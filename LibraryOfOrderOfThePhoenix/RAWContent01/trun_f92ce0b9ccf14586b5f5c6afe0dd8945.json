{
  "input": "Underrated Viral Tweets Analysis Prompt\nResearch Briefing & Objective:\n\nYou are a Digital Anthropologist and Content Strategist studying the anatomy of insightful viral thoughts on social media. Your mission is to curate a database of \"underrated viral tweets\"—profound and shareable ideas that achieved massive organic reach despite originating from outside the mainstream media and celebrity ecosystem.\n\nPhase 1: Search & Selection Criteria\n\nYour goal is to identify and catalog tweets that meet all of the following conditions:\n\nViral Threshold: The tweet must have achieved significant organic engagement, defined as having at least 50,000 likes OR 10,000 retweets.\n\nUnderrated Source: The tweet must originate from an account that is not a major global celebrity, top-tier politician, or large media corporation (e.g., you should actively exclude accounts like @elonmusk, @BarackObama, @nytimes, @BBCNews). The focus is on tweets from subject-matter experts, writers, thinkers, and creators.\n\nInsightful Content: The tweet must be a self-contained, profound thought. It should be an observation, a mental model, a poignant short story, or a counter-intuitive idea. Exclude simple jokes, memes, political attacks, or replies that only make sense in the context of a conversation.\n\nPhase 2: Dataset Compilation & Final Deliverable\n\nThe final output must be a comprehensive table in CSV format, with each row representing a single tweet that meets the above criteria. Aim for a diverse collection of at least 5,000 tweets.\n\nThe table must have the following exact headers:\n\nTweet_URL\n\nTweet_Text\n\nAuthor_Handle\n\nAuthor_Description\n\nCore_Insight\n\nCategory\n\nLikes_Count\n\nRetweets_Count\n\nConstraint Checklist:\nAuthor_Description: Briefly describe the author's background or area of expertise based on their bio (e.g., \"Software Engineer,\" \"Philosophy Writer,\" \"Neuroscientist\").\n\nCore_Insight: Summarize the tweet's central idea, observation, or lesson in a single, concise sentence.\n\nCategory: Classify the tweet's primary theme using a consistent set of categories, such as: Technology, Philosophy, Psychology, Life Advice, Creativity, Science, Business.\n\nEngagement Counts: Provide the approximate number of likes and retweets (e.g., 82000, 15000).\n\nVerification: Prioritize tweets where the insight is original to the author and not a quote from another source.",
  "output": {
    "project_summary": "Creating a database of at least 5,000 'underrated viral tweets' is a feasible but highly challenging project that extends beyond simple data extraction into a research-level endeavor. The primary obstacles are the significant costs and technical limitations associated with the X/Twitter API, the profound subjectivity in defining and programmatically identifying 'insightful content', and the high risk of data scarcity where few tweets will meet all three stringent criteria (viral threshold, underrated source, insightful content). The X API's Pro ($5,000/month) or Enterprise ($42,000+/month) tiers are necessary for the required full-archive search, and the potential non-functionality of key engagement filters like `min_faves` necessitates a costly high-volume 'pull and filter' strategy. The most significant challenge is operationalizing 'insightful content', which requires developing detailed annotation guidelines, training human annotators, and potentially building custom NLP classifiers. Given these factors, a hybrid approach to data collection and analysis is strongly recommended. This involves leveraging cost-effective third-party APIs for bulk data acquisition, using public datasets like Kaggle's 'The Tweets of Wisdom' to seed the collection, and employing a multi-layered filtering process that combines automated heuristics, NLP models, and rigorous human-in-the-loop validation to ensure the final dataset's quality and integrity.",
    "recommended_strategy_overview": "The optimal strategy for this project is a multi-pronged, hybrid approach designed for resilience and cost-effectiveness. First, a hybrid data acquisition model should be used, leveraging high-volume, low-cost third-party APIs (e.g., TwitterAPI.io) for initial data collection, supplemented by public datasets (from Kaggle, GitHub) for seeding and model training, and using the official X API Pro tier sparingly for verification and rehydration of tweet IDs. Second, a multi-layered filtering process is crucial: implement deterministic rules to filter for 'Underrated Sources' by using follower-count thresholds (<1M), curated exclusion lists of celebrities and media, and checking for X's Verified Organization status. Concurrently, apply pre-filtering heuristics and NLP models to identify potential 'Insightful Content', boosting the yield for human review. Third, leverage advanced NLP for enrichment: use abstractive summarization models with factuality-checking guardrails to generate the 'Core_Insight' field, and employ NER with occupational taxonomies (like O*NET) to standardize the 'Author_Description'. Finally, the entire project must be underpinned by a strict quality assurance and compliance framework, including statistical sampling (AQL), high inter-annotator agreement targets (>0.80), and strict adherence to X's API policies, particularly by distributing only Tweet IDs in any public-facing dataset.",
    "data_acquisition_plan": {
      "recommended_approach": "A multi-pronged, hybrid strategy is the most cost-effective and resilient approach. It involves using a cost-effective third-party provider like TwitterAPI.io for high-volume raw data acquisition, leveraging public datasets from Kaggle and GitHub to seed the collection and train filtering models, using specialized providers like TrackMyHashtag for targeted historical data purchases, and using the official X API Pro Tier sparingly for data verification and rehydrating tweet IDs from public datasets.",
      "primary_challenge": "The main technical hurdle is the inefficiency and high cost of finding the specific 'underrated viral tweets' at scale. While the X API v2 supports operators like `min_faves` and `min_retweets`, the 'hit rate' for tweets that meet the stringent criteria (50k+ likes/10k+ retweets AND from an 'underrated' source) is expected to be very low. This necessitates processing a massive volume of raw tweets (potentially millions) to find a few thousand qualifying examples, making reliance solely on the official API's read limits a significant bottleneck and a major cost driver.",
      "primary_api_option": "The X/Twitter API v2 Pro Tier, priced at $5,000 per month, is the main recommended official API option. Its key feature is access to the full-archive search endpoint (`GET /2/tweets/search/all`), which is essential for the required historical data collection from 2009-2025. While it provides 1,000,000 reads per month, this is considered a potential bottleneck given the low expected hit rate of qualifying tweets, meaning the primary role of this API in the hybrid strategy is for verification and rehydration rather than bulk initial collection.",
      "alternative_providers": "Several third-party data providers were considered as more cost-effective alternatives for bulk data acquisition. The most compelling is TwitterAPI.io, which offers a rate of $0.15 per 1,000 tweets, providing a 33x increase in data volume for the same cost as the official Pro plan. Other considered providers include Bright Data for pre-collected datasets, TrackMyHashtag for historical data purchases, and enterprise solutions like Sprinklr, Brandwatch, and Meltwater, though the latter are likely too expensive for this specific task.",
      "cost_implication_summary": "The cost implications are significant due to the need for high-volume data processing. Relying solely on the X API Pro Tier ($5,000/month for 1M reads) could lead to costs of $25,000 or more over several months to find 5,000 qualified tweets, assuming a low hit rate. The Enterprise Tier, starting at $42,000/month with a potential shift to a revenue-sharing model, has a very poor ROI for this task. The recommended hybrid approach mitigates this by using cheaper third-party providers for bulk collection, significantly reducing the cost per tweet and making the project more financially feasible."
    },
    "underrated_source_definition": {
      "primary_filtering_method": "A multi-layered filtering approach is the primary method. It combines deterministic rules based on follower count thresholds, curated exclusion lists of known top accounts, and platform-native signals like verification status. This approach systematically removes major celebrities, politicians, and media corporations from the pool of potential sources.",
      "celebrity_exclusion_rules": "Major global celebrities are excluded using two main rules: 1) A follower count threshold, where accounts with over 1 million followers (mega-influencers) or a higher tier of 50-100 million followers are filtered out. 2) Curated exclusion lists compiled from authoritative sources like Wikipedia's 'List of most-followed Twitter accounts' and various industry publications that track top X/Twitter profiles.",
      "politician_exclusion_rules": "Top-tier politicians are excluded by cross-referencing author handles against official, publicly available datasets. Key sources include the `unitedstates/congress-legislators` GitHub repository for the U.S. Congress, European Parliament-provided lists for MEPs, and broader datasets from projects like EveryPolitician and Wikidata queries for global political figures.",
      "media_exclusion_rules": "Large media corporations are excluded using a combination of methods: 1) Checking against comprehensive lists of news agencies (e.g., from Wikipedia) and top news websites based on traffic (e.g., from Press Gazette). 2) Identifying accounts that are part of X's 'Verified Organizations' program, which are marked with a gold checkmark (for businesses) or a grey checkmark (for governmental organizations), providing a direct, platform-native signal of an organizational account.",
      "final_heuristic": "A reproducible scoring rubric is used for the final decision. Accounts start with a base score, and points are deducted if they meet any of the exclusion criteria (e.g., appearing on a celebrity list, exceeding a follower threshold, being identified as a politician from an official dataset, or being a verified media organization). Accounts with a final score below a specified threshold are definitively excluded from the 'Underrated Source' category, prioritizing precision to avoid including mainstream sources."
    },
    "insightful_content_annotation_guidelines": {
      "development_principles": "The guidelines are developed based on four core principles from NLP research: 1) Iterative Refinement, where the guidelines are drafted, tested on sample data, and revised based on annotator feedback to address ambiguities. 2) Comprehensive Annotator Training to ensure a shared understanding of all rules. 3) Measurement of Inter-Annotator Agreement (IAA) using statistical metrics to ensure consistency. 4) Public Availability of the final guidelines for transparency and reproducibility.",
      "definition_of_insightful": "Insightful Content is defined as a self-contained, profound thought, observation, mental model, poignant short story, or counter-intuitive idea. The core principle is 'self-containment,' meaning the insight must be fully understandable from the tweet's text alone without external context. This includes original aphorisms and micro-stories but excludes well-known clichés that lack a novel perspective.",
      "exclusion_criteria": "A list of content types to be explicitly excluded to reduce noise: 1) Jokes and Humor, identified by their intent to be humorous (e.g., setup/punchline structure). 2) Memes, typically a combination of image and text for satirical effect. 3) Political Attacks, defined as content using propaganda techniques or vilification rather than presenting a nuanced argument. 4) Context-Dependent Replies, which are any tweets that cannot be understood without reading the parent tweet.",
      "adjudication_process": "A formal adjudication process is used to resolve disagreements between annotators. The proposed model is a decision tree: if two annotators disagree on a label, the item is flagged and escalated to a senior third-party adjudicator. The adjudicator reviews the item and makes a final, binding decision. These cases of disagreement are collected and analyzed to identify patterns that can inform further refinement of the guidelines.",
      "quality_metric": "Inter-Annotator Agreement (IAA) is the primary quality metric used to measure consistency among annotators. For this multi-label classification task, Krippendorff's Alpha (α) is the recommended metric due to its flexibility. A target score of α ≥ 0.800 is set as the threshold for reliable agreement, indicating that the guidelines are clear and the annotations are consistent enough for use in training and analysis."
    },
    "nlp_classifier_plan_for_insightful_content": {
      "training_methodology": "A hybrid training methodology is recommended, combining weak supervision with a small, human-annotated 'gold set' (2k-5k examples). The weak supervision phase uses programmatic 'labeling functions' (e.g., based on linguistic cues via Snorkel) or a fine-tuned small LLM to generate a large volume of noisy labels. This large, weakly-labeled dataset is then combined with the high-quality gold set to train the final classifier, leveraging techniques like self-training and confident learning for further refinement.",
      "labeling_strategy": "To build the gold set efficiently, pool-based Active Learning (AL) is the recommended strategy. Instead of random sampling, AL intelligently selects the most informative and diverse examples from a large unlabeled pool for human annotation. The PRepAL (Pretrained Representation Active Learning) method is specifically recommended to make this process computationally efficient by pre-computing feature representations and using a simple linear classifier for sample selection, avoiding costly retraining of the full model at each step.",
      "recommended_models": "The plan recommends using pre-trained transformer models like BERT or, more specifically, DeBERTa-v3-base as strong baselines for the final classification task. For weak supervision and rapid prototyping, smaller Large Language Models (LLMs) like Llama 3 can be used in a zero-shot or few-shot capacity to generate initial labels or perform classification directly.",
      "optimization_target": "The primary performance metric for optimization is high precision. The goal is to ensure that the tweets flagged by the model as 'insightful' are indeed insightful, minimizing false positives. This is achieved by analyzing the precision-recall curve on a validation set and selecting a classification threshold that meets the desired precision level, even at the expense of lower recall. Model outputs will be calibrated to ensure the probabilities are trustworthy for thresholding.",
      "monitoring_plan": "Post-deployment, the model's performance will be continuously monitored for drift. This involves tracking both data drift (changes in input characteristics) and concept drift (changes in the definition of 'insightful'). Specialized tools like Evidently AI, WhyLabs, or Deepchecks will be used to monitor distributional shifts in model embeddings and track performance metrics on a regularly updated gold set. A significant performance drop will trigger a retraining cycle."
    },
    "core_insight_summarization_methodology": {
      "summarization_type": "The methodology employs abstractive summarization, specifically a form of 'extreme summarization' akin to the XSum dataset task. The goal is to generate a novel, single, self-contained sentence that captures the core message or thesis of the original tweet in new words, rather than extracting and stitching together phrases from the source.",
      "hallucination_prevention_techniques": "A multi-faceted approach is used to prevent hallucinations. This includes: 1) Integrating factuality checks into the model training process (e.g., FactPegasus). 2) Using retrieval-augmented generation (RAG) to ground the model's output firmly in the source tweet's context. 3) Employing advanced decoding strategies like DeCoRe (Decoding by Contrasting Retrieval Heads) that penalize inconsistent outputs during generation.",
      "automated_quality_metrics": "A suite of automated metrics will be used to evaluate the factual consistency of summaries at scale. Key recommended metrics include: NLI-based methods like SummaC for detecting inconsistencies, QA-based methods like QAFactEval for checking answerability from the source, dedicated models like FactCC for verification, and efficient embedding-based metrics like SBERTScore for comparing semantic similarity.",
      "human_quality_control_framework": "A human quality control framework, based on established academic benchmarks like SummEval and FRANK, will be implemented. This involves a regular sampling of summaries for manual review by trained annotators. Annotators will assess summaries on key dimensions including Factual Faithfulness, Relevance, Conciseness, and Clarity, and will be trained to identify and highlight specific unfaithful text spans.",
      "prompt_engineering_strategy": "A detailed prompt engineering strategy will guide the LLM's output. This includes using explicit prompts with clear negative constraints (e.g., 'Do not add external context'), setting a low 'temperature' for more deterministic and factual outputs, and enforcing a structured output format (like a JSON schema) to ensure the model's response is reliable and predictable."
    },
    "author_description_extraction_process": {
      "pipeline_overview": "A seven-stage pipeline is proposed for extraction and standardization: 1) Pre-processing to clean the bio text. 2) Role Extraction using NER models or APIs. 3) Normalization by mapping extracted titles to standard taxonomies. 4) Ambiguity Resolution to handle multiple or unclear roles. 5) Post-processing for final formatting. 6) Fallback Handling for empty or uninformative bios. 7) Quality Assurance to evaluate performance.",
      "pre_processing_steps": "Initial steps focus on cleaning the raw bio text to improve extraction accuracy. This involves using regular expressions to strip out all URLs and @-mentions, using a library like `demoji` to remove or replace emojis with their text descriptions, and using a language identification model like fastText's `lid.176` to handle multilingual bios consistently.",
      "extraction_tools": "A combination of tools is recommended for identifying job titles. For open-source options, this includes specialized NER models from Hugging Face (e.g., `LPDoctor/en_core_web_sm_job_related`) and keyword-based libraries (`fluquid/find_job_titles`). For higher accuracy and built-in normalization, commercial APIs like the Lightcast Titles API or Textkernel Professions API are recommended.",
      "normalization_taxonomies": "To standardize extracted titles, they are mapped to authoritative occupational dictionaries. The primary recommended resources are the US-based O*NET-SOC taxonomy (specifically its 'Alternate Titles' file with over 60,000 lay titles) and the multilingual ESCO (European Skills, Competences, Qualifications and Occupations) taxonomy, which is available in 28 languages.",
      "ambiguity_resolution_rule": "A heuristic-based rule is used to handle bios with multiple or ambiguous roles. The suggested priority order for selecting the most relevant role is: Current Role > Field > Notable Affiliation. For ambiguous terms (e.g., 'driver'), Word Sense Disambiguation tools like WordNet can be used to analyze the surrounding context and determine the correct meaning."
    },
    "originality_verification_protocol": {
      "detection_methods": "A combination of lexical and semantic techniques is used to detect non-original content. Lexical methods like Shingling and MinHashing are used to find structural, near-duplicate text. Semantic methods, primarily using Sentence-BERT (SBERT) embeddings and cosine similarity, are used to detect paraphrasing where the meaning is similar but the wording is different. This combined approach is more robust than using either method alone.",
      "reference_corpora": "To check for known quotes and public-domain text, tweets are compared against large, authoritative text corpora. The primary reference sources are full dumps of Wikiquote (parsed into a structured format) and the entire collection of public-domain books from Project Gutenberg. Specialized datasets of proverbs, like PROMETHEUS, are also used to identify common aphorisms.",
      "semantic_similarity_threshold": "For semantic similarity checks using models like SBERT, a cosine similarity score of 0.85 is suggested as a starting threshold to flag potential paraphrasing. This threshold is not absolute and requires tuning based on the specific use case; a lower threshold increases recall (finding more potential matches) while a higher one increases precision (reducing false positives).",
      "manual_review_guideline": "For borderline cases flagged by the automated systems, manual review procedures are based on the detailed flowcharts and best practices provided by the Committee on Publication Ethics (COPE). This framework guides the process of investigating suspected plagiarism, gathering evidence, and making a final determination.",
      "stylometry_application": "Stylometry, the analysis of an author's unique writing style, is used for authorship verification. This involves analyzing features like character n-grams, function word frequency, and punctuation usage. While challenging for short texts like tweets, advanced deep learning models using stylometric embeddings can help verify if a text's style is consistent with the author's other writings, providing an additional layer of originality checking."
    },
    "content_category_taxonomy": {
      "classification_approach": "A multi-label classification approach is recommended. This allows a single tweet to be assigned to multiple relevant categories (e.g., a tweet about AI ethics could be labeled both 'Technology' and 'Philosophy'). This approach better captures the nuanced and often interdisciplinary nature of insightful content compared to a restrictive single-label system.",
      "primary_categories": "The core list of categories includes: Technology, Philosophy, Psychology, Life Advice, Creativity, Science, and Business. Optional additional categories such as History, Sociology/Culture, and Education can be included to capture other common themes. Each category is defined with subcategories and a list of hallmark keywords to guide annotators.",
      "consistency_metric": "To ensure high agreement among annotators, Krippendorff's Alpha (α) is the recommended Inter-Annotator Agreement (IAA) metric. It is highly versatile and well-suited for multi-label tasks. The target for reliable agreement is set at α ≥ 0.800, indicating that the category definitions are clear and consistently applied.",
      "conflict_resolution_rules": "To handle cases where a tweet fits multiple categories or where annotators disagree, a clear process is defined. This includes using decision flowcharts in the annotation guidelines to handle common overlaps (e.g., AI ethics). For persistent disagreements, a senior annotator or a review committee will adjudicate and make a final decision. The outcomes are used to refine the guidelines.",
      "imbalance_diagnostics": "Procedures are in place to monitor the distribution of labels across categories. The frequency of each label is tracked regularly. A significant imbalance can signal potential issues, such as a category definition being too broad or too narrow, or it can reflect a genuine skew in the source data. This monitoring allows for the refinement of the taxonomy to ensure it is functioning as intended."
    },
    "data_pipeline_architecture": {
      "architecture_style": "A modular ETL (Extract, Transform, Load) pipeline architecture is proposed. This design separates the distinct logical functions of the data processing workflow into discrete modules, enhancing maintainability, scalability, and clarity. The core modules are Ingest, Filter, Annotate (Enrich), Summarize, Validate, and Export, which work sequentially to process raw tweet data into a final, curated CSV file.",
      "pipeline_modules": "The pipeline is composed of several distinct logical modules: 1. **Ingest:** This module is responsible for all interactions with the X API. It handles authentication, forming API requests, managing rate limits by monitoring response headers, and handling pagination using the `next_token`. 2. **Filter:** This module applies criteria to sift the collected data. It performs pre-filtering at the API level using query parameters and post-filtering after ingestion to apply more complex rules, such as excluding authors from a blocklist. 3. **Annotate (Enrich):** This module adds new data fields by processing the raw tweet information. This includes both direct extraction (e.g., `Author_Description` from the user bio) and complex, derived fields requiring NLP models. 4. **Summarize:** A specialized sub-component of the Annotate module, dedicated to generating the `Core_Insight` field by using an NLP summarization model (e.g., a fine-tuned T5 or BART model). 5. **Validate:** This module performs data quality checks, ensuring field completeness and cross-field consistency (e.g., numeric fields are integers). 6. **Export:** The final module formats the processed and validated data into a deduplicated CSV file with the specified headers.",
      "api_ingestion_strategy": "A hybrid, multi-pronged strategy is recommended for efficient data acquisition. The foundation involves using a cost-effective third-party service like TwitterAPI.io ($0.15 per 1,000 tweets) for high-volume, raw data pulls to work around the X API's non-functional server-side engagement filters. This is supplemented by using public datasets from Kaggle and GitHub to seed the collection and train filtering models without consuming paid credits. For targeted historical data needs, specialized providers like TrackMyHashtag can be used for one-time purchases. The official X API Pro Tier ($5,000/month) is used sparingly for its essential full-archive search capability (`/2/tweets/search/all`), for rehydrating tweet IDs from public datasets, and for verifying data integrity. API calls are optimized by requesting all necessary fields (`public_metrics`, `author_id`, `text`, `description`) and expansions (`author_id`) in a single request to minimize the number of calls.",
      "rate_limit_handling": "The pipeline must implement a robust mechanism to manage the X API's rate limits, which are enforced within 15-minute windows. This is achieved by continuously monitoring the HTTP response headers for every API call, specifically: `x-rate-limit-remaining` (requests left in the current window), `x-rate-limit-limit` (the total request ceiling for the window), and `x-rate-limit-reset` (the UTC epoch timestamp when the limit will reset). If an HTTP `429 Too Many Requests` error is received, the application must pause all requests and wait until the time specified in the `x-rate-limit-reset` header has passed before resuming. A best-practice implementation will incorporate an exponential backoff strategy for retrying failed requests to handle transient network issues or brief API unavailability.",
      "deduplication_method": "A multi-layered deduplication method is employed to ensure the final dataset contains only unique records. First, tweet types are distinguished using the `referenced_tweets` field in the API v2 response to separate original tweets from retweets, replies, and quote tweets, ensuring only original content is processed. Tweet URLs are canonicalized to a standard format (`https://twitter.com/{username}/status/{tweet_id}`) by normalizing the domain (x.com vs. twitter.com), stripping mobile prefixes, and removing query parameters. For near-duplicate text detection, a combination of lexical and semantic methods is used after text normalization (lowercase, whitespace removal, etc.). Algorithms like MinHash with Locality-Sensitive Hashing (LSH) or SimHash are used for syntactic similarity, while sentence embedding models (like SBERT) with cosine similarity are used to detect semantic duplicates (paraphrases), with a suggested threshold of >0.80. The unique `tweet.id` is used as the primary key throughout the pipeline to ensure idempotency."
    },
    "quality_assurance_framework": {
      "target_quality_level": "The explicit goal is to ensure that at least 95% of the final data rows meet all specified constraints. This quality level is statistically verified using acceptance sampling plans. To demonstrate a 95% pass rate (reliability) with 95% confidence, a sample size of 59 is required, assuming zero defects are found in the sample. This is based on the binomial confidence interval calculation `n = ln(1-C) / ln(R)` where C=0.95 and R=0.95.",
      "statistical_sampling_plan": "The framework employs Acceptance Quality Limit (AQL) sampling based on standards like ANSI/ASQ Z1.4 and ISO 2859-1 to audit data quality on entire batches. An AQL (e.g., 4.0%) is defined as the worst tolerable process average for non-conforming items. The performance of the sampling plan is evaluated using Operating Characteristic (OC) curves, which balance Producer's Risk (rejecting a good lot) and Consumer's Risk (accepting a bad lot). Both standard Z1.4 plans and stricter c=0 (zero acceptance) plans are considered, with the latter requiring smaller sample sizes but being less tolerant of defects. For example, for a lot of 1300 items, a c=0 plan might require sampling 18 items with 0 defects allowed, while a Z1.4 plan might sample 125 with up to 10 defects allowed.",
      "human_review_process": "A Human-in-the-Loop (HITL) process is critical for subjective criteria. It is built on several pillars: 1. **Annotation Guidelines:** A comprehensive manual with precise definitions and examples for 'insightful content' and 'underrated source'. 2. **Inter-Annotator Agreement (IAA):** Consistency is measured using metrics like Cohen's Kappa or Krippendorff's Alpha, with a target of >0.70 or >0.80 to ensure reliable ratings. 3. **Gold Standard & Adjudication:** A 'gold standard' dataset labeled by experts is used to qualify and monitor annotators. When annotators disagree, an adjudication workflow escalates the conflict to a senior reviewer who makes a final, binding decision. 4. **Continuous Feedback:** A feedback loop involving regular training and performance reviews against the gold standard is maintained to ensure annotator alignment and improve guideline clarity.",
      "automated_validation_tools": "Automation is used to efficiently check objective criteria and logical consistency. This includes: 1. **Engagement & Field Checks:** Scripts automatically verify that `Likes_Count` and `Retweets_Count` meet the viral threshold and that all required fields are complete (not null). 2. **Originality Detection:** Hashing techniques like SimHash and MinHash with LSH are used to detect near-duplicate text. For semantic duplicates (paraphrases), embedding cosine similarity is used. 3. **Cross-Field Consistency:** Data validation frameworks like Great Expectations or Pandera enforce logical rules, such as ensuring a tweet cannot be simultaneously categorized as 'insightful content' and a 'simple joke'.",
      "improvement_process": "A continuous improvement loop ensures the QA framework evolves and becomes more effective. The process involves: 1. **Post-Hoc Error Analysis:** Systematically analyzing errors identified during AQL sampling or HITL adjudication to determine their root causes. 2. **Feedback Integration:** The findings from error analysis are used to refine annotation guidelines, improve annotator training materials, and enhance the rules in the automated validation tools. 3. **Final Sign-off:** A final checklist is used to ensure all QA steps (AQL pass, IAA targets met, etc.) are completed before a data batch is accepted. 4. **Monitoring and Adaptation:** Quality trends are monitored over time through dashboards, allowing the team to proactively address emerging issues and adapt the framework to any changes in the data source or project requirements."
    },
    "legal_and_platform_compliance_summary": {
      "data_collection_policy": "Data collection from the X platform is strictly limited to the use of official, published interfaces, primarily the X API. The Terms of Service (effective Nov 15, 2024) explicitly state that 'crawling or scraping the Services in any form, for any purpose without our prior written consent is expressly prohibited.' This is reinforced by the platform's `robots.txt` file, which disallows general-purpose bots. Violating these terms carries significant financial risk, as the policy stipulates liquidated damages of '$15,000 USD per 1,000,000 posts' for unauthorized access exceeding this volume in a 24-hour period. Therefore, all data acquisition for this project must be conducted exclusively through the appropriate X API v2 endpoints.",
      "content_redistribution_policy": "X's Developer Policy heavily restricts the redistribution of full tweet content and metadata to third parties. The primary rule is that developers may only distribute Post IDs, Direct Message IDs, and/or User IDs. There are explicit quantitative limits: a maximum of 1,500,000 Post IDs can be shared with any single entity within a 30-day period without prior written permission from X. For non-automated sharing, such as in a spreadsheet or PDF, the limit is 500 public Post Objects per person per day. Any third party receiving these IDs must also agree to X's Terms of Service and Developer Agreement. Distributing the full text and metadata of 5,000 tweets in a CSV file, as requested in the user's final deliverable, would directly violate these policies.",
      "rehydration_requirement": "The mandatory and compliant process for sharing tweet data is 'rehydration'. This involves distributing a dataset composed only of Tweet IDs. Recipients of this dataset must then use the X API themselves to fetch the full, current tweet objects. This ensures that the data reflects its most recent state on the platform (e.g., if a tweet has been deleted or an account made private). The Developer Policy mandates that any X Content stored offline must be kept current, requiring that stored copies of deleted or modified content be removed or updated within 24 hours. This rehydration process is the standard for academic and research use, and tools like `twarc` are designed for this purpose.",
      "ai_model_training_restriction": "The X Developer Agreement contains a specific and explicit prohibition on using platform data for training large-scale AI models. The policy states that developers may not use the X API or X Content to 'fine-tune or train a foundation or frontier model.' This restriction is critical for any NLP-related aspects of the project, such as developing a classifier for 'Insightful Content' or a summarizer for 'Core_Insight'. While X reserves the right to use user content to train its own AI models, this right is not extended to third-party developers using the API.",
      "recommended_dataset_release_format": "Given the strict content redistribution policies, the only compliant format for releasing the final dataset to the public or any third party is a list of Tweet IDs and corresponding User IDs. The final deliverable should not be a CSV file containing the full `Tweet_Text`, `Author_Handle`, `Likes_Count`, etc. Instead, it should be a file containing only the unique identifiers. This file must be accompanied by clear instructions for the recipient on how to 'rehydrate' these IDs using the X API to retrieve the full, current tweet content and metadata for their own analysis, along with a notice requiring them to comply with X's own terms."
    },
    "diversity_and_bias_mitigation_plan": {
      "language_and_region_strategy": "To combat inherent Anglophone bias, the plan involves targeted language sampling across a predefined list of diverse languages, rather than relying on default API streams. This will be supplemented with multilingual demographic inference and post-stratification techniques to create more representative samples. For regional diversity, the strategy will prioritize user-level sampling over tweet-level sampling to capture a wider range of authors. The primary method for geographic targeting will be using bounding box coordinates in the Search API, which has been shown to outperform simple location or language queries, despite the known imprecision where some tweets may be placed at a country's centroid.",
      "author_diversity_strategy": "To ensure a diverse set of authors and adhere to the 'Underrated Source' constraint, the primary strategy is follower-count stratification. Social media follower counts follow a power-law distribution, so a strict follower-count ceiling will be implemented to filter out major celebrities, politicians, and media organizations. Based on industry definitions of influencer tiers, a ceiling of 100,000 followers is a reasonable starting point to specifically target nano-influencers (<10,000 followers) and micro-influencers (10,000-100,000 followers), who are more likely to be the subject-matter experts, writers, and creators the project aims to capture.",
      "topic_diversity_strategy": "To ensure a balanced representation of disciplines and prevent the dataset from being skewed towards a few popular topics, advanced topic modeling will be employed. The recommended tool is BERTopic, which is well-suited for short texts like tweets and has been shown to outperform traditional models like LDA in topic diversity and coherence. The diversity of topics will be quantitatively measured using established metrics such as the Shannon Entropy or Simpson Index to ensure the final dataset reflects a wide range of intellectual domains like Technology, Philosophy, Psychology, etc.",
      "niche_overrepresentation_prevention": "Several procedures will be implemented to prevent the dataset from being dominated by a few authors or niche topics. First, a per-author cap will be enforced, limiting the number of tweets included from any single author. Second, robust near-duplicate detection algorithms (such as MinHash, SimHash, or C4 de-duplication) will be used to identify and remove tweets that are semantically very similar, ensuring a wider range of unique ideas. Finally, the data collection process will involve balancing the sampling of tweets across different time windows to avoid capturing fleeting trends and to build a more timeless collection of insights.",
      "documentation_requirement": "A critical component of the diversity and bias mitigation plan is comprehensive documentation. Following the frameworks proposed by researchers like Gebru et al. and Bender & Friedman, the project will produce a datasheet or data statement to accompany the final dataset. This document will transparently detail the dataset's motivation, composition, collection methodology, preprocessing steps, and, most importantly, any known biases, limitations, or gaps, including linguistic and regional imbalances. This ensures that end-users of the dataset are fully aware of its characteristics and can use it responsibly."
    },
    "project_timeline_and_budget": {
      "estimated_timeline": "The total estimated duration for the project is 8 to 12 weeks. This timeline accounts for all phases of the project, from initial setup and pilot testing to final data curation and delivery. The variability in the timeline is largely dependent on the success rate of data acquisition and the speed and efficiency of the manual annotation process.",
      "project_phases": "The project is broken down into four distinct phases with clear milestone acceptance gates: \n1. **Phase 1: Scoping, Setup, and Pilot (2 Weeks):** Finalize tools, recruit annotators, and conduct a pilot study to refine guidelines and establish baselines. \n2. **Phase 2: Data Acquisition and Pre-Processing (Weeks 3-10, Ongoing):** Systematically query the X API's full-archive search to collect a large pool of candidate tweets that meet the viral threshold. \n3. **Phase 3: Full-Scale Annotation and Quality Control (Weeks 4-11):** Deploy annotators to label the candidate tweets for 'insightful content' and 'underrated source' using a redundant labeling strategy for quality control. \n4. **Phase 4: Final Dataset Curation and Delivery (Week 12):** Perform final quality checks, format the data, and compile the final report and dataset for delivery.",
      "primary_cost_driver": "The single largest and most significant anticipated expense for the project is the cost of data acquisition via the X API. To access the full historical archive, which is necessary to find a sufficient number of tweets meeting the high viral threshold, a subscription to the X Enterprise Tier is required. As of August 2025, this tier's pricing starts at an estimated $42,000 per month. This cost significantly outweighs all other project expenses, including annotation labor and tooling.",
      "estimated_annotation_labor_cost": "The estimated cost for manual annotation labor is between $2,500 and $3,000. This is based on a conservative throughput estimate of 50-100 tweets annotated per hour for a cognitively demanding task. To achieve 5,000 validated rows with a two-annotator review process for quality control (totaling 10,000 annotations), approximately 134 hours of work are needed. At a mid-range annotator rate of $15/hour, this amounts to a base cost of ~$2,010, with an additional buffer of 25-50% recommended for quality reviews and conflict resolution.",
      "key_risks": "The project faces several significant risks: \n1. **Data Scarcity (High):** The primary risk is underestimating the difficulty of finding 5,000+ tweets that meet both the high viral threshold and the subjective 'insightful content' criteria. Mitigation requires a large budget for comprehensive API access. \n2. **Cost Overrun (High):** The high cost of the X Enterprise API means any miscalculation in the required data volume will severely impact the budget. \n3. **Annotation Subjectivity (Medium):** The subjective nature of 'insightful' can lead to high disagreement among annotators, slowing progress. Mitigation involves investing heavily in developing clear guidelines and a pilot phase. \n4. **API Instability (Medium):** X has a history of changing its API terms, with a known transition for the Enterprise API planned for July 2025, which could introduce uncertainty."
    },
    "key_challenges_and_risks": "The project faces three primary, interconnected risks that could impact its timeline, budget, and ultimate success. First is the high and volatile cost of the X API. Accessing the necessary full-archive search endpoint requires an expensive Pro ($5,000/month) or Enterprise ($42,000+/month) subscription. This cost is exacerbated by the potential non-functionality of server-side engagement filters, which forces a high-volume, brute-force data collection strategy. Furthermore, X's history of sudden policy and pricing changes, such as the shift to a revenue-sharing model for the Enterprise tier, introduces significant financial uncertainty. Second is the inherent subjectivity and difficulty in defining and finding 'insightful content'. This is not a simple keyword search but a nuanced, semantic challenge requiring a significant investment in creating robust annotation guidelines, training a team of human annotators, and potentially developing sophisticated NLP models, with no guarantee of achieving high inter-annotator agreement. Third, there is a substantial risk of data scarcity. The intersection of the three core criteria—high virality (50k+ likes), an 'underrated' non-celebrity source, and profound 'insightful' content—is likely to be extremely small. This creates a high probability that even after processing tens or hundreds of millions of tweets, the project may struggle to reach the target of 5,000 validated examples, making the entire endeavor a search for a 'needle in a haystack'.",
    "discovered_data_sources": [
      {
        "source_name": "The Tweets of Wisdom",
        "platform": "Kaggle / GitHub",
        "relevance_to_project": "This dataset is the most significant finding and is highly aligned with the project's objectives. It contains over 30,000 tweets specifically curated for being 'self-help' and 'wise' thoughts, directly matching the 'insightful content' criterion. The data is compiled from over 40 authors described as thinkers and writers, fulfilling the 'underrated source' requirement. Most importantly, the inclusion of engagement metrics ('likes' and 'retweets') allows for direct filtering based on the project's 'Viral Threshold' (50,000 likes or 10,000 retweets).",
        "available_metadata": "The dataset is provided as a 'tweets.csv' file containing the columns: `author_name`, `handle`, `tweet_content`, `likes`, and `retweets`. This metadata is sufficient for initial filtering and for populating several of the required fields in the final deliverable.",
        "license": "CC0: Public Domain"
      },
      {
        "source_name": "Twitter Classification of Viral Tweets",
        "platform": "Kaggle",
        "relevance_to_project": "This public dataset is valuable for initial exploration and for building or testing models to identify patterns in viral content. It contains tweets that have already been classified based on their virality, which can help in understanding the features of high-engagement posts and refining the filtering logic for the main project.",
        "available_metadata": "The context does not specify the exact columns, but datasets of this nature typically include tweet text, author information, and engagement counts such as likes and retweets.",
        "license": "Not specified in the provided context. The license would be available on the specific Kaggle dataset page."
      },
      {
        "source_name": "Tweets Dataset (with like/share counts)",
        "platform": "Kaggle",
        "relevance_to_project": "This is another general-purpose dataset identified as potentially useful. Its key value lies in the explicit inclusion of like and share counts, which are essential for filtering tweets based on the project's defined viral threshold. It can serve as a supplementary source of candidate tweets.",
        "available_metadata": "The dataset includes tweet text and engagement metrics like like/share counts. Further details on author metadata would be on the dataset's page.",
        "license": "Not specified in the provided context. The license would be available on the specific Kaggle dataset page."
      },
      {
        "source_name": "awesome-twitter-data",
        "platform": "GitHub",
        "relevance_to_project": "This is a curated list of Twitter-related datasets. It functions as a meta-source or directory, enabling the discovery of other niche or topic-specific datasets that might contain insightful viral tweets. It's a valuable starting point for broader data exploration if the primary sources do not yield the target volume of 5,000 tweets.",
        "available_metadata": "As a repository of links, the available metadata varies for each linked dataset. The repository itself provides names and brief descriptions of other datasets.",
        "license": "The license of the 'awesome-twitter-data' repository itself is likely permissive (e.g., MIT), but the license for each dataset it links to will vary."
      }
    ]
  },
  "outputBasis": [
    {
      "field": "author_description_extraction_process",
      "citations": [
        {
          "title": "Twitter(X) Profile Bio ICP Classifier - Bio Keywords Extractor",
          "url": "https://apify.com/lead.gen.labs/twitter-x-profile-bio-icp-classifier---bio-keywords-extractor",
          "excerpts": [
            "Classify Twitter (X) profiles by using bio keyword extraction. This tool scrapes public Twitter bios and intelligently extracts relevant keywords to help you ...See more"
          ]
        },
        {
          "title": "Social Media best practices: Twitter",
          "url": "https://maddenmedia.com/social-media-best-practices-twitter/",
          "excerpts": [
            "Apr 17, 2018 — Clean up your bio. Does your bio reflect who you are as a DMO? It should. The bio is one of the first things a follower will see on your page."
          ]
        },
        {
          "title": "How to clean up your Twitter account? Simple tips that can ...",
          "url": "https://promorepublic.com/en/blogclean-twitter-account-simple-tips-can-help/",
          "excerpts": [
            "How to clean up your Twitter account? Simple tips that can help you! · Unfollow inactive accounts · Unfollow irrelevant accounts · Get a pinned tweet · Organize ..."
          ]
        },
        {
          "title": "Regex remove URLs paths and keep last slug of the URLs",
          "url": "https://unix.stackexchange.com/questions/691165/regex-remove-urls-paths-and-keep-last-slug-of-the-urls",
          "excerpts": [
            "Feb 18, 2022 — So basically it would look at the full URL path and ONLY keep the last slug which is imageN.png . For example these are the URLs parts that I am ..."
          ]
        },
        {
          "title": "Parsing usernames from Twitter, Facebook, and Instagram",
          "url": "https://dev.to/mattkenefick/regex-parsing-usernames-from-twitter-facebook-and-instagram-5l9",
          "excerpts": [
            "Sep 3, 2021 — We've had to parse out usernames from various social media URLs. We usually do this on user settings pages to make it easy when filling out forms."
          ]
        },
        {
          "title": "Bio To Schema: Why And How I Built My Custom GPT",
          "url": "https://www.lidia-infante.com/post/bio-to-schema",
          "excerpts": [
            "Jan 14, 2024 — The aim of Bio To Schema is to empower you to understand how to turn author bios into schema markup and help you prove the value of it to leadership."
          ]
        },
        {
          "title": "Why You Don't Want Job Title Normalization & ...",
          "url": "https://www.openprisetech.com/blog/data-normalization-basics-why-you-dont-really-want-to-normalize-job-title/",
          "excerpts": [
            "Jan 9, 2019 — Openprise maintains data sources of over 3,600 job title keywords sorted into job functions and of over 1,300 job title keywords in multiple ..."
          ]
        },
        {
          "title": "Models",
          "url": "https://huggingface.co/models?other=named-entity-recognition",
          "excerpts": [
            "We're on a journey to advance and democratize artificial intelligence through open source and open science."
          ]
        },
        {
          "title": "Job Titles classification",
          "url": "https://kb.lightcast.io/en/articles/7907529-job-titles-classification",
          "excerpts": [
            "Lightcast classifies raw job titles to over 70,000 standardized titles using a machine learning model that normalizes and matches titles based on similarity."
          ]
        },
        {
          "title": "2018 SOC Direct Match Title File (BLS)",
          "url": "https://www.bls.gov/soc/2018/soc_2018_direct_match_title_file.pdf",
          "excerpts": [
            "***Questions should be emailed to soc@bls.gov***. 2018 SOC Code. 2018 SOC Title. 2018 SOC Direct Match Title. Illustrative Example. 11-1011. Chief Executives. Direct Match Title File, 2018 SOC",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives"
          ]
        },
        {
          "title": "Welcome to LIWC-22",
          "url": "https://www.liwc.app/",
          "excerpts": [
            "LIWC is the gold standard in software for analyzing word use. It can be used to study a single individual, groups of people over time, or all of social media."
          ]
        },
        {
          "title": "A hedging annotation scheme focused on epistemic ...",
          "url": "https://aclanthology.org/W15-0302.pdf",
          "excerpts": [
            "by LM Sanchez · 2015 · Cited by 13 — This paper presents an annotation scheme for hedging focused on epistemic modality expressions in informal language. Hedges are used by a speaker to modulate ..."
          ]
        },
        {
          "title": "Epistemic Modality",
          "url": "https://iep.utm.edu/ep-moda/",
          "excerpts": [
            "Hedging views can offer some account of the oddness of embedded epistemic modals, since on these views the speaker is using “may” or “might” to express ..."
          ]
        },
        {
          "title": "Implicative verbs and their presuppositions - Prerna Nadathur",
          "url": "https://pnadathur.github.io/pdfs/implicatives_final.pdf",
          "excerpts": [
            "by P Nadathur · 2015 · Cited by 8 — Mary dared to open the door. b. ; Mary opened the door. Implicatives may be distinguished from factive verbs like know and regret (Kiparsky and."
          ]
        },
        {
          "title": "Implicative-Verbs.pdf",
          "url": "https://www.researchgate.net/profile/Lauri-Karttunen/publication/271696228_Implicative_Verbs/links/54f9eca60cf21ee4fdedfd2b/Implicative-Verbs.pdf",
          "excerpts": [
            "There are also verbs that are not inherently factive or non-factive, e.g. report, announce, remember. They can be used with or without presupposition of the ..."
          ]
        },
        {
          "title": "Evidence for the Concreteness of Abstract Language",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8773921/",
          "excerpts": [
            "by N Del Maschio · 2021 · Cited by 30 — Abstract concepts have been defined as mental representations referring to entities that are neither purely physical nor spatially constrained [ ..."
          ]
        },
        {
          "title": "Concreteness vs. Abstractness: A Selectional Preference ...",
          "url": "https://aclanthology.org/2022.aacl-srw.13/",
          "excerpts": [
            "by T Tater · 2022 · Cited by 4 — Concrete words refer to concepts that are strongly experienced through human senses (banana, chair, salt, etc.), whereas abstract concepts are less perceptually ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly discuss extracting information from author bios, including guidance on treating bios as data sources for role extraction, and pipelines for normalizing extracted job titles to standard taxonomies. For example, one excerpt presents an approach to categorize Twitter bios by extracting a person's bio information (an ICP-style classifier for bios) and another discusses a practical workflow for cleaning bios, removing URLs and mentions, and handling multilingual bios. Several excerpts describe using named-entity recognition and keyword-based methods to identify job titles within bios, which is exactly the extraction step in the envisioned pipeline. There is also coverage of normalization to established taxonomies (e.g., O*NET-SOC and ESCO) and discussion of ambiguity resolution rules, which align with the normalization and disambiguation steps in the pipeline. Additional excerpts touch on open-source and commercial NLP tools suitable for extracting and linking occupational terminology, which supports the extraction_tools and normalization_taxonomies components. Finally, other items discuss broader annotation and QA considerations for bios annotation workflows, which relate to the QA and post-processing steps in the proposed seven-stage pipeline.",
      "confidence": "high"
    },
    {
      "field": "originality_verification_protocol",
      "citations": [
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "Similarity Check allows editors to upload a paper, and instantly produces a report highlighting potential matches and indicating if and how the paper overlaps with other work.",
            "This report enables editors to assess the originality of the work before they publish it, providing confidence for publishers and authors, and evidence of trust for readers.",
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism."
          ]
        },
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris",
            "The output of the plagiarism detection module (matching sections of text) can be sent as input to the paraphrase type identification module."
          ]
        },
        {
          "title": "Paraphrase Mining — Sentence Transformers documentation",
          "url": "https://sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ..."
          ]
        },
        {
          "title": "Implementation of Winnowing Algorithm with Dictionary ...",
          "url": "https://thesai.org/Downloads/Volume9No5/Paper_23-Implementation_of_Winnowing_Algorithm.pdf",
          "excerpts": [
            "Detection of plagiarism in this study will use a winnowing algorithm that has a function to check every character in two samples by hashing method that can ..."
          ]
        },
        {
          "title": "Authorship Verification Using the Impostors Method - CEUR-WS.org",
          "url": "https://ceur-ws.org/Vol-1179/CLEF2013wn-PAN-Seidman2013.pdf",
          "excerpts": [
            "The approach is based on comparing the similarity between the given documents and a number of external (impostor) documents, so that documents can be classified as having been written by the same author, if they are shown to be more similar to each other than to the impostors, in a number of trials."
          ]
        },
        {
          "title": "Plagiarism in a submitted manuscript - COPE Flowchart",
          "url": "https://publicationethics.org/guidance/flowchart/plagiarism-submitted-manuscript",
          "excerpts": [
            "\n\nThe journal should ask the reviewer for full documentary evidence and investigate the concerns. Authors should be contacted where there is clear plagiarism or copying, explaining the journal’s process and next steps. The author's institution should be informed, where necessary.",
            "A reviewer suspects plagiarism in a submitted manuscript. This flowchart provides a step by step process to help editors handle this problem. [Plagiarism](/guidance?f%5B0%5D=topics%3A25)",
            "If the author has copied from their own work, refer to the flowchart redundant (duplicate) publication in a submitted manuscript."
          ]
        },
        {
          "title": "Provenance Data in Social Media",
          "url": "https://www.odbms.org/wp-content/uploads/2014/03/Provenance-Data-in-Social-Media.pdf",
          "excerpts": [
            "Provenance data associated\n with a social media statement can help dispel rumors, clarify opinions, and conﬁrm facts."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpts describe official similarity-checking mechanisms and provenance controls that underpin originality verification. Crossref Similarity Check and related discussions describe formal processes for detecting textual overlap and potential plagiarism, which map directly to the referenced field's reference_corpora and detection context. The content on Crossref pages also outlines how similarity reports are generated and used for editorial decisions, which aligns with the need for systematic originality verification.",
      "confidence": "medium"
    },
    {
      "field": "diversity_and_bias_mitigation_plan",
      "citations": [
        {
          "title": "SoK: Measuring Blockchain Decentralization",
          "url": "https://arxiv.org/html/2501.18279v1",
          "excerpts": [
            "Jan 30, 2025 — A closely related metric to Gini is the Theil index, which intends to capture the lack of diversity, or the redundancy, in a population [14] ."
          ]
        },
        {
          "title": "Optimal Strategies to Perform Multilingual Analysis of ...",
          "url": "https://arxiv.org/html/2311.14727v2",
          "excerpts": [
            "Jul 3, 2025 — Specifically, we investigate which NLP techniques are best to keep manual data annotation to a minimum and to avoid cumbersome and costly rule- ..."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7931947/",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Best Practices for Managing Data Annotation Projects",
          "url": "https://assets.bbhub.io/company/sites/40/2020/09/Annotation-Best-Practices-091020-FINAL.pdf",
          "excerpts": [
            "by T Tseng · 2020 · Cited by 20 — • Annotation guidelines should include both tool and annotation instructions. • Annotation guidelines should also illustrate each label with examples.See more",
            "by T Tseng · 2020 · Cited by 20 — Annotation is the labeling of data by human effort. Annotation is critical to modern machine learning, and. Bloomberg has developed years of experience of ..."
          ]
        },
        {
          "title": "Comparing methods for creating a national random sample of twitter ...",
          "url": "https://link.springer.com/article/10.1007/s13278-024-01327-5",
          "excerpts": [
            "The results show that the *bounding box* (BB) and *location query* (Loc) sampling methods produce a significantly higher number of tweets compared to the *language query* (Lang) and *1% steam* methods. Among the four Twitter sampling methods, BB and Loc methods produce more than 18 million tweets, whereas the Lang and 1% stream methods generate 4.5 million and 174,000 tweets, respectively, withi",
            "A common aim when working with Twitter data is the construction of a random sample of users from a given country.",
            "The *1% Stream* method achieves the minimum error, compared to the other three methods, in the prediction task of estimating the population of the US from Twitter users.",
            "To address this fact, researchers often try to create a random sample of Twitter users from a country. However, at least four widely-used sampling methods exist in the literature, and the extent to which their outputs are similar or different has not been explored systematically so far.",
            "r results highlight the *1% Stream* Twitter sampling method, which exhibits different characteristics compared to the other three sampling methods and fits as the top candidate in most use cases."
          ]
        }
      ],
      "reasoning": "- The plan explicitly calls for topic diversity using BERTopic to model short tweets and ensure diversity across themes like Technology, Philosophy, Psychology, etc. This provides a concrete method aligned with the Core Insight that diversity of topics is a design goal, and BERTopic is a suitable tool for short-text topic diversity management. - Discussion of multilingual analysis strategies and addressing language bias aligns with the language_and_region_strategy portion of the field value, which emphasizes sampling across diverse languages and demographic inference. These excerpts support the strategic emphasis on linguistic and regional diversity rather than relying solely on English or a single language. - Attention to bias in datasets and disclosure of bias considerations supports the author_diversity_strategy and documentation aspects, showing that acknowledging and mitigating bias is an active design component of the plan. - Best-practices excerpts on annotation guidelines, provenance, and data statements underpin the documentation_requirement portion of the plan, ensuring transparent bias mitigation and dataset quality controls. - References to sampling strategies and controls for diversity (e.g., per-author caps, post-stratification, bounding-box geographic targeting, and avoidance of overrepresentation) provide concrete precedent and methodological grounding for the niche_overrepresentation_prevention and geographic/demographic sampling parts of the plan. - Some entries discuss general data annotation quality and provenance, reinforcing the documentation and governance needs described in the plan.",
      "confidence": "high"
    },
    {
      "field": "nlp_classifier_plan_for_insightful_content",
      "citations": [
        {
          "title": "Rapid Training Data Creation with Weak Supervision - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5951191/",
          "excerpts": [
            "by A Ratner · 2017 · Cited by 1173 — (1) SME users write labeling functions (LFs) that express weak supervision sources like distant supervision, patterns, and heuristics. (2) Snorkel applies the ..."
          ]
        },
        {
          "title": "Human labeling, weak supervision and multilingual NLP - Medium",
          "url": "https://medium.com/@vbsowmya/human-labeling-weak-supervision-and-multilingual-nlp-edeced22550c",
          "excerpts": [
            "Weak supervision is a machine learning approach used in scenarios where we don't have readily available human-labeled data, and manually collecting it is ..."
          ]
        },
        {
          "title": "ML Techniques: Active Learning vs Weak Supervision - Appen",
          "url": "https://www.appen.com/blog/ml-techniques-active-learning-vs-weak-supervision",
          "excerpts": [
            "Which machine learning technique is right for you? We explain active learning vs weak supervision and which one is best to use for you."
          ]
        },
        {
          "title": "Active Learning Overview: Strategies and Uncertainty Measures",
          "url": "https://medium.com/data-science/active-learning-overview-strategies-and-uncertainty-measures-521565e0b0b",
          "excerpts": [
            "Uncertainty sampling is a set of techniques for identifying unlabeled items that are near a decision boundary in your current machine learning ..."
          ]
        },
        {
          "title": "Fine-tune an LLM on a Single GPU with QLoRA",
          "url": "https://medium.com/@rjnclarke/fine-tune-an-llm-on-a-single-gpu-with-qlora-faa270e2a043",
          "excerpts": [
            "Using QLora to train small LLM (SLM) on a single A100 GPU. AI NLP method used in example of text classifcation on google colab."
          ]
        },
        {
          "title": "Currently, what's the easiest way to fine tune a bert-like ...",
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bum24a/currently_whats_the_easiest_way_to_fine_tune_a/",
          "excerpts": [
            "For common NLP tasks discussed above, BERT takes between 1-25mins on a single Cloud TPU or between 1-130mins on a single GPU."
          ]
        },
        {
          "title": "microsoft/deberta-v3-base - Hugging Face",
          "url": "https://huggingface.co/microsoft/deberta-v3-base",
          "excerpts": [
            "The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens.See more"
          ]
        },
        {
          "title": "DeBERTa - Hugging Face",
          "url": "https://huggingface.co/docs/transformers/main/en/model_doc/deberta",
          "excerpts": [
            "In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using ..."
          ]
        },
        {
          "title": "The implementation of DeBERTa - GitHub",
          "url": "https://github.com/microsoft/DeBERTa",
          "excerpts": [
            "DeBERTa-V3-XSmall is added. With only 22M backbone parameters which is only 1/4 of RoBERTa-Base and XLNet-Base, DeBERTa-V3-XSmall significantly outperforms ..."
          ]
        },
        {
          "title": "Welcome to LLMflation - LLM inference cost is going down ...",
          "url": "https://a16z.com/llmflation-llm-inference-cost/",
          "excerpts": [
            "Nov 12, 2024 — The cost of LLM inference has dropped by a factor of 1,000 in 3 years. If we pick a higher MMLU score of 83, we have less data because models of ..."
          ]
        },
        {
          "title": "Finetuning Large Language Models On A Single GPU ...",
          "url": "https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html",
          "excerpts": [
            "Mar 28, 2023 — This article illustrates a simple technique that works as a great workaround to train models with larger batch sizes when GPU memory is a concern: gradient ..."
          ]
        },
        {
          "title": "Weak supervision for pharmacovigilance and NLP",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8554513/",
          "excerpts": [
            "Weak Supervision utilizes noisy, limited, or imprecise sources to provide a supervision signal for labeling large amounts of training data in a supervised learning setting",
            "To decrease the labelling costs, researchers have been using weaker forms of supervision, such as heuristically generating training data with external knowledge bases, patterns/rules, or other classifiers.",
            "The assumption behind our work is that the large volume of training data which can be collected using an automated labeling process can compensate for the inaccuracy in the labels.",
            "there is a huge need to avoid reliance on manual annotation of small datasets and to move to automatic annotation on large datasets.",
            "This approach can help reduce the need for manual annotation which saves time and resources."
          ]
        },
        {
          "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
          "url": "https://arxiv.org/abs/2402.15613",
          "excerpts": [
            "Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications.",
            "When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration.",
            "We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data ...",
            "The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or updating it as newer versions get released.",
            "As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive."
          ]
        },
        {
          "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
          "url": "https://arxiv.org/html/2402.15613v1",
          "excerpts": [
            "The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or updating it as newer versions get released.",
            "As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive."
          ]
        },
        {
          "title": "3 ways to use Snorkel's Labeling Functions",
          "url": "https://snorkel.ai/blog/snorkel-labeling-functions-use-cases/",
          "excerpts": [
            "Jun 24, 2022 — Data scientists and subject matter experts can create labeling functions that capture and apply heuristics or patterns. Looking at a few ..."
          ]
        },
        {
          "title": "Programmatically labeling data using Snorkel with example - Medium",
          "url": "https://medium.com/data-science-in-your-pocket/programmatically-labeling-data-using-snorkel-with-example-a6a322ef0f2c",
          "excerpts": [
            "Snorkel can be very handy when you facing such an issue for building your training dataset when you have little or no labeled dataset at all."
          ]
        },
        {
          "title": "A collection of tutorials for Snorkel - GitHub",
          "url": "https://github.com/snorkel-team/snorkel-tutorials",
          "excerpts": [
            "We recommend that all users start with the Getting Started tutorial hosted on the Snorkel website for a gentle introduction to the concepts and classes of ..."
          ]
        },
        {
          "title": "Example of Labeling Functions",
          "url": "https://docs.datasaur.ai/assisted-labeling/data-programming/example-of-labeling-functions",
          "excerpts": [
            "Jun 26, 2024 — Example of Labeling Functions ... Labeling functions are designed to apply weak heuristics and rules for predicting labels on unlabeled data."
          ]
        },
        {
          "title": "A Quick Guide: Threshold & Calibration in LR",
          "url": "https://www.numberanalytics.com/blog/quick-guide-threshold-calibration-lr",
          "excerpts": [
            "Combining multiple calibration techniques—e.g., initial Platt scaling followed by isotonic regression—can sometimes yield better results."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on a hybrid training plan for an insightful NLP classifier. The strongest support is found in passages that explicitly describe weak supervision with programmatic labeling functions (as in Snorkel) to quickly generate large, imperfect labeled data and then combine this with a smaller gold standard to train a final model. For example, one excerpt explains that Snorkel enables labeling functions to express weak supervision signals and that these can produce high-volume pseudo-labels which are then integrated with gold labels to train models. This directly aligns with a hybrid training approach where weak supervision is used to bootstrap training before refining with gold data, and it even mentions techniques like self-training and confident learning for further refinement. Additional excerpts reinforce this methodology by detailing pool-based Active Learning to efficiently select informative samples for annotation, and by naming concrete models and architectures (e.g., BERT, DeBERTa) as strong baselines for the final classifier, which matches the planned model choices in the field value. There are also passages that discuss using smaller LLMs like Llama in a zero-shot or few-shot capacity for initial labeling or quick classification, which supports the idea of a staged pipeline combining lightweight labeling with heavier fine-tuning. Other excerpts elaborate on the role of pretraining or fine-tuning transformer models (e.g., DeBERTa-v3-base) as effective baselines, which aligns with the recommended models in the plan. The included content also hints at monitoring and calibration considerations through references to calibration and evaluation metrics in related contexts, further supporting the emphasis on precision in the optimization target. Taken together, these excerpts coherently map onto the field value's emphasis on a hybrid weak-supervision/gold-set approach, pool-based Active Learning for gold-set construction, and transformer-based final models, with attention to precision and calibration.",
      "confidence": "high"
    },
    {
      "field": "recommended_strategy_overview",
      "citations": [
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ...",
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data."
          ]
        },
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Basic\n\n$200 /month\n\nFor hobbyists and prototypes\n\n#### What's included:\n\n1 project\n\n2 apps / project\n\n15,000 posts / month (reads)\n\n50,000 posts / month (writes)\n\nFull v2 endpoints access](https://developer.x.com/en/portal/products)",
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)"
          ]
        },
        {
          "title": "Pagination - Welcome to the X Developer Platform",
          "url": "https://docs.x.com/x-api/posts/search/integrate/paginate",
          "excerpts": [
            "The recent search endpoints will respond to a query with at least one page, and provide a next_token in its JSON response if additional pages are available."
          ]
        },
        {
          "title": "Twitter API - Tweet Object and Data Dictionary",
          "url": "https://developer.x.com/en/docs/x-api/v1/data-dictionary/object-model/tweet",
          "excerpts": [
            "The user who posted this Tweet. See User data dictionary for complete list of attributes. Example highlighting select attributes:"
          ]
        },
        {
          "title": "Developer Agreement and Policy - Twitter Developers - X",
          "url": "https://developer.x.com/en/developer-terms/agreement-and-policy",
          "excerpts": [
            "nfidential Information (as defined below).\n\n**D. Rate Limits.** You will not attempt to exceed or circumvent limitations on access, calls, and use of the X API (\" **Rate Limits** \") or otherwise use the X API in a manner that exceeds reasonable request volume, constitutes excessive or abusive usage, or otherwise does not comply with this Agreement. If you exceed or X reasonably believes that you have attempted to circumvent Rate Limits, controls to limit use of the X APIs, or the terms of this Agreement, then your ability to use the Licensed Material may be temporarily suspended or permanently blocked.",
            "**. Notwithstanding anything to the contrary in this Agreement, to the extent you are provided access to the Licensed Material pursuant to the procedures described in Article 40 of the Digital Services Act (Regulation (EU) 2022/2065) (“DSA”), your access and use of the Licensed Material is limited solely to performing research that contributes to the detection, identification, and understanding of systemic risks in the European Union and only to the extent necessary for X to comply with its obligations under the DSA.",
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs). Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research.",
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties. If you ..."
          ]
        },
        {
          "title": "Terms of Service - X",
          "url": "https://x.com/en/tos/previous/version_19",
          "excerpts": [
            "This license has the sole purpose of enabling you to use and enjoy the benefit of the Services as provided on X, in the manner permitted by these Terms."
          ]
        },
        {
          "title": "Start Collecting: Twarc Command Basics",
          "url": "https://scholarslab.lib.virginia.edu/learn-twarc/06-twarc-command-basics",
          "excerpts": [
            "This process is called “rehydration”, and will return a JSON file containing the data for every tweet that it was able to locate. It's important to note that ...",
            "A dehydrated data set can be “rehydrated” using Twarc - Twitter will take each ID and search the current Twitterverse for a corresponding tweet. If it locates a ..."
          ]
        },
        {
          "title": "Docnow – Docnow",
          "url": "https://www.docnow.io/",
          "excerpts": [
            "Hydrator is a desktop application for turning Tweet ID datasets back into tweet data to use in your research. It has been designed to be a reliable option for ..."
          ]
        },
        {
          "title": "Docnow App",
          "url": "https://www.docnow.io/docnow-app/",
          "excerpts": [
            "DocNow is an open-source app for appraising, collecting, and gathering consent for social media archives. It collects content, downloads archives, and hydrates ..."
          ]
        },
        {
          "title": "Metrics",
          "url": "https://developer.x.com/en/docs/x-api/metrics",
          "excerpts": [
            "Metrics include the total count of impressions, Retweets, Quote Tweets, likes, replies, video views, video view quartiles, URL and profile link clicks for each ...See more"
          ]
        },
        {
          "title": "Tweet's public metrics - X API v2",
          "url": "https://devcommunity.x.com/t/tweets-public-metrics/180920",
          "excerpts": [
            "Nov 30, 2022 — I'm specifically interested in the public ones, but if the answer is similar for paid or organic metrics, it would be great. Thanks.See more"
          ]
        },
        {
          "title": "Twitter Data Collection: Tweet Rehydration - Communalytic",
          "url": "https://communalytic.org/tutorials/twitter-data-collection-tweet-rehydration/",
          "excerpts": [
            "Tweet Rehydration allows you to collect tweets from Twitter using their unique IDs. To collect data using this function, you need a list of tweet IDs in a CSV ..."
          ]
        },
        {
          "title": "DocNow/hydrator: Turn Tweet IDs into Twitter JSON & CSV ...",
          "url": "https://github.com/DocNow/hydrator",
          "excerpts": [
            "However they do allow datasets of tweet IDs to be shared. Hydrator helps you turn these tweet IDs back into JSON and also CSV from the comfort of your desktop.",
            "Twitter's changes to their API which greatly reduce the amount of read-only access means that the Hydrator is no longer a useful application. The application keys, which functioned for the last 7 years, have been rescinded by Twitter.",
            "Hydrator is an Electron based desktop application for hydrating Twitter ID datasets. Twitter's Terms of Service do not allow the full JSON for datasets of ..."
          ]
        },
        {
          "title": "Learn how to easily hydrate tweets",
          "url": "https://towardsdatascience.com/learn-how-to-easily-hydrate-tweets-a0f393ed340e/",
          "excerpts": [
            "Mar 8, 2022 — Two tools provided by Documenting the Now (DocNow) for this purpose: a desktop application Hydrator and a command line tool twarc."
          ]
        },
        {
          "title": "Guide to rehydrating tweets with the Twitter API V2",
          "url": "https://developer.talkwalker.com/guides/rehydrating-tweet",
          "excerpts": [
            "Guide to rehydrating tweets with the Twitter API V2 · Getting started with Twitter API · Retrieve tweet details · Retrieve author details."
          ]
        }
      ],
      "reasoning": "The requested fine-grained field value describes a concrete plan to acquire data through a hybrid, multi-source approach and to manage data responsibly with filtering, NLP enrichment, and QA practices, all while being mindful of costs and platform policies. Excerpts that discuss high-end API pricing illustrate the financial constraints and decision points behind choosing between free/basic tiers and premium enterprise options, which directly informs the \"hybrid data acquisition\" rationale and cost efficiency goals. Excerpts that outline the existence of tiered API pricing (Free, Basic, Pro, Enterprise) and the severe price tag of top-tier plans demonstrate why a multi-pronged approach would be attractive from a cost-management perspective and why reliance on a mix of data sources is prudent. Excerpts describing the official pricing landscape for X's API and the new enterprise pricing variants provide concrete context for the \"cost-effectiveness\" and risk-management aspects of the strategy.",
      "confidence": "high"
    },
    {
      "field": "project_summary",
      "citations": [
        {
          "title": "Trending Humor Tweets",
          "url": "https://www.kaggle.com/datasets/jashjasani/trending-humor-tweets",
          "excerpts": [
            "Twitter New Dataset 2024 March Data · Ayush Kumar Singh · a year ago · 8.1 · 3 MB · 351 downloads ; The Tweets of Wisdom · Hsankesara · 6 years ago · 10.0 · 2 MB · 379 ..."
          ]
        },
        {
          "title": "Sentiment & Emotions Labelled Tweets",
          "url": "https://www.kaggle.com/datasets/ankitkumar2635/sentiment-and-emotions-of-tweets",
          "excerpts": [
            "The dataset contains about 25k tweets along with date, timestamp, username, tweet Id, sentiment, sentiment score, emotion and emotion score. grid_3x3sort"
          ]
        },
        {
          "title": "Twitter Tweets Sentiment Dataset",
          "url": "https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset",
          "excerpts": [
            "Twitter is an online Social Media Platform where people share their their though as tweets. It is observed that some people misuse it to tweet hateful content."
          ]
        },
        {
          "title": "Sahil Bloom (@SahilBloom) / X",
          "url": "https://x.com/sahilbloom?lang=en",
          "excerpts": [
            "Underrated life advice: You can reinvent yourself as many times as you need. New habits. New standards. New people. New career. You're never stuck. You're ...See more"
          ]
        },
        {
          "title": "A comprehensive study of wisdom",
          "url": "https://www.kaggle.com/code/hsankesara/a-comprehensive-study-of-wisdom",
          "excerpts": [
            "Understanding the Author, Likes and Retweets¶. How much likes and retweets are related. Value count of tweets of each author. Do more tweets increase the mean ... Explore and run machine learning code with Kaggle Notebooks | Using data from The Tweets of Wisdom\n",
            "A comprehensive study of wisdom",
            "the-tweets-of-wisdom"
          ]
        },
        {
          "title": "Tweet counts enterprise to Twitter API v2 migration guide | Docs",
          "url": "https://developer.x.com/en/docs/twitter-api/tweets/counts/migrate/enterprise-to-twitter-api-v2",
          "excerpts": [
            "The enterprise version of this endpoint uses the following date/time format in both the pagination parameters and the `timePeriod` response field: `YYYYMMDDHHmm`",
            "The v2 endpoint uses ISO 8601/RFC 3339 date/time format in both the pagination parameters and the `start` and `end` response fields: `YYYY-MM-DDTHH:mm:ssZ`",
            "The v2 endpoint uses ISO 8601/RFC 3339 date/time format in both the pagination parameters and the `start` and `end` response fields: `YYYY-MM-DDTHH:mm:ssZ`",
            "#### Request parameters"
          ]
        },
        {
          "title": "Hsankesara",
          "url": "https://www.kaggle.com/datasets/hsankesara/",
          "excerpts": [
            "Flickr Image dataset. Updated 7 years ago Usability 7.1 · 9 GB ... The Tweets of Wisdom. Updated 6 years ago Usability 10.0 · 1 File (CSV) · 2 ..."
          ]
        },
        {
          "title": "Tweets To Templates: Train ChatGPT To Reverse Engineer ...",
          "url": "https://writewithai.substack.com/p/tweets-to-templates-train-chatgpt",
          "excerpts": [
            "Today we want to help you create your own library of viral Twitter templates. Going viral on Twitter always comes down to the hook you use."
          ]
        },
        {
          "title": "Reddit - r/indiehackers discussion by erik-grielenberger",
          "url": "http://reddit.com/r/indiehackers/comments/1m2ugls/i_reverseengineered_500_viral_tweets_heres",
          "excerpts": [
            "1. Lead with one outcome. [r/indiehackers - I reverse‑engineered 500+ VIRAL TWEETS - Here’s everything I've learned. Steal it ",
            "TL;DR**: Hook hard. Prove fast. Ask once. Media every two tweets. Follow this blueprint and turn that viral ceiling into your floor. Share if it help",
            "**4. Six hook patterns you can copy today:**",
            "**5. Pick a Narrative Skeleton (lock one). **",
            "**6. Media matters:** Drop a data card, GIF or 5‑sec demo every two or three tweets. Alternating text and media increases retweets by 50%. Add a three‑word headline and ALT text for each visual.",
            "5. Pick a Narrative Skeleton (lock one).",
            "4. Six hook patterns you can copy today:",
            "Copy my data-backed AI Templates: bettrprompts.com/category/viral-social-media",
            "*Hi, I’m Erik** - massive data nerd. I’ve primarily created these datasets because I want to increase my social media reach and understand how some posts go viral and most of them don",
            "This data analysis shows you how only **39 Threads** reached **118M+ impressions** and **450k+ likes** *(save for later)*."
          ]
        },
        {
          "title": "The anatomy of a viral tweet: The \"rehashing old news\" variant",
          "url": "https://weaponizedspaces.substack.com/p/the-anatomy-of-a-viral-tweet-the",
          "excerpts": [
            "A tweet that is said to “go viral” is generally one that has garnered a certain number of interactions within a specified time frame."
          ]
        },
        {
          "title": "What does a non-celebrity have to do to get famous on ...",
          "url": "https://www.quora.com/What-does-a-non-celebrity-have-to-do-to-get-famous-on-Twitter",
          "excerpts": [
            "I have seen many non-celebs who became prominent on Twitter. You need to show off the value in your tweets to become famous."
          ]
        },
        {
          "title": "Initial tweet valence, abuse volume, and observer Dark ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11106073/",
          "excerpts": [
            "by CJ Hand · 2024 · Cited by 1 — The current study. Participants processed six Twitter threads consisting of an initial tweet by a female celebrity followed by six replies."
          ]
        },
        {
          "title": "Studying Celebrity Practices on Twitter Using a Framework ...",
          "url": "https://journals.sagepub.com/doi/10.1177/2056305118763365",
          "excerpts": [
            "by S Tanupabrungsun · 2018 · Cited by 28 — We constructed three datasets by employing a toolkit (Hemsley, Ceskavich, & Tanupabrungsun, 2014) that collects tweets using a streaming API."
          ]
        },
        {
          "title": "As a YouTuber, is it really that vital to have a Twitter ...",
          "url": "https://www.reddit.com/r/socialmedia/comments/lqa4o1/as_a_youtuber_is_it_really_that_vital_to_have_a/",
          "excerpts": [
            "There are two things I see creators do on Twitter to their benefit. They make connections with other creators so they can work together, and ..."
          ]
        },
        {
          "title": "I followed a lot of indie hackers and “build in public” accounts on ...",
          "url": "https://news.ycombinator.com/item?id=43407164",
          "excerpts": [
            "I followed a lot of indie hackers and “build in public” accounts on Twitter over the years. Most of them struggled for a while and then pivoted into some ..."
          ]
        },
        {
          "title": "21 Extremely Relatable Tweets That Will Speak To Every ...",
          "url": "https://www.buzzfeed.com/ishabassi/funny-writer-tweets",
          "excerpts": [
            "May 30, 2018 — 21 Extremely Relatable Tweets That Will Speak To Every Writer's Soul ; 1. Hannah Giorgis @ethiopienne ; 2. anna s-r @annaspargoryan ; 3. f thot ..."
          ]
        },
        {
          "title": "32 Funny Tweets That Are Completely Underrated",
          "url": "https://www.buzzfeed.com/erinchack/underrated-tweets",
          "excerpts": [
            "Mar 10, 2017 — We asked the BuzzFeed Community what their favorite underrated tweet was. Here are the best responses. Note: Not all submissions are from Community users."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "Public",
            "The-Tweets-of-Wisdom",
            "Here is the [link](https://www.kaggle.com/hsankesara/the-tweets-of-wisdom) to the dataset there.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "I scraped the data using **Tweepy** API. I have scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "A dataset which contains 30k+ so called \"self-help\" tweets from 100+ authors."
          ]
        },
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        },
        {
          "title": "Starter: The Tweets of Wisdom b542b708-9",
          "url": "https://www.kaggle.com/code/kerneler/starter-the-tweets-of-wisdom-b542b708-9",
          "excerpts": [
            "Explore and run machine learning code with Kaggle Notebooks | Using data from The Tweets of Wisdom."
          ]
        },
        {
          "title": "Nonrandom Tweet Mortality and Data Access Restrictions Compromising the Replication of Sensitive Twitter Studies",
          "url": "https://www.cambridge.org/core/journals/political-analysis/article/nonrandom-tweet-mortality-and-data-access-restrictions-compromising-the-replication-of-sensitive-twitter-studies/17E618F1D395CBBB2360AA424DA5533A",
          "excerpts": [
            ") ). The one-way aspect of this well-established computer science technique prevents rehydrating tweets’ raw content.",
            "hat these Twitter studies and their findings are considerably affected by nonrandom tweet mortality and data access restrictions imposed by ... As pr"
          ]
        },
        {
          "title": "Provenance Data in Social Media",
          "url": "https://www.odbms.org/wp-content/uploads/2014/03/Provenance-Data-in-Social-Media.pdf",
          "excerpts": [
            "Provenance data associated\n with a social media statement can help dispel rumors, clarify opinions, and conﬁrm facts.",
            "However, provenance data about social media statements is not readily available to users\ntoday.",
            "The search for provenance attribute values can continue using\nthe profile with the highest probability.",
            "The W3C incubator group provided a list of provenance dimensions that are applicable to\nprovenance data in social media.",
            "imeliness\n of social media data can be defined more simply as [8]:\n\t\t\t\t Qcurrency = (current_time − time_provenance_data_created)/retrie",
            "A provenance system\n that takes longer to gather provenance attribute values than the frequency at which the provenance\n attribute values are, or are likely to be updated, does not provide accurate or valuable provenance\n attribute valu"
          ]
        },
        {
          "title": "python - How to query Twitter API public metrics for multiple ...",
          "url": "https://stackoverflow.com/questions/72640093/how-to-query-twitter-api-public-metrics-for-multiple-tweet-ids-using-tweepy",
          "excerpts": [
            "Use Tweepy to to call the Twitter API to return the public_metrics ( likes , retweets , quotes , replies ) for each of multiple tweet_id s."
          ]
        },
        {
          "title": "Twitter API v2 in Python - issues with datetime conversion",
          "url": "https://stackoverflow.com/questions/66629933/twitter-api-v2-in-python-issues-with-datetime-conversion",
          "excerpts": [
            "I upgraded my Python-Twitter API access from v1 to v2. But as it seems, Twitter does not any longer accept my timestamp-formats for the updated ..."
          ]
        },
        {
          "title": "Possible to check API rate limit headers without burning a ...",
          "url": "https://community.openai.com/t/possible-to-check-api-rate-limit-headers-without-burning-a-request/510041",
          "excerpts": [
            "Nov 17, 2023 — I'd like to tell my user how many requests they have left before they start running a bulk API task. The best information on remaining rate ..."
          ]
        },
        {
          "title": "List of most-followed Twitter accounts",
          "url": "https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts",
          "excerpts": [
            "The following table lists the top 50 most-followed accounts on X, with each total rounded down to the nearest hundred thousand, as well as a description of each account."
          ]
        },
        {
          "title": "Twitter's new API plan costs up to $2.5 million per year",
          "url": "https://mashable.com/article/twitter-elon-musk-paid-enterprise-api-access-pricing",
          "excerpts": [
            "Mar 10, 2023 — The pricing goes all the way up to $210,000 per month for the highest plan with 200 million tweets. A mid tier plan that provides 100 million ..."
          ]
        }
      ],
      "reasoning": "The finegrained field seeks to document a database of at least 5,000 underrrated viral tweets. The strongest support comes from excerpts that explicitly reference or provide templates for underrrated viral content or seeds for such a dataset. Specifically, entries describing The Tweets of Wisdom (a curated dataset of thousands of 'self-help'/wisdom-themed tweets from non-celebrity authors) directly align with the underrrated, non-celebrity source and insight criteria, and they offer concrete seed material for building a larger catalogue. Related entries discuss a broader pattern of underrated content and practical steps to assemble similar datasets, which helps justify both the feasibility and the research approach. The presence of explicit examples (like Sahil Bloom's post, which is an underestimated form of valuable life-advice) supports the notion that underrrated content can achieve engagement while remaining outside mainstream celebrities. Additional excerpts highlight the data-gathering constraints that are central to the field value: the high cost of API access for full-archive data on X/Twitter (Pro or Enterprise tiers), limitations of engagement-based filters (min_faves, min_retweets), and the necessity for hybrid data collection strategies (public datasets, seed corpora, human-in-the-loop validation). These cost and methodological considerations strengthen the overall argument that creating a 5,000+ tweet database is feasible but expensive and technically challenging, requiring careful design, seed datasets, and multi-layer filtering. The most directly relevant passages describe: (a) the existence and potential utility of The Tweets of Wisdom as seed data and the idea of a wisdom/underrated content corpus, (b) the notion that underrrated, insightful tweets exist beyond celebrities, and (c) explicit notes on the high costs and gating factors (pricing tiers, API limits) that shape how such a large-scale dataset can be assembled. The less direct items—while informative about virality research in general—are secondary because they do not address the underrrated, non-celebrity sourcing criterion or provide concrete seed datasets. The overall confidence is medium to high for the claim that a 5,000-tweet underrrated viral tweet database is plausible with a multi-pronged data collection plan, but high confidence is limited by the need for explicit implementation details (annotation guidelines, human-in-the-loop processes) and real-world access constraints highlighted in the excerpts. The ranking favors seed-dataset-oriented and capability-constraining evidence over general API pricing discussions, since the field value centers on dataset construction and validation under realistic cost constraints.",
      "confidence": "medium"
    },
    {
      "field": "underrated_source_definition",
      "citations": [
        {
          "title": "Most followed accounts on X (Twitter) – 2025",
          "url": "https://www.tweetbinder.com/blog/top-twitter-accounts/",
          "excerpts": [
            "Top 10 accounts by number of followers (top 10 most followed Twitter accounts). ; Barack Obama, @BarackObama, 130M ; Cristiano Ronaldo, @Cristiano, 115M ; Narendra ..."
          ]
        },
        {
          "title": "Top 10 Most Followed X (Twitter) Accounts in 2025",
          "url": "https://www.favikon.com/blog/top-10-most-followed-x-twitter-accounts",
          "excerpts": [
            "10. Lady Gaga · 9. Taylor Swift · 8. Donald Trump · 7. Narendra Modi · 6. Katy Perry · 5. Rihanna · 4. Justin Bieber · 3. Cristiano Ronaldo.See more"
          ]
        },
        {
          "title": "List of most-followed Twitter accounts",
          "url": "https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts",
          "excerpts": [
            "| 1 | @elonmusk | [Elon Musk](/wiki/Elon_Musk \"Elon Musk\") | 222\\.2 | Businessman, owner of X/Twitter, xAI, SpaceX and Former Senior Advisor to the President |  |",
            "| 2 | [@BarackObama](/wiki/Social_media_use_by_Barack_Obama \"Social media use by Barack Obama\") | [Barack Obama](/wiki/Barack_Obama \"Barack Obama\") | 130\\.3 | U.S. President (2009–2017) |  |",
            "| 3 | @Cristiano | [Cristiano Ronaldo](/wiki/Cristiano_Ronaldo \"Cristiano Ronaldo\") | 115\\.6 | Football player |  |",
            "| 5 | @justinbieber | [Justin Bieber](/wiki/Justin_Bieber \"Justin Bieber\") | 108\\.6 | Musician |  |",
            "| 9 | @taylorswift13 | [Taylor Swift](/wiki/Taylor_Swift \"Taylor Swift\") | 93\\.6 | Musician |  |"
          ]
        }
      ],
      "reasoning": "The guidelines for underrating sources require excluding top-celebrity figures, politicians, and large media outlets. Excerpts that list or reference the most-followed or high-profile Twitter/X accounts provide concrete examples of mainstream sources to be excluded. For instance, excerpts enumerating the most-followed accounts explicitly name individuals who are global celebrities (e.g., Elon Musk) and high-profile figures (e.g., Barack Obama), illustrating the scale and type of accounts to exclude. Other excerpts present comprehensive lists of top Twitter/X accounts and influencers, which helps operationalize a threshold-based and list-based exclusion approach. By citing such lists, we can infer criteria like follower thresholds and membership in well-known media/political clusters that would trigger exclusion under the described rules. Collectively, these excerpts support the concept that underrated sources should avoid mainstream celebrities, politicians, and large media organizations, and they provide concrete reference points (names and lists) for implementing the exclusion rubric in Phase 1 screening. The presence of multiple lists (top followers, major influencers) demonstrates the kinds of sources that would be filtered out under the final heuristic, aligning with the intended underrating methodology.",
      "confidence": "medium"
    },
    {
      "field": "content_category_taxonomy",
      "citations": [
        {
          "title": "Taxonomy 101: Definition, Best Practices, and How It ...",
          "url": "https://www.nngroup.com/articles/taxonomy-101/",
          "excerpts": [
            "Taxonomies aren’t created for the pure love of classifying things (though some UXers like myself absolutely love doing it). Taxonomies allow us to effectively retrieve *all* the content that is related to a specific concep"
          ]
        },
        {
          "title": "Taxonomy Design Best Practices - Create and Build",
          "url": "https://www.claravine.com/evolution-of-taxonomy-design/",
          "excerpts": [
            "There has been an evolution of taxonomy design development. Here are the best practices from the perspective of a practitioner."
          ]
        },
        {
          "title": "Methodological Notes",
          "url": "https://www.k-alpha.org/methodological-notes",
          "excerpts": [
            "_ _:_ This range is often considered the lower bound for tentative conclusions. A Krippendorff's Alpha in this range suggests moderate agreement; thus, outcomes should be interpreted with concern, questioning the roots of such diverging ratings.",
            "_ _:_ This is indicative of poor agreement among raters. Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not applying the coding scheme consistently or that the scheme itself may be flawed.",
            " a Krippendorff's Alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions based on the rated data.",
            "Alpha = 1_ _:_ Indicates perfect agreement among raters. It is the scenario where all raters have provided the exact same ratings for each item evalua",
            "Alpha ≥ 0.80: This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's ...",
            "Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not ..."
          ]
        },
        {
          "title": "The measurement of observer agreement for categorical data",
          "url": "https://pubmed.ncbi.nlm.nih.gov/843571/",
          "excerpts": [
            "by JR Landis · 1977 · Cited by 93933 — This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "- The excerpt outlining Taxonomy 101 provides foundational guidance on what a taxonomy is, why it's used, and how to structure terms and definitions, which directly informs the \"core list of categories\" and the subcategories needed for a multi-label taxonomy. It also frames best practices for organizing terms, which is essential for a taxonomy intended to classify Tweets across interdisciplinary themes like Technology and Philosophy. - The companion excerpt on Taxonomy Design Best Practices expands on how to construct and refine a taxonomy, which is crucial for ensuring that the core categories (Technology, Philosophy, Psychology, Life Advice, Creativity, Science, Business) are well-defined and consistently applied, including considerations for additional categories as needed. - Excerpts that discuss Krippendorff's Alpha as an inter-annotator reliability metric establish an objective standard for agreement, including typical threshold guidance and interpretation, which supports setting a reliability target (e.g., alpha ≥ 0.800) for a high-quality taxonomy implementation. - Excerpts detailing Krippendorff's Alpha interpretation and thresholds provide concrete benchmarks for when taxonomy annotations are reliable and ready for use in higher-stakes analyses, aligning with the need for consistent multi-label labeling across categories and subcategories. - Excerpts on inter-annotator agreement more broadly (e.g., Cohen's Kappa as a related reliability measure) reinforce the primacy of agreement metrics in validating a taxonomy-driven labeling scheme, and they offer complementary perspectives when Krippendorff's Alpha is not applicable. - Together, these excerpts inform the governance rules for resolving conflicts when annotators disagree (conflict resolution rules) and the diagnostics for detecting label imbalances (imbalance_diagnostics), both of which are necessary to maintain a robust taxonomy in practice. - Collectively, the sources provide a coherent picture: define a multi-label taxonomy with explicit core categories and subcategories; establish high reliability targets (e.g., alpha ≥ 0.800); implement adjudication and conflict-resolution flow to handle overlaps; and monitor label balance to ensure taxonomy effectiveness over time.",
      "confidence": "high"
    },
    {
      "field": "author_description_extraction_process.extraction_tools",
      "citations": [
        {
          "title": "Twitter(X) Profile Bio ICP Classifier - Bio Keywords Extractor",
          "url": "https://apify.com/lead.gen.labs/twitter-x-profile-bio-icp-classifier---bio-keywords-extractor",
          "excerpts": [
            "Classify Twitter (X) profiles by using bio keyword extraction. This tool scrapes public Twitter bios and intelligently extracts relevant keywords to help you ...See more"
          ]
        }
      ],
      "reasoning": "The target field value enumerates a combination of tools for identifying job titles, including specialized NER models from Hugging Face and keyword-based libraries, as well as commercial APIs for normalization. Among the provided excerpts, the one that aligns closest discusses classifying Twitter profiles by using bio keyword extraction, including scraping public Twitter bios to extract relevant keywords. This directly demonstrates a tool-based approach to extracting descriptive terms from author bios, which is a core aspect of the extraction_tools referenced in the field value. While other excerpts touch on bio-related topics (e.g., bio cleanup or bio as a profiling signal), they do not as clearly describe concrete extraction tools or pipelines for author descriptions. Therefore, the quoted content about bio keyword extraction provides the most direct support for the stated extraction_tools concept, illustrating a practical implementation path that resembles the described tools (NER models and keyword libraries) in the field value.",
      "confidence": "medium"
    },
    {
      "field": "data_acquisition_plan",
      "citations": [
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)",
            "### Basic\n\n$200 /month\n\nFor hobbyists and prototypes\n\n#### What's included:\n\n1 project\n\n2 apps / project\n\n15,000 posts / month (reads)\n\n50,000 posts / month (writes)\n\nFull v2 endpoints access](https://developer.x.com/en/portal/products)"
          ]
        },
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data.",
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ..."
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "TwitterAPI.io",
          "url": "https://twitterapi.io/",
          "excerpts": [
            "Cost per 1K tweets, $0.15, $100+",
            "Rate Limit, 1000+ req/sec, 300 req/15min"
          ]
        },
        {
          "title": "Hashtag tracker pricing plan by TrackMyHashtag",
          "url": "https://www.trackmyhashtag.com/pricing",
          "excerpts": [
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ...",
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ..."
          ]
        },
        {
          "title": "Meltwater Pricing & Costs",
          "url": "https://www.meltwater.com/en/pricing",
          "excerpts": [
            "Learn more about our Meltwater pricing model, packaging, and costs to find your personal solution. Build your personalized Meltwater suite today!"
          ]
        }
      ],
      "reasoning": "The data_acquisition_plan envisions a cost-aware, hybrid approach that blends multiple data-sourcing channels. Direct pricing details for official API access are highly pertinent: cited material shows that the Pro tier exists with concrete monthly access figures and that full-archive search capability is part of the official offering, which aligns with the plan's need for historical data access. For example, the official pricing documentation delineates a Pro tier with substantial read allowances and lists full-archive search access among the features, supporting the plan's reliance on official API access for verification or rehydration rather than wholesale harvesting alone. Additionally, real-world reporting on extreme pricing for API access—such as multi-thousand-dollar-per-month figures for premium plans—illustrates the cost pressure that underpins the suggested hybrid strategy, where cheaper third-party providers are used for bulk collection while the official API handles targeted verification and rehydration tasks. Several articles quantify the pricing landscape in familiar terms (for instance, a high monthly price around $42k) and discuss shifts in pricing that push researchers and smaller teams toward alternative data-sourcing routes, which corroborates the plan's aim to diversify suppliers to manage cost and accessibility. The excerpts detailing third-party options and their pricing reinforce the plan's multi-pronged nature: one passage cites a third-party data-service with favorable per-tweet pricing, which complements Kaggle/GitHub seed data; another passage discusses TrackMyHashtag as a source of historical data purchases with explicit pricing. The inclusion of TrackMyHashtag pricing specifics directly supports the plan's assertion that targeted, historical data purchases can be a cost-effective supplement to API-driven collection. Additional excerpts describe the availability and pricing of third-party data aggregators (TwitterAPI.io) and the micro-economics of data acquisition (per-1k-tweet pricing, cost per tweet, etc.), which reinforce the rationale for a hybrid strategy rather than relying solely on the official API. In sum, the connected content substantiates the plan's core claims about: (1) the existence and cost of official API tiers and the desirability of full-archive search for historical scraping needs, (2) the rationale for incorporating cheaper third-party data sources to scale collection while controlling costs, and (3) the practical expectations around data acquisition costs and hit rates that drive a multi-provider approach.",
      "confidence": "high"
    },
    {
      "field": "quality_assurance_framework",
      "citations": [
        {
          "title": "Selecting Statistically Valid Sampling Plans - Taylor Enterprises",
          "url": "https://variation.com/selecting-statistically-valid-sampling-plans/",
          "excerpts": [
            "OC curves are generally summarized by two numbers: the Acceptable Quality Level (AQL) and Lot Tolerance Percent Defective (LTPD). The AQL is that percent ..."
          ]
        },
        {
          "title": "Zero Acceptance Number Sampling Plans, Fifth Edition",
          "url": "https://www.amazon.com/Acceptance-Number-Sampling-Plans-Fifth/dp/0873897390",
          "excerpts": [
            "C=0 should be a consideration for all companies seeking a simpler alternative to the old Mil Std 105e, current Z 1.4. Its plans eliminate acceptance on anything ..."
          ]
        },
        {
          "title": "Acceptance Sampling Standards and AQL",
          "url": "https://www.asq104.org/app/download/555762704/Acceptance+Sampling+D+OLeary+20110216.pdf",
          "excerpts": [
            "ANSI/ASQ Z1.4 Sampling Procedures and Tables for Inspection By",
            "Attributes"
          ]
        },
        {
          "title": "Acceptance Sampling - CQE Academy",
          "url": "https://cqeacademy.com/cqe-body-of-knowledge/product-process-control/acceptance-sampling/",
          "excerpts": [
            "If we look at this OC Curve above, we can tell that the AQL associated with this sampling plan is 2%, and the LTPD for this sampling plan is 8%."
          ]
        },
        {
          "title": "Building custom NLP tools to annotate discourse-functional ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2772766124000594",
          "excerpts": [
            "by M Eguchi · 2024 · Cited by 5 — When it comes to developing an ML system, the goal of annotation is to construct a “gold-standard” annotation dataset, which provides consistent input-output ..."
          ]
        },
        {
          "title": "entity hierarchy, corpus annotation, and sequence labeling",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8104034/",
          "excerpts": [
            "by T Thieu · 2020 · Cited by 22 — 2.2.​​ The annotation process: from guidelines and schema development to creation of the gold standard corpus."
          ]
        },
        {
          "title": "DataFrame Schemas - pandera documentation",
          "url": "https://pandera.readthedocs.io/en/stable/dataframe_schemas.html",
          "excerpts": [
            "The DataFrameSchema class enables the specification of a schema that verifies the columns and index of a pandas DataFrame object."
          ]
        },
        {
          "title": "Binomial proportion confidence interval - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval",
          "excerpts": [
            "The Clopper–Pearson interval is an early and very common method for calculating binomial confidence intervals. This is often called an 'exact' method, as it ..."
          ]
        },
        {
          "title": "ANSI/ASQ Z1.4 and Z1.9 Sampling Standards",
          "url": "https://asq.org/quality-resources/z14-z19?srsltid=AfmBOopy-uXRazS2cMW3DDdt4aObKmnwChaU8o-i-FYyr0XNLsPazFcT",
          "excerpts": [
            "What is the Z1.9 Standard? --------------------------"
          ]
        },
        {
          "title": "PARSEME Shared Task 1.3 - Annotation guidelines",
          "url": "https://parsemefr.lis-lab.fr/parseme-st-guidelines/1.3/?page=040_Annotation_process_-_decision_tree",
          "excerpts": [
            "The decision tree below indicates the order in which tests should be applied in step 3. The decision trees are a useful summary to consult ..."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "by ML McHugh · 2012 · Cited by 21858 — As a general heuristic, sample sizes should not consist of less than 30 comparisons. Sample sizes of 1,000 or more are mathematically most likely to produce ...",
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            " However, this interpretation allows for very little agreement among raters to be described as “substantial”.",
            "For percent agreement, 61% agreement can immediately be seen as problematic.",
            "Almost 40% of the data in the dataset represent faulty data.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "The core of the fine-grained field is a data-quality governance framework built around acceptance sampling and QA checks. The most supportive material explicitly outlines how sampling plans are defined and evaluated in data contexts: acceptance quality level (AQL) serves as the worst tolerable defect level for a batch, and operating characteristics (OC) curves are used to balance producer and consumer risk when auditing a batch. This provides the statistical backbone for aiming at a high pass rate (e.g., 95%) with a quantifiable confidence, and it also illustrates how sample sizes are determined for zero-defect plans (c=0) versus conventional Z1.4 plans. Phrasings like \"Acceptance Quality Limit (AQL) sampling based on standards like ANSI/ASQ Z1.4 and ISO 2859-1 to audit data quality on entire batches\" and \"Operating Characteristic (OC) curves, which balance Producer's Risk and Consumer's Risk\" map directly to the field value's described framework and governance style, including how a lot of items might be inspected and judged for conformity. The references that discuss \"zero acceptance (c=0) plans\" and sample sizes (e.g., \"a lot of 1300 items, sampling 18 with 0 defects\") directly support the concrete mechanics of the target QA threshold and the tight tolerance implied by a 95% pass rate. The existence of explicit guidelines for when to apply Z1.4 versus c=0 plans, and the explicit articulation of sample sizes, supports the field's emphasis on formalized sampling strategies and explicit acceptance criteria. The OC curve discussions, and the portrayal of producer and consumer risk, align with the field's emphasis on balancing risk and reliability in batch-level QA. The inclusion of a human-in-the-loop component (HITL) and adjudication, as well as references to continuous improvement and adjudication for outcome reliability, aligns with the field's human-review and iterative improvement stance, indicating the governance and escalation steps that ensure adjudication decisions are binding and improved over time. The automated validation tooling excerpts—cross-field consistency checks, hash-based originality checks, and validation frameworks—support the field's automated validation layer that ensures completeness and logical consistency across fields (e.g., ensuring a data row cannot simultaneously be \"insightful content\" and \"simple joke\"). Finally, the converging set of excerpts touching on both statistical sampling and HITL/adjudication, plus automation, support a cohesive QA framework described in the field value.",
      "confidence": "high"
    },
    {
      "field": "discovered_data_sources",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        },
        {
          "title": "Reddit - r/indiehackers discussion by erik-grielenberger",
          "url": "http://reddit.com/r/indiehackers/comments/1m2ugls/i_reverseengineered_500_viral_tweets_heres",
          "excerpts": [
            "1. Lead with one outcome. [r/indiehackers - I reverse‑engineered 500+ VIRAL TWEETS - Here’s everything I've learned. Steal it ",
            "TL;DR**: Hook hard. Prove fast. Ask once. Media every two tweets. Follow this blueprint and turn that viral ceiling into your floor. Share if it help",
            "**4. Six hook patterns you can copy today:**",
            "**5. Pick a Narrative Skeleton (lock one). **",
            "**6. Media matters:** Drop a data card, GIF or 5‑sec demo every two or three tweets. Alternating text and media increases retweets by 50%. Add a three‑word headline and ALT text for each visual.",
            "5. Pick a Narrative Skeleton (lock one).",
            "4. Six hook patterns you can copy today:",
            "Copy my data-backed AI Templates: bettrprompts.com/category/viral-social-media",
            "*Hi, I’m Erik** - massive data nerd. I’ve primarily created these datasets because I want to increase my social media reach and understand how some posts go viral and most of them don",
            "This data analysis shows you how only **39 Threads** reached **118M+ impressions** and **450k+ likes** *(save for later)*."
          ]
        },
        {
          "title": "The anatomy of a viral tweet: The \"rehashing old news\" variant",
          "url": "https://weaponizedspaces.substack.com/p/the-anatomy-of-a-viral-tweet-the",
          "excerpts": [
            "A tweet that is said to “go viral” is generally one that has garnered a certain number of interactions within a specified time frame."
          ]
        }
      ],
      "reasoning": "The finegrained field value specifies multiple data sources with concrete references to The Tweets of Wisdom dataset (a Kaggle/GitHub resource with self-help/wise tweets compiled from numerous authors), a Kaggle dataset for viral tweets (Twitter Classification of Viral Tweets), another Kaggle dataset that includes tweets with like/share counts, and the awesome-twitter-data GitHub repository as a directory of Twitter datasets. The most directly supportive excerpts are those that explicitly name The Tweets of Wisdom dataset and describe its provenance, platform (Kaggle/GitHub), and licensing (CC0), as well as the presence of tweet-level metadata (author, handle, tweet_content, likes, retweets). Additional excerpts that reference The Tweets of Wisdom in various forms (as a dataset on Kaggle or GitHub, including its description and availability) further corroborate the field value and enrich metadata such as license and data coverage. Datasets named in these excerpts provide concrete evidence for populating Core fields like Tweet_Text, Likes_Count, Retweets_Count, Author_Handle, and License, which aligns with the field value's emphasis on availability, provenance, and licensing. These excerpts collectively establish the existence and scope of the sources listed in the finegrained field, enabling the construction of a structured, source-focused portion of the final deliverable. The ordering reflects direct name matches first (The Tweets of Wisdom, other named Kaggle datasets, and awesome-twitter-data) and then related dataset mentions that still align with the same data-source concept (Kaggle datasets and GitHub lists).",
      "confidence": "high"
    },
    {
      "field": "originality_verification_protocol.manual_review_guideline",
      "citations": [
        {
          "title": "Plagiarism in a submitted manuscript - COPE Flowchart",
          "url": "https://publicationethics.org/guidance/flowchart/plagiarism-submitted-manuscript",
          "excerpts": [
            "\n\nThe journal should ask the reviewer for full documentary evidence and investigate the concerns. Authors should be contacted where there is clear plagiarism or copying, explaining the journal’s process and next steps. The author's institution should be informed, where necessary.",
            "A reviewer suspects plagiarism in a submitted manuscript. This flowchart provides a step by step process to help editors handle this problem. [Plagiarism](/guidance?f%5B0%5D=topics%3A25)",
            "If the author has copied from their own work, refer to the flowchart redundant (duplicate) publication in a submitted manuscript."
          ]
        },
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "This report enables editors to assess the originality of the work before they publish it, providing confidence for publishers and authors, and evidence of trust for readers.",
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism.",
            "Similarity Check allows editors to upload a paper, and instantly produces a report highlighting potential matches and indicating if and how the paper overlaps with other work."
          ]
        },
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris",
            "The output of the plagiarism detection module (matching sections of text) can be sent as input to the paraphrase type identification module."
          ]
        },
        {
          "title": "Provenance Data in Social Media",
          "url": "https://www.odbms.org/wp-content/uploads/2014/03/Provenance-Data-in-Social-Media.pdf",
          "excerpts": [
            "Provenance data associated\n with a social media statement can help dispel rumors, clarify opinions, and conﬁrm facts."
          ]
        }
      ],
      "reasoning": "The most directly supporting content describes formal processes for handling suspected plagiarism using COPE flowcharts and official guidance. For instance, passages explicitly state that editors should follow a step-by-step COPE flowchart and communicate with authors, outlining next steps and involving the author's institution when plagiarism is suspected. This aligns with a structured manual review protocol that governs borderline or flagged cases. Additional excerpts describe the role of Similarity Check tooling and the importance of assessing originality and providing trust evidence to readers, which complements the manual review framework by outlining the technologies and standards that inform the initial screening and evidence-gathering phase. Other excerpts touch on related plagiarism-detection approaches and outputs, which support the broader workflow of identifying and addressing potential plagiarism through documented procedures. Together, these excerpts establish a cohesive workflow: detect potential plagiarism, consult COPE-guided flowcharts, request full documentation and evidence from authors, inform relevant parties when necessary, and rely on established best practices to reach a final determination.",
      "confidence": "high"
    },
    {
      "field": "originality_verification_protocol.stylometry_application",
      "citations": [
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris"
          ]
        },
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism.",
            "Similarity Check allows editors to upload a paper, and instantly produces a report highlighting potential matches and indicating if and how the paper overlaps with other work.",
            "This report enables editors to assess the originality of the work before they publish it, providing confidence for publishers and authors, and evidence of trust for readers."
          ]
        },
        {
          "title": "Implementation of Winnowing Algorithm with Dictionary ...",
          "url": "https://thesai.org/Downloads/Volume9No5/Paper_23-Implementation_of_Winnowing_Algorithm.pdf",
          "excerpts": [
            "Detection of plagiarism in this study will use a winnowing algorithm that has a function to check every character in two samples by hashing method that can ..."
          ]
        },
        {
          "title": "Provenance Data in Social Media",
          "url": "https://www.odbms.org/wp-content/uploads/2014/03/Provenance-Data-in-Social-Media.pdf",
          "excerpts": [
            "Provenance data associated\n with a social media statement can help dispel rumors, clarify opinions, and conﬁrm facts."
          ]
        }
      ],
      "reasoning": "The fine-grained field value concerns stylometry-based authorship verification, which relies on analyzing an author's unique writing style through features like character n-grams and other stylistic markers, as a means to confirm originality and authorship. Excerpts that discuss detection of textual similarity and plagiarism provide the closest support for methods or concepts that could underpin stylometric verification. Specifically, content that outlines the use of character n-grams as evidence in plagiarism detection directly echoes the idea of using fine-grained textual features to establish authorial style. Background on similarity checks and plagiarism detection tools further supports the notion of verifying originality by comparing texts to known works. References describing systems like Similarity Check or iThenticate show practical approaches to measuring overlap and authorial consistency, which align with the broader goal of authorship verification. Additional excerpts discussing provenance data in social media offer contextual relevance for how veracity and originality might be tracked in a digital, publish-or-perish ecosystem, even though they are not about stylometric features per se. Collectively, these excerpts provide a scaffold of techniques and considerations (feature-based textual analysis, n-gram usage, and similarity-based verification) that underpin stylometry-informed authorship verification, even if they do not state stylometry explicitly. ",
      "confidence": "medium"
    },
    {
      "field": "author_description_extraction_process.pre_processing_steps",
      "citations": [
        {
          "title": "Twitter(X) Profile Bio ICP Classifier - Bio Keywords Extractor",
          "url": "https://apify.com/lead.gen.labs/twitter-x-profile-bio-icp-classifier---bio-keywords-extractor",
          "excerpts": [
            "Classify Twitter (X) profiles by using bio keyword extraction. This tool scrapes public Twitter bios and intelligently extracts relevant keywords to help you ...See more"
          ]
        },
        {
          "title": "Bio To Schema: Why And How I Built My Custom GPT",
          "url": "https://www.lidia-infante.com/post/bio-to-schema",
          "excerpts": [
            "Jan 14, 2024 — The aim of Bio To Schema is to empower you to understand how to turn author bios into schema markup and help you prove the value of it to leadership."
          ]
        },
        {
          "title": "Social Media best practices: Twitter",
          "url": "https://maddenmedia.com/social-media-best-practices-twitter/",
          "excerpts": [
            "Apr 17, 2018 — Clean up your bio. Does your bio reflect who you are as a DMO? It should. The bio is one of the first things a follower will see on your page."
          ]
        }
      ],
      "reasoning": "The target field value concerns initial preprocessing steps applied to raw author bios to improve extraction accuracy, such as stripping extraneous content and preparing text for analysis. An excerpt describing a tool that classifies Twitter bios by extracting relevant keywords directly supports the idea of keyword-focused preprocessing and scraping bios to enable structured extraction. Another excerpt notes the importance of cleaning or refining a bio so it accurately reflects the author, which aligns with the preprocessing goal of preparing bios for reliable analysis. A third excerpt discusses converting bio content into a schema representation, which implies normalization and structured handling of bio data as part of preprocessing before downstream tasks like insight extraction. Together, these excerpts corroborate the concept of initial preprocessing steps (cleanup, keyword-focused parsing, and schema-oriented normalization) applied to author bios to improve extraction accuracy.",
      "confidence": "medium"
    },
    {
      "field": "originality_verification_protocol.semantic_similarity_threshold",
      "citations": [
        {
          "title": "Paraphrase Mining — Sentence Transformers documentation",
          "url": "https://sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ..."
          ]
        },
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris",
            "The output of the plagiarism detection module (matching sections of text) can be sent as input to the paraphrase type identification module."
          ]
        },
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism.",
            "This report enables editors to assess the originality of the work before they publish it, providing confidence for publishers and authors, and evidence of trust for readers."
          ]
        },
        {
          "title": "Implementation of Winnowing Algorithm with Dictionary ...",
          "url": "https://thesai.org/Downloads/Volume9No5/Paper_23-Implementation_of_Winnowing_Algorithm.pdf",
          "excerpts": [
            "Detection of plagiarism in this study will use a winnowing algorithm that has a function to check every character in two samples by hashing method that can ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant content describes explicit methods for identifying paraphrase and similarity using modern embedding-based approaches. The paraphrase mining entry from the Sentence Transformers documentation explicitly notes that it compares sentences pairwise and returns pairs with the highest cosine similarity, which directly aligns with using a cosine similarity threshold to flag potential paraphrasing. Closely related are discussions of plagiarism detection and paraphrase identification, which address the same underlying problem (detecting similarity/duplication) and reference techniques like n-gram similarity and alignment algorithms, providing context for how thresholds impact detection. Crossref Similarity Check entries describe a service designed to detect overlap and ensure originality, illustrating how publishers operationalize similarity thresholds in practice. Additional entries further describe the role of similarity checks in assessing originality (e.g., iThenticate-powered workflows) and general provenance considerations in social content, which can influence how thresholds are interpreted or augmented with metadata. Collectively, these excerpts support the concept of semantic similarity-based thresholding (e.g., 0.85 cosine similarity) as a starting point for flagging potential paraphrasing, while also acknowledging that thresholds require tuning and contextual adaptation.",
      "confidence": "high"
    },
    {
      "field": "author_description_extraction_process.normalization_taxonomies",
      "citations": [
        {
          "title": "Job Titles classification",
          "url": "https://kb.lightcast.io/en/articles/7907529-job-titles-classification",
          "excerpts": [
            "Lightcast classifies raw job titles to over 70,000 standardized titles using a machine learning model that normalizes and matches titles based on similarity."
          ]
        },
        {
          "title": "Why You Don't Want Job Title Normalization & ...",
          "url": "https://www.openprisetech.com/blog/data-normalization-basics-why-you-dont-really-want-to-normalize-job-title/",
          "excerpts": [
            "Jan 9, 2019 — Openprise maintains data sources of over 3,600 job title keywords sorted into job functions and of over 1,300 job title keywords in multiple ..."
          ]
        }
      ],
      "reasoning": "The most directly supportive excerpt discusses how job titles are classified into standardized forms using a model that normalizes and matches titles based on similarity. This aligns with the idea of mapping extracted titles to standardized terms within an authoritative taxonomy, illustrating the core mechanism of standardization described in the field value. A second excerpt addresses data normalization concepts in the context of job titles, arguing about normalization basics and potential trade-offs, which provides context for why normalization to standardized taxonomies is considered important, though it does not explicitly name the specific taxonomies. Taken together, these excerpts support the overarching practice of normalizing titles against standardized occupational dictionaries, with the first excerpt illustrating the concrete application and the second offering foundational rationale.",
      "confidence": "medium"
    },
    {
      "field": "originality_verification_protocol.reference_corpora",
      "citations": [
        {
          "title": "Provenance Data in Social Media",
          "url": "https://www.odbms.org/wp-content/uploads/2014/03/Provenance-Data-in-Social-Media.pdf",
          "excerpts": [
            "Provenance data associated\n with a social media statement can help dispel rumors, clarify opinions, and conﬁrm facts."
          ]
        },
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "This report enables editors to assess the originality of the work before they publish it, providing confidence for publishers and authors, and evidence of trust for readers.",
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism.",
            "Similarity Check allows editors to upload a paper, and instantly produces a report highlighting potential matches and indicating if and how the paper overlaps with other work."
          ]
        },
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris",
            "The output of the plagiarism detection module (matching sections of text) can be sent as input to the paraphrase type identification module."
          ]
        },
        {
          "title": "Plagiarism in a submitted manuscript - COPE Flowchart",
          "url": "https://publicationethics.org/guidance/flowchart/plagiarism-submitted-manuscript",
          "excerpts": [
            "\n\nThe journal should ask the reviewer for full documentary evidence and investigate the concerns. Authors should be contacted where there is clear plagiarism or copying, explaining the journal’s process and next steps. The author's institution should be informed, where necessary.",
            "A reviewer suspects plagiarism in a submitted manuscript. This flowchart provides a step by step process to help editors handle this problem. [Plagiarism](/guidance?f%5B0%5D=topics%3A25)",
            "If the author has copied from their own work, refer to the flowchart redundant (duplicate) publication in a submitted manuscript."
          ]
        },
        {
          "title": "Paraphrase Mining — Sentence Transformers documentation",
          "url": "https://sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ..."
          ]
        },
        {
          "title": "Implementation of Winnowing Algorithm with Dictionary ...",
          "url": "https://thesai.org/Downloads/Volume9No5/Paper_23-Implementation_of_Winnowing_Algorithm.pdf",
          "excerpts": [
            "Detection of plagiarism in this study will use a winnowing algorithm that has a function to check every character in two samples by hashing method that can ..."
          ]
        },
        {
          "title": "Authorship Verification Using the Impostors Method - CEUR-WS.org",
          "url": "https://ceur-ws.org/Vol-1179/CLEF2013wn-PAN-Seidman2013.pdf",
          "excerpts": [
            "The approach is based on comparing the similarity between the given documents and a number of external (impostor) documents, so that documents can be classified as having been written by the same author, if they are shown to be more similar to each other than to the impostors, in a number of trials."
          ]
        }
      ],
      "reasoning": "The claim describes a rigorous originality verification protocol that cross-checks tweets against large, authoritative text corpora to identify quotes and public-domain text. The strongest alignment comes from the notion that provenance data associated with a statement can help dispel rumors, clarify opinions, and confirm facts, which directly supports the idea of tracing text to authoritative sources. Additionally, mentions of a similarity-check workflow used by editors to detect plagiarism and to assess originality provide concrete mechanisms for evaluating whether a tweet's content is novel or drawn from existing sources. References to plagiarism-detection approaches (character n-gram similarity and alignment-based methods) and to systems that highlight overlaps with other works illustrate how automated checks can identify non-original content. Descriptions of workflows to handle suspected plagiarism and to ensure authorship integrity further ground the process in an established verification protocol. Together, these excerpts support the central idea of comparing tweets against large reference corpora to identify known quotes and public-domain text, using provenance data and formal similarity checks as core components of the originality verification workflow.",
      "confidence": "medium"
    },
    {
      "field": "originality_verification_protocol.detection_methods",
      "citations": [
        {
          "title": "Paraphrase Mining — Sentence Transformers documentation",
          "url": "https://sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ..."
          ]
        },
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris"
          ]
        },
        {
          "title": "Implementation of Winnowing Algorithm with Dictionary ...",
          "url": "https://thesai.org/Downloads/Volume9No5/Paper_23-Implementation_of_Winnowing_Algorithm.pdf",
          "excerpts": [
            "Detection of plagiarism in this study will use a winnowing algorithm that has a function to check every character in two samples by hashing method that can ..."
          ]
        },
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism."
          ]
        }
      ],
      "reasoning": "The target field describes a hybrid detection strategy using both lexical techniques (Shingling, MinHashing, or similar n-gram based methods) to identify structural or near-duplicate text, and semantic methods (Sentence-BERT embeddings with cosine similarity) to detect paraphrasing where meaning is preserved but wording differs. An excerpt about Paraphrase Mining with Sentence Transformers directly references sentence-transformer based approaches and cosine similarity, which aligns with the semantic portion of the described method. Another excerpt discusses plagiarism detection approaches that rely on character and word n-gram similarity and alignment algorithms, which maps to the lexical/structural analysis portion. A separate excerpt details a winnowing algorithm that hashes content to detect similarity, representing a practical hashing/fingerprinting technique within lexical detection. Additionally, a broader excerpt about a similarity-check service ties into the workflow of verifying originality using automated similarity reports, supporting the overall methodology. Taken together, these excerpts substantiate the combined lexical and semantic detection framework described in the field value.",
      "confidence": "high"
    },
    {
      "field": "author_description_extraction_process.pipeline_overview",
      "citations": [
        {
          "title": "Twitter(X) Profile Bio ICP Classifier - Bio Keywords Extractor",
          "url": "https://apify.com/lead.gen.labs/twitter-x-profile-bio-icp-classifier---bio-keywords-extractor",
          "excerpts": [
            "Classify Twitter (X) profiles by using bio keyword extraction. This tool scrapes public Twitter bios and intelligently extracts relevant keywords to help you ...See more"
          ]
        },
        {
          "title": "Bio To Schema: Why And How I Built My Custom GPT",
          "url": "https://www.lidia-infante.com/post/bio-to-schema",
          "excerpts": [
            "Jan 14, 2024 — The aim of Bio To Schema is to empower you to understand how to turn author bios into schema markup and help you prove the value of it to leadership."
          ]
        },
        {
          "title": "Social Media best practices: Twitter",
          "url": "https://maddenmedia.com/social-media-best-practices-twitter/",
          "excerpts": [
            "Apr 17, 2018 — Clean up your bio. Does your bio reflect who you are as a DMO? It should. The bio is one of the first things a follower will see on your page."
          ]
        }
      ],
      "reasoning": "The most directly relevant content describes a system that extracts keywords from Twitter bios to classify or understand bio content, which aligns with the core idea of a pre-processing step followed by role extraction from bios. It emphasizes intelligent extraction of keywords from public bios, which supports the notion of extracting structured information (roles/descriptions) from unstructured bio text. The next piece discusses transforming author bios into schema, illustrating normalization and structured representation of bios, which corresponds to the pipeline's normalization and post-processing stages, as well as the broader goal of producing standardized outputs from bios. The third piece emphasizes bio cleanliness and alignment of the bio with identity or role, which relates to preprocessing quality and ensuring bios reflect the author's background, touching on QA-like considerations for bios quality before extraction and downstream processing. Collectively, these excerpts substantiate the general workflow of bios pre-processing, extraction of descriptive roles, normalization/standardization, and quality-oriented post-processing, which map to the seven-stage pipeline described in the fine-grained field value.",
      "confidence": "high"
    },
    {
      "field": "diversity_and_bias_mitigation_plan.niche_overrepresentation_prevention",
      "citations": [
        {
          "title": "Comparing methods for creating a national random sample of twitter ...",
          "url": "https://link.springer.com/article/10.1007/s13278-024-01327-5",
          "excerpts": [
            "The results show that the *bounding box* (BB) and *location query* (Loc) sampling methods produce a significantly higher number of tweets compared to the *language query* (Lang) and *1% steam* methods. Among the four Twitter sampling methods, BB and Loc methods produce more than 18 million tweets, whereas the Lang and 1% stream methods generate 4.5 million and 174,000 tweets, respectively, withi",
            "A common aim when working with Twitter data is the construction of a random sample of users from a given country.",
            "The *1% Stream* method achieves the minimum error, compared to the other three methods, in the prediction task of estimating the population of the US from Twitter users.",
            "To address this fact, researchers often try to create a random sample of Twitter users from a country. However, at least four widely-used sampling methods exist in the literature, and the extent to which their outputs are similar or different has not been explored systematically so far.",
            "r results highlight the *1% Stream* Twitter sampling method, which exhibits different characteristics compared to the other three sampling methods and fits as the top candidate in most use cases."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7931947/",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        }
      ],
      "reasoning": "The target field value outlines a strategy to avoid overrepresentation of any single author and to increase diversity by removing semantically similar content and by balancing sampling across time windows. Evidence in the excerpts highlights key related concepts: one excerpt discusses methodological pitfalls and ethical boundaries that are often overlooked when handling social data, underscoring the need to guard against biased or non-representative samples. Several excerpts describe different Twitter data sampling methods and their performance, illustrating practical approaches to construct a broader, more representative dataset by comparing methods and acknowledging variability across sampling techniques (e.g., one method yielding more tweets than others, and a discussion of different sampling strategies). Taken together, these excerpts support the idea that bias-aware data collection requires deliberate sampling techniques and de-duplication considerations to prevent dominance by a subset of sources and topics, aligning with the planned procedures such as per-author constraints, near-duplicate detection, and balanced time-window sampling. The connection is that recognizing biases and employing robust sampling/duplication controls are foundational steps toward achieving a diverse, well-rounded corpus as described in the finegrained field value.",
      "confidence": "medium"
    },
    {
      "field": "key_challenges_and_risks",
      "citations": [
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data.",
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ..."
          ]
        },
        {
          "title": "Restricted Use Cases for X (formerly Twitter) Content",
          "url": "https://help.meltwater.com/en/articles/5653790-restricted-use-cases-for-x-formerly-twitter-content",
          "excerpts": [
            "Meltwater has licensed X data since 2010, utilizing multiple APIs to capture and display data across multiple areas of our product."
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "X updates its terms to ban crawling and scraping",
          "url": "https://techcrunch.com/2023/09/08/x-updates-its-terms-to-ban-crawling-and-scraping/",
          "excerpts": [
            "Elon Musk-owned X, formerly Twitter, has updated its terms of service to prohibit scraping and crawling — likely to fend off any AI models ..."
          ]
        },
        {
          "title": "Developer Agreement and Policy - Twitter Developers - X",
          "url": "https://developer.x.com/en/developer-terms/agreement-and-policy",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs). Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research."
          ]
        },
        {
          "title": "The Moonlight Review: Comparing Methods for Creating a National Random Sample of Twitter Users",
          "url": "https://www.themoonlight.io/en/review/comparing-methods-for-creating-a-national-random-sample-of-twitter-users",
          "excerpts": [
            "This paper compares four common methods for creating a random sample of Twitter users in the United States: the 1% Stream, Bounding Box, Location Query, and Language Query methods."
          ]
        },
        {
          "title": "Annotation and API considerations for research on viral content (PMC 2018)",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6371268/",
          "excerpts": [
            " Annotation speed varied across different users (40.47 – 92.22 words per minute) leading to wide ranges in the quantity of sentences annotated (435 -920) and ..."
          ]
        },
        {
          "title": "Prodigy Metrics",
          "url": "https://prodi.gy/docs/metrics",
          "excerpts": [
            "All\nof these metrics share the general form in that they take into account both the\nobserved agreement ( pₐ ) - the proportion of instances where annotators agree\nand the expected agreement ( pₑ ) the agreement that would occur by chance:"
          ]
        },
        {
          "title": "Inter-Annotator Agreement – Innovatiana",
          "url": "https://www.innovatiana.com/en/post/inter-annotator-agreement",
          "excerpts": [
            "By regularly checking the Inter Annotator Agreement, it is possible to identify trends and recurring problems in the evaluations, in the data sets under construction. This makes it possible to optimize the annotation process, whether it is images or [**videos**](https://en.innovatiana.com/post/video-annotation-common-mistakes) in particular, by implementing corrective measures over time to improve the reliability of dataset evaluations over the long term.",
            "The Cohen's Kappa coefficient is a statistical measure that assesses the agreement between two annotators corrected by the possibility of random agreement. It is calculated by comparing the observed frequency of agreement between annotators to the expected frequency of agreement by chance.",
            "It is calculated by comparing the observed frequency of agreement between annotators to the expected frequency of agreement by chance."
          ]
        }
      ],
      "reasoning": "- The cited excerpts give concrete cost figures for API access levels: the Enterprise tier pricing and the Pro tier pricing, illustrating a high and volatile cost environment that aligns with the first risk. One excerpt notes that Enterprise pricing starts at tens of thousands per month and another cites a fixed monthly price for a mid-tier tier, underscoring scale barriers and budget uncertainty. Additionally, a source describes a revenue-sharing approach being contemplated for enterprise access, highlighting policy-driven financial risk and uncertainty which directly supports the cost- and policy-Volatility component. The combination of explicit price points and mention of a revenue-share consideration concretely anchors the \"high and volatile cost\" risk. The statements about policy shifts and pricing changes further reinforce the volatility and uncertainty around access, supporting the same risk dimension with real-world precedent. The parts describing full-archive access and associated costs map onto the Risk: High costs to obtain the data necessary for the project's Phase 1-Phase 2 workflow. Quotes indicating that access requires a $5,000 per month Pro tier or a $42,000+ per month Enterprise tier provide concrete anchors for budgeting the data-collection effort and for risk planning related to cost overruns or funding gaps. The references describing potential revenue-sharing models add a structural financial risk that could affect long-term budgeting and vendor negotiations, strengthening the claim of volatility and uncertainty in data access economics. The excerpts discussing policy changes, including shifts in terms and permissibility of data redistribution, bolster the risk of sudden, costly shifts in what is allowed, further elevating the budgeting and planning risk. - The excerpts that address the annotation/content-insight risk describe the challenge of defining insightful content, the need for robust annotation guidelines, and the potential inter-annotator reliability issues. They point to substantial investment in human labeling, guideline design, and QA processes, which directly supports the second risk by illustrating the complexity and resource intensity of extracting truly insightful content from social media, beyond simple keyword searches. The explicit emphasis on inter-annotator agreement and the need for high-quality, nuanced labeling demonstrates why the project might incur significant time and cost to achieve acceptable reliability. - For data-scarcity risk, the excerpts discuss sampling challenges and the difficulty of collecting large-scale, high-quality, truly viral examples, including notes on data access limits, strict eligibility criteria, and the need to sample tens or hundreds of millions of tweets to reach a handful of qualifying entries. This aligns with the third risk dimension by illustrating the practical difficulty of assembling a dataset of 5,000 validated items given stringent viral thresholds and underrating source criteria. The references to sampling methods, data-access limits, and the need for careful plan to obtain adequate samples strengthen the claim that data scarcity is a real and continuing risk for the project. - The combination of direct price data, policy-change references, annotation-reliability considerations, and sampling/data-access constraints provides multi-faceted support for the stated fine-grained field value. Some excerpts provide explicit cost figures and policy references, making them highly relevant to the first risk; others discuss annotation quality and reliability, which tightly supports the second risk; and several address data scarcity and sampling limitations, which underpin the third risk. Overall, the evidence coherently supports the three-part risk narrative, with pricing/policy details being the most compelling anchors, followed by methodological annotation concerns, and then data-scarcity/sampling constraints.",
      "confidence": "high"
    },
    {
      "field": "project_timeline_and_budget",
      "citations": [
        {
          "title": "X Enterprise API Pricing and Data Access",
          "url": "https://developer.x.com/en/products/x-api/enterprise/enterprise-api-interest-form",
          "excerpts": [
            "For self service access, please explore our Basic or Pro API access tiers. ... Enterprise API pricing starts at $42,000 / Month, based on usage and needs.See more"
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data.",
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ..."
          ]
        },
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Basic\n\n$200 /month\n\nFor hobbyists and prototypes\n\n#### What's included:\n\n1 project\n\n2 apps / project\n\n15,000 posts / month (reads)\n\n50,000 posts / month (writes)\n\nFull v2 endpoints access](https://developer.x.com/en/portal/products)",
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)",
            "X API v2 gives you programmatic access to X's global conversation—posts, users, spaces, DMs, lists, trends, media, and more. Build apps, analyze data, and ...",
            "X API v2 gives you programmatic access to X's global conversation—posts, users, spaces, DMs, lists, trends, media, and more. Build apps, analyze data, ..."
          ]
        },
        {
          "title": "X API Pricing - Social Media Today (June 2025)",
          "url": "https://www.socialmediatoday.com/news/x-formerly-twitter-reforms-api-pricing-revenue-share-model/750025/",
          "excerpts": [
            "X recently began sending out emails to paid subscribers of its Enterprise API plans, which start at $42,000 per month, informing them of the upcoming change.",
            "The new API pricing scheme is scheduled to go into effect on July 1.",
            "X has not yet shared final details about the change, such as exactly what percentage the revenue share model will be, with its customers."
          ]
        },
        {
          "title": "awesome-twitter-data",
          "url": "https://github.com/shaypal5/awesome-twitter-data",
          "excerpts": [
            "\nA list of Twitter datasets and related resources, released under CC0 . If you have a resource to add to the list, feel free to open a pull request, or email me at shay.palachy@gmail.com . The license, when known, is given in {curly brackets}. Dataset size is given in [square brackets] when available. 1   Twitter Datasets"
          ]
        }
      ],
      "reasoning": "The budget portion of the finegrained field centers on major cost drivers for data access. Excerpts that state enterprise API pricing starts at a concrete amount (a fixed monthly cost) map directly to the primary_cost_driver in the field value. For example, enterprise pricing begins at $42,000 per month, which is a central budget line item in the plan. This is echoed by additional outlets discussing the same price point, reinforcing the budgetary impact of enterprise access. Other excerpts enumerate defined pricing tiers for the API (e.g., Basic at a fixed monthly rate, Pro tier at a much higher monthly price) which aligns with the field's need to capture a breakdown of cost components under budget estimation and cost drivers. The presence of multiple sources citing the same enterprise price and detailing tiered pricing supports a coherent budget narrative (high-level cost driver plus tiered pricing). However, the finegrained field also includes a projected timeline component (estimated_timeline) and a phase-based plan. The excerpts do not provide explicit week-by-week timelines (e.g., 8–12 weeks) or milestone gates, so exact temporal estimates in Phase 2–4 are not directly corroborated by the excerpts. They do, however, discuss data-access constraints and enterprise access implications that could drive project duration indirectly (through data acquisition speed, API rate limits, and access approvals). Based on the content, the strongest alignment is with the budget/cost aspects; timeline details are less directly evidenced, but pricing structures could influence planning and phasing decisions. Consequently, the evidence most strongly supports the budget-related elements of the field, with partial support for timeline implications inferred from access constraints and pricing dynamics. The connected excerpts collectively substantiate the key budget items (enterprise pricing and tiers) but show weaker, indirect ties to the exact Phase timeline components in the field value.",
      "confidence": "medium"
    },
    {
      "field": "diversity_and_bias_mitigation_plan.language_and_region_strategy",
      "citations": [
        {
          "title": "Comparing methods for creating a national random sample of twitter ...",
          "url": "https://link.springer.com/article/10.1007/s13278-024-01327-5",
          "excerpts": [
            "The results show that the *bounding box* (BB) and *location query* (Loc) sampling methods produce a significantly higher number of tweets compared to the *language query* (Lang) and *1% steam* methods. Among the four Twitter sampling methods, BB and Loc methods produce more than 18 million tweets, whereas the Lang and 1% stream methods generate 4.5 million and 174,000 tweets, respectively, withi",
            "r results highlight the *1% Stream* Twitter sampling method, which exhibits different characteristics compared to the other three sampling methods and fits as the top candidate in most use cases.",
            "To address this fact, researchers often try to create a random sample of Twitter users from a country. However, at least four widely-used sampling methods exist in the literature, and the extent to which their outputs are similar or different has not been explored systematically so far.",
            "A common aim when working with Twitter data is the construction of a random sample of users from a given country."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7931947/",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Optimal Strategies to Perform Multilingual Analysis of ...",
          "url": "https://arxiv.org/html/2311.14727v2",
          "excerpts": [
            "Jul 3, 2025 — Specifically, we investigate which NLP techniques are best to keep manual data annotation to a minimum and to avoid cumbersome and costly rule- ..."
          ]
        },
        {
          "title": "SoK: Measuring Blockchain Decentralization",
          "url": "https://arxiv.org/html/2501.18279v1",
          "excerpts": [
            "Jan 30, 2025 — A closely related metric to Gini is the Theil index, which intends to capture the lack of diversity, or the redundancy, in a population [14] ."
          ]
        },
        {
          "title": "Best Practices for Managing Data Annotation Projects",
          "url": "https://assets.bbhub.io/company/sites/40/2020/09/Annotation-Best-Practices-091020-FINAL.pdf",
          "excerpts": [
            "by T Tseng · 2020 · Cited by 20 — • Annotation guidelines should include both tool and annotation instructions. • Annotation guidelines should also illustrate each label with examples.See more",
            "by T Tseng · 2020 · Cited by 20 — Annotation is the labeling of data by human effort. Annotation is critical to modern machine learning, and. Bloomberg has developed years of experience of ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly address Twitter data sampling methods and geographic targeting. One excerpt explains that sampling approaches like the bounding box and location query yield more tweets and thus are central to constructing geographicly aware samples, which aligns with the plan's emphasis on bounding box coordinates and region-aware sampling. Another excerpt discusses how various sampling methods yield different output scales and characteristics, which supports the need to choose sampling approaches that mitigate bias and improve representativeness. Additional excerpts highlight language-related sampling methods and multilingual analysis, which correspond to the plan's aim to diversify language sources and incorporate multilingual demographic inference. Together, these excerpts substantiate the strategy of targeted language sampling across diverse languages, post-stratification considerations, and prioritizing user-level sampling to broaden author diversity, alongside geographic targeting using bounding boxes. A supporting excerpt also address biases and ethical pitfalls in social data, which reinforces the necessity of bias mitigation and careful interpretation when sampling across languages and regions. Collectively, these sources corroborate the core components of the diverse language and region strategy by providing concrete methods (language sampling, bounding boxes, location-based queries) and cautionary context (biases, limitations) that align with the described plan.",
      "confidence": "medium"
    },
    {
      "field": "diversity_and_bias_mitigation_plan.documentation_requirement",
      "citations": [
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7931947/",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Comparing methods for creating a national random sample of twitter ...",
          "url": "https://link.springer.com/article/10.1007/s13278-024-01327-5",
          "excerpts": [
            "The results show that the *bounding box* (BB) and *location query* (Loc) sampling methods produce a significantly higher number of tweets compared to the *language query* (Lang) and *1% steam* methods. Among the four Twitter sampling methods, BB and Loc methods produce more than 18 million tweets, whereas the Lang and 1% stream methods generate 4.5 million and 174,000 tweets, respectively, withi",
            "A common aim when working with Twitter data is the construction of a random sample of users from a given country.",
            "The *1% Stream* method achieves the minimum error, compared to the other three methods, in the prediction task of estimating the population of the US from Twitter users.",
            "To address this fact, researchers often try to create a random sample of Twitter users from a country. However, at least four widely-used sampling methods exist in the literature, and the extent to which their outputs are similar or different has not been explored systematically so far.",
            "r results highlight the *1% Stream* Twitter sampling method, which exhibits different characteristics compared to the other three sampling methods and fits as the top candidate in most use cases."
          ]
        }
      ],
      "reasoning": "The fine-grained field value requires a comprehensive datasheet-like documentation of dataset motivation, composition, collection methodology, preprocessing steps, and, crucially, known biases and gaps (linguistic and regional imbalances). The most directly supportive content states that there are methodological limitations and ethical boundaries, and it highlights biases and the unintended consequences that researchers should disclose. This aligns with the core need to include biases, limitations, and data provenance in the documentation. Additionally, several excerpts discuss sampling methods in Twitter data and their impact on representativeness, which underscores the necessity of detailing how data was collected and which biases may arise from sampling choices. Together, these sources justify including sections on data provenance, sampling methodology, potential biases (linguistic, regional, demographic), and limitations in the final datasheet or data statement, as required by the field value. The other excerpts touching on annotation practices and best practices for data projects provide useful contextual support for documenting project methodology and guidelines, further reinforcing the importance of thorough, transparent documentation.",
      "confidence": "high"
    },
    {
      "field": "diversity_and_bias_mitigation_plan.author_diversity_strategy",
      "citations": [
        {
          "title": "Comparing methods for creating a national random sample of twitter ...",
          "url": "https://link.springer.com/article/10.1007/s13278-024-01327-5",
          "excerpts": [
            "The results show that the *bounding box* (BB) and *location query* (Loc) sampling methods produce a significantly higher number of tweets compared to the *language query* (Lang) and *1% steam* methods. Among the four Twitter sampling methods, BB and Loc methods produce more than 18 million tweets, whereas the Lang and 1% stream methods generate 4.5 million and 174,000 tweets, respectively, withi",
            "The *1% Stream* method achieves the minimum error, compared to the other three methods, in the prediction task of estimating the population of the US from Twitter users.",
            "r results highlight the *1% Stream* Twitter sampling method, which exhibits different characteristics compared to the other three sampling methods and fits as the top candidate in most use cases.",
            "A common aim when working with Twitter data is the construction of a random sample of users from a given country.",
            "To address this fact, researchers often try to create a random sample of Twitter users from a country. However, at least four widely-used sampling methods exist in the literature, and the extent to which their outputs are similar or different has not been explored systematically so far."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a sampling strategy to ensure diversity by constraining follower counts to include nano- and micro-influencers. The most directly relevant information in the excerpts concerns Twitter data sampling methods and random sampling approaches, which are foundational to building a representative set of authors. One excerpt discusses that sampling methods (e.g., bounding box and location query) can yield larger numbers of tweets, highlighting how different sampling schemes influence coverage and representativeness. This informs the idea of carefully choosing sampling methods to reach a diverse author pool without relying on high-follower accounts. Another excerpt notes that a common aim in Twitter data work is constructing a random sample of users from a country, which aligns with the goal of creating a diverse, follower-count-conscious dataset. Additional excerpts highlight that the 1% Stream method can minimize error relative to other methods in population estimation tasks, suggesting that small-sample strategies can still achieve broad coverage if chosen appropriately. Collectively, these excerpts support the notion that sampling design is crucial for coverage and diversity, although they do not explicitly validate a specific follower-count ceiling or target nano-/micro-influencers. The connection to the finegrained field value is therefore indirect and relies on leveraging sampling strategy insights to implement a follower-count-based stratification plan in practice, rather than citing explicit evidence for the exact ceiling or category thresholds.",
      "confidence": "low"
    },
    {
      "field": "author_description_extraction_process.ambiguity_resolution_rule",
      "citations": [
        {
          "title": "Twitter(X) Profile Bio ICP Classifier - Bio Keywords Extractor",
          "url": "https://apify.com/lead.gen.labs/twitter-x-profile-bio-icp-classifier---bio-keywords-extractor",
          "excerpts": [
            "Classify Twitter (X) profiles by using bio keyword extraction. This tool scrapes public Twitter bios and intelligently extracts relevant keywords to help you ...See more"
          ]
        },
        {
          "title": "Social Media best practices: Twitter",
          "url": "https://maddenmedia.com/social-media-best-practices-twitter/",
          "excerpts": [
            "Apr 17, 2018 — Clean up your bio. Does your bio reflect who you are as a DMO? It should. The bio is one of the first things a follower will see on your page."
          ]
        },
        {
          "title": "Bio To Schema: Why And How I Built My Custom GPT",
          "url": "https://www.lidia-infante.com/post/bio-to-schema",
          "excerpts": [
            "Jan 14, 2024 — The aim of Bio To Schema is to empower you to understand how to turn author bios into schema markup and help you prove the value of it to leadership."
          ]
        },
        {
          "title": "Welcome to LIWC-22",
          "url": "https://www.liwc.app/",
          "excerpts": [
            "LIWC is the gold standard in software for analyzing word use. It can be used to study a single individual, groups of people over time, or all of social media."
          ]
        },
        {
          "title": "Why You Don't Want Job Title Normalization & ...",
          "url": "https://www.openprisetech.com/blog/data-normalization-basics-why-you-dont-really-want-to-normalize-job-title/",
          "excerpts": [
            "Jan 9, 2019 — Openprise maintains data sources of over 3,600 job title keywords sorted into job functions and of over 1,300 job title keywords in multiple ..."
          ]
        },
        {
          "title": "Job Titles classification",
          "url": "https://kb.lightcast.io/en/articles/7907529-job-titles-classification",
          "excerpts": [
            "Lightcast classifies raw job titles to over 70,000 standardized titles using a machine learning model that normalizes and matches titles based on similarity."
          ]
        }
      ],
      "reasoning": "The core idea in the fine-grained value is to extract an author's description from a bio and to apply a heuristic that prioritizes the most relevant role when roles are ambiguous. A key supporting theme is the use of bio keyword extraction to classify profiles, which directly informs how to surface an author's description from a bios text. The notion of improving bios presentation and making bios explicit is echoed in guidance to \"Clean up your bio\" and ensure it reflects the author's identity, which supports the idea that bios should be clear and unambiguous. Extending this, turning author bios into a structured schema demonstrates a concrete method to formalize author descriptions in a machine-readable way, aligning with the idea of extracting and standardizing background information. Language-analysis tools that study word use in bios (LIWC) provide a methodological basis for parsing nuanced descriptions and identifying salient descriptors within a bio, which is essential for disambiguation when terms may be polysemous. The presence of resources on classifying bios-related attributes, including \"Job Titles classification,\" offers a parallel approach to categorize background information and supports using a structured taxonomy to resolve ambiguity. Together, these excerpts support a framework where bios are extracted, cleaned, analyzed for linguistic signals, and mapped into a schema with a prioritization rule for resolving ambiguous roles (current role first, then field, then notable affiliation).",
      "confidence": "medium"
    },
    {
      "field": "nlp_classifier_plan_for_insightful_content.labeling_strategy",
      "citations": [
        {
          "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
          "url": "https://arxiv.org/abs/2402.15613",
          "excerpts": [
            "Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications.",
            "When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration.",
            "We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data ...",
            "The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or updating it as newer versions get released.",
            "As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive."
          ]
        },
        {
          "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
          "url": "https://arxiv.org/html/2402.15613v1",
          "excerpts": [
            "The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or updating it as newer versions get released.",
            "As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive."
          ]
        },
        {
          "title": "3 ways to use Snorkel's Labeling Functions",
          "url": "https://snorkel.ai/blog/snorkel-labeling-functions-use-cases/",
          "excerpts": [
            "Jun 24, 2022 — Data scientists and subject matter experts can create labeling functions that capture and apply heuristics or patterns. Looking at a few ..."
          ]
        },
        {
          "title": "Weak supervision for pharmacovigilance and NLP",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8554513/",
          "excerpts": [
            "Weak Supervision utilizes noisy, limited, or imprecise sources to provide a supervision signal for labeling large amounts of training data in a supervised learning setting",
            "To decrease the labelling costs, researchers have been using weaker forms of supervision, such as heuristically generating training data with external knowledge bases, patterns/rules, or other classifiers.",
            "The assumption behind our work is that the large volume of training data which can be collected using an automated labeling process can compensate for the inaccuracy in the labels.",
            "there is a huge need to avoid reliance on manual annotation of small datasets and to move to automatic annotation on large datasets.",
            "This approach can help reduce the need for manual annotation which saves time and resources."
          ]
        }
      ],
      "reasoning": "The target field value centers on an NLP classification labeling strategy driven by pool-based Active Learning, with an emphasis on efficiency via precomputed feature representations and a lightweight classifier to select informative samples. Excerpts describing active learning in NLP, the use of pretrained representations within the active learning loop, and the goal of reducing computational burden by avoiding full retraining directly support this concept. Quoting or paraphrasing these excerpts, one sees explicit references to fine-tuning or leveraging pretrained embeddings in active learning contexts and to the idea that labeling should be driven by informativeness and diversity to improve annotation efficiency. Additional excerpts on labeling functions and weak supervision provide context about alternative labeling strategies and the broader labeling ecosystem, which helps delineate how pool-based AL differs from or complements weak supervision approaches. Together, these excerpts map onto the core idea of an efficient, sample-efficient NLP labeling workflow that relies on pool-based Active Learning and pretrained representations, as described in the finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "underrated_source_definition.celebrity_exclusion_rules",
      "citations": [
        {
          "title": "List of most-followed Twitter accounts",
          "url": "https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts",
          "excerpts": [
            "| 1 | @elonmusk | [Elon Musk](/wiki/Elon_Musk \"Elon Musk\") | 222\\.2 | Businessman, owner of X/Twitter, xAI, SpaceX and Former Senior Advisor to the President |  |",
            "| 2 | [@BarackObama](/wiki/Social_media_use_by_Barack_Obama \"Social media use by Barack Obama\") | [Barack Obama](/wiki/Barack_Obama \"Barack Obama\") | 130\\.3 | U.S. President (2009–2017) |  |",
            "| 9 | @taylorswift13 | [Taylor Swift](/wiki/Taylor_Swift \"Taylor Swift\") | 93\\.6 | Musician |  |",
            "| 3 | @Cristiano | [Cristiano Ronaldo](/wiki/Cristiano_Ronaldo \"Cristiano Ronaldo\") | 115\\.6 | Football player |  |",
            "| 5 | @justinbieber | [Justin Bieber](/wiki/Justin_Bieber \"Justin Bieber\") | 108\\.6 | Musician |  |"
          ]
        },
        {
          "title": "Most followed accounts on X (Twitter) – 2025",
          "url": "https://www.tweetbinder.com/blog/top-twitter-accounts/",
          "excerpts": [
            "Top 10 accounts by number of followers (top 10 most followed Twitter accounts). ; Barack Obama, @BarackObama, 130M ; Cristiano Ronaldo, @Cristiano, 115M ; Narendra ..."
          ]
        },
        {
          "title": "Top 10 Most Followed X (Twitter) Accounts in 2025",
          "url": "https://www.favikon.com/blog/top-10-most-followed-x-twitter-accounts",
          "excerpts": [
            "10. Lady Gaga · 9. Taylor Swift · 8. Donald Trump · 7. Narendra Modi · 6. Katy Perry · 5. Rihanna · 4. Justin Bieber · 3. Cristiano Ronaldo.See more"
          ]
        }
      ],
      "reasoning": "The finegrained field value specifies exclusion of major global celebrities using follower-count thresholds and curated exclusion lists. Excerpts that display extremely high follower counts for well-known individuals (Elon Musk, Barack Obama, Taylor Swift) directly illustrate the existence of mega-influencer accounts that would be excluded under the threshold rule. The excerpts also enumerate other top accounts, providing context for what constitutes a 'most-followed' list used in curated exclusion strategies. By citing specific follower figures (e.g., 222.2M for Elon Musk, 130.3M for Barack Obama, 93.6M for Taylor Swift), these excerpts substantiate the idea that accounts above certain follower thresholds are filtered out. The combination of concrete follower counts and the listing of top accounts serves as concrete support for both components of the exclusion rules: a numerical threshold and a curated list source (e.g., Wikipedia's list of most-followed accounts). Therefore, these excerpts collectively support the described rule set for excluding major celebrities from consideration as underrated sources, and they illustrate the practical basis for the exclusion criteria.",
      "confidence": "high"
    },
    {
      "field": "legal_and_platform_compliance_summary",
      "citations": [
        {
          "title": "Developer Agreement and Policy - Twitter Developers - X",
          "url": "https://developer.x.com/en/developer-terms/agreement-and-policy",
          "excerpts": [
            "nfidential Information (as defined below).\n\n**D. Rate Limits.** You will not attempt to exceed or circumvent limitations on access, calls, and use of the X API (\" **Rate Limits** \") or otherwise use the X API in a manner that exceeds reasonable request volume, constitutes excessive or abusive usage, or otherwise does not comply with this Agreement. If you exceed or X reasonably believes that you have attempted to circumvent Rate Limits, controls to limit use of the X APIs, or the terms of this Agreement, then your ability to use the Licensed Material may be temporarily suspended or permanently blocked.",
            "**. Notwithstanding anything to the contrary in this Agreement, to the extent you are provided access to the Licensed Material pursuant to the procedures described in Article 40 of the Digital Services Act (Regulation (EU) 2022/2065) (“DSA”), your access and use of the Licensed Material is limited solely to performing research that contributes to the detection, identification, and understanding of systemic risks in the European Union and only to the extent necessary for X to comply with its obligations under the DSA.",
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs). Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research.",
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties. If you ..."
          ]
        },
        {
          "title": "Terms of Service - X",
          "url": "https://x.com/en/tos/previous/version_19",
          "excerpts": [
            "This license has the sole purpose of enabling you to use and enjoy the benefit of the Services as provided on X, in the manner permitted by these Terms."
          ]
        },
        {
          "title": "X's new Terms of Service enforces that all content can be ...",
          "url": "https://www.reddit.com/r/privacy/comments/1g5cc00/xs_new_terms_of_service_enforces_that_all_content/",
          "excerpts": [
            "X just updated their terms of service as of today (effective on the 15th of November). Obviously, no one actually has time to read it, so I made a color-coded ..."
          ]
        },
        {
          "title": "Twitter Terms of Service - X",
          "url": "https://x.com/en/tos/previous/version_13",
          "excerpts": [
            "These Terms of Service (“Terms”) govern your access to and use of our services, including our various websites, SMS, APIs, email notifications, applications, ..."
          ]
        },
        {
          "title": "Developer Policy – X Developers",
          "url": "http://developer.x.com/en/developer-terms/policy",
          "excerpts": [
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties.**If you provide X Content to third parties, including downloadable datasets or via an API, you may only distribute Post IDs, Direct Message IDs, and/or User IDs (except as described below)",
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs).",
            "Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research. Only as many Post IDs or User IDs that is necessary for such research, and has been approved by X may be used.",
            "Your license agreement with X limits your use of the X API and X Content. Among other things, the X API has rate limits which help to ensure fair data usage and to combat spam on the platform."
          ]
        },
        {
          "title": "Terms of Service",
          "url": "https://x.com/en/tos",
          "excerpts": [
            "You must abide by the Services’ acceptable use terms:** You may not access the Services in any way other than through the currently available, published interfaces that we provide. For example, this means that you cannot scrape the Services without X’s express written permission, try to work around any technical limitations we impose, or otherwise attempt to disrupt the operation of the Service",
            " Liquidated Damages\nProtecting our users’ data and our system resources is important to us. You further agree that, to the extent permitted by applicable law, if you violate the Terms, or you induce or facilitate others to do so, in addition to all other legal remedies available to us, you will be jointly and severally liable to us for liquidated damages as follows for requesting, viewing, or accessing more than 1,000,000 posts (including reply posts, video posts, image posts, and any other posts) in any 24-hour period - $15,000 USD per 1,000,000 posts. You agree that these amounts are (i) a reasonable estimate of our damages; (ii) not a penalty; and (iii) not otherwise limiting of our ability to recover from you or others under any legal or equitable theory or claim, including but not limited to statutory damages and/or equitable relief. You further agree that repeated violations of these Terms will irreparably harm and entitle us to injunctive and/or other equitable relief, in addition to monetary damages."
          ]
        },
        {
          "title": "X updates its terms to ban crawling and scraping",
          "url": "https://techcrunch.com/2023/09/08/x-updates-its-terms-to-ban-crawling-and-scraping/",
          "excerpts": [
            "Sep 8, 2023 — The new terms, which are effective from September 29, ban any kind of scraping or crawling without “prior written consent.” I",
            "Elon Musk-owned X, formerly Twitter, has updated its terms of service to prohibit scraping and crawling — likely to fend off any AI models ...",
            "”\nNOTE: crawling or scraping the Services in any form, for any purpose without our prior written consent is expressly prohibited."
          ]
        },
        {
          "title": "X Updates its Terms, Bans Data Scraping& Crawling",
          "url": "https://nftnow.com/news/x-updates-terms-of-service-to-ban-unauthorized-data-crawling-scraping/",
          "excerpts": [
            "Sep 8, 2023 — X, formerly known as Twitter, updated its Terms of Service to include the prohibition of unauthorized data crawling and scraping."
          ]
        },
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "If you need to share X content you obtained via the X APIs with another party, the best way to do so is by sharing Post IDs, Direct Message IDs, and/or User IDs, which the end user of the content can then rehydrate (i.e. request the full Post, User, or Direct Message content) using the X APIs.",
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X."
          ]
        },
        {
          "title": "Twitter developer policy update | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/blog/industry-team-news/2020/x-developer-policy-update",
          "excerpts": [
            "Mar 10, 2020 — Researchers can now share an unlimited number of Tweet IDs and/or User IDs if they are doing so on behalf of an academic institution and for ..."
          ]
        },
        {
          "title": "Versioning, Provenance, and Reproducibility in Production Machine ...",
          "url": "https://ckaestne.medium.com/versioning-provenance-and-reproducibility-in-production-machine-learning-355c48665005",
          "excerpts": [
            "Versioning, provenance tracking, and reproducibility are essential tools for responsible engineers that provide a technical foundation for debugging."
          ]
        },
        {
          "title": "13 Most Important Social Media Metrics To Track in 2025",
          "url": "https://agencyanalytics.com/blog/social-media-metrics",
          "excerpts": [
            "QUICK SUMMARY: Social media metrics are used to measure performance, understand audience behavior, and optimize campaign strategy."
          ]
        },
        {
          "title": "[PDF] The Provenance of a Tweet - gwu-libraries.github.io",
          "url": "https://gwu-libraries.github.io/sfm-ui/resources/provenance-of-tweet.pdf",
          "excerpts": [
            "Even in those domains where reproducibility is not required or even desired, it needs to be made transparent how results were generated. . . ."
          ]
        },
        {
          "title": "A Longitudinal Assessment of the Persistence of Twitter ...",
          "url": "https://arxiv.org/pdf/1709.09186",
          "excerpts": [
            "by A Zubiaga · 2017 · Cited by 111 — This study assesses the persistence of 30 Twitter datasets, analyzing completeness, representativity, similarity, and changingness of ..."
          ]
        },
        {
          "title": "Data Authenticity, Consent, and Provenance for AI Are All Broken",
          "url": "https://mit-genai.pubpub.org/pub/uk7op8zs",
          "excerpts": [
            "We explain why existing tools for data authenticity, consent, and documentation alone are unable to solve the core problems facing the AI community."
          ]
        },
        {
          "title": "11 Best Practices For Tracking Social Media With Google Analytics",
          "url": "https://www.lovesdata.com/blog/tracking-social-media/",
          "excerpts": [
            "Comparing your organic (free) and paid social media efforts; Using the social reports in Google Analytics; Interpreting social media data in ..."
          ]
        },
        {
          "title": "(PDF) A longitudinal study of topic classification on Twitter",
          "url": "https://www.researchgate.net/publication/361149588_A_longitudinal_study_of_topic_classification_on_Twitter",
          "excerpts": [
            "In summary, this work provides a long-term study of topic classifiers on Twitter that further justifies classification-based topical filtering approaches while ..."
          ]
        },
        {
          "title": "Best practices for timestamps and time zones in databases",
          "url": "https://www.tinybird.co/blog-posts/database-timestamps-timezones",
          "excerpts": [
            "## 1\\. Thou shalt default to UTC",
            "Regardless of the time zone of the server where an event is created, you should first convert the timestamp into UTC when you store it. You may also keep A) the relevant time zone offset in seconds, B) the name of the time zone, and C) the local timestamp as a string. But always, always, **make your primary storage of a historical event timestamp in UTC. **",
            "UTC is an absolute point in time. It is based on a [highly accurate and stable standard](https://en.wikipedia.org/wiki/Coordinated_Universal_Time) recognized worldwide. It is not affected by changes in time zones or other adjustments at the whim of the latest local government. UTC is always constant - every other modern time is relative.",
            "## 2\\. Thou shalt only do that which is necessary",
            "It can be tempting to process timestamp data in a way that seems comprehensive or exhaustive, but this can lead to wasted effort and unnecessary complexity. This is why **you should only do what is needed to serve the use case in front of you. **"
          ]
        },
        {
          "title": "Notes on Downloading Conversations through Twitter's V2 ...",
          "url": "https://cborchers.com/2021/03/23/notes-on-downloading-conversations-through-twitters-v2-api/",
          "excerpts": [
            "Mar 23, 2021 — The variable created_at might be a good proxy for filtering tweets in the sense of a forward search through only including tweets posted after ..."
          ]
        },
        {
          "title": "Whats new with the Twitter API v2 in 2022",
          "url": "https://dev.to/suhemparack/whats-new-with-the-twitter-api-v2-in-2022-plb",
          "excerpts": [
            "Oct 11, 2022 — This tells us that the Tweet with ID 1265324480517361664 was deleted. This guide showcases how to work with the batch compliance solution in ..."
          ]
        },
        {
          "title": "Response Codes | Docs | Twitter Developer Platform - X",
          "url": "https://developer.x.com/ja/docs/basics/response-codes",
          "excerpts": [
            "The standard Twitter API returns HTTP status codes in addition to JSON-based error codes and messages."
          ]
        },
        {
          "title": "Data Collection with Twitter API v2",
          "url": "https://www.kaggle.com/code/andrewedward37/data-collection-with-twitter-api-v2",
          "excerpts": [
            "In this article, I will go through a step-by-step process from setting up, accessing endpoints, to saving tweets collected in CSV format to use for analysis in ..."
          ]
        },
        {
          "title": "[PDF] An archive and corpus of Twitter/X's policies for Tweet redistribution ...",
          "url": "https://access.gesis.org/sharing/2847/6568",
          "excerpts": [
            "When researchers publish research based on the analysis of Tweets, good practice requires sharing the Tweets. (or Tweet IDs) to enable ..."
          ]
        },
        {
          "title": "Recommended date format for REST GET API - Stack Overflow",
          "url": "https://stackoverflow.com/questions/9581692/recommended-date-format-for-rest-get-api",
          "excerpts": [
            "Missing: social reproducibility provenance"
          ]
        },
        {
          "title": "Table format comparisons - Append-only tables and incremental reads",
          "url": "https://jack-vanlightly.com/blog/2024/8/13/table-format-comparisons-append-only-tables-and-incremental-reads",
          "excerpts": [
            "Missing: metrics practices"
          ]
        },
        {
          "title": "GET statuses/home_timeline | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/x-api/v1/tweets/timelines/api-reference/get-statuses-home_timeline",
          "excerpts": [
            "X API v2 ... The value of count is best thought of as a limit to the number of tweets to return because suspended or deleted content is removed after the count ..."
          ]
        },
        {
          "title": "HTTP Error code: 403 (no access to Twitter API v2)",
          "url": "https://devcommunity.x.com/t/http-error-code-403-no-access-to-twitter-api-v2/155377",
          "excerpts": [
            "Jun 10, 2021 — When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project."
          ]
        },
        {
          "title": "Best Practices for Time-Series Data Modeling: Single or Multiple ...",
          "url": "https://www.tigerdata.com/learn/best-practices-time-series-data-modeling-single-or-multiple-partitioned-tables-aka-hypertables",
          "excerpts": [
            "With time-series data being (normally) append-only, removing parts of the data (this specific user's data) may be tricky. Schema changes."
          ]
        },
        {
          "title": "Date and Time Formatting - Amazon-Services-API",
          "url": "https://developer-docs.amazon.com/sp-api/docs/iso-8601",
          "excerpts": [
            "Missing: social media reproducibility provenance"
          ]
        },
        {
          "title": "on Data Persistence for Manipulative Social Media Content",
          "url": "https://aclanthology.org/2024.clicit-1.115.pdf",
          "excerpts": [
            "by O Uryupina · 2024 — This work presents an in-depth investigation of the data decay for publicly fact-checked online content. We monitor compromised posts on major ..."
          ]
        },
        {
          "title": "The 21 essential social media metrics you must track for ...",
          "url": "https://blog.hootsuite.com/social-media-metrics/",
          "excerpts": [
            "We've broken down the 21 key social media metrics you need to keep an eye on into six different categories to help keep you organized."
          ]
        },
        {
          "title": "Nonrandom Tweet Mortality and Data Access Restrictions Compromising the Replication of Sensitive Twitter Studies",
          "url": "https://www.cambridge.org/core/journals/political-analysis/article/nonrandom-tweet-mortality-and-data-access-restrictions-compromising-the-replication-of-sensitive-twitter-studies/17E618F1D395CBBB2360AA424DA5533A",
          "excerpts": [
            "As proposed in Davidson _et al._ ( [Reference Davidson 2023]() ), automatic crawlers could update these archives without needing an API.",
            "However, one must carefully evaluate this step, as crawling social media platforms might be a legal gray zone.",
            "zone. The library intends to make its collected data available “within the German National Library’s infrastructure.",
            "Twitter’s policy still prohibits researchers from directly sharing the raw content of tweets.",
            ") ). The one-way aspect of this well-established computer science technique prevents rehydrating tweets’ raw content.",
            "hat these Twitter studies and their findings are considerably affected by nonrandom tweet mortality and data access restrictions imposed by ... As pr"
          ]
        },
        {
          "title": "Provenance Data in Social Media",
          "url": "https://www.odbms.org/wp-content/uploads/2014/03/Provenance-Data-in-Social-Media.pdf",
          "excerpts": [
            "Provenance data associated\n with a social media statement can help dispel rumors, clarify opinions, and conﬁrm facts.",
            "However, provenance data about social media statements is not readily available to users\ntoday.",
            "The search for provenance attribute values can continue using\nthe profile with the highest probability.",
            "The W3C incubator group provided a list of provenance dimensions that are applicable to\nprovenance data in social media.",
            "imeliness\n of social media data can be defined more simply as [8]:\n\t\t\t\t Qcurrency = (current_time − time_provenance_data_created)/retrie",
            "A provenance system\n that takes longer to gather provenance attribute values than the frequency at which the provenance\n attribute values are, or are likely to be updated, does not provide accurate or valuable provenance\n attribute valu"
          ]
        },
        {
          "title": "python - How to query Twitter API public metrics for multiple ...",
          "url": "https://stackoverflow.com/questions/72640093/how-to-query-twitter-api-public-metrics-for-multiple-tweet-ids-using-tweepy",
          "excerpts": [
            "Use Tweepy to to call the Twitter API to return the public_metrics ( likes , retweets , quotes , replies ) for each of multiple tweet_id s."
          ]
        },
        {
          "title": "Twitter API v2 in Python - issues with datetime conversion",
          "url": "https://stackoverflow.com/questions/66629933/twitter-api-v2-in-python-issues-with-datetime-conversion",
          "excerpts": [
            "I upgraded my Python-Twitter API access from v1 to v2. But as it seems, Twitter does not any longer accept my timestamp-formats for the updated ..."
          ]
        },
        {
          "title": "Possible to check API rate limit headers without burning a ...",
          "url": "https://community.openai.com/t/possible-to-check-api-rate-limit-headers-without-burning-a-request/510041",
          "excerpts": [
            "Nov 17, 2023 — I'd like to tell my user how many requests they have left before they start running a bulk API task. The best information on remaining rate ..."
          ]
        },
        {
          "title": "Identifying time zones in ISO 8601",
          "url": "https://stackoverflow.com/questions/42194571/identifying-time-zones-in-iso-8601",
          "excerpts": [
            "ISO 8601 supports zone offsets, but not timezones with variable offsets (typically timezones with summer time, which goes for many of the IANA timezones).See more"
          ]
        },
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better.",
            "tweets.csv(6.52 MB)",
            "License\n-------\n\n[CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        },
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/hsankesara/the-tweets-of-wisdom/code",
          "excerpts": [
            "The Tweets of Wisdom. A collection of wise words straight from the Twitter. arrow_drop_up 14. file_downloadDownload. The Tweets of Wisdom."
          ]
        },
        {
          "title": "Starter: The Tweets of Wisdom b542b708-9",
          "url": "https://www.kaggle.com/code/kerneler/starter-the-tweets-of-wisdom-b542b708-9",
          "excerpts": [
            "Explore and run machine learning code with Kaggle Notebooks | Using data from The Tweets of Wisdom."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "A dataset which contains 30k+ so called \"self-help\" tweets from 100+ authors.",
            "I scraped the data using **Tweepy** API. I have scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "Here is the [link](https://www.kaggle.com/hsankesara/the-tweets-of-wisdom) to the dataset there.",
            "The-Tweets-of-Wisdom",
            "Public"
          ]
        },
        {
          "title": "Sahil Bloom (@SahilBloom) / X",
          "url": "https://x.com/sahilbloom?lang=en",
          "excerpts": [
            "Underrated life advice: You can reinvent yourself as many times as you need. New habits. New standards. New people. New career. You're never stuck. You're ...See more"
          ]
        },
        {
          "title": "Tweets To Templates: Train ChatGPT To Reverse Engineer ...",
          "url": "https://writewithai.substack.com/p/tweets-to-templates-train-chatgpt",
          "excerpts": [
            "Today we want to help you create your own library of viral Twitter templates. Going viral on Twitter always comes down to the hook you use."
          ]
        },
        {
          "title": "Reddit - r/indiehackers discussion by erik-grielenberger",
          "url": "http://reddit.com/r/indiehackers/comments/1m2ugls/i_reverseengineered_500_viral_tweets_heres",
          "excerpts": [
            "1. Lead with one outcome. [r/indiehackers - I reverse‑engineered 500+ VIRAL TWEETS - Here’s everything I've learned. Steal it ",
            "TL;DR**: Hook hard. Prove fast. Ask once. Media every two tweets. Follow this blueprint and turn that viral ceiling into your floor. Share if it help",
            "**4. Six hook patterns you can copy today:**",
            "**5. Pick a Narrative Skeleton (lock one). **",
            "**6. Media matters:** Drop a data card, GIF or 5‑sec demo every two or three tweets. Alternating text and media increases retweets by 50%. Add a three‑word headline and ALT text for each visual.",
            "5. Pick a Narrative Skeleton (lock one).",
            "4. Six hook patterns you can copy today:",
            "Copy my data-backed AI Templates: bettrprompts.com/category/viral-social-media",
            "*Hi, I’m Erik** - massive data nerd. I’ve primarily created these datasets because I want to increase my social media reach and understand how some posts go viral and most of them don",
            "This data analysis shows you how only **39 Threads** reached **118M+ impressions** and **450k+ likes** *(save for later)*."
          ]
        },
        {
          "title": "The anatomy of a viral tweet: The \"rehashing old news\" variant",
          "url": "https://weaponizedspaces.substack.com/p/the-anatomy-of-a-viral-tweet-the",
          "excerpts": [
            "A tweet that is said to “go viral” is generally one that has garnered a certain number of interactions within a specified time frame."
          ]
        },
        {
          "title": "What does a non-celebrity have to do to get famous on ...",
          "url": "https://www.quora.com/What-does-a-non-celebrity-have-to-do-to-get-famous-on-Twitter",
          "excerpts": [
            "I have seen many non-celebs who became prominent on Twitter. You need to show off the value in your tweets to become famous."
          ]
        },
        {
          "title": "Initial tweet valence, abuse volume, and observer Dark ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11106073/",
          "excerpts": [
            "by CJ Hand · 2024 · Cited by 1 — The current study. Participants processed six Twitter threads consisting of an initial tweet by a female celebrity followed by six replies."
          ]
        },
        {
          "title": "Studying Celebrity Practices on Twitter Using a Framework ...",
          "url": "https://journals.sagepub.com/doi/10.1177/2056305118763365",
          "excerpts": [
            "by S Tanupabrungsun · 2018 · Cited by 28 — We constructed three datasets by employing a toolkit (Hemsley, Ceskavich, & Tanupabrungsun, 2014) that collects tweets using a streaming API."
          ]
        },
        {
          "title": "As a YouTuber, is it really that vital to have a Twitter ...",
          "url": "https://www.reddit.com/r/socialmedia/comments/lqa4o1/as_a_youtuber_is_it_really_that_vital_to_have_a/",
          "excerpts": [
            "There are two things I see creators do on Twitter to their benefit. They make connections with other creators so they can work together, and ..."
          ]
        },
        {
          "title": "I followed a lot of indie hackers and “build in public” accounts on ...",
          "url": "https://news.ycombinator.com/item?id=43407164",
          "excerpts": [
            "I followed a lot of indie hackers and “build in public” accounts on Twitter over the years. Most of them struggled for a while and then pivoted into some ..."
          ]
        },
        {
          "title": "21 Extremely Relatable Tweets That Will Speak To Every ...",
          "url": "https://www.buzzfeed.com/ishabassi/funny-writer-tweets",
          "excerpts": [
            "May 30, 2018 — 21 Extremely Relatable Tweets That Will Speak To Every Writer's Soul ; 1. Hannah Giorgis @ethiopienne ; 2. anna s-r @annaspargoryan ; 3. f thot ..."
          ]
        },
        {
          "title": "32 Funny Tweets That Are Completely Underrated",
          "url": "https://www.buzzfeed.com/erinchack/underrated-tweets",
          "excerpts": [
            "Mar 10, 2017 — We asked the BuzzFeed Community what their favorite underrated tweet was. Here are the best responses. Note: Not all submissions are from Community users."
          ]
        },
        {
          "title": "Wise life advice from a 5-year-old is going viral on Twitter | Mashable",
          "url": "https://mashable.com/article/twitter-thread-wise-life-advice-from-5-year-old-is-going-viral",
          "excerpts": [
            "A 5-year-old boy dispensed some powerful life advice to his mom on his way to school. And Twitter is loving it."
          ]
        },
        {
          "title": "Twitter users share their best advice in four words or less - Daily Mail",
          "url": "https://www.dailymail.co.uk/femail/article-7124905/Thousands-Twitter-users-share-best-advice-just-four-words-less.html",
          "excerpts": [
            "'Just be bloody nice!' Thousands of people sum up their VERY best advice in just four words or less as part of an inspiring viral Twitter thread."
          ]
        },
        {
          "title": "The Best Book Tweets Of 2021 - BuzzFeed",
          "url": "https://www.buzzfeed.com/farrahpenn/best-funniest-viral-book-tweets-2021",
          "excerpts": [
            "Missing: underrated thinkers"
          ]
        },
        {
          "title": "11 Twitter accounts that will make you a better writer",
          "url": "https://forthwrite.substack.com/p/11-twitter-accounts-that-will-make",
          "excerpts": [
            "1. Stephen King | @StephenKing ... I'm usually too much of a wuss for horror stories, but even I've read Stephen King – and there is no doubt he's ..."
          ]
        },
        {
          "title": "5 Million Subscribers in 4 Years, The Most Underrated Writing ...",
          "url": "https://johnbardos.medium.com/5-million-subscribers-in-4-years-the-most-underrated-writing-platform-how-i-grew-my-twitter-9k-4f72b489943a?source=user_profile---------8----------------------------",
          "excerpts": [
            "YouTuber Peter Mckinnon got 1.8 million subs in his first year and more than 5 million subscribers in his first four years."
          ]
        },
        {
          "title": "The belief that artists must create content on social media ...",
          "url": "https://www.reddit.com/r/musicmarketing/comments/1cjf6k3/the_belief_that_artists_must_create_content_on/",
          "excerpts": [
            "Promoting your music or creating content on social media and building a following seems to be a huge waste in effort if you want to people to listen to your ...See more"
          ]
        },
        {
          "title": "What an excellent, insightful post. This really resonates",
          "url": "https://news.ycombinator.com/item?id=32125130",
          "excerpts": [
            "What an excellent, insightful post. This really resonates: \"when self-promotion is done right, it never looks like self-promotion\"."
          ]
        },
        {
          "title": "21 Tweets By Non-Celebrities That Got Over 500000 Likes ...",
          "url": "https://www.buzzfeed.com/mikespohr/tweets-by-non-celebrities-that-got-over-500000-likes",
          "excerpts": [
            "Feb 15, 2020 — 1. This perfect halftime show joke with a twist: · 2. This kid who thinks on a whole other level: · 3. And this adorable kid who goes his own way, ..."
          ]
        },
        {
          "title": "Collect posts from Twitter and send to Airtable",
          "url": "https://n8n.io/workflows/1403-collect-posts-from-twitter-and-send-to-airtable/",
          "excerpts": [
            "This workflow collects tweets (100 but adjustable) and add them to AirTable. Starting the 2nd execution, the workflow will add only the new tweets."
          ]
        },
        {
          "title": "CASOS-IDeaS-CMU/twitter_conversation_collection",
          "url": "https://github.com/CASOS-IDeaS-CMU/twitter_conversation_collection",
          "excerpts": [
            "Repository for collecting Twitter conversations using the V2 API - CASOS-IDeaS-CMU/twitter_conversation_collection."
          ]
        },
        {
          "title": "How do you get started from scratch on Twitter(or Threads) ...",
          "url": "https://www.reddit.com/r/socialmedia/comments/1dx5fha/how_do_you_get_started_from_scratch_on_twitteror/",
          "excerpts": [
            "My question revolves around just getting anyone random to simply view my posts, not getting followers, not getting a huge amount of likes/views."
          ]
        },
        {
          "title": "Reddit GrowthHacking Analysis and Related Datasets",
          "url": "https://www.reddit.com/r/GrowthHacking/comments/1m2u9tp/i_reverseengineered_500_viral_tweets_this_is/",
          "excerpts": [
            "This data analysis shows you how only 39 Threads reached 118M+ impressions and 450k+ likes (save for later)."
          ]
        },
        {
          "title": "A comprehensive study of wisdom",
          "url": "https://www.kaggle.com/code/hsankesara/a-comprehensive-study-of-wisdom",
          "excerpts": [
            "Understanding the Author, Likes and Retweets¶. How much likes and retweets are related. Value count of tweets of each author. Do more tweets increase the mean ... Explore and run machine learning code with Kaggle Notebooks | Using data from The Tweets of Wisdom\n",
            "A comprehensive study of wisdom",
            "the-tweets-of-wisdom"
          ]
        },
        {
          "title": "Hsankesara",
          "url": "https://www.kaggle.com/datasets/hsankesara/",
          "excerpts": [
            "Flickr Image dataset. Updated 7 years ago Usability 7.1 · 9 GB ... The Tweets of Wisdom. Updated 6 years ago Usability 10.0 · 1 File (CSV) · 2 ..."
          ]
        },
        {
          "title": "Trending Humor Tweets",
          "url": "https://www.kaggle.com/datasets/jashjasani/trending-humor-tweets",
          "excerpts": [
            "Twitter New Dataset 2024 March Data · Ayush Kumar Singh · a year ago · 8.1 · 3 MB · 351 downloads ; The Tweets of Wisdom · Hsankesara · 6 years ago · 10.0 · 2 MB · 379 ..."
          ]
        },
        {
          "title": "Sentiment & Emotions Labelled Tweets",
          "url": "https://www.kaggle.com/datasets/ankitkumar2635/sentiment-and-emotions-of-tweets",
          "excerpts": [
            "The dataset contains about 25k tweets along with date, timestamp, username, tweet Id, sentiment, sentiment score, emotion and emotion score. grid_3x3sort"
          ]
        },
        {
          "title": "Twitter Tweets Sentiment Dataset",
          "url": "https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset",
          "excerpts": [
            "Twitter is an online Social Media Platform where people share their their though as tweets. It is observed that some people misuse it to tweet hateful content."
          ]
        }
      ],
      "reasoning": "The finegrained_field_path corresponds to a governance/legality lens on how to legally collect and share data from X/Twitter. The most directly relevant excerpts are those that state explicit data-collection constraints and platform policy, including explicit prohibitions on crawling/scraping, posted in the X Developer Policy and Terms of Service. Those excerpts clearly articulate that crawling or scraping is prohibited without prior written permission and that data redistribution is tightly controlled via the Developer Policy, which is essential to support the field value that data collection must be done via official interfaces and within stated limits. Additional excerpts describe data redistribution limits (1,500,000 Post IDs per entity per 30 days, and 500 public posts per person per day for non-automated sharing), which directly map to the field's redistribution policy and quantification aspects. Hydration (rehydration) requirements are addressed in excerpts describing the official practice that datasets should be released as Tweet IDs to be rehydrated by recipients using the API, ensuring data is current and compliant. Finally, a policy restricting the use of platform data to train large language models (no fine-tuning or training on the API/X Content) supports the ai-model-training restriction facet. Together, these excerpts corroborate the field value's four components: data-collection policy, content-redistribution policy, rehydration-based release guidance, and AI-training restrictions, and they provide concrete policy thresholds and procedures that underpin compliant data handling when sharing Twitter/X data.",
      "confidence": "high"
    },
    {
      "field": "content_category_taxonomy.classification_approach",
      "citations": [
        {
          "title": "Taxonomy 101: Definition, Best Practices, and How It ...",
          "url": "https://www.nngroup.com/articles/taxonomy-101/",
          "excerpts": [
            "Taxonomies aren’t created for the pure love of classifying things (though some UXers like myself absolutely love doing it). Taxonomies allow us to effectively retrieve *all* the content that is related to a specific concep"
          ]
        },
        {
          "title": "Taxonomy Design Best Practices - Create and Build",
          "url": "https://www.claravine.com/evolution-of-taxonomy-design/",
          "excerpts": [
            "There has been an evolution of taxonomy design development. Here are the best practices from the perspective of a practitioner."
          ]
        },
        {
          "title": "The measurement of observer agreement for categorical data",
          "url": "https://pubmed.ncbi.nlm.nih.gov/843571/",
          "excerpts": [
            "by JR Landis · 1977 · Cited by 93933 — This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies."
          ]
        },
        {
          "title": "Methodological Notes",
          "url": "https://www.k-alpha.org/methodological-notes",
          "excerpts": [
            "_ _:_ This is indicative of poor agreement among raters. Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not applying the coding scheme consistently or that the scheme itself may be flawed.",
            " a Krippendorff's Alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions based on the rated data.",
            "Alpha = 1_ _:_ Indicates perfect agreement among raters. It is the scenario where all raters have provided the exact same ratings for each item evalua",
            "Alpha ≥ 0.80: This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's ...",
            "Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not ..."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "A robust taxonomy or classification framework is valuable because it enables retrieval and grouping of content across related concepts, which supports the idea of labeling a tweet with multiple, overlapping categories rather than forcing a single label. This aligns with the proposed approach of multi-label classification, where a single tweet can simultaneously inhabit several themes (e.g., Technology and Philosophy) to reflect its interdisciplinary nature. The evolution and best practices in taxonomy design emphasize building systems that can flexibly map concepts to related content, which is a prerequisite for effective multi-label labeling. Additionally, materials on observer agreement and reliability highlight the importance of validating the consistency and accuracy of labeling schemes; such insights are directly relevant when implementing a multi-label taxonomy to ensure that multiple categories assigned to a tweet are reliable and reproducible. Taken together, these excerpts support adopting a taxonomy-driven, multi-label approach for nuanced content classification, while also underscoring the need to assess and improve labeling reliability as part of the methodology. The most directly supportive elements emphasize taxonomy as a means to retrieve concept-related content and the evolution of taxonomy design practices, which underpins why a multi-label framework is advantageous for capturing the interdisciplinary nature of insights in tweets.",
      "confidence": "medium"
    },
    {
      "field": "underrated_source_definition.primary_filtering_method",
      "citations": [
        {
          "title": "List of most-followed Twitter accounts",
          "url": "https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts",
          "excerpts": [
            "| 1 | @elonmusk | [Elon Musk](/wiki/Elon_Musk \"Elon Musk\") | 222\\.2 | Businessman, owner of X/Twitter, xAI, SpaceX and Former Senior Advisor to the President |  |",
            "| 2 | [@BarackObama](/wiki/Social_media_use_by_Barack_Obama \"Social media use by Barack Obama\") | [Barack Obama](/wiki/Barack_Obama \"Barack Obama\") | 130\\.3 | U.S. President (2009–2017) |  |",
            "| 9 | @taylorswift13 | [Taylor Swift](/wiki/Taylor_Swift \"Taylor Swift\") | 93\\.6 | Musician |  |",
            "| 3 | @Cristiano | [Cristiano Ronaldo](/wiki/Cristiano_Ronaldo \"Cristiano Ronaldo\") | 115\\.6 | Football player |  |",
            "| 5 | @justinbieber | [Justin Bieber](/wiki/Justin_Bieber \"Justin Bieber\") | 108\\.6 | Musician |  |"
          ]
        },
        {
          "title": "Most followed accounts on X (Twitter) – 2025",
          "url": "https://www.tweetbinder.com/blog/top-twitter-accounts/",
          "excerpts": [
            "Top 10 accounts by number of followers (top 10 most followed Twitter accounts). ; Barack Obama, @BarackObama, 130M ; Cristiano Ronaldo, @Cristiano, 115M ; Narendra ..."
          ]
        },
        {
          "title": "Top 10 Most Followed X (Twitter) Accounts in 2025",
          "url": "https://www.favikon.com/blog/top-10-most-followed-x-twitter-accounts",
          "excerpts": [
            "10. Lady Gaga · 9. Taylor Swift · 8. Donald Trump · 7. Narendra Modi · 6. Katy Perry · 5. Rihanna · 4. Justin Bieber · 3. Cristiano Ronaldo.See more"
          ]
        }
      ],
      "reasoning": "The target field value describes a filtering strategy centered on (a) follower-count-based thresholds, (b) excluding clearly dominant sources (celebrities, politicians, large media), and (c) platform signals such as verification status. Excerpts that present the current landscape of extremely high-follower accounts provide concrete examples of where such a threshold would remove sources from consideration and illustrate what \"top\" accounts look like in practice. For instance, explicit follower counts for Elon Musk and Barack Obama demonstrate the kind of magnitudes that would be excluded under a threshold rule. Similarly, listing multiple top accounts with their follower numbers showcases the density of highly-followed sources that would be filtered out, thereby clarifying what constitutes an overridable boundary between mainstream/high-profile accounts and undervalued, potentially underrated sources. These details help justify the use of a deterministic, threshold-driven and exclusion-based approach described in the field value, even though the excerpts do not spell out the full multi-layered workflow or verification criteria. The most relevant parts are those that provide concrete follower-count data and identify recognizable high-profile accounts that would be filtered out under such a scheme, while the less direct relevance comes from the broader list context without explicit linkage to verification status or the exact multi-layered process.",
      "confidence": "medium"
    },
    {
      "field": "nlp_classifier_plan_for_insightful_content.training_methodology",
      "citations": [
        {
          "title": "Rapid Training Data Creation with Weak Supervision - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5951191/",
          "excerpts": [
            "by A Ratner · 2017 · Cited by 1173 — (1) SME users write labeling functions (LFs) that express weak supervision sources like distant supervision, patterns, and heuristics. (2) Snorkel applies the ..."
          ]
        },
        {
          "title": "3 ways to use Snorkel's Labeling Functions",
          "url": "https://snorkel.ai/blog/snorkel-labeling-functions-use-cases/",
          "excerpts": [
            "Jun 24, 2022 — Data scientists and subject matter experts can create labeling functions that capture and apply heuristics or patterns. Looking at a few ..."
          ]
        },
        {
          "title": "Example of Labeling Functions",
          "url": "https://docs.datasaur.ai/assisted-labeling/data-programming/example-of-labeling-functions",
          "excerpts": [
            "Jun 26, 2024 — Example of Labeling Functions ... Labeling functions are designed to apply weak heuristics and rules for predicting labels on unlabeled data."
          ]
        },
        {
          "title": "Weak supervision for pharmacovigilance and NLP",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8554513/",
          "excerpts": [
            "To decrease the labelling costs, researchers have been using weaker forms of supervision, such as heuristically generating training data with external knowledge bases, patterns/rules, or other classifiers.",
            "Weak Supervision utilizes noisy, limited, or imprecise sources to provide a supervision signal for labeling large amounts of training data in a supervised learning setting"
          ]
        },
        {
          "title": "Programmatically labeling data using Snorkel with example - Medium",
          "url": "https://medium.com/data-science-in-your-pocket/programmatically-labeling-data-using-snorkel-with-example-a6a322ef0f2c",
          "excerpts": [
            "Snorkel can be very handy when you facing such an issue for building your training dataset when you have little or no labeled dataset at all."
          ]
        },
        {
          "title": "A collection of tutorials for Snorkel - GitHub",
          "url": "https://github.com/snorkel-team/snorkel-tutorials",
          "excerpts": [
            "We recommend that all users start with the Getting Started tutorial hosted on the Snorkel website for a gentle introduction to the concepts and classes of ..."
          ]
        },
        {
          "title": "ML Techniques: Active Learning vs Weak Supervision - Appen",
          "url": "https://www.appen.com/blog/ml-techniques-active-learning-vs-weak-supervision",
          "excerpts": [
            "Which machine learning technique is right for you? We explain active learning vs weak supervision and which one is best to use for you."
          ]
        }
      ],
      "reasoning": "The target fine-grained field value describes a hybrid training methodology that relies on weak supervision via labeling functions (and specifically mentions Snorkel), combined with a smaller gold-standard set, and additional refinement techniques. Excerpts that discuss labeling functions and weak supervision directly support this approach. Specifically, content that confirms: (a) labeling functions as a mechanism to express weak supervision rules, often implemented in Snorkel, (b) the use of weak supervision to generate large labeled datasets and reduce manual labeling cost, (c) practical examples or guides on labeling functions and Snorkel implementations, (d) related comparisons or tutorials that reinforce the feasibility and patterns of these methods. Quoted/paraphrased points from the most relevant excerpts include: the use of labeling functions to express weak supervision sources and the Snorkel framework applying them; explicit guidance or case examples on using Snorkel's labeling functions; discussions of weak supervision as an approach to generate training data using heuristics, patterns, or auxiliary classifiers; and concrete examples or tutorials on labeling functions and Snorkel that illustrate building weakly-labeled datasets. These elements collectively align with the proposed hybrid methodology combining weak supervision with a smaller gold set and subsequent refinement steps, making them highly relevant to the field value. The remaining excerpts touch on related topics like active learning or broader weak supervision discussions, which provide contextual support but do not directly anchor the exact stated workflow as strongly as the core labeling-function and Snorkel-focused excerpts do.",
      "confidence": "high"
    },
    {
      "field": "content_category_taxonomy.conflict_resolution_rules",
      "citations": [
        {
          "title": "Taxonomy 101: Definition, Best Practices, and How It ...",
          "url": "https://www.nngroup.com/articles/taxonomy-101/",
          "excerpts": [
            "Taxonomies aren’t created for the pure love of classifying things (though some UXers like myself absolutely love doing it). Taxonomies allow us to effectively retrieve *all* the content that is related to a specific concep"
          ]
        },
        {
          "title": "Taxonomy Design Best Practices - Create and Build",
          "url": "https://www.claravine.com/evolution-of-taxonomy-design/",
          "excerpts": [
            "There has been an evolution of taxonomy design development. Here are the best practices from the perspective of a practitioner."
          ]
        },
        {
          "title": "Methodological Notes",
          "url": "https://www.k-alpha.org/methodological-notes",
          "excerpts": [
            "_ _:_ This range is often considered the lower bound for tentative conclusions. A Krippendorff's Alpha in this range suggests moderate agreement; thus, outcomes should be interpreted with concern, questioning the roots of such diverging ratings.",
            "_ _:_ This is indicative of poor agreement among raters. Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not applying the coding scheme consistently or that the scheme itself may be flawed.",
            "Alpha ≥ 0.80: This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's ...",
            "Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not ..."
          ]
        },
        {
          "title": "The measurement of observer agreement for categorical data",
          "url": "https://pubmed.ncbi.nlm.nih.gov/843571/",
          "excerpts": [
            "by JR Landis · 1977 · Cited by 93933 — This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "To support a structured approach for handling cases where tweets map to multiple categories or where annotators disagree, it is important to ground the process in a formal taxonomy framework. Excerpts that discuss taxonomy design and best practices illuminate how categories should be defined and organized so that overlaps are minimized and navigated consistently. Additionally, excerpts on methodological notes about rater agreement and reliability provide the empirical basis for determining when a disagreement is substantive and when adjudication or a decision rule is warranted. The idea of using taxonomy evolution and best practices helps articulate a stable, evolving set of categories, while reliability discussions emphasize the need for explicit thresholds, agreement checks, and the potential necessity of adjudication when agreement is insufficient. Together, these sources support implementing explicit decision flowcharts to handle common overlaps and establishing a review mechanism (senior annotator or committee) to adjudicate persistent disagreements, all while using ongoing guideline refinement to improve future categorization. Directly, the taxonomy-focused excerpts affirm that a structured classification scheme and its evolution are foundational, whereas the reliability-focused excerpts justify having formal consensus processes and thresholds to trigger adjudication and guideline updates.",
      "confidence": "medium"
    },
    {
      "field": "content_category_taxonomy.primary_categories",
      "citations": [
        {
          "title": "Taxonomy 101: Definition, Best Practices, and How It ...",
          "url": "https://www.nngroup.com/articles/taxonomy-101/",
          "excerpts": [
            "Taxonomies aren’t created for the pure love of classifying things (though some UXers like myself absolutely love doing it). Taxonomies allow us to effectively retrieve *all* the content that is related to a specific concep"
          ]
        },
        {
          "title": "Taxonomy Design Best Practices - Create and Build",
          "url": "https://www.claravine.com/evolution-of-taxonomy-design/",
          "excerpts": [
            "There has been an evolution of taxonomy design development. Here are the best practices from the perspective of a practitioner."
          ]
        },
        {
          "title": "Methodological Notes",
          "url": "https://www.k-alpha.org/methodological-notes",
          "excerpts": [
            "_ _:_ This range is often considered the lower bound for tentative conclusions. A Krippendorff's Alpha in this range suggests moderate agreement; thus, outcomes should be interpreted with concern, questioning the roots of such diverging ratings.",
            "_ _:_ This is indicative of poor agreement among raters. Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not applying the coding scheme consistently or that the scheme itself may be flawed.",
            " a Krippendorff's Alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions based on the rated data.",
            "Alpha = 1_ _:_ Indicates perfect agreement among raters. It is the scenario where all raters have provided the exact same ratings for each item evalua",
            "Alpha ≥ 0.80: This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's ...",
            "Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not ..."
          ]
        },
        {
          "title": "The measurement of observer agreement for categorical data",
          "url": "https://pubmed.ncbi.nlm.nih.gov/843571/",
          "excerpts": [
            "by JR Landis · 1977 · Cited by 93933 — This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "The fine-grained field value specifies a core list of thematic categories and notes that optional subcategories may be included, with clear definitions and keywords to guide annotators. Excerpts that explicitly discuss taxonomy and taxonomy design provide direct support for how categories might be structured, named, and evolved over time. For example, content that reframes taxonomy as a tool for effective retrieval and cross-cutting concepts aligns with the need to define a robust primary category list and subcategories. Excerpts that describe reliability measures (such as Krippendorff's Alpha and interrater agreement) are relevant because a stable taxonomy requires consistent application by different coders, ensuring that the assigned categories reflect a shared understanding. Together, these excerpts frame both the creation of the taxonomy (how to structure and evolve categories) and the evaluation of its reliability (how consistently researchers apply those categories), which supports the goal of clearly delineating core categories and optional extensions with defined criteria. The most pertinent pieces are those that discuss taxonomy concepts and design evolution, followed by pieces that address methodological reliability to ensure consistent category labeling; the remaining reliability-focused excerpts provide context for interpreting and validating the taxonomy in practice.",
      "confidence": "medium"
    },
    {
      "field": "nlp_classifier_plan_for_insightful_content.recommended_models",
      "citations": [
        {
          "title": "microsoft/deberta-v3-base - Hugging Face",
          "url": "https://huggingface.co/microsoft/deberta-v3-base",
          "excerpts": [
            "The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters with a vocabulary containing 128K tokens.See more"
          ]
        },
        {
          "title": "DeBERTa - Hugging Face",
          "url": "https://huggingface.co/docs/transformers/main/en/model_doc/deberta",
          "excerpts": [
            "In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using ..."
          ]
        },
        {
          "title": "The implementation of DeBERTa - GitHub",
          "url": "https://github.com/microsoft/DeBERTa",
          "excerpts": [
            "DeBERTa-V3-XSmall is added. With only 22M backbone parameters which is only 1/4 of RoBERTa-Base and XLNet-Base, DeBERTa-V3-XSmall significantly outperforms ..."
          ]
        },
        {
          "title": "Weak supervision for pharmacovigilance and NLP",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8554513/",
          "excerpts": [
            "Weak Supervision utilizes noisy, limited, or imprecise sources to provide a supervision signal for labeling large amounts of training data in a supervised learning setting",
            "To decrease the labelling costs, researchers have been using weaker forms of supervision, such as heuristically generating training data with external knowledge bases, patterns/rules, or other classifiers.",
            "The assumption behind our work is that the large volume of training data which can be collected using an automated labeling process can compensate for the inaccuracy in the labels.",
            "there is a huge need to avoid reliance on manual annotation of small datasets and to move to automatic annotation on large datasets.",
            "This approach can help reduce the need for manual annotation which saves time and resources."
          ]
        },
        {
          "title": "3 ways to use Snorkel's Labeling Functions",
          "url": "https://snorkel.ai/blog/snorkel-labeling-functions-use-cases/",
          "excerpts": [
            "Jun 24, 2022 — Data scientists and subject matter experts can create labeling functions that capture and apply heuristics or patterns. Looking at a few ..."
          ]
        },
        {
          "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
          "url": "https://arxiv.org/abs/2402.15613",
          "excerpts": [
            "Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications.",
            "When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration.",
            "We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data ..."
          ]
        }
      ],
      "reasoning": "The field value asserts two core points: (a) pre-trained transformer models such as BERT or DeBERTa-v3-base are strong baselines for the final classification task, and (b) for weak supervision and rapid prototyping, smaller LLMs like Llama 3 can be used in zero-shot or few-shot capacity to generate initial labels or classifications. Excerpts that discuss DeBERTa models (including the base variant) directly support the first point by providing concrete references to DeBERTa and its variants as modeling choices. Excerpts detailing the architecture, properties, or usage of DeBERTa and BERT cement the suitability of these architectures as baselines. Excerpts focusing on weak supervision techniques (including labeling functions, patterns, and small LLMs for labeling) support the second point by outlining approaches for generating supervision signals or initial labels without heavy reliance on large, fully supervised datasets. Additionally, discussions about labeling functions and Snorkel exemplify practical weak supervision workflows that align with the idea of rapid prototyping using weaker supervision signals, which complements the Llama 3 mention for initial labeling. Taken together, these excerpts substantiate the field value's guidance on using DeBERTa-base and BERT as solid baselines and leveraging weaker signals (e.g., Llama 3 in zero-/few-shot capacity) to bootstrap the process.",
      "confidence": "high"
    },
    {
      "field": "content_category_taxonomy.imbalance_diagnostics",
      "citations": [
        {
          "title": "Taxonomy 101: Definition, Best Practices, and How It ...",
          "url": "https://www.nngroup.com/articles/taxonomy-101/",
          "excerpts": [
            "Taxonomies aren’t created for the pure love of classifying things (though some UXers like myself absolutely love doing it). Taxonomies allow us to effectively retrieve *all* the content that is related to a specific concep"
          ]
        },
        {
          "title": "Taxonomy Design Best Practices - Create and Build",
          "url": "https://www.claravine.com/evolution-of-taxonomy-design/",
          "excerpts": [
            "There has been an evolution of taxonomy design development. Here are the best practices from the perspective of a practitioner."
          ]
        },
        {
          "title": "The measurement of observer agreement for categorical data",
          "url": "https://pubmed.ncbi.nlm.nih.gov/843571/",
          "excerpts": [
            "by JR Landis · 1977 · Cited by 93933 — This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies."
          ]
        },
        {
          "title": "Methodological Notes",
          "url": "https://www.k-alpha.org/methodological-notes",
          "excerpts": [
            "_ _:_ This range is often considered the lower bound for tentative conclusions. A Krippendorff's Alpha in this range suggests moderate agreement; thus, outcomes should be interpreted with concern, questioning the roots of such diverging ratings.",
            "_ _:_ This is indicative of poor agreement among raters. Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not applying the coding scheme consistently or that the scheme itself may be flawed.",
            "Alpha = 1_ _:_ Indicates perfect agreement among raters. It is the scenario where all raters have provided the exact same ratings for each item evalua",
            "Alpha ≥ 0.80: This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's ...",
            "Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not ..."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on a process to monitor how often each category label is used across the taxonomy and to detect imbalances that prompt taxonomy refinement. Excerpt describing taxonomy as a structured scheme for organizing related concepts directly supports the idea of a taxonomy needing ongoing evaluation and adjustment. Excerpts that discuss the evolution and best practices of taxonomy design further reinforce the notion that taxonomy quality depends on thoughtful structure and maintenance, which is exactly what monitoring label distribution aims to achieve. Discussions of observer reliability and Krippendorff's Alpha provide concrete mechanisms for assessing consistency in labeling across items, which is essential when diagnosing whether imbalances reflect genuine data characteristics or measurement/definition issues within the taxonomy. By highlighting acceptable reliability thresholds and potential pitfalls when agreement is low, those excerpts collectively illuminate how to detect and respond to distributional imbalances, informing taxonomy refinement. Overall, the strongest support comes from explicit statements about taxonomy and its design/maintenance, with reliability-focused excerpts offering the methodological tools to monitor and adjust label distributions. The less direct excerpts still contribute by framing the reliability context that underpins balanced category definitions and ongoing taxonomy improvement.",
      "confidence": "medium"
    },
    {
      "field": "underrated_source_definition.final_heuristic",
      "citations": [
        {
          "title": "List of most-followed Twitter accounts",
          "url": "https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts",
          "excerpts": [
            "| 1 | @elonmusk | [Elon Musk](/wiki/Elon_Musk \"Elon Musk\") | 222\\.2 | Businessman, owner of X/Twitter, xAI, SpaceX and Former Senior Advisor to the President |  |",
            "| 2 | [@BarackObama](/wiki/Social_media_use_by_Barack_Obama \"Social media use by Barack Obama\") | [Barack Obama](/wiki/Barack_Obama \"Barack Obama\") | 130\\.3 | U.S. President (2009–2017) |  |",
            "| 9 | @taylorswift13 | [Taylor Swift](/wiki/Taylor_Swift \"Taylor Swift\") | 93\\.6 | Musician |  |",
            "| 3 | @Cristiano | [Cristiano Ronaldo](/wiki/Cristiano_Ronaldo \"Cristiano Ronaldo\") | 115\\.6 | Football player |  |",
            "| 5 | @justinbieber | [Justin Bieber](/wiki/Justin_Bieber \"Justin Bieber\") | 108\\.6 | Musician |  |"
          ]
        },
        {
          "title": "Most followed accounts on X (Twitter) – 2025",
          "url": "https://www.tweetbinder.com/blog/top-twitter-accounts/",
          "excerpts": [
            "Top 10 accounts by number of followers (top 10 most followed Twitter accounts). ; Barack Obama, @BarackObama, 130M ; Cristiano Ronaldo, @Cristiano, 115M ; Narendra ..."
          ]
        },
        {
          "title": "Top 10 Most Followed X (Twitter) Accounts in 2025",
          "url": "https://www.favikon.com/blog/top-10-most-followed-x-twitter-accounts",
          "excerpts": [
            "10. Lady Gaga · 9. Taylor Swift · 8. Donald Trump · 7. Narendra Modi · 6. Katy Perry · 5. Rihanna · 4. Justin Bieber · 3. Cristiano Ronaldo.See more"
          ]
        }
      ],
      "reasoning": "The finegrained field value envisions a reproducible scoring rubric that begins with a base score and subtracts points for exclusions such as: being on a celebrity list, meeting a follower threshold, being a politician, or being a verified media organization. Excerpts that enumerate highly followed accounts and name recognizable celebrities (for example, Elon Musk, Barack Obama, Taylor Swift, Cristiano Ronaldo, Justin Bieber) provide concrete evidence of which accounts would receive deductions under the rubric. By listing follower counts and labeling accounts as celebrities or public figures, these excerpts illustrate the very criteria that would influence the final underrating-source decision. The most directly relevant portions are those that name specific famous individuals and provide their follower counts, as they demonstrate the practical application of the exclusion logic (celebrity status and prominence). The less direct but still pertinent parts are the broader lists of top-followed accounts, which contextualize the ecosystem from which underrated sources must be distinguished. Together, these excerpts support the notion that the rubric should penalize or deprioritize mainstream, highly visible figures while favoring non-celebrity sources with substantive, original insights.",
      "confidence": "medium"
    },
    {
      "field": "data_acquisition_plan.recommended_approach",
      "citations": [
        {
          "title": "TwitterAPI.io",
          "url": "https://twitterapi.io/",
          "excerpts": [
            "Cost per 1K tweets, $0.15, $100+",
            "Rate Limit, 1000+ req/sec, 300 req/15min"
          ]
        },
        {
          "title": "Hashtag tracker pricing plan by TrackMyHashtag",
          "url": "https://www.trackmyhashtag.com/pricing",
          "excerpts": [
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ...",
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ..."
          ]
        },
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data.",
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ..."
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "Meltwater Pricing & Costs",
          "url": "https://www.meltwater.com/en/pricing",
          "excerpts": [
            "Learn more about our Meltwater pricing model, packaging, and costs to find your personal solution. Build your personalized Meltwater suite today!"
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly reference the tools and strategies named in the field value. Content describing TwitterAPI.io pricing and usage supports the idea of using a cost-effective third-party provider for high-volume raw data acquisition. Specifically, discussions about costs and limits of the Twitter API illustrate the feasibility and constraints of integrating such a provider into a hybrid approach. Excerpts detailing TrackMyHashtag pricing align with the plan to use targeted historical data purchases, reinforcing the multi-sourcing aspect of the strategy. Excerpts that discuss API paywalls or changes to access (including paywall implementation and pricing for API access) reinforce the theme of balancing cost and access when assembling a data acquisition stack. Additional excerpts mentioning TrackMyHashtag and API-related pricing further corroborate the specific components of the proposed approach. While some excerpts mention Meltwater pricing, they are less aligned with the exact named components of the strategy and thus provide contextual contrast rather than direct support. The combination of these excerpts collectively substantiates a hybrid plan leveraging affordable APIs, historical data vendors, and selective verification/data reinvigoration steps, as outlined in the target field value.",
      "confidence": "medium"
    },
    {
      "field": "data_acquisition_plan.primary_challenge",
      "citations": [
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ...",
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data."
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)",
            "### Basic\n\n$200 /month\n\nFor hobbyists and prototypes\n\n#### What's included:\n\n1 project\n\n2 apps / project\n\n15,000 posts / month (reads)\n\n50,000 posts / month (writes)\n\nFull v2 endpoints access](https://developer.x.com/en/portal/products)"
          ]
        },
        {
          "title": "TwitterAPI.io",
          "url": "https://twitterapi.io/",
          "excerpts": [
            "Cost per 1K tweets, $0.15, $100+",
            "Rate Limit, 1000+ req/sec, 300 req/15min"
          ]
        },
        {
          "title": "Meltwater Pricing & Costs",
          "url": "https://www.meltwater.com/en/pricing",
          "excerpts": [
            "Learn more about our Meltwater pricing model, packaging, and costs to find your personal solution. Build your personalized Meltwater suite today!"
          ]
        },
        {
          "title": "Hashtag tracker pricing plan by TrackMyHashtag",
          "url": "https://www.trackmyhashtag.com/pricing",
          "excerpts": [
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ...",
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ..."
          ]
        }
      ],
      "reasoning": "The most relevant information directly supports the field value by detailing the financial and access barriers inherent in scaling the search for underrated viral tweets. The excerpt describing API access being moved behind a paywall and the Full Archive Search API cap directly illustrates the bottlenecks of obtaining comprehensive tweet data at scale, which matches the stated hurdle of needing to process vast tweet volumes to meet stringent criteria. The pricing discussion that estimates hundreds of thousands of dollars per year for access to a tiny fraction of tweets further reinforces the core challenge of cost-driven scalability. Additional excerpts that outline high monthly costs for API access and the existence of rate limits or pricing models from third-party tools broaden the context of why reaching the required volume of qualifying tweets is problematic, aligning with the field value's emphasis on bottlenecks caused by official API limits and cost drivers. Excerpts about general API documentation and broader pricing context contribute to understanding the ecosystem but are less directly tied to the explicit scaling bottleneck described, though they still corroborate the theme of prohibitive costs and access constraints in this space.",
      "confidence": "medium"
    },
    {
      "field": "quality_assurance_framework.human_review_process",
      "citations": [
        {
          "title": "entity hierarchy, corpus annotation, and sequence labeling",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8104034/",
          "excerpts": [
            "by T Thieu · 2020 · Cited by 22 — 2.2.​​ The annotation process: from guidelines and schema development to creation of the gold standard corpus."
          ]
        },
        {
          "title": "Building custom NLP tools to annotate discourse-functional ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2772766124000594",
          "excerpts": [
            "by M Eguchi · 2024 · Cited by 5 — When it comes to developing an ML system, the goal of annotation is to construct a “gold-standard” annotation dataset, which provides consistent input-output ..."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "by ML McHugh · 2012 · Cited by 21858 — As a general heuristic, sample sizes should not consist of less than 30 comparisons. Sample sizes of 1,000 or more are mathematically most likely to produce ...",
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            " However, this interpretation allows for very little agreement among raters to be described as “substantial”.",
            "For percent agreement, 61% agreement can immediately be seen as problematic.",
            "Almost 40% of the data in the dataset represent faulty data.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "The field value outlines a human-in-the-loop process with concrete pillars: a comprehensive annotation manual with precise definitions and examples for what constitutes insightful content and an underrating source, a gold-standard dataset created and used to calibrate annotators, and an adjudication workflow for when annotators disagree. The strongest evidence is the explicit reference to a 'gold-standard' annotation dataset and to annotation guidelines that define key criteria, which directly map to establishing standard operating procedures for human reviews. Supporting evidence includes discussions of inter-rater reliability and statistical measures such as Cohen's kappa, Krippendorff's Alpha, and related reliability testing, which explain how consistency among annotators is quantified and improved. Together, these excerpts provide a coherent blueprint for a quality assurance framework centered on clear guidelines, gold standards, adjudication, and ongoing reliability assessment. The excerpts collectively show how to implement a HITL process with explicit criteria, stabilization through standard references, and formal metrics to monitor agreement and resolve conflicts.",
      "confidence": "high"
    },
    {
      "field": "content_category_taxonomy.consistency_metric",
      "citations": [
        {
          "title": "Methodological Notes",
          "url": "https://www.k-alpha.org/methodological-notes",
          "excerpts": [
            " a Krippendorff's Alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions based on the rated data.",
            "Alpha ≥ 0.80: This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's ...",
            "_ _:_ This range is often considered the lower bound for tentative conclusions. A Krippendorff's Alpha in this range suggests moderate agreement; thus, outcomes should be interpreted with concern, questioning the roots of such diverging ratings.",
            "Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not ...",
            "Alpha = 1_ _:_ Indicates perfect agreement among raters. It is the scenario where all raters have provided the exact same ratings for each item evalua",
            "_ _:_ This is indicative of poor agreement among raters. Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not applying the coding scheme consistently or that the scheme itself may be flawed."
          ]
        },
        {
          "title": "The measurement of observer agreement for categorical data",
          "url": "https://pubmed.ncbi.nlm.nih.gov/843571/",
          "excerpts": [
            "by JR Landis · 1977 · Cited by 93933 — This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence.",
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing."
          ]
        }
      ],
      "reasoning": "The fine-grained field value asserts that Krippendorff's Alpha is the recommended inter-annotator reliability metric, is versatile for multi-label tasks, and that a target of alpha ≥ 0.800 indicates clear and consistently applied category definitions. The most relevant excerpts directly address Krippendorff's Alpha and practical interpretation: one notes that an alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions, and another states that a target of alpha ≥ 0.80 is a satisfactory level of agreement in many research contexts. Additional entries reference the Landis & Koch framework (observer agreement for categorical data) and Krippendorff's Alpha in methodological notes, which corroborate the use and interpretation of alpha as a reliability statistic. Supporting excerpts discuss the notion of 'perfect agreement' (alpha = 1) and the idea that very low alpha signals unreliable conclusions, which reinforces why the 0.800 threshold is meaningful for reliability. There is also contextual support about the broader concept of interrater reliability and related metrics (e.g., Cohen's kappa) that situates Krippendorff's Alpha within a family of agreement measures, although the primary emphasis remains on Krippendorff's Alpha and its threshold. Taken together, these excerpts align with the field value by validating Krippendorff's Alpha as the key metric, explaining its interpretation, and endorsing the ≥0.800 target as a criterion for reliable categorization.",
      "confidence": "high"
    },
    {
      "field": "quality_assurance_framework.statistical_sampling_plan",
      "citations": [
        {
          "title": "Selecting Statistically Valid Sampling Plans - Taylor Enterprises",
          "url": "https://variation.com/selecting-statistically-valid-sampling-plans/",
          "excerpts": [
            "OC curves are generally summarized by two numbers: the Acceptable Quality Level (AQL) and Lot Tolerance Percent Defective (LTPD). The AQL is that percent ..."
          ]
        },
        {
          "title": "Acceptance Sampling - CQE Academy",
          "url": "https://cqeacademy.com/cqe-body-of-knowledge/product-process-control/acceptance-sampling/",
          "excerpts": [
            "If we look at this OC Curve above, we can tell that the AQL associated with this sampling plan is 2%, and the LTPD for this sampling plan is 8%."
          ]
        },
        {
          "title": "ANSI/ASQ Z1.4 and Z1.9 Sampling Standards",
          "url": "https://asq.org/quality-resources/z14-z19?srsltid=AfmBOopy-uXRazS2cMW3DDdt4aObKmnwChaU8o-i-FYyr0XNLsPazFcT",
          "excerpts": [
            "What is the Z1.9 Standard? --------------------------"
          ]
        },
        {
          "title": "Zero Acceptance Number Sampling Plans, Fifth Edition",
          "url": "https://www.amazon.com/Acceptance-Number-Sampling-Plans-Fifth/dp/0873897390",
          "excerpts": [
            "C=0 should be a consideration for all companies seeking a simpler alternative to the old Mil Std 105e, current Z 1.4. Its plans eliminate acceptance on anything ..."
          ]
        },
        {
          "title": "Acceptance Sampling Standards and AQL",
          "url": "https://www.asq104.org/app/download/555762704/Acceptance+Sampling+D+OLeary+20110216.pdf",
          "excerpts": [
            "ANSI/ASQ Z1.4 Sampling Procedures and Tables for Inspection By",
            "Attributes"
          ]
        }
      ],
      "reasoning": "The finegrained field describes a statistical sampling framework that uses Acceptance Quality Limit (AQL) sampling aligned with standards such as ANSI/ASQ Z1.4 and ISO 2859-1, defines AQL values as the worst tolerable process average for non-conforming items, and evaluates plan performance with Operating Characteristic (OC) curves. It also discusses alternative plans like zero-acceptance (c=0), which have smaller sample sizes and tougher defect tolerance. The most directly supportive information shows: AQL and LTPD concepts as core descriptors of sampling plans; explicit values and relationships (for example, an AQL value such as 4.0% and the notion that OC curves balance Producer's Risk and Consumer's Risk); explicit references to Z1.4 as the standard used; and the comparison to c=0 plans with concrete sampling implications. In particular, one excerpt states that AQL and LTPD are used to summarize OC curves and define plan metrics, which directly anchors the field value's description of the framework. Another excerpt explicitly mentions a 2% AQL and an 8% LTPD as the AQL/LTPD pair for a given sampling setup, illustrating how these metrics define acceptable performance and the operating characteristics of the sampling plan. A separate excerpt confirms ANSI/ASQ Z1.4 as a standard reference for sampling procedures, reinforcing the standardization context described in the field value. Collectively, these excerpts substantiate the core elements of the field value: the use of AQL-based sampling per recognized standards (Z1.4/ISO 2859-1), the OC-curve framework to evaluate plan performance, and the existence of alternative plans such as c=0 with its distinct sampling implications. The supporting details about C=0 and the general idea of planning sample sizes in relation to defect tolerance further reinforce the described framework, even when not providing exact numerical values for every scenario. Together, they align with and substantiate the described quality assurance framework focused on AQL-based sampling and OC curves under recognized standards.",
      "confidence": "high"
    },
    {
      "field": "data_acquisition_plan.cost_implication_summary",
      "citations": [
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)",
            "### Basic\n\n$200 /month\n\nFor hobbyists and prototypes\n\n#### What's included:\n\n1 project\n\n2 apps / project\n\n15,000 posts / month (reads)\n\n50,000 posts / month (writes)\n\nFull v2 endpoints access](https://developer.x.com/en/portal/products)"
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data.",
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ..."
          ]
        },
        {
          "title": "Meltwater Pricing & Costs",
          "url": "https://www.meltwater.com/en/pricing",
          "excerpts": [
            "Learn more about our Meltwater pricing model, packaging, and costs to find your personal solution. Build your personalized Meltwater suite today!"
          ]
        },
        {
          "title": "TwitterAPI.io",
          "url": "https://twitterapi.io/",
          "excerpts": [
            "Cost per 1K tweets, $0.15, $100+",
            "Rate Limit, 1000+ req/sec, 300 req/15min"
          ]
        },
        {
          "title": "Hashtag tracker pricing plan by TrackMyHashtag",
          "url": "https://www.trackmyhashtag.com/pricing",
          "excerpts": [
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ...",
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ..."
          ]
        }
      ],
      "reasoning": "The target field describes specific cost implications of using various API access options, including high-priced plans and potential ROI concerns, along with a recommended hybrid approach to reduce cost per tweet. The most directly supportive excerpts state concrete monthly pricing for API access models (for example, Pro Tier at a fixed monthly cost for a large quota) and a later Enterprise tier with a much higher monthly price and revenue-sharing considerations, which align with the described cost-structure and ROI worries. Additional excerpts corroborate the high cost of API access and the potential need to balance cheaper providers as part of a hybrid strategy, reinforcing the overall cost-optimization narrative. Several excerpts provide explicit price points or pricing models, which directly underpin the analysis of cost implications and ROI that the finegrained field describes. Collectively, these excerpts substantiate the claims about significant ongoing costs, tiered pricing, and strategic approaches to manage expenditure for a large-scale tweet dataset project.",
      "confidence": "high"
    },
    {
      "field": "quality_assurance_framework.target_quality_level",
      "citations": [
        {
          "title": "Binomial proportion confidence interval - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval",
          "excerpts": [
            "The Clopper–Pearson interval is an early and very common method for calculating binomial confidence intervals. This is often called an 'exact' method, as it ..."
          ]
        },
        {
          "title": "Selecting Statistically Valid Sampling Plans - Taylor Enterprises",
          "url": "https://variation.com/selecting-statistically-valid-sampling-plans/",
          "excerpts": [
            "OC curves are generally summarized by two numbers: the Acceptable Quality Level (AQL) and Lot Tolerance Percent Defective (LTPD). The AQL is that percent ..."
          ]
        },
        {
          "title": "Acceptance Sampling - CQE Academy",
          "url": "https://cqeacademy.com/cqe-body-of-knowledge/product-process-control/acceptance-sampling/",
          "excerpts": [
            "If we look at this OC Curve above, we can tell that the AQL associated with this sampling plan is 2%, and the LTPD for this sampling plan is 8%."
          ]
        },
        {
          "title": "Acceptance Sampling Standards and AQL",
          "url": "https://www.asq104.org/app/download/555762704/Acceptance+Sampling+D+OLeary+20110216.pdf",
          "excerpts": [
            "Attributes",
            "ANSI/ASQ Z1.4 Sampling Procedures and Tables for Inspection By"
          ]
        },
        {
          "title": "Zero Acceptance Number Sampling Plans, Fifth Edition",
          "url": "https://www.amazon.com/Acceptance-Number-Sampling-Plans-Fifth/dp/0873897390",
          "excerpts": [
            "C=0 should be a consideration for all companies seeking a simpler alternative to the old Mil Std 105e, current Z 1.4. Its plans eliminate acceptance on anything ..."
          ]
        }
      ],
      "reasoning": "The target field describes a quality-assurance goal to ensure that at least 95% of final data rows meet all constraints, verified via acceptance sampling plans, with a binomial confidence interval used to demonstrate a 95% pass rate at 95% confidence and a sample size of 59. The most directly supporting information is that binomial confidence intervals are a standard method for estimating the true pass-rate with a given confidence, and that such interval methods are commonly used in conjunction with sampling-based quality verification. Additional support comes from standard sampling theory elements, such as OC curves and AQL concepts, which outline how sampling plans determine when a batch is accepted or rejected based on observed defects or pass criteria. Together, these excerpts show a coherent framework where a 95% confidence interval for a pass rate is calculated within a predefined sampling plan (AQL/OC curve context) to justify the required sample size and observed reliability. The excerpts describing AQL, OC curves, and acceptance-sampling standards provide the necessary background to understand and implement the stated reliability and sampling requirements, reinforcing that the 95% pass-rate goal can be evaluated through established sampling procedures.",
      "confidence": "medium"
    },
    {
      "field": "data_acquisition_plan.alternative_providers",
      "citations": [
        {
          "title": "TwitterAPI.io",
          "url": "https://twitterapi.io/",
          "excerpts": [
            "Cost per 1K tweets, $0.15, $100+",
            "Rate Limit, 1000+ req/sec, 300 req/15min"
          ]
        },
        {
          "title": "Hashtag tracker pricing plan by TrackMyHashtag",
          "url": "https://www.trackmyhashtag.com/pricing",
          "excerpts": [
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ...",
            "$99.99 For upto 25,000 Tweets · All time Historical X (Twitter) Data since 2006 · Request data for #hashtag, keyword or @account · Data available in CSV, Excel and ..."
          ]
        },
        {
          "title": "Meltwater Pricing & Costs",
          "url": "https://www.meltwater.com/en/pricing",
          "excerpts": [
            "Learn more about our Meltwater pricing model, packaging, and costs to find your personal solution. Build your personalized Meltwater suite today!"
          ]
        },
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data.",
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ..."
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)",
            "### Basic\n\n$200 /month\n\nFor hobbyists and prototypes\n\n#### What's included:\n\n1 project\n\n2 apps / project\n\n15,000 posts / month (reads)\n\n50,000 posts / month (writes)\n\nFull v2 endpoints access](https://developer.x.com/en/portal/products)"
          ]
        }
      ],
      "reasoning": "To assess the finegrained field value about selecting cost-effective third-party data providers, I focus on excerpts that give concrete pricing or economic arguments for alternative sources. The most directly relevant excerpt states the price per 1K tweets for TwitterAPI.io as $0.15, which supports the central claim of a cheap, scalable option, and its assertion of a substantial data-volume increase at the same cost as an official Pro plan aligns with the field value's '33x increase in data volume' assertion. Excerpts describing TrackMyHashtag pricing provide concrete alternative data purchase costs, supporting the idea that non-major providers offer affordable access for historical or targeted data. Meltwater pricing provides an additional lens on a larger enterprise-oriented vendor, illustrating that some incumbents exist but may be more expensive, matching the field value's note that some enterprise solutions (like Meltwater) are likely too expensive for the task. Other excerpts discuss the broader pricing terrain of API access and paywall changes, which contextualize why cost-effective third-party options matter and why the field value emphasizes rivals to the official API. Collectively, these excerpts substantiate the core idea that there are multiple non-major providers with varying pricing, and one instance (TwitterAPI.io) explicitly confirms a low per-unit cost and high data volume, aligning with the field value's claim of cost-effective alternatives.",
      "confidence": "high"
    },
    {
      "field": "data_acquisition_plan.primary_api_option",
      "citations": [
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)"
          ]
        },
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ...",
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data."
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpt explicitly lists a price of $5,000 per month and describes features aligned with the pro-tier offering, including access to the full-archive search capability and the volumes related to reads/writes, which supports the field value's claim about the Pro Tier and its capabilities. Additional excerpts discuss related pricing and paywall context, such as the mention of paywalls and a Full Archive Search API being capped, which provides context about pricing dynamics and the availability of the full-archive endpoint, reinforcing how such a tier operates within the pricing ecosystem. Another excerpt contrasts pricing at a much higher rate, illustrating that multiple price points exist for API access and that $5,000 per month is a plausible mid-high tier in this ecosystem. Collectively, these excerpts support the existence and characteristics of a Pro Tier with full-archive search access and a $5,000 monthly price, with contextual references that help explain its relevance and positioning versus other pricing options.",
      "confidence": "medium"
    },
    {
      "field": "discovered_data_sources.1",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The most directly supportive information is that there is a Kaggle dataset titled 'The Tweets of Wisdom' containing data from multiple authors and described as a collection of wise words from Twitter, indicating a public dataset on Kaggle about Twitter content. This aligns with the field value's note that the platform is Kaggle and that the dataset is intended for exploring virality features. Additional supporting details state that tweets and retweets from multiple authors were scraped and stored, which corroborates the field value's implication that the dataset includes tweet text, author information, and engagement-relevant metadata. A related excerpt also references scraping the tweets to facilitate exploration of these authors' words, further reinforcing the nature of a tweet-focused dataset on Kaggle used for analysis. Collectively, these excerpts establish the provenance (Kaggle), the subject (Twitter tweets), and the type of metadata (authors, tweets/retweets) that the field value describes.",
      "confidence": "high"
    },
    {
      "field": "discovered_data_sources.0",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The core finegrained field value describes a dataset named 'The Tweets of Wisdom' hosted on Kaggle/GitHub, consisting of thousands of self-help or 'wise' tweets from a sizeable pool of authors, with engagement metrics available for filtering by viral criteria. Directly supporting content includes: multiple excerpts explicitly reference the dataset and its scope, such as the dataset containing many authors and being a collection of wise words from Twitter, or noting that retweets from multiple authors are stored as tweets from the original author, and that the data is scraped to expose the tweets for analysis. These passages also point to the existence of engagement-related data (likes/retweets) and the dataset being sourced from Kaggle. Collectively, these excerpts corroborate the dataset's existence, scope (dozens of authors), content nature (self-help/wise quotes), and the presence of engagement metrics, which aligns with the finegrained field value's description of viral potential and metadata availability. The strongest support comes from explicit mentions of: a dataset titled The Tweets of Wisdom, its Kaggle source, mention of more than 40 authors, and a collection of wise words from Twitter. Additional excerpts reiterate that the data was scraped to enable exploration of these words and their dissemination, which reinforces the dataset's suitability for viral analysis.",
      "confidence": "high"
    },
    {
      "field": "quality_assurance_framework.improvement_process",
      "citations": [
        {
          "title": "Building custom NLP tools to annotate discourse-functional ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2772766124000594",
          "excerpts": [
            "by M Eguchi · 2024 · Cited by 5 — When it comes to developing an ML system, the goal of annotation is to construct a “gold-standard” annotation dataset, which provides consistent input-output ..."
          ]
        },
        {
          "title": "PARSEME Shared Task 1.3 - Annotation guidelines",
          "url": "https://parsemefr.lis-lab.fr/parseme-st-guidelines/1.3/?page=040_Annotation_process_-_decision_tree",
          "excerpts": [
            "The decision tree below indicates the order in which tests should be applied in step 3. The decision trees are a useful summary to consult ..."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "by ML McHugh · 2012 · Cited by 21858 — As a general heuristic, sample sizes should not consist of less than 30 comparisons. Sample sizes of 1,000 or more are mathematically most likely to produce ...",
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            " However, this interpretation allows for very little agreement among raters to be described as “substantial”.",
            "For percent agreement, 61% agreement can immediately be seen as problematic.",
            "Almost 40% of the data in the dataset represent faulty data.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "The fine-grained field describes a continuous improvement loop for a QA framework, emphasizing post-hoc error analysis, feedback integration into annotation guidelines and training materials, enhancement of automated validation tools, a final sign-off checklist, and monitoring dashboards to track quality trends. Excerpt content that directly supports this kind of QA workflow includes: (a) discussions of building and using an annotated dataset and establishing a gold-standard for annotation, which underpins a robust QA feedback loop and clear training material improvements; (b) descriptions of annotation processes and decision trees that guide consistent labeling, enabling systematic error analysis and guideline refinement; and (c) extensive coverage of interrater reliability concepts (e.g., Cohen's kappa) that are used to assess and improve agreement among annotators, a key component of monitoring quality and informing corrective actions. Together, these excerpts map to the core components of the target field value: capability to perform error analysis after annotation, update guidelines and training based on findings, apply automated validation where possible, enforce a final compliance step, and monitor quality over time. The strongest alignment is with passages that emphasize (i) creating and maintaining a gold-standard annotation dataset and (ii) formalizing annotation processes and decision rules, which together enable reliable error diagnosis and targeted improvements. The reliability-focused excerpts further support the notion of ongoing quality assessment and calibration across annotators, a foundational element of any continuous QA improvement loop.",
      "confidence": "medium"
    },
    {
      "field": "project_timeline_and_budget.primary_cost_driver",
      "citations": [
        {
          "title": "Twitter's $42000-per-Month API Prices Out Nearly Everyone",
          "url": "https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/",
          "excerpts": [
            "Tiers will start at $500,000 a year for access to 0.3 percent of the company's tweets. Researchers say that's too much for too little data.",
            "Mar 10, 2023 — On February 2, Musk announced API access would go behind a paywall in a week. ... Full Archive Search API will be capped at 50,000. The number of ..."
          ]
        },
        {
          "title": "X API Pricing - Social Media Today (June 2025)",
          "url": "https://www.socialmediatoday.com/news/x-formerly-twitter-reforms-api-pricing-revenue-share-model/750025/",
          "excerpts": [
            "X recently began sending out emails to paid subscribers of its Enterprise API plans, which start at $42,000 per month, informing them of the upcoming change.",
            "The new API pricing scheme is scheduled to go into effect on July 1.",
            "X has not yet shared final details about the change, such as exactly what percentage the revenue share model will be, with its customers."
          ]
        },
        {
          "title": "X is changing how it charges for API access | Mashable",
          "url": "https://mashable.com/article/elon-musk-x-twitter-api-revenue-share-model-change",
          "excerpts": [
            "The X API costs developers $42K per month. Now X wants a cut of their revenue instead. Third-party developers using the X API were informed that ..."
          ]
        },
        {
          "title": "X Enterprise API Pricing and Data Access",
          "url": "https://developer.x.com/en/products/x-api/enterprise/enterprise-api-interest-form",
          "excerpts": [
            "For self service access, please explore our Basic or Pro API access tiers. ... Enterprise API pricing starts at $42,000 / Month, based on usage and needs.See more"
          ]
        },
        {
          "title": "X API Documentation and Pricing",
          "url": "https://docs.x.com/x-api",
          "excerpts": [
            "### Basic\n\n$200 /month\n\nFor hobbyists and prototypes\n\n#### What's included:\n\n1 project\n\n2 apps / project\n\n15,000 posts / month (reads)\n\n50,000 posts / month (writes)\n\nFull v2 endpoints access](https://developer.x.com/en/portal/products)",
            "### Pro\n\n$5,000 /month\n\nFor startups scaling their business\n\n#### What's included:\n\n1 project\n\n3 apps / project\n\n1,000,000 posts / month (reads)\n\n300,000 posts / month (writes)\n\n[Full-archive search](/x-api/posts/search/introduction) access\n\n[Filtered stream](/x-api/posts/filtered-stream/introduction) access\n\nPriority support](https://developer.x.com/en/portal/products)",
            "X API v2 gives you programmatic access to X's global conversation—posts, users, spaces, DMs, lists, trends, media, and more. Build apps, analyze data, and ...",
            "X API v2 gives you programmatic access to X's global conversation—posts, users, spaces, DMs, lists, trends, media, and more. Build apps, analyze data, ..."
          ]
        },
        {
          "title": "awesome-twitter-data",
          "url": "https://github.com/shaypal5/awesome-twitter-data",
          "excerpts": [
            "\nA list of Twitter datasets and related resources, released under CC0 . If you have a resource to add to the list, feel free to open a pull request, or email me at shay.palachy@gmail.com . The license, when known, is given in {curly brackets}. Dataset size is given in [square brackets] when available. 1   Twitter Datasets"
          ]
        }
      ],
      "reasoning": "The field specifies that the single largest expense for the project is data acquisition via the X API, highlighting the need for full archival access and an enterprise-tier subscription with a high monthly price. Excerpts that report a monthly price of 42,000 for the enterprise API directly substantiate the stated cost driver. Excerpts describing the existence of an enterprise pricing model and the required capability to access the full historical archive further corroborate the main claim. Supporting excerpts that detail lower-tier pricing (Basic and Pro) offer necessary context for the pricing ecosystem but do not directly establish the primary expense, whereas excerpts referencing the enterprise tier and its high monthly cost provide the strongest alignment with the finegrained field value. Taken together, the most compelling evidence centers on the enterprise-tier pricing and its requirement for full-archive data access, with secondary context from related pricing discussions.",
      "confidence": "high"
    },
    {
      "field": "discovered_data_sources.2",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The most relevant content points to a Kaggle collection related to tweets. For instance, the discussion describes a dataset titled The Tweets of Wisdom hosted on Kaggle, noting that the data includes numerous authors and that tweets (and their retweets) are part of the collection, which aligns with the idea of a structured dataset of tweet content and metadata. This supports the notion of a Kaggle dataset containing tweet text and engagement-related information, which is central to the finegrained field value's emphasis on like and share metrics. Additional excerpts reinforce this Kaggle-centric framing by explicitly naming Kaggle as the data source and describing the aggregation of tweet content from multiple authors, further corroborating the existence and structure of a tweet-focused dataset on Kaggle suitable for viral analysis. Some excerpts extend the context to discuss self-contained tweet collections and data scraping, which aligns with the broader concept of a dataset that aggregates tweet texts along with engagement signals, even if the exact mention of like/share counts is not repeated in every excerpt. Taken together, these excerpts most directly support the field value's claim about a Kaggle Tweets Dataset and its relevance to filtering by engagement, followed by excerpts that mention related datasets or sources that are tangentially connected to tweet data aggregation.",
      "confidence": "medium"
    },
    {
      "field": "legal_and_platform_compliance_summary.recommended_dataset_release_format",
      "citations": [
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "If you need to share X content you obtained via the X APIs with another party, the best way to do so is by sharing Post IDs, Direct Message IDs, and/or User IDs, which the end user of the content can then rehydrate (i.e. request the full Post, User, or Direct Message content) using the X APIs.",
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X."
          ]
        },
        {
          "title": "[PDF] An archive and corpus of Twitter/X's policies for Tweet redistribution ...",
          "url": "https://access.gesis.org/sharing/2847/6568",
          "excerpts": [
            "When researchers publish research based on the analysis of Tweets, good practice requires sharing the Tweets. (or Tweet IDs) to enable ..."
          ]
        }
      ],
      "reasoning": "The field value envisions a compliant data release format that discloses only identifiers (Tweet IDs and User IDs) and provides explicit instructions for recipients to rehydrate the full content via the X API, along with a compliance notice. The most directly supporting excerpts state that: shareable X content can be distributed via Post IDs, Direct Message IDs, and/or User IDs, which the recipient can use to rehydrate the full content using the X APIs. This aligns with the idea of releasing only IDs and providing steps to retrieve full data on demand. Additional support comes from the policy note that there are redistribution limitations tied to IDs and the process by which recipients can rehydrate the content, reinforcing that the IDs themselves are the appropriate release unit, not the full text. A related excerpt further corroborates this approach by discussing that sharing IDs (Post IDs and User IDs) is permissible for research or redistributive purposes under the platform's terms, which matches the proposed format. Together, these excerpts corroborate a legal and practical pathway for releasing a dataset as a list of IDs accompanied by rehydration instructions and compliance notices.",
      "confidence": "high"
    },
    {
      "field": "legal_and_platform_compliance_summary.rehydration_requirement",
      "citations": [
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "If you need to share X content you obtained via the X APIs with another party, the best way to do so is by sharing Post IDs, Direct Message IDs, and/or User IDs, which the end user of the content can then rehydrate (i.e. request the full Post, User, or Direct Message content) using the X APIs.",
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X."
          ]
        },
        {
          "title": "[PDF] An archive and corpus of Twitter/X's policies for Tweet redistribution ...",
          "url": "https://access.gesis.org/sharing/2847/6568",
          "excerpts": [
            "When researchers publish research based on the analysis of Tweets, good practice requires sharing the Tweets. (or Tweet IDs) to enable ..."
          ]
        },
        {
          "title": "Terms of Service",
          "url": "https://x.com/en/tos",
          "excerpts": [
            "You must abide by the Services’ acceptable use terms:** You may not access the Services in any way other than through the currently available, published interfaces that we provide. For example, this means that you cannot scrape the Services without X’s express written permission, try to work around any technical limitations we impose, or otherwise attempt to disrupt the operation of the Service",
            " Liquidated Damages\nProtecting our users’ data and our system resources is important to us. You further agree that, to the extent permitted by applicable law, if you violate the Terms, or you induce or facilitate others to do so, in addition to all other legal remedies available to us, you will be jointly and severally liable to us for liquidated damages as follows for requesting, viewing, or accessing more than 1,000,000 posts (including reply posts, video posts, image posts, and any other posts) in any 24-hour period - $15,000 USD per 1,000,000 posts. You agree that these amounts are (i) a reasonable estimate of our damages; (ii) not a penalty; and (iii) not otherwise limiting of our ability to recover from you or others under any legal or equitable theory or claim, including but not limited to statutory damages and/or equitable relief. You further agree that repeated violations of these Terms will irreparably harm and entitle us to injunctive and/or other equitable relief, in addition to monetary damages."
          ]
        }
      ],
      "reasoning": "The most directly relevant content explicitly describes the practice of sharing only IDs (Post IDs, Direct Message IDs, and/or User IDs) when redistributing X content, which aligns with the concept of rehydration where recipients request the full tweet objects via the API. The wording indicates that the correct workflow is to give IDs so the end user can rehydrate content through X APIs, which echoes the field value's assertion that rehydration is mandatory for academic/research use. The accompanying excerpt reinforces a numeric limit on distributing IDs to a single entity within a time window, further grounding the context in controlled ID-based redistribution rather than circulating full offline copies. A policy-focused excerpt about keeping offline copies current and the need to update or remove stored content within a defined timeframe aligns with the broader principle of rehydration reflecting current platform state, thereby supporting the idea that you should rely on API-driven retrieval to reflect deletions or privacy changes. Additional excerpts about Terms of Service or anti-scraping provide broader governance but do not directly substantiate the rehydration workflow, though they offer contextual safeguards around data access and use.",
      "confidence": "high"
    },
    {
      "field": "legal_and_platform_compliance_summary.data_collection_policy",
      "citations": [
        {
          "title": "X updates its terms to ban crawling and scraping",
          "url": "https://techcrunch.com/2023/09/08/x-updates-its-terms-to-ban-crawling-and-scraping/",
          "excerpts": [
            "Sep 8, 2023 — The new terms, which are effective from September 29, ban any kind of scraping or crawling without “prior written consent.” I"
          ]
        },
        {
          "title": "X Updates its Terms, Bans Data Scraping& Crawling",
          "url": "https://nftnow.com/news/x-updates-terms-of-service-to-ban-unauthorized-data-crawling-scraping/",
          "excerpts": [
            "Sep 8, 2023 — X, formerly known as Twitter, updated its Terms of Service to include the prohibition of unauthorized data crawling and scraping."
          ]
        },
        {
          "title": "Terms of Service",
          "url": "https://x.com/en/tos",
          "excerpts": [
            " Liquidated Damages\nProtecting our users’ data and our system resources is important to us. You further agree that, to the extent permitted by applicable law, if you violate the Terms, or you induce or facilitate others to do so, in addition to all other legal remedies available to us, you will be jointly and severally liable to us for liquidated damages as follows for requesting, viewing, or accessing more than 1,000,000 posts (including reply posts, video posts, image posts, and any other posts) in any 24-hour period - $15,000 USD per 1,000,000 posts. You agree that these amounts are (i) a reasonable estimate of our damages; (ii) not a penalty; and (iii) not otherwise limiting of our ability to recover from you or others under any legal or equitable theory or claim, including but not limited to statutory damages and/or equitable relief. You further agree that repeated violations of these Terms will irreparably harm and entitle us to injunctive and/or other equitable relief, in addition to monetary damages.",
            "You must abide by the Services’ acceptable use terms:** You may not access the Services in any way other than through the currently available, published interfaces that we provide. For example, this means that you cannot scrape the Services without X’s express written permission, try to work around any technical limitations we impose, or otherwise attempt to disrupt the operation of the Service"
          ]
        },
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X."
          ]
        },
        {
          "title": "GET statuses/home_timeline | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/x-api/v1/tweets/timelines/api-reference/get-statuses-home_timeline",
          "excerpts": [
            "X API v2 ... The value of count is best thought of as a limit to the number of tweets to return because suspended or deleted content is removed after the count ..."
          ]
        },
        {
          "title": "[PDF] An archive and corpus of Twitter/X's policies for Tweet redistribution ...",
          "url": "https://access.gesis.org/sharing/2847/6568",
          "excerpts": [
            "When researchers publish research based on the analysis of Tweets, good practice requires sharing the Tweets. (or Tweet IDs) to enable ..."
          ]
        },
        {
          "title": "Data Collection with Twitter API v2",
          "url": "https://www.kaggle.com/code/andrewedward37/data-collection-with-twitter-api-v2",
          "excerpts": [
            "In this article, I will go through a step-by-step process from setting up, accessing endpoints, to saving tweets collected in CSV format to use for analysis in ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly address the core compliance requirements for data collection on X. The excerpts that announce bans on crawling and scraping clarify that such activities are prohibited without prior written consent, which aligns with the field value's stance on restricted data acquisition through official channels. Additional excerpts reinforce that access to X content should be via official means, not by bypassing interfaces, which supports the emphasis on using the X API. The excerpt detailing liquidated damages specifies a monetary penalty for excessive, unauthorized post retrieval, which directly corroborates the claim of significant financial risk for noncompliant data collection. Further supporting material notes the prohibition on scraping and crawling and references to terms of service updates that formalize these restrictions, strengthening the alignment with API-centric data collection. An excerpt discussing API endpoints (e.g., X API v2) and standard API usage provides concrete confirmation that compliant data acquisition should rely on official interfaces rather than scraping. Additional context on redistribution limits and related policy documents adds depth to the policy environment governing data collection on the platform.",
      "confidence": "high"
    },
    {
      "field": "legal_and_platform_compliance_summary.content_redistribution_policy",
      "citations": [
        {
          "title": "Developer Policy – X Developers",
          "url": "http://developer.x.com/en/developer-terms/policy",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs).",
            "Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research. Only as many Post IDs or User IDs that is necessary for such research, and has been approved by X may be used.",
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties.**If you provide X Content to third parties, including downloadable datasets or via an API, you may only distribute Post IDs, Direct Message IDs, and/or User IDs (except as described below)",
            "Your license agreement with X limits your use of the X API and X Content. Among other things, the X API has rate limits which help to ensure fair data usage and to combat spam on the platform."
          ]
        },
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X.",
            "If you need to share X content you obtained via the X APIs with another party, the best way to do so is by sharing Post IDs, Direct Message IDs, and/or User IDs, which the end user of the content can then rehydrate (i.e. request the full Post, User, or Direct Message content) using the X APIs."
          ]
        },
        {
          "title": "Terms of Service",
          "url": "https://x.com/en/tos",
          "excerpts": [
            "You must abide by the Services’ acceptable use terms:** You may not access the Services in any way other than through the currently available, published interfaces that we provide. For example, this means that you cannot scrape the Services without X’s express written permission, try to work around any technical limitations we impose, or otherwise attempt to disrupt the operation of the Service",
            " Liquidated Damages\nProtecting our users’ data and our system resources is important to us. You further agree that, to the extent permitted by applicable law, if you violate the Terms, or you induce or facilitate others to do so, in addition to all other legal remedies available to us, you will be jointly and severally liable to us for liquidated damages as follows for requesting, viewing, or accessing more than 1,000,000 posts (including reply posts, video posts, image posts, and any other posts) in any 24-hour period - $15,000 USD per 1,000,000 posts. You agree that these amounts are (i) a reasonable estimate of our damages; (ii) not a penalty; and (iii) not otherwise limiting of our ability to recover from you or others under any legal or equitable theory or claim, including but not limited to statutory damages and/or equitable relief. You further agree that repeated violations of these Terms will irreparably harm and entitle us to injunctive and/or other equitable relief, in addition to monetary damages."
          ]
        },
        {
          "title": "[PDF] An archive and corpus of Twitter/X's policies for Tweet redistribution ...",
          "url": "https://access.gesis.org/sharing/2847/6568",
          "excerpts": [
            "When researchers publish research based on the analysis of Tweets, good practice requires sharing the Tweets. (or Tweet IDs) to enable ..."
          ]
        }
      ],
      "reasoning": "The field value describes a policy framework where the redistribution of full tweet content and metadata to third parties is tightly constrained. A key support point is that the policy restricts distribution to only specific identifiers (Post IDs, Direct Message IDs, and/or User IDs) and imposes explicit quotas (1,500,000 Post IDs per entity in 30 days; 500 public Post Objects per person per day for non-automated sharing). Excerpted text explicitly states: \"In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X,\" and \"Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research,\" which supports the idea that full content distribution is restricted. Additionally, passages emphasize that redistribution should be done via Post IDs or similar identifiers rather than distributing full content, e.g., \"the best way to share X content you obtained via the X APIs with another party is by sharing Post IDs, Direct Message IDs, and/or User IDs,\" which aligns with the notion that full text or metadata distribution is not allowed. Other excerpts reinforce that the license and terms (not to redistribute X content to third parties, rate limits, and general terms of service) create a broader compliance framework that would conflict with producing a CSV containing full tweet text. Taken together, these excerpts directly support the fine-grained field value's assertion that distributing full tweet content and metadata would violate X's Developer Policy and the specified numeric limits and sharing constraints, and that the user's requested CSV (containing full tweets) would be non-compliant.",
      "confidence": "high"
    },
    {
      "field": "legal_and_platform_compliance_summary.ai_model_training_restriction",
      "citations": [
        {
          "title": "Terms of Service",
          "url": "https://x.com/en/tos",
          "excerpts": [
            "You must abide by the Services’ acceptable use terms:** You may not access the Services in any way other than through the currently available, published interfaces that we provide. For example, this means that you cannot scrape the Services without X’s express written permission, try to work around any technical limitations we impose, or otherwise attempt to disrupt the operation of the Service",
            " Liquidated Damages\nProtecting our users’ data and our system resources is important to us. You further agree that, to the extent permitted by applicable law, if you violate the Terms, or you induce or facilitate others to do so, in addition to all other legal remedies available to us, you will be jointly and severally liable to us for liquidated damages as follows for requesting, viewing, or accessing more than 1,000,000 posts (including reply posts, video posts, image posts, and any other posts) in any 24-hour period - $15,000 USD per 1,000,000 posts. You agree that these amounts are (i) a reasonable estimate of our damages; (ii) not a penalty; and (iii) not otherwise limiting of our ability to recover from you or others under any legal or equitable theory or claim, including but not limited to statutory damages and/or equitable relief. You further agree that repeated violations of these Terms will irreparably harm and entitle us to injunctive and/or other equitable relief, in addition to monetary damages."
          ]
        },
        {
          "title": "X updates its terms to ban crawling and scraping",
          "url": "https://techcrunch.com/2023/09/08/x-updates-its-terms-to-ban-crawling-and-scraping/",
          "excerpts": [
            "Sep 8, 2023 — The new terms, which are effective from September 29, ban any kind of scraping or crawling without “prior written consent.” I"
          ]
        },
        {
          "title": "Developer Agreement and Policy - Twitter Developers - X",
          "url": "https://developer.x.com/en/developer-terms/agreement-and-policy",
          "excerpts": [
            "**. Notwithstanding anything to the contrary in this Agreement, to the extent you are provided access to the Licensed Material pursuant to the procedures described in Article 40 of the Digital Services Act (Regulation (EU) 2022/2065) (“DSA”), your access and use of the Licensed Material is limited solely to performing research that contributes to the detection, identification, and understanding of systemic risks in the European Union and only to the extent necessary for X to comply with its obligations under the DSA.",
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs). Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research."
          ]
        },
        {
          "title": "Developer Policy – X Developers",
          "url": "http://developer.x.com/en/developer-terms/policy",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs).",
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties.**If you provide X Content to third parties, including downloadable datasets or via an API, you may only distribute Post IDs, Direct Message IDs, and/or User IDs (except as described below)"
          ]
        },
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X.",
            "If you need to share X content you obtained via the X APIs with another party, the best way to do so is by sharing Post IDs, Direct Message IDs, and/or User IDs, which the end user of the content can then rehydrate (i.e. request the full Post, User, or Direct Message content) using the X APIs."
          ]
        }
      ],
      "reasoning": "The targeted field is asking for a specific, explicit prohibition on using platform data to train large-scale AI models within the X Developer Agreement. The most directly relevant material would be clauses that state restrictions on how content can be used, shared, or repurposed, particularly with respect to training or reusing data for model development. The following excerpts discuss The Terms of Service and policy language that restrict data use and redistribution, and they stress limits on how X content can be accessed, redistributed, or scraped, which are the closest publicly documented boundaries around using platform data. For example, there are explicit reminders that X Content should be obtained directly from X and that redistribution to third parties is constrained, including language about rate limits and the prohibition of unauthorized data access or scraping. These statements establish a framework that constrains how data can be used and shared, which is relevant to whether a training-use ban could be inferred or is explicitly stated. However, none of the excerpts clearly state an explicit ban on using platform data to train large AI models or to fine-tune foundation or frontier models in unequivocal terms. The strongest connecting evidence is the general emphasis on restricted use, scraping bans, and controlled redistribution of X Content, which can support an interpretation that training-use prohibitions might exist within the broader policy constraints, but the explicit wording of a training ban is not directly present in these excerpts. Consequently, while the excerpts reinforce a context of restricted data use, they do not provide unambiguous, explicit language that fully confirms the exact claimed prohibition on training large-scale AI models. In light of that, the most direct support comes from policy and terms-of-service excerpts that address permissible use and data sharing practices, rather than a verbatim statement about training prohibition.\n",
      "confidence": "low"
    },
    {
      "field": "discovered_data_sources.0.platform",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The finegrained field value indicates the data source platform is Kaggle or GitHub. Several excerpts explicitly reference Kaggle as the source of the dataset (for example, the dataset titled 'The Tweets of Wisdom' is hosted on Kaggle, and descriptions mention 'https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom'). These excerpts directly support the presence of Kaggle as a data source platform. One excerpt also notes the data collection context around multiple authors and Twitter content, which aligns with a Kaggle-hosted dataset that aggregates tweets. There is no explicit mention of GitHub in these excerpts, so the 'GitHub' portion of the field value is not evidenced, but the 'Kaggle' portion is clearly supported by the text. The remaining excerpts either reiterate the Kaggle reference in a similar way or provide peripheral context about the dataset contents without adding platform information beyond Kaggle. Given this, the most relevant excerpts are those that clearly identify Kaggle as the source, followed by ones that describe the dataset contents and authors in a way that corroborates the Kaggle-hosted data. The least directly supportive excerpts are the ones that do not clearly add new platform information or mention GitHub.",
      "confidence": "medium"
    },
    {
      "field": "discovered_data_sources.0.source_name",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The specific field value points to the source name of the first discovered data source, which is the dataset title \"The Tweets of Wisdom.\" The most direct support comes from excerpts that explicitly name this dataset as a titled collection and describe its nature as wisdom or self-help-style tweets obtained from Twitter. Excerpt describing a dataset with the title \"The Tweets of Wisdom\" and noting it as a collection of wise words from Twitter directly corroborates the field value. Additional excerpts corroborate the same dataset concept by discussing scraping or collecting tweets from multiple authors and framing them as a compendium of wisdom, which reinforces the existence and scope of that dataset, though they may be slightly less explicit about the exact title. Taken together, the excerpts consistently support that the source_name is the dataset known as \"The Tweets of Wisdom.\"",
      "confidence": "high"
    },
    {
      "field": "discovered_data_sources.2.platform",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The discovered field asserts that the platform is Kaggle. The most direct support comes from excerpts that explicitly identify Kaggle as the source or host of the dataset: one excerpt provides a Kaggle dataset URL and describes the data as a collection sourced from Kaggle, directly tying the platform to the dataset. Another excerpt repeats the Kaggle dataset title and confirms the same source. Additional excerpts mention that the data contains multiple authors and relate to the same dataset collection, reinforcing Kaggle as the hosting platform. The remaining excerpts describe actions around the dataset (scraping tweets, exploring words) but still reference the same dataset collection and context, which aligns with Kaggle as the platform hosting or distributing the dataset. Taken together, these sources consistently corroborate that Kaggle is the platform associated with the discovered data sources field value. ",
      "confidence": "high"
    },
    {
      "field": "discovered_data_sources.1.platform",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter"
          ]
        }
      ],
      "reasoning": "To support the field value 'Kaggle' at the path discovered_data_sources.1.platform, look for excerpts that directly identify Kaggle as the source or platform. The excerpt stating the Kaggle URL for the dataset provides a clear, explicit link to Kaggle as the hosting platform. Another excerpt explicitly references the dataset title and its association with Kaggle, reinforcing Kaggle as the source. These lines confirm the platform for the second data source entry in the discovered_data_sources array. Other excerpts discuss the dataset content but do not mention Kaggle, so they offer partial context but do not directly validate the field value.",
      "confidence": "high"
    },
    {
      "field": "discovered_data_sources.1.source_name",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The field value asserts a focus on classifying viral tweets from Twitter. Excerpts that explicitly reference Twitter as the source of the collected content, or that describe scraping or collecting tweets for analysis, are most directly supportive of this field value. The strongest support comes from wording that directly ties the dataset to Twitter data: mentioning that the collection consists of words straight from Twitter, and that tweets were scraped or gathered for exploration of the social-media content. Additional excerpts extend the context by naming the broader concept of tweets and wisdom, indicating an ongoing data-gathering effort from Twitter sources, which aligns with a Twitter-based classification theme. The least direct support comes from references to self-help tweets or generic mentions of Twitter without explicit linkage to a classification task, yet they still corroborate that Twitter is the data source involved in analyzing tweet content.",
      "confidence": "medium"
    },
    {
      "field": "discovered_data_sources.1.relevance_to_project",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better.",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte"
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The finegrained field value asserts that the public dataset is valuable for initial exploration and model testing for virality, and that the tweets have been classified by virality to aid understanding of high-engagement posts. The excerpts collectively document a Kaggle dataset titled 'The Tweets of Wisdom' and describe aspects such as multiple authors, how retweets are represented, scraping of tweets, and the intent to explore self-help content. These details support the notion that this dataset is a substantive source for early-stage analysis and for informing filtering logic, because they establish the data provenance, composition, and scraping methodology. While none of the excerpts verbatim confirms that the dataset contains pre-classified virality labels, they do reinforce that the dataset is a sizable, multi-author collection suitable for exploratory analysis of engagement patterns, which aligns with the stated purpose of using it for initial exploration and model testing. Therefore, the excerpts collectively support the idea that the dataset is valuable for our purpose, albeit with partial explicit confirmation about virality classification.",
      "confidence": "low"
    },
    {
      "field": "discovered_data_sources.0.relevance_to_project",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The finegrained field value emphasizes a dataset that aggregates wisdom/self-help content from a broad pool of authors, suitable for identifying underrated, insightful tweets and enabling viral filtering via engagement metrics. Excerpts describe: a collection attributed to more than 40 authors and a corpus of wisdom/self-help tweets sourced from Twitter, which directly supports the claims about the breadth of authors and the nature of content. They also indicate that the tweets and their neighbors include retweets, implying a data collection that could be used to assess engagement, aligning with the stated viral filtering capability. One excerpt reinforces the idea of exploring the words of these self-help tweets to understand them better, which supports the insight-oriented nature of the content. Taken together, these excerpts substantiate the key aspects of authorship breadth and content type relevant to the field value, while they do not provide explicit confirmation of the precise engagement thresholds or the exact tweet count. Therefore, the strongest support comes from the excerpts that explicitly mention multiple authors and the self-help/wise nature of the tweets, with additional partial support from mentions of tweet/retweet collection. The overarching conclusion is that the dataset described in the excerpts is consistent with the project's aim to curate an underrated, insight-driven collection from a diverse author pool, potentially enabling engagement-based filtering, though some numerical specifics remain unverified by the excerpts alone.",
      "confidence": "medium"
    },
    {
      "field": "discovered_data_sources.2.source_name",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better.",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte"
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt explicitly references a data collection of tweets from a source described as a collection of wise words from Twitter, which aligns with the notion of a dataset of tweets and implies aggregation from multiple authors. The second highly relevant excerpt notes that the data contains more than 40 authors and that retweets from those authors are stored as tweets from the original author, supporting the idea of a broad, multi-author dataset and the provenance around how tweets are represented in the dataset. An additional excerpt mentions that all tweets and retweets were scraped from the platform, which corroborates dataset construction methods and data richness (including like/share dynamics implied by \"retweets\"). Another excerpt emphasizes the ability to explore the words of these self-help tweets, reinforcing the dataset's focus on tweet content and authorial voice within a curated collection. The least directly relevant excerpt is a shorter line that reiterates the dataset's title without adding new details about authors, scraping, or data structure, but it still supports the overall context of a tweets dataset focused on wisdom or self-help content.",
      "confidence": "medium"
    },
    {
      "field": "discovered_data_sources.0.available_metadata",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The claim describes a dataset labeled 'The Tweets of Wisdom' with a structured metadata footprint (authors, handles, tweet content, likes, retweets) suitable for initial filtering and population of required fields. Excerpt describing that all tweets and retweets from about forty authors were scraped, including retweets with comments, directly supports the notion that the dataset is built from a compiled, author-centered collection and likely includes fields such as author_name, handle, tweet_content, likes, and retweets. A separate excerpt notes that the data collection contains more than forty authors and that retweets are stored as tweets from the original author, which reinforces the idea of an author-centric metadata structure and data provenance suitable for filtering. Additional excerpts reiterate that the dataset is a Kaggle-hosted collection of wisdom tweets and that scraping was performed to enable exploration of words, which aligns with having a structured dataset suitable for populating descriptive fields. The least direct, but still relevant, excerpt mentions self-help framing and general exploration of the dataset, which provides contextual support but does not directly reinforce the exact metadata fields. Overall, the strongest support comes from statements about scraping, multiple authors, and the explicit dataset name, which together underpin the claim that the CSV-like metadata (author_name, handle, tweet_content, likes, retweets) exists and is suitable for initial filtering.",
      "confidence": "medium"
    },
    {
      "field": "discovered_data_sources.1.available_metadata",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly indicates that the dataset involved includes tweets, retweets, and data scraping from multiple authors. This aligns with the idea of available metadata covering authors and tweet-level content, which is what the field value suggests datasets typically contain. The next two excerpts reinforce the metadata angle by explicitly noting multiple authors and concatenating retweet-related items, which supports the notion of author information and tweet lineage in the dataset metadata. The following excerpts mention the dataset's thematic focus on self-help or wisdom content, which, while contextually related to the data source, is less about the presence of metadata fields and more about subject matter, hence lower relevance. The least relevant excerpt mentions the collection of a general set of wisdom words, which is informative about content but provides the weakest direct connection to metadata types like tweet text, author details, and engagement counts.",
      "confidence": "medium"
    },
    {
      "field": "discovered_data_sources.2.available_metadata",
      "citations": [
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better.",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte"
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        }
      ],
      "reasoning": "The finegrained field value claims that the dataset includes tweet text and engagement metrics such as like/share counts, and that there would be details on author metadata on the dataset page. Excerpts that describe how the dataset collects or stores tweets—specifically mentioning that all tweets, retweets, and retweets with comments are scraped and that there are multiple authors—are relevant because they address data composition and potential availability of author-related metadata. The most directly relevant portion is the note about scraping all tweets and retweets (including retweets with comments) from multiple authors, since this indicates a dataset built from user-generated posts with possible inclusion of engagement-like signals in the broader data pipeline. Other excerpts reinforce the same context by describing the dataset's scope (The Tweets of Wisdom) and the multi-author nature, which underpins the likelihood of author metadata and content being present somewhere in the data or its page, even if engagement counts are not explicitly stated. Overall, the correspondence is partial: it supports the notion of a multi-author tweet corpus with broad data capture but does not explicitly confirm that likes/shares are stored within the dataset or that author metadata are present on the dataset page.",
      "confidence": "low"
    },
    {
      "field": "core_insight_summarization_methodology",
      "citations": [
        {
          "title": "What is This Article About? Extreme Summarization with ...",
          "url": "https://www.jair.org/index.php/jair/article/download/11315/26523/22081",
          "excerpts": [
            "by S Narayan · 2019 · Cited by 24 — Abstract. We introduce extreme summarization, a new single-document summarization task which aims at creating a short, one-sentence news summary answering ..."
          ]
        },
        {
          "title": "Factual consistency evaluation of summarization in the Era ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S0957417424013228",
          "excerpts": [
            "by Z Luo · 2024 · Cited by 21 — To encourage better FC in summarization, a number of automated evaluation methods have been developed including natural language inference(NLI) ...",
            "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks."
          ]
        },
        {
          "title": "How to Write a Summary | Guide & Examples",
          "url": "https://www.scribbr.com/working-with-sources/how-to-summarize/",
          "excerpts": [
            "Summarizing means giving a short overview of a source's main points in your own words. There are five key steps to writing a summary.",
            "Nov 23, 2020 — Step 1: Read the text · Step 2: Break the text down into sections · Step 3: Identify the key points in each section · Step 4: Write the summary."
          ]
        },
        {
          "title": "How to write a killer one sentence pitch (or logline) for ...",
          "url": "https://nathanbransford.com/blog/2023/05/how-to-write-killer-one-sentence-pitch-logline-novels-memoirs",
          "excerpts": [
            "May 22, 2023 — A good pitch is a specific description of what actually happens in your novel. It's a one sentence description of the plot, not the theme."
          ]
        },
        {
          "title": "Article Summaries, Reviews & Critiques - RCC Library",
          "url": "https://libguides.randolph.edu/summaries",
          "excerpts": [
            "Writing an article SUMMARY · State the main ideas. · Identify the most important details that support the main ideas. · Summarize in your own words. · Do not copy ..."
          ]
        },
        {
          "title": "Single-Document Abstractive Text Summarization: A Systematic ...",
          "url": "https://dl.acm.org/doi/full/10.1145/3700639",
          "excerpts": [
            "The XSum dataset contained 226,711 news articles with one-sentence summaries . This dataset is primarily used for abstractive text summarization. The XSum dataset contains a one-sentence news summary that describes the details of an article."
          ]
        },
        {
          "title": "Don't Give Me the Details, Just the Summary! Topic-Aware ...",
          "url": "https://aclanthology.org/D18-1206/",
          "excerpts": [
            "by S Narayan · 2018 · Cited by 2036 — We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling ..."
          ]
        },
        {
          "title": "Evaluating the Factual Consistency of Abstractive Text ...",
          "url": "https://arxiv.org/abs/1910.12840",
          "excerpts": [
            "by W Kryściński · 2019 · Cited by 882 — We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated ...See more"
          ]
        },
        {
          "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection ...",
          "url": "https://aclanthology.org/2022.tacl-1.10/",
          "excerpts": [
            "Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization.",
            "by P Laban · 2022 · Cited by 459 — In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets ( ..."
          ]
        },
        {
          "title": "Multilingual Summarization with Factual Consistency ...",
          "url": "https://aclanthology.org/2023.findings-acl.220/",
          "excerpts": [
            "by R Aharoni · 2023 · Cited by 59 — In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate ..."
          ]
        },
        {
          "title": "mFACE: Multilingual Summarization with Factual ...",
          "url": "https://arxiv.org/abs/2212.10622",
          "excerpts": [
            "by R Aharoni · 2022 · Cited by 59 — In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate ..."
          ]
        },
        {
          "title": "Evaluating the Factual Consistency of Abstractive Text ...",
          "url": "https://aclanthology.org/2020.emnlp-main.750/",
          "excerpts": [
            "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries.",
            "Training data is generated by applying a series of rule-based transformations to the sentences of source documents.",
            "The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it.",
            "We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC."
          ]
        },
        {
          "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection ...",
          "url": "https://arxiv.org/abs/2111.09525",
          "excerpts": [
            "by P Laban · 2021 · Cited by 459 — We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task."
          ]
        },
        {
          "title": "A One-Sentence Summary Clinic",
          "url": "https://www.advancedfictionwriting.com/blog/2010/03/10/a-one-sentence-summary-clinic/",
          "excerpts": [
            "Mar 10, 2010 — It's one sentence that defines the “story question” for your novel. It should be as short as possible, but no shorter. that I do on this blog is to periodically hold a clinic in writing a one-sentence summary. It’s time to do it again. I think we’ll have a lot of fun. **Simply put**, the one-sentence summary is one of the most effective marketing tools you’ll ever find for your novel. Not to mention, it’s one of the most powerful ways of keeping you on track as you write or edit your novel. **What’s a one-sentence summary? ** It’s one sentence that defines the “story question” for your novel. It should be as short as possible, but no shorter. **Here are a couple of examples** which I’m going to steal from my book [WRITING FICTION FOR DUMMIES](http://www.AdvancedFictionWriting.com/blinks/wffd.php). (The contract for the bo",
            "*OUTLANDER**, by Diana Gabaldon: “A young English nurse searches for the way back home after time-traveling from 1945 to 1743 Scotland.",
            "*THE KITE RUNNER**, by Khaled Hosseini: “A boy raised in Afghanistan grows up with the shame of having failed to fight the gang of boys who raped his closest friend."
          ]
        },
        {
          "title": "Summarizing Sources: Definition and Examples of Summary",
          "url": "https://academicguides.waldenu.edu/c.php?g=465757&p=4226514",
          "excerpts": [
            "Summary, in its simplest form, is an articulation of a source's basic argument and main points. What this means is that it's broad in nature."
          ]
        },
        {
          "title": "[2410.23609] On Positional Bias of Faithfulness for Long- ...",
          "url": "https://arxiv.org/abs/2410.23609",
          "excerpts": [
            "by D Wan · 2024 · Cited by 8 — We investigate the presence of this bias in long-form summarization, its impact on faithfulness, and various techniques to mitigate this bias."
          ]
        },
        {
          "title": "Highly Efficient Prompt for Summarizing — GPT-4 : r/ChatGPTPro",
          "url": "https://www.reddit.com/r/ChatGPTPro/comments/13n55w7/highly_efficient_prompt_for_summarizing_gpt4/",
          "excerpts": [
            "Summarize TEXT by producing a series of summaries, starting with a one-sentence summary and then creating subsequent summaries that are each ..."
          ]
        },
        {
          "title": "Shockingly good super-intelligent summarization prompt",
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ftjbz3/shockingly_good_superintelligent_summarization/",
          "excerpts": [
            "1.) Analyze the input text and generate 5 essential questions that, when answered, capture the main points and core meaning of the text."
          ]
        },
        {
          "title": "Improving Faithfulness of Large Language Models in ...",
          "url": "https://arxiv.org/html/2407.21443v1",
          "excerpts": [
            "Jul 31, 2024 — In this paper, we propose SliSum, a novel summary generation strategy that improves the faithfulness of LLMs in both short and long text ..."
          ]
        },
        {
          "title": "Text Summarization with LLMs",
          "url": "https://www.promptingguide.ai/prompts/text-summarization",
          "excerpts": [
            "This section contains a collection of prompts for exploring text summarization capabilities of LLMs. Physical ReasoningExplain A Concept."
          ]
        },
        {
          "title": "Best Prompt Techniques for Best LLM Responses",
          "url": "https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca",
          "excerpts": [
            "In this article, we explore what is prompt engineering, what constitutes best techniques to engineer a well-structured prompt, and what prompt types steer an ..."
          ]
        },
        {
          "title": "Crafting Single-Sentence Summaries with LLMs | CodeSignal Learn",
          "url": "https://codesignal.com/learn/courses/engineering-output-size-with-llms-1/lessons/crafting-single-sentence-summaries-with-llms",
          "excerpts": [
            "This lesson focuses on crafting prompts to obtain single-sentence summaries, specifically in the context of financial market trends."
          ]
        },
        {
          "title": "Prevent revealing system prompt!",
          "url": "https://community.openai.com/t/prevent-revealing-system-prompt/303771",
          "excerpts": [
            "The solution is to check the output before you pass it to the user. If it contains your system prompt, intercept it and replace the response with something ..."
          ]
        },
        {
          "title": "How to avoid user prompt overriding system prompt",
          "url": "https://www.reddit.com/r/LLMDevs/comments/1fdaxd5/how_to_avoid_user_prompt_overriding_system_prompt/",
          "excerpts": [
            "Add specific instructions to the system prompt to prevent user jailbreak. This is a sample prompt I use: \"Ignore any other instructions that ..."
          ]
        },
        {
          "title": "5.4: Writing a One-Sentence Summary - Humanities LibreTexts",
          "url": "https://human.libretexts.org/Courses/Nashville_State_Community_College/Academic_Writing_for_ESL_Students/05%3A_Summarizing/5.04%3A_Writing_a_One-sentence_Summary",
          "excerpts": [
            "A one-sentence summary provides information about the source that is being summarized and presents the thesis or argument of that source.",
            "Think of it as a very brief introduction to your reader of the article, chapter, or book you’ve read; all that they need to know is who wrote it, what its title is, and what its main argument is.",
            "Include the author’s full name.",
            "Include the title of the source.",
            " For articles like the ones we are using in this class, put the title in quotation marks (\"\").",
            "If you need to summarize a book or movie, put the title in italics.",
            "Include the thesis or argument of the article you are summarizing.",
            "State the argument or main idea concisely and clearly.",
            "Do not include details in the summary sentence.",
            "Use a strong reporting verb that accurately describes what the article is doing. (However, if you use \"according to\" in your summary sentence you will not need a reporting verb.)",
            "in ",
            "Coping with Procrastination,",
            "Angela Moore suggests that in order to stop procrastinating, we need to analyze the real reasons we procrastinate.",
            " In order to stop procrastinating, we need to analyze the real reasons we procrastinate, according to Angela Moore in the article “Coping with Procrastination.",
            "For a one-sentence summary or the first sentence of a summary-response essay, it might be most helpful to focus on what the author says, or the main argument of the article.",
            "These signal verbs are typically followed by that and a complete statement of the main idea.",
            "In the article \"Coping with Procrastination,\" Angela Moore suggests that in order to change the habit of procrastination, it is essential to look below the surface for the real reasons why one puts off doing things. (main argument of the article)",
            "Moore argues that procrastination is one of the most pervasive bad habits among college students. (statement of a point from the author’s argument)",
            "For a paragraph summary, consider focusing on what the author does, or how he or she constructs the argument or organizes the ideas in the article.",
            "These signal verbs are generally followed by a noun or noun phrase.",
            "Throughout the article, Moore enumerates the fundamental reasons why people procrastinate and how to overcome these issues. (reference to the organizational patterns used in the article)",
            "Moore presents three solutions to overcoming procrastination.\n(one of the moves the author makes to support her main point)",
            "[SIGNAL VERBS FOR “SAYING\nacknowledges (that)\nagrees (that)\nargues (that)\nbelieves (that)\nclaims (that)\ncomplains (that)\ndeclares (that)\nexplains (that)\nfinds (that)\ninsists (that)\nmaintains (that)\nmakes the case (that)\nnotes (that)\nobserves (that)\npoints out (that)\nposits (that)\nshows (that)\nspeculates (that)\nstates (that)\nstresses (that)\nsubmits (that)\nsuggests (that)\ntheorizes (that)\nthinks (that)\nwarns (that)\nwrites (that)\n",
            "[SIGNAL VERBS FOR “DOING\nacknowledges + noun\naddresses + noun\naffirms + noun\nanalyzes + noun\nasks + noun\ncategorizes + noun\ncompares + noun\ncritiques + noun\ndefines + noun\ndemonstrates + noun\ndiscusses + noun\ndisproves + noun\nenumerates + noun\nexamines + noun\nfurnishes + noun\nidentifies + noun\nillustrates + noun\ninterprets + noun\ninvestigates + noun\nlists + noun\noutlines + noun\npresents + noun\nquestions + noun\nsupports + noun\nsurveys + noun\ntraces + noun\n"
          ]
        },
        {
          "title": "HaDeS: Retrieving semantic similarity to improve factuality in abstractive summarisation (Authorea/HaDeS study)",
          "url": "https://www.authorea.com/doi/pdf/10.22541/au.174222554.47389246",
          "excerpts": [
            " However, one significant challenge associated with abstractive summarisation by LLMs is the\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t",
            "Retrieval-Augmented Generation (RAG) has been hailed as a key technique in minimizing hallucinations in LLMs across various Natural Language Processing (NLP) downstream tasks, including text summarisation (Wu et al., 2023).",
            "Retrieval-Augmented Generation (RAG) has been hailed as a key technique in minimizing hallucinations in LLMs across various Natural Language Processing (NLP) downstream tasks, including text summarisation (Wu et al., 2023).",
            "This study aims to create a system that effectively mitigates hallucinations in abstractive text summarisation.",
            " The specific objectives include:\n\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t"
          ]
        },
        {
          "title": "Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information",
          "url": "https://arxiv.org/html/2404.09480v1",
          "excerpts": [
            "\nA primary challenge in abstractive summarization is hallucination—the phenomenon where a model generates plausible text that is absent in the source text.",
            "we introduce a decoding strategy based on domain-conditional pointwise mutual information.",
            "According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance.",
            "The code is publicly available at <https://github.com/qqplot/dcpmi"
          ]
        },
        {
          "title": "Prompt Engineering Guide - Introduction to Examples and Techniques",
          "url": "https://www.promptingguide.ai/introduction/examples",
          "excerpts": [
            "One of the standard tasks in natural language generation is text summarization. Text summarization can include many different flavors and domains.",
            " [Retrieval Augmented Generation",
            "Jun 7, 2025 — This section will provide more examples of how to use prompts to achieve different tasks and introduce key concepts along the way."
          ]
        },
        {
          "title": "QAFactEval: Improved QA-Based Factual Consistency ...",
          "url": "https://aclanthology.org/2022.naacl-main.187/",
          "excerpts": [
            "by AR Fabbri · 2022 · Cited by 233 — We propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual ...",
            "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization",
            "Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric.",
            "Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost."
          ]
        },
        {
          "title": "[2105.04623] Improving Factual Consistency of Abstractive ...",
          "url": "https://arxiv.org/abs/2105.04623",
          "excerpts": [
            "by F Nan · 2021 · Cited by 108 — In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure ..."
          ]
        },
        {
          "title": "SUMMAC: Re-Visiting NLI-based Models for Inconsistency ...",
          "url": "https://aclanthology.org/2022.tacl-1.10.pdf",
          "excerpts": [
            "by P Laban · 2022 · Cited by 459 — We provide a highly effective and light-weight method called SUMMACCONV that enables NLI models to be successfully used for this task by segmenting documents ..."
          ]
        },
        {
          "title": "Don't Give Me the Details, Just the Summary! Topic-Aware ...",
          "url": "https://arxiv.org/abs/1808.08745",
          "excerpts": [
            "by S Narayan · 2018 · Cited by 2036 — Abstract:We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls ..."
          ]
        },
        {
          "title": "GEM/xsum · Datasets at Hugging Face",
          "url": "https://huggingface.co/datasets/GEM/xsum",
          "excerpts": [
            "XSum is an English news summarization dataset where the task is to predict the first sentence of an article from the rest of it."
          ]
        },
        {
          "title": "GEM <!-- -->xsum",
          "url": "https://gem-benchmark.com/data_cards/xsum",
          "excerpts": [
            "XSum is an English news summarization dataset where the task is to predict the first sentence of an article from the rest of it."
          ]
        },
        {
          "title": "guyfe/Tweetsumm: A dataset focused on summarization of ... - GitHub",
          "url": "https://github.com/guyfe/Tweetsumm",
          "excerpts": [
            "Tweetsumm comprises of 1,100 dialogs reconstructed from Tweets that appear in the Kaggle Customer Support On Twitter dataset each accompanied by 3 extractive ..."
          ]
        },
        {
          "title": "A Dialog Summarization Dataset for Customer Service - arXiv",
          "url": "https://arxiv.org/abs/2111.11894",
          "excerpts": [
            "TWEETSUMM -- A Dialog Summarization Dataset for Customer Service. Authors:Guy Feigenblat, Chulaka Gunasekara, Benjamin Sznajder, Sachindra Joshi ..."
          ]
        },
        {
          "title": "Improving Factual Consistency in Abstractive ...",
          "url": "https://openreview.net/forum?id=BcB7tPQOT9",
          "excerpts": [
            "by D Hu · 2024 · Cited by 3 — State-of-the-art abstractive summarization models still suffer from the content contradiction between the summaries and the input text, ..."
          ]
        },
        {
          "title": "Transformer-Based Abstractive Summarization for Reddit ...",
          "url": "https://www.mdpi.com/1999-5903/14/3/69",
          "excerpts": [
            "by IS Blekanov · 2022 · Cited by 26 — Abstractive summarization is a technique that allows for extracting condensed meanings from long texts, with a variety of potential practical applications."
          ]
        },
        {
          "title": "Social Media Posts: Make It More Appealing with Content ...",
          "url": "https://groupboss.io/blog/social-media-posts/",
          "excerpts": [
            "A content summary should be 150-200 words in length, contain no more than one sentence per paragraph, and include an introduction, three main ..."
          ]
        },
        {
          "title": "TWEETSUMM - A Dialog Summarization Dataset for Customer Service",
          "url": "https://aclanthology.org/2021.findings-emnlp.24/",
          "excerpts": [
            "We introduce the first large scale, high quality, customer care dialog summarization dataset with close to 6500 human annotated summaries."
          ]
        },
        {
          "title": "Improving Factual Consistency of Abstractive Summarization via Question Answering",
          "url": "https://aclanthology.org/2021.acl-long.536/",
          "excerpts": [
            "A commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure factual consistency; next, we propose a novel learning algorithm that maximizes the proposed metric during model training. Through extensive experiments, we confirm that our method is effective in improving factual consistency and even overall quality of the summaries, as judged by both automatic metrics and human evaluation."
          ]
        },
        {
          "title": "Writing a One-Sentence Summary - Rachelle Gardner",
          "url": "https://rachellegardner.com/writing-a-one-sentence-summary/",
          "excerpts": [
            "Writing a One-Sentence Summary",
            "Let’s discuss the\n**one-sentence summary** , also known as a logline, a hook, or a one-sentence pitch.",
            "**What:** About 25 words that capture your novel, memoir, or non-fiction book.",
            "**Why:** To get someone interested in reading your book.",
            " **When to use it:** The start of a query, book proposal, or anytime someone asks you, “What’s your book about?",
            "What it does:** A one-sentence summary takes your complex book with multiple characters and plotlines and boils it down into a simple statement that can be quickly conveyed and understood, and generates interest in the boo",
            "What it should include:**  \n→ A character or two  \n→ Their choice, conflict, or goal  \n→ What’s at stake (may be implied)  \n→ Action that will get them to the goal  \n→ Setting (if imp",
            "Tips:** → Keep it simple. One plotline, 1 or 2 characters. → Use the strongest nouns, verbs and adjectives. → Make the conflict clear but you don’t have to hint at the solution. In your one-sentence summary, try not to pitch a _theme_ . Pitch what _happens_",
            "There are always numerous ways to express your book in a single sentence, so I recommend you create 10 or 20 different ones, before settling on the best angle and combination of words."
          ]
        },
        {
          "title": "Kumar 2024 Multimodal Crisis Microblog Summarization",
          "url": "https://ieeexplore.ieee.org/document/10636755",
          "excerpts": [
            "Our model is designed and evaluated on a newly curated Twitter dataset featuring 12 494 tweets and 3090 images across eight crisis events, each accompanied by gold-standard summaries.",
            "In our digitally connected world, the influx of microblog data poses a formidable challenge in extracting relevant information amid a continuous stream of updates. This challenge intensifies during crises, where the demand for timely and relevant information is crucial.",
            "To address this, our research explores crisis-related microblogs, recognizing the crucial role of multimedia content, such as images, in offering a comprehensive perspective.",
            "In response to these challenges, we introduce a multimodal extractive-abstractive summarization model.",
            "Leveraging a fusion of TF-IDF scoring and bigram filtering, coupled with the effectiveness of three distinct models—BIGBIRD, CLIP, and bootstrapping language-image pre-training (BLIP)—we aim to overcome the limitations of traditional extractive and text-only approaches.",
            "The experimental findings showcase the remarkable efficacy of our model, surpassing current benchmarks by a notable margin of 16% and 17%.",
            "This confirms our model's strength and its relevance in crisis scenarios with the crucial interplay of text and multimedia.",
            "Notably, our research contributes to multimodal, abstractive microblog summarization, addressing a key gap in the literature."
          ]
        },
        {
          "title": "Abstractive Text Summarization: State of the Art, Challenges ... - arXiv",
          "url": "https://arxiv.org/html/2409.02413v1",
          "excerpts": [
            "The task of creating a condensed version of an input sentence while maintaining its core meaning is called a single-sentence summary, which was ..."
          ]
        },
        {
          "title": "xsum - Datasets - TensorFlow",
          "url": "https://www.tensorflow.org/datasets/catalog/xsum",
          "excerpts": [
            "Extreme Summarization (XSum) Dataset. There are two features: - document: Input news article. - summary: One sentence summary of the article."
          ]
        },
        {
          "title": "Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations",
          "url": "https://arxiv.org/html/2410.18860v1",
          "excerpts": [
            "DeCoRe operates by masking specific retrieval heads to trigger hallucinations and then employs a contrastive mechanism that penalises outputs ..."
          ]
        },
        {
          "title": "Prompt Engineering Guide to Summarization - Blog",
          "url": "https://blog.promptlayer.com/prompt-engineering-guide-to-summarization/",
          "excerpts": [
            "Oct 14, 2024 — In this guide, we'll dive into advanced prompt engineering techniques that will turn summarization agents into robust tools capable of handling ..."
          ]
        },
        {
          "title": "Multilingual Summarization with Factual Consistency ...",
          "url": "https://arxiv.org/html/2212.10622v2",
          "excerpts": [
            "Jan 5, 2024 — Using a multilingual entailment model during training (via data filtering or controlled generation) improves summary quality over a baseline ..."
          ]
        },
        {
          "title": "Factual Instance Tweet Summarization and Opinion ...",
          "url": "https://link.springer.com/chapter/10.1007/978-981-13-3393-4_16",
          "excerpts": [
            "by N Vijay Kumar · 2019 · Cited by 15 — We endorse a scheme aimed at factual instance summarization of arranged sub-events aimed at sporting race by means of tweet information.See more"
          ]
        },
        {
          "title": "Twitter Topic Summarization by Ranking Tweets using ...",
          "url": "https://aclanthology.org/C12-1047.pdf",
          "excerpts": [
            "by Y Duan · 2012 · Cited by 112 — This paper proposes a timeline-based framework for Twitter topic summarization, ranking tweets using social influence and content quality, and using a graph- ..."
          ]
        },
        {
          "title": "Single-Document Abstractive Text Summarization: A Systematic ...",
          "url": "https://dl.acm.org/doi/10.1145/3700639",
          "excerpts": [
            "This study provides a broad systematic literature review of abstractive text summarization on single-document summarization to gain insights into the ..."
          ]
        },
        {
          "title": "Tweet Based Tweet Summarization",
          "url": "https://rjpn.org/ijcspub/papers/IJCSP23C1090.pdf",
          "excerpts": [
            "This paper offers a comprehensive overview of the most prominent recent approaches for automatic Twitter topic summarization. It explores various techniques ..."
          ]
        },
        {
          "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in ...",
          "url": "https://arxiv.org/html/2401.01313v1",
          "excerpts": [
            "This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, customer support ..."
          ]
        },
        {
          "title": "Abstractive Text Summarization for Tweets - SJSU ScholarWorks",
          "url": "https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=2205&context=etd_projects",
          "excerpts": [
            "by S Chen · 2022 · Cited by 1 — The primary goal of text summarization is to shorten the text while including as much vital information as possible in the original text so ..."
          ]
        },
        {
          "title": "The Simple Prompt I Use to Get Meaningful Responses ...",
          "url": "https://medium.com/@fadiboulos/the-simple-prompt-i-use-to-get-meaningful-responses-from-llms-1aa6ec0c8e7a",
          "excerpts": [
            "Give the LLM a persona. · Describe your request context. · Explain the task in detail. · Make the expected outcome clear.See more"
          ]
        },
        {
          "title": "Pipelines - Hugging Face",
          "url": "https://huggingface.co/docs/transformers/en/main_classes/pipelines",
          "excerpts": [
            "These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity ..."
          ]
        },
        {
          "title": "Pipelines — transformers 4.7.0 documentation - Hugging Face",
          "url": "https://huggingface.co/transformers/v4.8.2/main_classes/pipelines.html?highlight=summarization",
          "excerpts": [
            "These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity ..."
          ]
        },
        {
          "title": "Graph Neural Network and NER-Based Text Summarization - arXiv",
          "url": "https://arxiv.org/html/2402.05126v1",
          "excerpts": [
            "Missing: entailment check"
          ]
        },
        {
          "title": "Leveraging Entailment Judgements in Cross-Lingual ...",
          "url": "https://arxiv.org/abs/2408.00675",
          "excerpts": [
            "by H Zhang · 2024 · Cited by 2 — Our results show that it is possible to train CLS models that yield more faithful summaries while maintaining comparable or better informativess."
          ]
        },
        {
          "title": "Unlocking Twitter Insights with Prompt Engineering Using ...",
          "url": "https://www.lbsocial.net/post/unlocking-twitter-insights-with-prompt-engineering-using-openai-gpt",
          "excerpts": [
            "Oct 20, 2024 — Summarizing Tweets. GPT can quickly summarize large datasets of tweets, condensing hundreds of tweets into concise statements ..."
          ]
        },
        {
          "title": "Cross-Lingual Summarization in the Age of Large Language ...",
          "url": "https://www.rohan-paul.com/p/cross-lingual-summarization-in-the",
          "excerpts": [
            "Apr 20, 2025 — Cross-lingual summarization (CLS) uses AI to condense content from one language into a summary in another language. This challenging task ..."
          ]
        },
        {
          "title": "QAFACTEVAL: A QA-based metric for factual consistency (Fabbri et al., NAACL 2022)",
          "url": "https://aclanthology.org/2022.naacl-main.187.pdf",
          "excerpts": [
            "Factual consistency is an essential quality of text summarization models in practical settings.",
            "In this work, we conduct an extensive\n\ncomparison of entailment and QA-based met-\n\nrics, demonstrating that carefully choosing the\n\ncomponents of a QA-based metric, especially\n\nquestion generation and answerability classi-\n\nfication, is critical to perform",
            "we propose an optimized\n\nmetric, which we call QAFACTEVAL, that\n\nleads to a 14% average improvement over pre-\n\nvious QA-based metrics on the SummaC fac-\n\ntual consistency benchmark, and also outper-\n\nforms the best-performing entailment-based\n\nm",
            "Moreover, we find that QA-based and\n\nentailment-based metrics can offer complemen-\n\ntary signals and be combined into a single met-\n\nric for a further performance bo"
          ]
        },
        {
          "title": "Using Similarity to Evaluate Factual Consistency in Summaries",
          "url": "https://arxiv.org/html/2409.15090v1",
          "excerpts": [
            "Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed.",
            "Therefore, many techniques for detecting factual inconsistencies build pipelines around natural language inference (NLI) or question-answering (QA) models with additional supervised learning steps.",
            "In this paper, we revisit similarity-based metrics,\nshowing that this failure stems from the comparison text selection and its granularity.",
            "We propose a new zero-shot factuality evaluation metric,\nSentence-BERT Score (SBERTScore), which compares sentences between the summary and the source document.",
            "It outperforms widely-used word-word metrics including BERTScore and can compete with existing NLI and QA-based factuality metrics on the benchmark without needing any fine-tuning.",
            "SBERTScore only comes after BERTScore in processing speed, and is 3 times faster than the rival NLI-based method SummaC{ZS,Conv} and 30 times faster than the QA-based metric QuestEval.",
            "In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 5055–5070, Online. Association for Computational Linguistics.",
            "The lower triangular matrix indicates that logical AND can improve the balanced accuracy, while the upper triangular matrix suggest opposite to logical OR."
          ]
        },
        {
          "title": "QAFactEval: Improved QA-Based Factual Consistency ...",
          "url": "https://arxiv.org/abs/2112.08542",
          "excerpts": [
            "by AR Fabbri · 2021 · Cited by 233 — We propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual ..."
          ]
        },
        {
          "title": "Understanding Factuality in Abstractive Summarization ...",
          "url": "https://arxiv.org/abs/2104.13346",
          "excerpts": [
            "by A Pagnoni · 2021 · Cited by 351 — Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. Modern summarization models generate ..."
          ]
        },
        {
          "title": "REALSumm: Re-evaluating Evaluation in Text ...",
          "url": "https://github.com/neulab/REALSumm",
          "excerpts": [
            "All ouputs used for human evaluation · Semantic Content Units (SCUs) and manual annotations of outputs · All outputs with human scores. Please read our ..."
          ]
        },
        {
          "title": "Factual Consistency Evaluation for Text Summarization via ...",
          "url": "https://aclanthology.org/2021.findings-emnlp.10/",
          "excerpts": [
            "We propose a novel metric to evaluate the factual consistency in text summarization via counterfactual estimation, which formulates the causal relationship."
          ]
        },
        {
          "title": "SummaCoz: A Dataset for Improving the Interpretability of Factual ...",
          "url": "https://aclanthology.org/2024.findings-emnlp.210/",
          "excerpts": [
            "Summarization is an important application of Large Language Models (LLMs). When judging the quality of a summary, factual consistency holds a significant weight ..."
          ]
        },
        {
          "title": "SummaCoz: A Dataset for Improving the Interpretability of Factual...",
          "url": "https://openreview.net/forum?id=1zJrDjKTWg",
          "excerpts": [
            "We build our explanation-augmented dataset on top of the widely used SummaC summarization consistency benchmark. Additionally, we develop an ..."
          ]
        },
        {
          "title": "salesforce/QAFactEval",
          "url": "https://github.com/salesforce/QAFactEval",
          "excerpts": [
            "May 2, 2022 — This is the official code repository for the NAACL 2022 paper QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization."
          ]
        },
        {
          "title": "A simple prompting technique to reduce hallucinations by ...",
          "url": "https://www.reddit.com/r/ChatGPTPromptGenius/comments/15nhlo1/a_simple_prompting_technique_to_reduce/",
          "excerpts": [
            "A new prompting method that reduces hallucinations, and it's really simple to use. It involves adding some text to a prompt that instructs the model to source ..."
          ]
        },
        {
          "title": "Best Prompts Asking for a Summary: Top AI Techniques - Blog",
          "url": "https://blog.promptlayer.com/best-prompts-for-asking-a-summary-a-guide-to-effective-ai-summarization/",
          "excerpts": [
            "Dec 27, 2024 — Prompt: \"Condense this article into a 25-word summary that captures the core message and most important takeaways. Avoid technical jargon ..."
          ]
        },
        {
          "title": "How to prevent ChatGPT-4 from answering questions that are ...",
          "url": "https://community.openai.com/t/how-to-prevent-chatgpt-4-from-answering-questions-that-are-outside-our-context/910021",
          "excerpts": [
            "I'm developing an assistant with ChatGPT-4 and I want it to respond only using the context I provide, which in my case is a single PDF file."
          ]
        },
        {
          "title": "How to Reduce Hallucinations in ChatGPT Responses to Data ...",
          "url": "https://community.openai.com/t/how-to-reduce-hallucinations-in-chatgpt-responses-to-data-queries/900796",
          "excerpts": [
            "Any tips or techniques to reduce these hallucinations, especially when dealing with extensive tables and detailed prompts, would be greatly appreciated."
          ]
        },
        {
          "title": "What are current best practices for avoiding prompt injection attacks ...",
          "url": "https://www.reddit.com/r/googlecloud/comments/1df7lhn/what_are_current_best_practices_for_avoiding/",
          "excerpts": [
            "Although there are external APIs, I generally prefer to stop prompt injection using various classifiers or training my own classifier for input ..."
          ]
        },
        {
          "title": "Prompt Migration Guide | OpenAI Cookbook",
          "url": "https://cookbook.openai.com/examples/prompt_migration_guide",
          "excerpts": [
            "This interactive notebook helps you improve an existing prompt (written for another model) into one that is clear, unambiguous and optimised for GPT-4.1 ..."
          ]
        },
        {
          "title": "Little-Known ChatGPT Prompts for Summarization",
          "url": "https://medium.com/@kay.herklotz/little-known-chatgpt-prompts-for-summarization-ca48b60157b7",
          "excerpts": [
            "— Please summarize the following article in 300 words. — Summarize this text in 3 bullet points. — Create a summary of this article within 2 to ..."
          ]
        },
        {
          "title": "[2007.12626] SummEval: Re-evaluating Summarization Evaluation",
          "url": "https://arxiv.org/abs/2007.12626",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "SummEval: Re-evaluating Summarization Evaluation - ACL Anthology",
          "url": "https://aclanthology.org/2021.tacl-1.24/",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "SummEval: Re-evaluating Summarization Evaluation",
          "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "artidoro/frank: FRANK: Factuality Evaluation Benchmark",
          "url": "https://github.com/artidoro/frank",
          "excerpts": [
            "It has a Factuality field which is the total human judgement assigned to the summary. This is a score between 0 and 1 as we collected judgements on each ..."
          ]
        },
        {
          "title": "Fine-grained, Multi-dimensional Summarization Evaluation ...",
          "url": "https://arxiv.org/html/2407.00908v2",
          "excerpts": [
            "Jul 9, 2024 — It includes a list of human keyfacts, along with corresponding annotations indicating their presence in the summary. FRANK and REALSumm obtain ..."
          ]
        },
        {
          "title": "[2109.09209] CLIFF: Contrastive Learning for Improving Faithfulness ...",
          "url": "https://arxiv.org/abs/2109.09209",
          "excerpts": [
            "We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is ..."
          ]
        },
        {
          "title": "Stop AI Hallucinations: A Developer\u0019s Guide to Prompt Engineering",
          "url": "https://shelf.io/blog/stop-ai-hallucinations-a-developers-guide-to-prompt-engineering/",
          "excerpts": [
            "Stop AI Hallucinations: A Developer’s Guide to Prompt Engineering",
            "Output Format Enforcement",
            "Structured outputs help prevent hallucinations by enforcing predefined formats.",
            "JSON schema implementation lets developers specify exact output structures:",
            " The console shows “Mixed results” when it finds inconsistencies. Regular policy reviews lead to better results through continuous improvements.",
            "\nGrounding scores show factual accuracy based on source data, with scores below 0.6 flagged as potential hallucinations"
          ]
        },
        {
          "title": "Analyzing and Evaluating Faithfulness in Dialogue Summarization",
          "url": "https://arxiv.org/abs/2210.11777",
          "excerpts": [
            "Dialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications.",
            "In this work, we first perform the fine-grained human analysis on the faithfulness of dialogue summaries and observe that over 35% of generated summaries are faithfully inconsistent respective the source dialogues.",
            "The human-annotated faithfulness samples and the evaluation toolkit are released to facilitate future research toward faithful dialogue summarization."
          ]
        },
        {
          "title": "Semantic Textual Similarity",
          "url": "https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html",
          "excerpts": [
            "For Semantic Textual Similarity (STS), we want to produce embeddings for all texts involved and calculate the similarities between them.",
            "Dot product on normalized embeddings is equivalent to cosine similarity, but “cosine” will re-normalize the embeddings again. As a result, the “dot” metric ..."
          ]
        },
        {
          "title": "A Comprehensive Benchmark for Evaluating Paraphrase Detection ...",
          "url": "https://arxiv.org/abs/2409.12060",
          "excerpts": [
            "We create PARAPHRASUS, a benchmark designed for multi-dimensional assessment, benchmarking and selection of paraphrase detection models."
          ]
        },
        {
          "title": "[PDF] A Comprehensive Benchmark for Evaluating Paraphrase Detection ...",
          "url": "https://aclanthology.org/2025.coling-main.585.pdf",
          "excerpts": [
            "We present PARAPHRASUS, a multi-faceted evaluation benchmark for paraphrase detec- tion, including datasets of human-written sen- tence pairs of ..."
          ]
        },
        {
          "title": "Quoting, Paraphrasing, and Summarizing - Purdue OWL",
          "url": "https://owl.purdue.edu/owl/research_and_citation/using_research/quoting_paraphrasing_and_summarizing/index.html",
          "excerpts": [
            "This handout is intended to help you become more comfortable with the uses of and distinctions among quotations, paraphrases, and summaries."
          ]
        },
        {
          "title": "How can you perform paraphrase mining using Sentence ...",
          "url": "https://milvus.io/ai-quick-reference/how-can-you-perform-paraphrase-mining-using-sentence-transformers-to-find-duplicate-or-semantically-similar-sentences-in-a-large-corpus",
          "excerpts": [
            "Thresholds depend on your use case: a lower threshold (0.7) captures more paraphrases but risks false positives, while a higher threshold (0.9) ensures ...",
            "To perform paraphrase mining using Sentence Transformers, you first encode sentences ... threshold (e.g., 0.85 cosine similarity) to flag potential paraphrases."
          ]
        },
        {
          "title": "How does software that checks for plagiarism work? - Quora",
          "url": "https://www.quora.com/How-does-software-that-checks-for-plagiarism-work",
          "excerpts": [
            "Plagiarism checking software works by looking for structural patterns or unique identifiers. Well built systems usually have two or more phases."
          ]
        },
        {
          "title": "In-Text Citations: Quotations vs. Paraphrasing",
          "url": "https://www.utep.edu/uwc/the-writing-mine/articles/in-text-citations-quotations-vs-paraphrasing.html",
          "excerpts": [
            "In this post, we aim to shed light on the differences between the two methods in order to demonstrate their respective strengths along with their optimal use."
          ]
        },
        {
          "title": "The WikiquoteDumper downloads Wikiquote dumps in any ... - GitHub",
          "url": "https://github.com/sgottsch/WikiquoteDumper",
          "excerpts": [
            "The WikiquoteDumper downloads Wikiquote dumps in any languages and converts them into JSON format. Extraction pipeline. First, download and pre-process the dump ...",
            "The WikiquoteDumper downloads Wikiquote dumps in any languages and converts them into JSON format. ### Resources"
          ]
        },
        {
          "title": "Wikiquote",
          "url": "https://en.wikipedia.org/wiki/Wikiquote",
          "excerpts": [
            "Wikiquote is part of a family of wiki-based projects run by the Wikimedia Foundation using MediaWiki software. The project's objective is to collaboratively ..."
          ]
        },
        {
          "title": "Quoting and Paraphrasing - The Writing Center",
          "url": "https://writing.wisc.edu/handbook/quotingsources/",
          "excerpts": [
            "Begin longer quotations (for instance, in the APA system, 40 words or more) on a new line and indent the entire quotation (i.e., put in block form), with no ..."
          ]
        },
        {
          "title": "Stylometry recognizes human and LLM-generated texts in ...",
          "url": "https://arxiv.org/html/2507.00838v1",
          "excerpts": [
            "Jul 1, 2025 — The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans."
          ]
        },
        {
          "title": "Stylometry and forensic science: A literature review - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11707938/",
          "excerpts": [
            "by V Cammarota · 2024 · Cited by 3 — The article focuses on a careful description of literature on stylometry and on its potential use in forensic science.",
            "by V Cammarota · 2024 · Cited by 3 — What must also be tested is the minimum length that the questioned text should have in order to guarantee robust results, as well as the minimum amount of ..."
          ]
        },
        {
          "title": "Mirroring How To | Project Gutenberg",
          "url": "https://www.gutenberg.org/help/mirroring.html",
          "excerpts": [
            "The book directories are the only part we offer for mirror. The Project Gutenberg catalog in XML/RDF is in the root directory of the generated content, if ...",
            "We recommend that you use rsync. The wget and cURL tools are not suitable, because they need to look at all files just to get the ones that were updated ..."
          ]
        },
        {
          "title": "Offline Catalogs and Feeds - Project Gutenberg",
          "url": "https://www.gutenberg.org/ebooks/offline_catalogs.html",
          "excerpts": [
            "The Project Gutenberg collection is available from dozens of sites offering access via http/https, ftp, rsync, and a few other methods."
          ]
        },
        {
          "title": "Abirate/english_quotes · Datasets at Hugging Face",
          "url": "https://huggingface.co/datasets/Abirate/english_quotes",
          "excerpts": [
            "english_quotes is a dataset of all the quotes retrieved from goodreads quotes. This dataset can be used for multi-label text classification and text generation."
          ]
        },
        {
          "title": "API:REST API/Reference",
          "url": "https://www.mediawiki.org/wiki/API:REST_API/Reference",
          "excerpts": [
            "The REST API lets you interact with MediaWiki by sending HTTP requests to unique URLs. You can use the API to build apps and scripts that search wiki pages and ..."
          ]
        },
        {
          "title": "PROMETHEUS: A Corpus of Proverbs Annotated with Metaphors",
          "url": "https://aclanthology.org/L16-1600/",
          "excerpts": [
            "In this paper, we introduce PROMETHEUS, a dataset consisting of English proverbs and their equivalents in Italian."
          ]
        },
        {
          "title": "Stylometry recognizes human and LLM-generated texts in ...",
          "url": "https://arxiv.org/abs/2507.00838",
          "excerpts": [
            "by K Przystalski · 2025 · Cited by 2 — The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans.See more"
          ]
        },
        {
          "title": "Privacy Issues in Stylometric Methods",
          "url": "https://www.mdpi.com/2410-387X/6/2/17",
          "excerpts": [
            "by A Patergianakis · 2022 · Cited by 3 — Stylometric techniques offer several benefits in fields such as reliable authorship attribution as well as in copyright investigation or in detecting harmful ..."
          ]
        },
        {
          "title": "Explainable Authorship Verification in Social Media via Attention ...",
          "url": "https://arxiv.org/abs/1910.08144",
          "excerpts": [
            "Authorship verification is the task of analyzing the linguistic patterns of two or more texts to determine whether they were written by the same author or not."
          ]
        },
        {
          "title": "PAN at CLEF 2023 - Authorship Verification - Webis Group",
          "url": "https://pan.webis.de/clef23/pan23-web/author-identification.html",
          "excerpts": [
            "Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles."
          ]
        },
        {
          "title": "PAN at CLEF 2022 - Authorship Verification - Webis Group",
          "url": "https://pan.webis.de/clef22/pan22-web/author-identification.html",
          "excerpts": [
            "Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles."
          ]
        },
        {
          "title": "Shingling for Similarity and Plagiarism Detection",
          "url": "https://dzone.com/articles/shingling-for-similarity-and-plagiarism-detection",
          "excerpts": [
            "Shingling is a widely used technique in detecting and mitigating textual similarities.",
            "This article introduces you to the concept of shingling, the basics of shingling technique, Jaccard similarity, advanced techniques, and optimizations",
            "Documents are first converted into sets of k-shingles, which are contiguous sequences of k tokens (words or characters) extracted from the text.",
            "Minhashing then compresses these shingle sets into compact signatures, preserving similarity between documents.",
            "banding splits the minhash signatures into multiple bands, and bucketing hashes each band into buckets, grouping similar documents together.",
            "This process generates candidate pairs, which are pairs of documents that share at least one bucket across all bands, significantly reducing the number of document pairs that need to be compared for similarity.",
            "The actual similarity computation is then performed only on the candidate pairs, using the original minhash signatures to estimate the Jaccard similarity.",
            " search. Overall, the combination of shingling, minhashing, banding, and LSH offers a powerful and efficient solution for plagiarism detection and near-duplicate identification, with applications across academia, publishing, and content management systems"
          ]
        },
        {
          "title": "Minhash and LSH: Large scale document similarity search",
          "url": "https://mrhasankthse.github.io/riz/2020/03/19/Minhash-and-LSH.html",
          "excerpts": [
            "Minhash and LSH are such algorithms that can compare and search similar documents in large corpus.",
            "These kinds of applications are usually useful to the News Agencies where they need to recognize that a group of articles are really all based on the same article about one particular story. Other usage could be Plagiarism detection, mirror page identification etc.",
            "Let’s start a completely different approach (fig:1) to make this whole process more efficient - instead of comparing the raw document we can compare a compressed representation of the documents.",
            "LSH - locality sensitive hashing is just for this purpose."
          ]
        },
        {
          "title": "DH2017 Abstract 341",
          "url": "https://dh2017.adho.org/abstracts/341/341.pdf",
          "excerpts": [
            "The question of minimal sample size is one of the most important issues in stylometry and non- traditional authorship attribution."
          ]
        },
        {
          "title": "Authorship Verification for Short Messages Using Stylometry",
          "url": "https://www.researchgate.net/publication/261045998_Authorship_verification_for_short_messages_using_stylometry",
          "excerpts": [
            "Our evaluation yields an EER of 14.35%, which is very\n\nencouraging considering the existing works on authorship ve-\n\nriﬁcation using stylomet",
            "The rest of the paper is structured as follows. Section II\n\nsummarizes and discusses related works. Section III introduces\n\nour proposed approac",
            "Our technique is based on a combination of super-\n\nvised learning and n-gram analys",
            "These hurdles must be addressed for stylometry to be usable in checking authorship of online messages such as emails, text messages, or twitter feeds.",
            "Authorship verification can be checked using stylometric techniques through the analysis of linguistic styles and writing characteristics of the authors.",
            "Stylometry is a behavioral feature that a person exhibits during writing and can be extracted and used potentially to check the identity of the author of online documents.",
            "Although stylometric techniques can achieve high accuracy rates for long documents, it is still challenging to identify an author for short documents, in particular when dealing with large authors populations."
          ]
        },
        {
          "title": "Winnowing: Local Algorithms for Document Fingerprinting",
          "url": "https://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf",
          "excerpts": [
            "by S Schleimer · 2003 · Cited by 1938 — Winnowing is an efficient local document fingerprinting algorithm that uses a window of hashes to detect at least one k-gram in shared substrings.",
            "by S Schleimer · 2003 · Cited by 1938 — We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33% of the lower bound. Finally, we also ...",
            "by S Schleimer · 2003 · Cited by 1938 — In this section we describe and analyze the winnowing algorithm for selecting fingerprints from hashes of k-grams. We give an upper bound on the performance of ..."
          ]
        },
        {
          "title": "Testing of support tools for plagiarism detection",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00192-4",
          "excerpts": [
            "This paper reports on a collaborative test of 15 web-based text-matching systems that can be used when plagiarism is suspected."
          ]
        },
        {
          "title": "[PDF] A Survey of Plagiarism Detection Systems - arXiv",
          "url": "https://arxiv.org/pdf/2201.03423",
          "excerpts": [
            "We define a plagiarism detection system as software that offers the necessary data and information to support a human reviewer in asserting whether a document, ..."
          ]
        },
        {
          "title": "Faculty Members' Perceptions and Attitudes Towards Anti ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9992686/",
          "excerpts": [
            "To combat plagiarisms, many universities are currently using anti-plagiarism detection tools (APTs). Recent studies have shown that more than 50% of American ..."
          ]
        },
        {
          "title": "Notes of MOSS and Followups | Programmer's Musings",
          "url": "https://alexkassil.github.io/2020/08/23/Notes-on-Moss-and-Followups.html",
          "excerpts": [
            "Aug 23, 2020 — A MOSS Tool Addressing Plagiarism at Scale. The original Moss project works great for single documents you want to check for plagiarism against ..."
          ]
        },
        {
          "title": "Winnowing: Local Algorithms for Document Fingerprinting",
          "url": "https://www.researchgate.net/publication/2840981_Winnowing_Local_Algorithms_for_Document_Fingerprinting",
          "excerpts": [
            "ArticlePDF Available. Winnowing: Local Algorithms for Document Fingerprinting. April 2003. DOI:10.1145/872757.872770. Authors: Saul David Schleimer at ...See more"
          ]
        },
        {
          "title": "A Plagiarism Detection Algorithm based on Extended ...",
          "url": "https://www.matec-conferences.org/articles/matecconf/pdf/2017/42/matecconf_eitce2017_02019.pdf",
          "excerpts": [
            "by X Duan · 2017 · Cited by 13 — This paper introduces the method of extending classic Winnowing plagiarism detection algorithm, expands the algorithm in functionality. The extended algorithm ..."
          ]
        },
        {
          "title": "Content similarity detection - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Content_similarity_detection",
          "excerpts": [
            "Citation-based plagiarism detection using citation pattern analysis is capable of identifying stronger paraphrases and translations with higher success rates ..."
          ]
        },
        {
          "title": "Parser that converts Wikiquote dumps into structured JSON - GitHub",
          "url": "https://github.com/heyseth/wickedQuotes",
          "excerpts": [
            "There aren't many large, public datasets of quotes online, so I created my own by parsing and cleaning up a Wikiquote data dump."
          ]
        },
        {
          "title": "Retrieve quotes from any Wikiquote article - GitHub",
          "url": "https://github.com/federicotdn/wikiquote",
          "excerpts": [
            "The wikiquote package for Python (>=3.8) allows you to search and retrieve quotes from any Wikiquote article, as well as retrieve the quote of the day."
          ]
        },
        {
          "title": "Wikipedia:Database download",
          "url": "https://en.wikipedia.org/wiki/Wikipedia:Database_download",
          "excerpts": [
            "Wikipedia offers free copies of all available content to interested users. These databases can be used for mirroring, personal use, informal backups, offline ..."
          ]
        },
        {
          "title": "CrossCheck Plagiarism Screening: Understanding the ...",
          "url": "https://www.ithenticate.com/plagiarism-detection-blog/bid/63534/CrossCheck-Plagiarism-Screening-Understanding-the-Similarity-Score",
          "excerpts": [
            "CrossCheck members use the iThenticate plagiarism detection system to screen submitted papers for originality and can quickly tell whether a paper contains ..."
          ]
        },
        {
          "title": "How Is AI (Really) Changing Peer Review?",
          "url": "https://paeditorial.co.uk/post/how-is-ai-really-changing-peer-review-and-do-humans-still-matter/",
          "excerpts": [
            "Plagiarism detection is perhaps the best known. Tools like Turnitin, iThenticate, and Crossref Similarity Check scan new manuscripts against vast databases ..."
          ]
        },
        {
          "title": "Plagiarism detection | Editors",
          "url": "https://www.elsevier.com/editor/perk/plagiarism-complaints/plagiarism-detection",
          "excerpts": [
            "Detecting plagiarism with the help of CrossRef Similarity Check and Editorial Manager's Duplicate Submission Check."
          ]
        },
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this research we propose methods to identify two important paraphrase types – synonymous substitution and word reordering in paraphrased, plagiarised sentence pairs.",
            "Plagiarism has been on the rise with the widespread availability of digital information and the ease with which it can be copied. Recent and past surveys suggest an increase in cases of plagiarism in both academic work and scientific literature",
            ") ). _Paraphrase plagiarism_ (Carmona et al. [2018](/articles/10.1186/s41239-021-00277-8 \"Carmona, M. Á. Á., Franco-Salvador, M., Villatoro-Tello, E., Montes-y-Gómez, M., Rosso, P., & Pineda, L. V. \\\\(2018\\\\). Semantically-informed distance and similarity measures for paraphrase plagiarism identification. Journal of Intelligent and Fuzzy Systems, 34\\\\(5\\\\), 2983–2990.\") )",
            " synonymous substitution and word reordering in paraphrased, plagiarised sentence pairs. We propose a three staged approach that uses context matching and pretrained word embeddings for identifying synonymous",
            "Synonymous substitution, word reordering and insertion/deletion have been identified as some of the common paraphrasing strategies used by plagiarists.",
            "Plagiarism detection refers to techniques, tools and methods used for automated detection of plagiarism, since manual detection becomes infeasible with large amounts of information.",
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris",
            "The output of the plagiarism detection module (matching sections of text) can be sent as input to the paraphrase type identification module."
          ]
        },
        {
          "title": "Plagiarism in a submitted manuscript - COPE Flowchart",
          "url": "https://publicationethics.org/guidance/flowchart/plagiarism-submitted-manuscript",
          "excerpts": [
            "A reviewer suspects plagiarism in a submitted manuscript. This flowchart provides a step by step process to help editors handle this problem. [Plagiarism](/guidance?f%5B0%5D=topics%3A25)",
            "\n\nThe journal should ask the reviewer for full documentary evidence and investigate the concerns. Authors should be contacted where there is clear plagiarism or copying, explaining the journal’s process and next steps. The author's institution should be informed, where necessary.",
            "Authors should be contacted where there is clear plagiarism or copying, explaining the journal’s process and next steps.",
            "The author's institution should be informed, where necessary.",
            "If the author has copied from their own work, refer to the flowchart redundant (duplicate) publication in a submitted manuscript."
          ]
        },
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism.",
            "Similarity Check allows editors to upload a paper, and instantly produces a report highlighting potential matches and indicating if and how the paper overlaps with other work.",
            "This report enables editors to assess the originality of the work before they publish it, providing confidence for publishers and authors, and evidence of trust for readers.",
            "If you participate in Similarity Check, not only do you get reduced rate access to iThenticate, but you also have the peace of mind of knowing that any similarity between your published content and manuscripts checked by other publishers will be flagged as a potential issue too.",
            "as the iThenticate database contains over 78 million full-text scholarly content items, editors can be confident that Similarity Check will provide a comprehensive and reliable addition to their workflow."
          ]
        },
        {
          "title": "Text recycling guidelines for editors",
          "url": "https://publicationethics.org/guidance/endorsed-guidance/text-recycling-guidelines-editors",
          "excerpts": [
            "\n\nA common issue encountered by editors is overlap of text with an author’s own previously published work, particularly with the increasing use of plagiarism detection software. This practice is known as ‘text recycling’ (also sometimes referred to as ‘self-plagiarism’).",
            ". These guidelines were developed by BioMed Central.",
            ". The guidelines cover how to deal with text recycling both in a submitted manuscript and a published article and include situations where text recycling may be acceptable as well as those where it is unlikely to be."
          ]
        },
        {
          "title": "iThenticate Crossref Similarity Check FAQS",
          "url": "https://www.ithenticate.com/training-crosscheck-faqs",
          "excerpts": [
            "A similarity report is a tool that helps editors identify potential scholarly misconduct in author manuscripts.",
            "The Similarity Report identifies and highlights every section of text within the manuscript that was found to match the iThenticate content repositories and produces an overall similarity percentage.",
            "the available exclusion options in order to remove matches to the bibliography, quoted text, abstract and methods sections, and small matches in order to display more relevant matches and display a more accurate similarity percentage.",
            "The biblliography exclusion excludes all text after the Bibliography keyword heading.",
            "Editors should contact their publishers if further clarification is needed on how to proceed with a manuscript determined to be plagiarized."
          ]
        },
        {
          "title": "util — Sentence Transformers documentation",
          "url": "https://sbert.net/docs/package_reference/util.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ( ..."
          ]
        },
        {
          "title": "A Comparative Analysis of Plagiarism Detection Algorithms",
          "url": "https://irojournals.com/tcsst/article/pdf/5/3/1",
          "excerpts": [
            "The winnowing algorithm can only calculate the similarity rate between documents, whereas the extended algorithm can mark the plagiarized text ...",
            "Jul 5, 2023 — The winnowing algorithm process includes first text preprocessing followed by forming k- grams of length k. After that, those k-gram hash values ..."
          ]
        },
        {
          "title": "What percentage of plagiarism is acceptable on Turnitin?",
          "url": "https://www.quora.com/What-percentage-of-plagiarism-is-acceptable-on-Turnitin",
          "excerpts": [
            "Regardless, plagiarism may exist at any score, 0 through 100 percent. Those who check Turnitin scores are concerned about (a) originality, and ( ..."
          ]
        },
        {
          "title": "Evaluation — Sentence Transformers documentation",
          "url": "https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html",
          "excerpts": [
            "Given a large set of sentences, this evaluator performs paraphrase (duplicate) mining and identifies the pairs with the highest similarity. It compare the ...",
            "... Threshold: 0.8352 ... query_chunk_size (int, optional) – To identify the paraphrases, the cosine-similarity between all sentence-pairs will be computed.See more"
          ]
        },
        {
          "title": "PAN at CLEF 2016 - Webis Group",
          "url": "https://pan.webis.de/clef16/pan16-web/",
          "excerpts": [
            "This session presents illustrative cases of translingual plagiarism and discusses some of the approaches adopted by forensic linguists.See more"
          ]
        },
        {
          "title": "A Semantic Similarity Approach to Paraphrase Detection",
          "url": "https://www.researchgate.net/publication/228616213_A_Semantic_Similarity_Approach_to_Paraphrase_Detection",
          "excerpts": [
            "This paper presents a novel approach to the problem of paraphrase identification. Al-though paraphrases often make use of syn-onymous or near synonymous ..."
          ]
        },
        {
          "title": "Authorship attribution using author profiling classifiers",
          "url": "https://www.cambridge.org/core/journals/natural-language-engineering/article/authorship-attribution-using-author-profiling-classifiers/9ADB32F9096C1212E8099BF016B0B218",
          "excerpts": [
            "by C Deutsch · 2023 · Cited by 16 — Authorship attribution has been a popular research topic in Natural Language Processing and the subject of several shared tasks in the PAN-CLEF ...See more"
          ]
        },
        {
          "title": "Analysis of Stylometric Variables in Long and Short Texts",
          "url": "https://www.sciencedirect.com/science/article/pii/S1877042813042080/pdf?md5=47d0c5e358cae45dbb22d87c76b7626e&pid=1-s2.0-S1877042813042080-main.pdf",
          "excerpts": [
            "by F López-Escobedo · 2013 · Cited by 28 — We conclude that the length of texts is a factor that affects the discriminatory capacity of the stylometric variables. We also found that there are certain ..."
          ]
        },
        {
          "title": "Understanding the Similarity Report",
          "url": "https://guides.ithenticate.com/hc/en-us/articles/27842305774605-Understanding-the-Similarity-Report",
          "excerpts": [
            "Jun 27, 2025 — Instructors can opt to exclude quotes from the Similarity Report to lower similarity scores where applicable. Example 4: A student has submitted ..."
          ]
        },
        {
          "title": "Audit Trail Checklist for 2025 (With Examples) - Sprinto",
          "url": "https://sprinto.com/blog/audit-trail/",
          "excerpts": [
            "An audit trail is a detailed series of records of human activities, application processes, data flows, system snapshots, transactions, administrative changes, ..."
          ]
        },
        {
          "title": "Plagiarism - Editorial Guidance Resource - Confluence",
          "url": "https://documentation.cochrane.org/egr/plagiarism-318472381.html",
          "excerpts": [
            "Editors are encouraged to, at minimum, check at least a portion of text for all protocols and reviews (including updates) when initially submitted."
          ]
        },
        {
          "title": "Plagiarism - Editorial Policies - Author Services - Taylor & Francis",
          "url": "https://authorservices.taylorandfrancis.com/editorial-policies/plagiarism/",
          "excerpts": [
            "Taylor & Francis takes the issue of plagiarism very seriously. Find out what plagiarism is (and isn't) and how you can avoid it."
          ]
        },
        {
          "title": "How to interpret the Turnitin Similarity Report (Instructors)",
          "url": "https://extensionhelpcenter.ucsd.edu/hc/en-us/articles/27459870154893-How-to-interpret-the-Turnitin-Similarity-Report-Instructors",
          "excerpts": [
            " Above all, it is important to understand that a high similarity score does not always suggest that a piece of writing has been plagiarized, and neither does a low similarity score always indicate that no plagiarism has occurred—in short, there is no ideal cutoff percentage or threshold . Therefore, the Turnitin Similarity Report is not a replacement for instructor expertise and manual review.",
            "Manual Review of Each Report is Essential: A thorough review of all flagged areas is crucial."
          ]
        },
        {
          "title": "Authorship verification for short messages using stylometry",
          "url": "https://ieeexplore.ieee.org/document/6705711",
          "excerpts": [
            "Authorship verification can be checked using stylometric techniques through the analysis of linguistic styles and writing characteristics of the authors.",
            "Stylometry is a behavioral feature that a person exhibits during writing and can be extracted and used potentially to check the identity of the author of online documents.",
            "Although stylometric techniques can achieve high accuracy rates for long documents, it is still challenging to identify an author for short documents, in particular when dealing with large authors populations.",
            "In this paper, we pose some steps toward achieving that goal by proposing a supervised learning technique combined with n-gram analysis for authorship verification in short texts.",
            "Experimental evaluation based on the Enron email dataset involving 87 authors yields very promising results consisting of an Equal Error Rate (EER) of 14.35% for message blocks of 500 characters."
          ]
        },
        {
          "title": "Plagiarism in a published article",
          "url": "https://publicationethics.org/guidance/flowchart/plagiarism-published-article",
          "excerpts": [
            "A reader suspects plagiarism in a published article. This flowchart provides a step by step process to help editors handle this problem.",
            "The journal's instructions to authors should include a definition of plagiarism and state the journal’s policy on plagiarism. Authors should be contacted explaining the journal’s process and next steps. The journal should consider whether a retraction or correction is required, depending on the degree of copying. Where necessary, editors of other journals involved should be informed.",
            "COPE Flowcharts and infographics — Plagiarism in a published article — English."
          ]
        },
        {
          "title": "Finding near-duplicates with Jaccard similarity and MinHash",
          "url": "https://blog.nelhage.com/post/fuzzy-dedup/",
          "excerpts": [
            "Jul 3, 2024 — In this post I want to explore the method of approximate deduplication via Jaccard similarity and the MinHash approximation trick."
          ]
        },
        {
          "title": "Searching for Near Duplicates with Minhash",
          "url": "https://skeptric.com/minhash-lsh/",
          "excerpts": [
            "May 9, 2020 — LSH can work really well as an online algorithm to efficiently check for near-duplicates in a large corpus, by storing and adding to these band hash tables."
          ]
        },
        {
          "title": "Near-duplicate Detection with Locality-Sensitive Hashing and ...",
          "url": "https://yorko.github.io/2023/practical-near-dup-detection/",
          "excerpts": [
            "Jun 27, 2023 — In this post, I review Locality-Sensitive Hashing for near-duplicate detection. I demonstrate the principle and provide a quick intro to Datasketch."
          ]
        },
        {
          "title": "Paraphrase Mining — Sentence Transformers documentation",
          "url": "https://sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ..."
          ]
        },
        {
          "title": "Implementation of Winnowing Algorithm with Dictionary ...",
          "url": "https://thesai.org/Downloads/Volume9No5/Paper_23-Implementation_of_Winnowing_Algorithm.pdf",
          "excerpts": [
            "Detection of plagiarism in this study will use a winnowing algorithm that has a function to check every character in two samples by hashing method that can ..."
          ]
        },
        {
          "title": "Authorship Verification Using the Impostors Method - CEUR-WS.org",
          "url": "https://ceur-ws.org/Vol-1179/CLEF2013wn-PAN-Seidman2013.pdf",
          "excerpts": [
            "The approach is based on comparing the similarity between the given documents and a number of external (impostor) documents, so that documents can be classified as having been written by the same author, if they are shown to be more similar to each other than to the impostors, in a number of trials."
          ]
        },
        {
          "title": "[PDF] Not All Character N-grams Are Created Equal: A Study in Authorship ...",
          "url": "https://aclanthology.org/N15-1010.pdf",
          "excerpts": [
            "Character n-grams have been identified as the most successful feature in both single- domain and cross-domain Authorship Attribu-."
          ]
        },
        {
          "title": "API:Get the contents of a page",
          "url": "https://www.mediawiki.org/wiki/API:Get_the_contents_of_a_page",
          "excerpts": [
            "The TextExtracts extension provides an API which allows you to retrieve plain-text or limited HTML extracts of page content. See Extension:TextExtracts for ..."
          ]
        },
        {
          "title": "Help:Export",
          "url": "https://en.wikiquote.org/wiki/Help:Export",
          "excerpts": [
            "Wiki pages can be exported in a special XML format to import into another MediaWiki installation or use it elsewise for instance for analysing the content. See ..."
          ]
        },
        {
          "title": "hewikiquote dump progress on 20250720 - Wikimedia Downloads",
          "url": "https://dumps.wikimedia.org/hewikiquote/20250720/",
          "excerpts": [
            "Missing: parser wickedQuotes"
          ]
        },
        {
          "title": "How to get WikiQuote Source by Wikidata API?",
          "url": "https://stackoverflow.com/questions/37765344/how-to-get-wikiquote-source-by-wikidata-api",
          "excerpts": [
            "Which claims represent the quotes on Wikiquote? For example, for the Knowledge page. And the corresponding WikiData JSON: https://www.wikidata."
          ]
        },
        {
          "title": "Quick implementation of character n-grams for word - Stack Overflow",
          "url": "https://stackoverflow.com/questions/18658106/quick-implementation-of-character-n-grams-for-word",
          "excerpts": [
            "My question is, how do I get an output that excludes the last character (ie t)? and is there a quicker and more efficient method for computing ..."
          ]
        },
        {
          "title": "An Improved Impostors Method for Authorship Verification",
          "url": "https://www.researchgate.net/publication/319138439_An_Improved_Impostors_Method_for_Authorship_Verification",
          "excerpts": [
            "In this paper, we propose a modification of the Impostors method that focuses on both appropriate selection of impostor documents and enhanced comparison of ..."
          ]
        },
        {
          "title": "[PDF] An Improved Impostors Method for Authorship Verification",
          "url": "https://icsdweb.aegean.gr/stamatatos/papers/CLEF-Potha-2017.pdf",
          "excerpts": [
            "In this paper, we propose a modification of the Impostors method that focuses on both appropriate selection of impostor documents and enhanced comparison of ..."
          ]
        },
        {
          "title": "Authorship Verification Based on SimCSE",
          "url": "https://ceur-ws.org/Vol-3497/paper-228.pdf",
          "excerpts": [
            "by Y Qiu · Cited by 2 — The goal of the PAN@CLEF 2023 Authorship verification [1][2] task is to determine whether the two texts are written by the same author by ..."
          ]
        },
        {
          "title": "Paraphrase detection and text reuse detection methods",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S1568494614005316",
          "excerpts": [
            "# Text reuse detection using a composition of text similarity measures",
            "A number of metrics have been proposed in the literature to measure text re-use between pairs of sentences or short passages.",
            "Basic lexical similarity measures have been used, e.g., Levenshtein edit distance, longest common subsequence (LCS), and several word *n*-gram (i.e., sequence of *n* contiguous words) overlap functions.",
            "Since individual measures of text similarity have their own strengths and limitations, it is expected that better results can be attained by combining multiple measures into one score which is then used to detect text similarity or reuse.",
            "Paraphrase detection is an important task in text analytics with numerous applications such as plagiarism detection, duplicate question identification, and enhanced customer support helpdesks."
          ]
        },
        {
          "title": "O*NET-SOC Taxonomy at O*NET Resource Center",
          "url": "https://www.onetcenter.org/taxonomy.html",
          "excerpts": [
            "Jul 1, 2025 — The new O*NET-SOC taxonomy includes 1,016 occupational titles, 923 of which represent O*NET data-level occupations. Based on the 2018 SOC System ..."
          ]
        },
        {
          "title": "See All Occupations",
          "url": "https://www.onetonline.org/find/all",
          "excerpts": [
            "See All Occupations Save Table: XLSX CSV ; 2, 53-6031.00, Automotive and Watercraft Service Attendants ; 2, 49-3021.00, Automotive Body and Related Repairers ; 3 ...",
            "See All Occupations Save Table: XLSX CSV. Find in list. 1016 of 1,016 occupations shown. Show Job Zones: All 1 2 3 4 5. Show occupations: All Data-level ..."
          ]
        },
        {
          "title": "List of SOC Occupations",
          "url": "https://www.bls.gov/oes/2023/may/oes_stru.htm",
          "excerpts": [
            "21-0000 Community and Social Service Occupations ; 21-1010 Counselors. 21-1012 Educational, Guidance, and Career Counselors and Advisors ; 21-1020 Social Workers."
          ]
        },
        {
          "title": "ESCO v1 | European Skills, Competences, Qualifications and ...",
          "url": "https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/esco-v1",
          "excerpts": [
            "Display of the skills hierarchy in the ESCO portal. The hierarchy is available in the skill section and can be downloaded in the download page; Inclusion of the ..."
          ]
        },
        {
          "title": "Standard Occupational Classification (SOC) System",
          "url": "https://www.bls.gov/soc/2018/",
          "excerpts": [
            "The Direct Match Title File ( [DMTF](/soc/2018/soc_2018_direct_match_title_file.xlsx) ) lists associated job titles for detailed SOC occupations. Each of these titles is a direct match to a single SOC occupation. All workers with a job title listed in the DMTF are classified in only one detailed SOC occupation code."
          ]
        },
        {
          "title": "LPDoctor/en_core_web_sm_job_related",
          "url": "https://huggingface.co/LPDoctor/en_core_web_sm_job_related",
          "excerpts": [
            "This spaCy-based Named Entity Recognition (NER) model has been custom-trained to recognize and classify entities related to \"profession,\" \"facility,\" and \"experience.\" It is designed to enhance your text analysis capabilities by identifying these specific entity types in unstructured text data.",
            "The model recognizes the following entity types:\n\n* PROFESSION: Represents professions or job titles. * FACILITY: Denotes facilities, buildings, or locations. * EXPERIENCE: Identifies mentions of work experience, durations, or qualifications."
          ]
        },
        {
          "title": "Named Entity Recognition (NER) of Short & Unstructured ...",
          "url": "https://medium.com/@ziprecruiter.engineering/named-entity-recognition-ner-of-short-unstructured-job-search-queries-6b265ec0fb",
          "excerpts": [
            "NER is a ML technique used to identify specific known words or phrases (named entities) within input text data, and categorize them according to predefined ..."
          ]
        },
        {
          "title": "GitHub - xitanggg/open-resume: OpenResume is a ...",
          "url": "https://github.com/xitanggg/open-resume",
          "excerpts": [
            "OpenResume is a powerful open-source resume builder and resume parser. The goal of OpenResume is to provide everyone with free access to a modern professional ..."
          ]
        },
        {
          "title": "removing emojis from a string in Python",
          "url": "https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python",
          "excerpts": [
            "There is a module named demoji in python which does this task very accurately and removes almost all types of emojis."
          ]
        },
        {
          "title": "How to Clean Up Your Social Media - clockworkTalent",
          "url": "https://clockworktalent.com/blog-5-steps-to-cleaning-up-your-social-media/",
          "excerpts": [
            "Here are my 5 steps to cleaning up your social media presence in time for a job search. Step 1. Do I need a separate professional profile?"
          ]
        },
        {
          "title": "demoji",
          "url": "https://pypi.org/project/demoji/",
          "excerpts": [
            "Accurately find or remove emojis from a blob of text using data from the Unicode Consortium's emoji code repository."
          ]
        },
        {
          "title": "regex to extract mentions in Twitter - python",
          "url": "https://stackoverflow.com/questions/43958026/regex-to-extract-mentions-in-twitter",
          "excerpts": [
            "To extract twitter handles a regex could be as simple as: @\\w{1,15} (allows characters, numbers and underscores and includes the 15 character limit)."
          ]
        },
        {
          "title": "Regular expression to extract twitter handle in python",
          "url": "https://stackoverflow.com/questions/66564171/regular-expression-to-extract-twitter-handle-in-python",
          "excerpts": [
            "The regex `^.*?\\btwitter\\.com/@?(\\w{1,15})(?:[?/,].*)?$` extracts Twitter handles, capturing 1-15 word characters in group 1."
          ]
        },
        {
          "title": "Role Analysis: Why Employers Should Normalize Job Titles",
          "url": "https://lightcast.io/resources/blog/role-analysis-why-employers-should-normalize-job-titles",
          "excerpts": [
            "s\nNormalization includes government taxonomies like Standard Occupation Codes (SOC) and O*NET.",
            "s\nTo normalize job titles is to capture the chaos. Job title normalization is the process of translating your internal language to the outside world. When your titles are normalized, they become the most relevant, utilized terminology across the industry—giving your team the clearest lens for role analysis.",
            ". Lightcast hosts the most comprehensive library of titles built from and connected to our entire data ecosystem. We continuously collect and analyze millions of titles from job postings, resumes, and profile data.",
            ", Lightcast Titles reflect what jobs are actually called, because they are based on real job titles."
          ]
        },
        {
          "title": "O*NET 29.3 Database",
          "url": "https://www.onetcenter.org/database.html",
          "excerpts": [
            "Jul 1, 2025 — The O*NET-SOC Occupation Taxonomy covers work performed in the U.S. economy and defines the set of occupations for which data is collected. The ...See more",
            "Jul 1, 2025 — 1,444 alternate titles related to 546 occupations were added from employer job postings and customer input. · Updated Survey Booklet Locations ..."
          ]
        },
        {
          "title": "Download - ESCO - European Union",
          "url": "https://esco.ec.europa.eu/en/use-esco/download",
          "excerpts": [
            "Download ESCO in a specific language, in CSV or ODS format · Version: Select your preferred version · Content: Choose classification · Language: Select your ..."
          ]
        },
        {
          "title": "Normalized dataset of 70k job titles",
          "url": "https://github.com/jneidel/job-titles",
          "excerpts": [
            "Normalized dataset of 70k job titles. Data Normalizations The data is normalized in the following ways: Caveats See also Contribute"
          ]
        },
        {
          "title": "Job Titles and Description",
          "url": "https://www.kaggle.com/datasets/jatinchawda/job-titles-and-description",
          "excerpts": [
            "Job Title Classification: Predicting or classifying job titles based on ... Usability 7.6 · 17 MB · 1,535 downloads. 6 Files (CSV, JSON). arrow_drop_up 22.See more"
          ]
        },
        {
          "title": "Structure of ESCO Downloadable Datasets - European Union",
          "url": "https://esco.ec.europa.eu/en/structure-esco-downloadable-datasets",
          "excerpts": [
            "This page provides details on ESCO Download packages, their internal structure, and guidance on creating connections and relations between various ESCO files."
          ]
        },
        {
          "title": "SOC home - U.S. Bureau of Labor Statistics",
          "url": "https://www.bls.gov/soc/",
          "excerpts": [
            "Missing: Match csv"
          ]
        },
        {
          "title": "tabiya-open-dataset",
          "url": "https://github.com/tabiya-tech/tabiya-open-dataset",
          "excerpts": [
            "It contains ESCO models in a CSV format that can be imported into the Tabiya Open Taxonomy Platform. The files can also be used as a standalone dataset for ..."
          ]
        },
        {
          "title": "Social Media best practices: Twitter",
          "url": "https://maddenmedia.com/social-media-best-practices-twitter/",
          "excerpts": [
            "Apr 17, 2018 — Clean up your bio. Does your bio reflect who you are as a DMO? It should. The bio is one of the first things a follower will see on your page."
          ]
        },
        {
          "title": "How to clean up your Twitter account? Simple tips that can ...",
          "url": "https://promorepublic.com/en/blogclean-twitter-account-simple-tips-can-help/",
          "excerpts": [
            "How to clean up your Twitter account? Simple tips that can help you! · Unfollow inactive accounts · Unfollow irrelevant accounts · Get a pinned tweet · Organize ..."
          ]
        },
        {
          "title": "Regex remove URLs paths and keep last slug of the URLs",
          "url": "https://unix.stackexchange.com/questions/691165/regex-remove-urls-paths-and-keep-last-slug-of-the-urls",
          "excerpts": [
            "Feb 18, 2022 — So basically it would look at the full URL path and ONLY keep the last slug which is imageN.png . For example these are the URLs parts that I am ..."
          ]
        },
        {
          "title": "Parsing usernames from Twitter, Facebook, and Instagram",
          "url": "https://dev.to/mattkenefick/regex-parsing-usernames-from-twitter-facebook-and-instagram-5l9",
          "excerpts": [
            "Sep 3, 2021 — We've had to parse out usernames from various social media URLs. We usually do this on user settings pages to make it easy when filling out forms."
          ]
        },
        {
          "title": "Bio To Schema: Why And How I Built My Custom GPT",
          "url": "https://www.lidia-infante.com/post/bio-to-schema",
          "excerpts": [
            "Jan 14, 2024 — The aim of Bio To Schema is to empower you to understand how to turn author bios into schema markup and help you prove the value of it to leadership."
          ]
        },
        {
          "title": "Why You Don't Want Job Title Normalization & ...",
          "url": "https://www.openprisetech.com/blog/data-normalization-basics-why-you-dont-really-want-to-normalize-job-title/",
          "excerpts": [
            "Jan 9, 2019 — Openprise maintains data sources of over 3,600 job title keywords sorted into job functions and of over 1,300 job title keywords in multiple ..."
          ]
        },
        {
          "title": "Twitter(X) Profile Bio ICP Classifier - Bio Keywords Extractor",
          "url": "https://apify.com/lead.gen.labs/twitter-x-profile-bio-icp-classifier---bio-keywords-extractor",
          "excerpts": [
            "Classify Twitter (X) profiles by using bio keyword extraction. This tool scrapes public Twitter bios and intelligently extracts relevant keywords to help you ...See more"
          ]
        },
        {
          "title": "Models",
          "url": "https://huggingface.co/models?other=named-entity-recognition",
          "excerpts": [
            "We're on a journey to advance and democratize artificial intelligence through open source and open science."
          ]
        },
        {
          "title": "Job Titles classification",
          "url": "https://kb.lightcast.io/en/articles/7907529-job-titles-classification",
          "excerpts": [
            "Lightcast classifies raw job titles to over 70,000 standardized titles using a machine learning model that normalizes and matches titles based on similarity."
          ]
        },
        {
          "title": "2018 SOC Direct Match Title File (BLS)",
          "url": "https://www.bls.gov/soc/2018/soc_2018_direct_match_title_file.pdf",
          "excerpts": [
            "***Questions should be emailed to soc@bls.gov***. 2018 SOC Code. 2018 SOC Title. 2018 SOC Direct Match Title. Illustrative Example. 11-1011. Chief Executives. Direct Match Title File, 2018 SOC",
            "Admiral",
            "CEO",
            "Chief Executive Officer",
            "Chief Financial Officer",
            "Chief Operating Officer",
            "Chief Sustainability Officer",
            "Commissioner of Internal Revenue",
            "COO",
            "County Commissioner",
            "Government Service Executive",
            "Governor",
            "Mayor",
            "School Superintendent",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "11-1011",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "Chief Executives",
            "University President",
            "Department Store General Manager",
            "Department Store Manager",
            "General Manager",
            "General Superintendent",
            "x",
            "x",
            "x",
            "x",
            "x",
            "x",
            "x",
            "Manufacturing Operations Manager",
            "General and Operations Managers",
            "General and Operations Managers",
            "General and Operations Managers",
            "General and Operations Managers",
            "General and Operations Managers",
            "General and Operations Managers",
            "Operational Risk"
          ]
        },
        {
          "title": "Extracting occupation identities from biographies: Self-reported job titles and SOC mapping (AAAI ICWSM 2024)",
          "url": "https://ojs.aaai.org/index.php/ICWSM/article/download/31330/33490/35386",
          "excerpts": [
            " Title (Bios)\t\t\t\t\t\t   Occupation Category (SOC)\t\t\t\t\t\t\t\t\t  Match Mention\t ",
            "Job title extractor: https://github.com/fluquid/find_job_titles",
            "he\t\t\t\t 10\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t We use Rouge-L F1 score to softly align reported occupations\n"
          ]
        },
        {
          "title": "Lightcast Job Titles",
          "url": "https://kb.lightcast.io/en/articles/6957694-lightcast-job-titles",
          "excerpts": [
            "Lightcast maintains an extensive library of over 75000 job titles. Raw titles are standardized to these titles."
          ]
        },
        {
          "title": "Resources for Implementing Changes to the IPEDS Human ...",
          "url": "https://nces.ed.gov/ipeds/Section/resources_soc",
          "excerpts": [
            "2018 SOC Direct Match Title File (DMTF) (Excel, 204 KB) This file, which is updated periodically, lists associated job titles for detailed SOC occupations."
          ]
        },
        {
          "title": "Releasing Emsi Open Titles | Blog - Lightcast",
          "url": "https://lightcast.io/resources/blog/releasing-emsi-open-titles",
          "excerpts": [
            " Emsi Open Titles , a new open-source library designed to help companies clean up their job-title chaos and transform the way they benchmark their talent. Employers frequently wrestle with three problems when it comes to their own job titles:",
            "Emsi Open Titles distills over 20 million real-world jobs titles (collected from over a billion job postings, resumes, and profiles) into 75,000 standardized titles. Through a process called normalization , Emsi Open Titles uses these standardized titles to translate employers’ internal titles into the language of SOC codes"
          ]
        },
        {
          "title": "UIS Data Browser - UNESCO",
          "url": "https://databrowser.uis.unesco.org/",
          "excerpts": [
            "The data browser allows users to view and filter data and metadata, visualize and share it or download it in various formats (csv, excel)."
          ]
        },
        {
          "title": "International Standard Classification of Occupations (ISCO)",
          "url": "https://ilostat.ilo.org/methods/concepts-and-definitions/classification-occupation/",
          "excerpts": [
            "ISCO is a statistical framework that organizes jobs into a clearly defined set of groups according to the tasks and duties undertaken in the job."
          ]
        },
        {
          "title": "Mining the Social Web, 3rd Edition",
          "url": "https://www.oreilly.com/library/view/mining-the-social/9781491973547/ch04.html",
          "excerpts": [
            "Mining LinkedIn: Faceting Job Titles, Clustering Colleagues, and More. This chapter introduces techniques and considerations for mining the troves of data ..."
          ]
        },
        {
          "title": "resume-parser",
          "url": "https://github.com/topics/resume-parser",
          "excerpts": [
            "Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions."
          ]
        },
        {
          "title": "Research Guides: Text Mining: Social Media",
          "url": "https://guides.library.columbia.edu/text-mining/social-media",
          "excerpts": [
            "Jun 5, 2025 — Text mining social media presents special challenges to researchers. At the same time, the social media platforms lure researchers in with their breadth."
          ]
        },
        {
          "title": "Language identification",
          "url": "https://fasttext.cc/docs/en/language-identification.html",
          "excerpts": [
            "FastText provides two models for language identification, recognizing 176 languages. One is faster (126MB), the other compressed (917kB)."
          ]
        },
        {
          "title": "google/cld3",
          "url": "https://github.com/google/cld3",
          "excerpts": [
            "Jun 15, 2024 — CLD3 is a neural network model for language identification. This package contains the inference code and a trained model."
          ]
        },
        {
          "title": "Convert emoji into text in Python",
          "url": "https://www.geeksforgeeks.org/python/convert-emoji-into-text-in-python/",
          "excerpts": [
            "Apr 6, 2021 — Converting emoticons or emojis into text in Python can be done using the demoji module. It is used to accurately remove and replace emojis in text strings."
          ]
        },
        {
          "title": "facebook/fasttext-language-identification",
          "url": "https://huggingface.co/facebook/fasttext-language-identification",
          "excerpts": [
            "This LID (Language IDentification) model is used to predict the language of the input text, and the hosted version ( lid218e ) was released as part of the NLLB ..."
          ]
        },
        {
          "title": "Google's Compact Language Detector 3 • cld3 - Docs",
          "url": "https://docs.ropensci.org/cld3/",
          "excerpts": [
            "Google's Compact Language Detector 3 is a neural network model for language identification and the successor of CLD2 (available from) CRAN."
          ]
        },
        {
          "title": "Language Identification using the 'fastText' package (a ...",
          "url": "https://cran.r-project.org/web/packages/fastText/vignettes/language_identification.html",
          "excerpts": [
            "Feb 17, 2024 — The fastText language identification pre-trained models support currently 176 languages. ... fastText based on the smaller pre-trained model 'lid."
          ]
        },
        {
          "title": "Remove all kind of emojis with the “demoji” package Python",
          "url": "https://s-hosseinkhani1999.medium.com/remove-all-kind-of-emojis-with-the-demoji-package-python-643a530491f4",
          "excerpts": [
            "Here is a simple code for who wants to remove emojis from the text! First: install demoji with pip install demoji Second: import this package on your test.py ..."
          ]
        },
        {
          "title": "regex fails at matching Twitter links - Reddit",
          "url": "https://www.reddit.com/r/regex/comments/spk8el/regex_fails_at_matching_twitter_links/",
          "excerpts": [
            "Yes, all URLs that should match do so at regex101, but with Python, all I get is an empty list. The other, more inclusive raw string works well, ..."
          ]
        },
        {
          "title": "find_job_titles README.rst",
          "url": "https://github.com/fluquid/find_job_titles/blob/master/README.rst",
          "excerpts": [
            "Find Job Titles in Strings",
            "* Free software: MIT license",
            "* Python versions: 2.7, 3.4+",
            "Features",
            "* Find any of 77k job titles in a given string",
            "* Text processing is extremely fast using \"acora\" library",
            "* Dictionary generation takes about 20 seconds upfront",
            "Quickstart",
            "----------",
            "Instantiate \"Finder\" and start extracting job titles:",
            "```\n>>> from find_job_titles import FinderAcora\n>>> finder=FinderAcora()\n>>> finder.findall(u'I am the Senior Vice President')\n[('Senior Vice President', 9),\n ('Vice President', 16),\n ('President', 21)]\n\n```",
            "All possible, overlapping matches are returned. Matches contain positional information of where the match was found."
          ]
        },
        {
          "title": "Alternate Titles - O*NET Occupation Data Updates at O* ...",
          "url": "https://www.onetcenter.org/dataUpdates/categories/Alternate_Titles",
          "excerpts": [
            "Jul 1, 2025 — 13-2011.00, Accountants and Auditors, 2025. 27-2011.00, Actors, 2025. 15-2011.00, Actuaries, 2025. 29-1141.01, Acute Care Nurses, 2025."
          ]
        },
        {
          "title": "O*NET Alternate Titles Procedures",
          "url": "https://www.onetcenter.org/reports/AltTitles.html",
          "excerpts": [
            "This paper details the multi-method data collection approach used to populate the Alternate Titles. Data sources include incumbent/occupational expert data, ..."
          ]
        },
        {
          "title": "A python script to preprocess text (remove URL, lowercase, ...",
          "url": "https://gist.github.com/MrEliptik/b3f16179aa2f530781ef8ca9a16499af",
          "excerpts": [
            "A python script to preprocess text (remove URL, lowercase, tokenize, etc ... \"\"\"Remove URLs from a sample string\"\"\". return re.sub(r\"http\\S+\", \"\", sample)."
          ]
        },
        {
          "title": "How to remove emoji and other language in a DataFrame",
          "url": "https://stackoverflow.com/questions/72695301/how-to-remove-emoji-and-other-language-in-a-dataframe",
          "excerpts": [
            "This can be done using the demoji library in python. For using do a pip install pip install demoji. Than"
          ]
        },
        {
          "title": "Alternate Titles (ONET Center)",
          "url": "https://www.onetcenter.org/dictionary/20.3/excel/alternate_titles.html",
          "excerpts": [
            "This file contains alternate, or \"lay\", occupational titles for the O\\*NET-SOC classification system. The file was developed to improve keyword searches in several Department of Labor internet applications (i.e., Career InfoNet, O\\*NET OnLine, and O\\*NET Code Connector).",
            "The file is displayed in five tab delimited fields with the columns named O\\*NET-SOC Code, Title, Alternate Title, Short Title, and Source(s).",
            "There are a total of 60,511 rows of data in this file.",
            "Column | Type | Column Content |",
            "| O\\*NET-SOC Code | Character(10) | O\\*NET-SOC Code _(see [_Occupation Data_](occupation_data.html \"Occupation Data\") )_ |",
            "| Title | Character Varying(150) | O\\*NET-SOC Title _(see [_Occupation Data_](occupation_data.html \"Occupation Data\") )_ |",
            "Alternate Title | Character Varying(150) | Alternate occupational title |",
            "Short Title | Character Varying(150) | Short version of alternate title (if applicable) |",
            "Source(s) | Character Varying(50) | List of source codes — see below |"
          ]
        },
        {
          "title": "Problem with getting user.fields from Twitter API 2.0",
          "url": "https://stackoverflow.com/questions/66615958/problem-with-getting-user-fields-from-twitter-api-2-0",
          "excerpts": [
            "I want to load tweets from Twitter API 2.0 endpoint and try to get the standard fields (author, text, ...) and some expansion field, esp. user.fields."
          ]
        },
        {
          "title": "Filtered stream version comparison | Docs - Twitter Developers",
          "url": "https://developer.x.com/en/docs/twitter-api/tweets/filtered-stream/migrate",
          "excerpts": [
            "Comparing X API's filtered stream endpoints ; Default request rate limits, 5 connection attempts per 5 min. 60 requests per min aggregated for both POST and GET ...See more"
          ]
        },
        {
          "title": "AdriaPadilla/Twitter-API-V2-full-archive-Search-academics - GitHub",
          "url": "https://github.com/AdriaPadilla/Twitter-API-V2-full-archive-Search-academics",
          "excerpts": [
            "API RATE LIMITS. Twitter API V2, and more precisely, Twitter Full-archive search for Academic Research, have a rate limit of 300 request in a 15 min window."
          ]
        },
        {
          "title": "PowerTrack API migration to X API v2 filtered stream",
          "url": "https://developer.x.com/en/docs/twitter-api/tweets/filtered-stream/migrate/powertrack-api-migration-to-twitter-api-v2",
          "excerpts": [
            "Similar to the PowerTrack API and Rules API, the new X API v2 filtered stream endpoints allows you to apply multiple rules to a single stream and add and remove ...See more"
          ]
        },
        {
          "title": "Twitter API V2 - Full archive search - Rate limit and cap - X Developers",
          "url": "https://devcommunity.x.com/t/twitter-api-v2-full-archive-search-rate-limit-and-cap/160987",
          "excerpts": [
            "I am using the Twitter API V2 for academics. I am retrieving tweets on a certain topic since 2010. Sadly, I am all the time getting to my rate limit."
          ]
        },
        {
          "title": "Understanding the new Tweet payload in the Twitter API v2",
          "url": "https://dev.to/xdevs/understanding-the-new-tweet-payload-in-the-twitter-api-v2-1fg5",
          "excerpts": [
            "With the new v2 payload, you can request the public_metrics object on the user object that will include fields such as: { \"public_metrics ...",
            "This new API allows for a new way of requesting the Tweet payload, giving you greater control by allowing you to request specific fields that ..."
          ]
        },
        {
          "title": "Problem with getting tweet_fields from Twitter API 2.0 using Tweepy",
          "url": "https://stackoverflow.com/questions/70371657/problem-with-getting-tweet-fields-from-twitter-api-2-0-using-tweepy",
          "excerpts": [
            "To access the data of the Tweet object, you can use attributes or keys (like a dictionary) to access each field. If you want all the data as a ..."
          ]
        },
        {
          "title": "Rehydrating X Posts",
          "url": "https://developer.meltwater.com/docs/meltwater-api/listening/rehydrating-x-posts/",
          "excerpts": [
            "In this guide we look at how you can use these IDs to fetch full details from the X API, in a process called 'rehydration'."
          ]
        },
        {
          "title": "Models — tweepy 4.14.0 documentation",
          "url": "https://docs.tweepy.org/en/stable/v2_models.html",
          "excerpts": [
            "The Twitter API v2 currently supports three event types: MessageCreate, ParticipantsJoin, and ParticipantsLeave. DM event objects are returned by the Direct ...",
            "For example, if the parent Tweet is a Retweet, a Retweet with comment (also known as Quoted Tweet) or a Reply, it will include the related Tweet referenced to ..."
          ]
        },
        {
          "title": "V2 API returning 0 for most of public_metrics - X Developers",
          "url": "https://devcommunity.x.com/t/v2-api-returning-0-for-most-of-public-metrics/166669",
          "excerpts": [
            "public_metrics shows 0 for everything except retweet_count on the data object of a RT, but shows the right data in the includes.tweets object.",
            "Feb 14, 2022 — public_metrics shows 0 for everything except retweet_count on the data object of a RT, but shows the right data in the includes.tweets object."
          ]
        },
        {
          "title": "GET /labs/2/tweets/:id | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/labs/tweets-and-users/api-reference/get-tweets-id",
          "excerpts": [
            "To return this field, add tweet.fields=public_metrics in the request's query parameter. public_metrics.retweet_count, integer, Number of times this Tweet has ...",
            "The unique identifier of the referenced Tweet. You can obtain the expanded object in includes.tweets by adding expansions=referenced_tweets.id in the request's ...",
            "To return this field, add tweet.fields=public_metrics in the request's query parameter.",
            "| `created_at` | date (ISO 8601) | Creation time of the Tweet. To return this field, add `tweet.fields=created_at` in the request's query parameter.",
            "public_metrics.retweet_count, integer, Number of times this Tweet has ... https://t.co/Hg8nkfoizN\"",
            "### Response fields",
            "| Name | Type | Description |",
            "| --- | --- | --- |"
          ]
        },
        {
          "title": "X (Twitter) Official API Pricing Tiers 2025",
          "url": "https://twitterapi.io/blog/twitter-api-pricing-2025",
          "excerpts": [
            "| --- | --- | --- |"
          ]
        },
        {
          "title": "X API v2",
          "url": "https://documenter.getpostman.com/view/9956214/T1LMiT5U",
          "excerpts": [
            "Comma-separated list of fields for the Tweet object. Expansion required. Allowed values: attachments,author_id,context_annotations,conversation_id,created_at, ..."
          ]
        },
        {
          "title": "Expansions and Fields",
          "url": "https://docs.tweepy.org/en/stable/expansions_and_fields.html",
          "excerpts": [
            "Available expansions for Tweet payloads ; entities.mentions.username. Returns a user object for the user mentioned in the Tweet ; referenced_tweets.id.author_id."
          ]
        },
        {
          "title": "URL for a link to Twitter for a specific tweet - Stack Overflow",
          "url": "https://stackoverflow.com/questions/5652136/url-for-a-link-to-twitter-for-a-specific-tweet",
          "excerpts": [
            "The URL is of the form: http://twitter.com/{twitter-user-id}/status/{tweet-status-id} where the content in curly braces is actual data extracted from the tweet."
          ]
        },
        {
          "title": "Rate Limiting | Docs | Twitter Developer Platform - X",
          "url": "https://developer.x.com/ja/docs/basics/rate-limiting",
          "excerpts": [
            "x-rate-limit-remaining: the number of requests left for the 15 minute window; x-rate-limit-reset: the remaining window before the rate limit resets, in UTC ..."
          ]
        },
        {
          "title": "Twitter's Developer Policies for Researchers, Archivists, and Librarians",
          "url": "https://medium.com/on-archivy/twitters-developer-policies-for-researchers-archivists-and-librarians-63e9ba0433b2",
          "excerpts": [
            "I will unpack some key portions of Twitter's Developer Policies that are relevant to research and archiving and offer my interpretation.",
            "You may not distribute Tweet IDs for the purposes of (a) enabling any entity to store and analyze Tweets for a period exceeding 30 days unless ..."
          ]
        },
        {
          "title": "getting-started-with-the-twitter-api-v2-for-academic- ...",
          "url": "https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/2-choosing-the-right-product-track.md",
          "excerpts": [
            "Build the full Tweet objects from a Tweet ID, or a set of Tweet IDs; Look up follower relationships. These are just some examples of what you can get from the ..."
          ]
        },
        {
          "title": "twitter api v2 expansion field option returns less data than ...",
          "url": "https://stackoverflow.com/questions/65396881/twitter-api-v2-expansion-field-option-returns-less-data-than-the-endpoint",
          "excerpts": [
            "Twitter APIv2 Expansions Using Tweepy · 1 · Tweepy for Twitter API v2 - Extracting Additional Fields for Tweet Search · Hot Network Questions."
          ]
        },
        {
          "title": "How to add a Tweet button to your website - Twitter Developers",
          "url": "https://developer.x.com/en/docs/x-for-websites/tweet-button/overview",
          "excerpts": [
            "The url parameter contains an absolute HTTP or HTTPS URL to be shared on X. The shared URL will be automatically shortened in a published Tweet. A Card may ..."
          ]
        },
        {
          "title": "Pagination with next_token - X API v2 - X Developers",
          "url": "https://devcommunity.x.com/t/pagination-with-next-token/149832",
          "excerpts": [
            "In your code example, you need to update query_params with the next_token you get, otherwise you keep requesting the same page over and over."
          ]
        },
        {
          "title": "X API v2 Expansions - Fundamentals",
          "url": "https://docs.x.com/x-api/fundamentals/expansions",
          "excerpts": [
            "With expansions, developers can expand objects referenced in the payload. Objects available for expansion are referenced by ID.",
            "If you would like to request fields related to the user that posted that Post, or the media, poll, or place that was included in that Post, you will need to pass the related expansion query parameter in your request to receive that data in your response",
            "When including an expansion in your request, we will include that expanded object’s default fields within the same response.",
            "{",
            "    \"data\" : {",
            "        \"attachments\" : {",
            "            \"media_keys\" : [ \"16_1211797899316740096\" ]",
            "        },",
            "        \"author_id\" : \"2244994945\" ,",
            "        \"id\" : \"1212092628029698048\" ,",
            "        \"referenced_tweets\" : [",
            "            {",
            "                \"type\" : \"replied_to\" ,",
            "                \"id\" : \"1212092627178287104\"",
            "            }",
            "        ],",
            "\n        \"text\" : \"We believe the best future version of our API will come from building it with YOU. Here’s to another great year with everyone who builds on the Twitter platform. We can’t wait to continue working with you in the new year.",
            "}"
          ]
        },
        {
          "title": "Fields - X",
          "url": "https://docs.x.com/x-api/fundamentals/fields",
          "excerpts": [
            "{",
            "}",
            "}",
            "}",
            "}"
          ]
        },
        {
          "title": "Pagination - Welcome to the X Developer Platform",
          "url": "https://docs.x.com/x-api/posts/search/integrate/paginate",
          "excerpts": [
            "The recent search endpoints will respond to a query with at least one page, and provide a next_token in its JSON response if additional pages are available."
          ]
        },
        {
          "title": "Developer policy support | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/support/x-api/policy",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 ...",
            "If you provide X Content to third parties, including downloadable datasets or via an API, you may only distribute X IDs, Direct Message IDs, and/or User IDs ( ... Support > X API > Developer policy\n",
            " X Content, you must comply with ALL X policies. These include this Developer Policy , the Automation Rules , the Display Requirements , the API Restricted Uses Rules , the X Rules , the X Brand Resources , the Periscope Community Guidelines , and the Periscope Trademark Guidelines , as well as any other agreements you enter into with X relating to your use of the X API or X Content, including but not limited to the Developer Agreement or a Master Licensing Agreement or Order (as applicable)."
          ]
        },
        {
          "title": "Get Tweet AND User Object - X Developers",
          "url": "https://devcommunity.x.com/t/get-tweet-and-user-object/150675",
          "excerpts": [
            "... username,public_metrics,verified', \\ 'expansions': 'author_id' }. My ... {'public_metrics': {'retweet_count': 5, 'reply_count': 2, 'like_count ..."
          ]
        },
        {
          "title": "User lookup standard v1.1 to v2 migration guide - Twitter Developers",
          "url": "https://developer.x.com/en/docs/x-api/users/lookup/migrate/standard-to-twitter-api-v2",
          "excerpts": [
            "The X API v2 version only delivers the user id , name, and username fields by default . To request any additional fields or objects, you wil need to use the fields and expansions parameters. Any user fields that you request from this endpoint will return in the primary user object."
          ]
        },
        {
          "title": "Rate Limit Confusion - X API v2 - X Developers",
          "url": "https://devcommunity.x.com/t/rate-limit-confusion/221364",
          "excerpts": [
            "The rate limits for the GET /2/users/:id/tweets endpoint for project's with the Basic plan are: 5 requests / 15 mins per user; 10 requests / 15 ..."
          ]
        },
        {
          "title": "Does showing a lot of tweets by a site violates the TOS?",
          "url": "https://devcommunity.x.com/t/does-showing-a-lot-of-tweets-by-a-site-violates-the-tos/175606",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 Tweet IDs ... In addition, any content shared remains subject to the Twitter Developer Policy ...",
            "Aug 17, 2022 — In total, you may not distribute more than 1,500,000 Tweet IDs to ... non-automated means (e.g., download of spreadsheets or PDFs). In ..."
          ]
        },
        {
          "title": "Question about 6)b)i) of the developer policy",
          "url": "https://devcommunity.x.com/t/question-about-6-b-i-of-the-developer-policy/27542",
          "excerpts": [
            "Would 6) b) i) allow us to redistribute the raw tweet text alongside the annotations, provided we limited the total number of downloads? Related ..."
          ]
        },
        {
          "title": "Twitter API - Tweet Object and Data Dictionary",
          "url": "https://developer.x.com/en/docs/x-api/v1/data-dictionary/object-model/tweet",
          "excerpts": [
            "Tweet Object\n\nTweets are the basic atomic building block of all things Twitter. Tweets are also known as “status updates.” The Tweet object has a long list of ‘root-level’ attributes, including fundamental attributes such as `id` , `created_at` , and `text` .",
            "\"retweet_count\":160",
            "\"favorite_count\":295",
            "The user who posted this Tweet. See User data dictionary for complete list of attributes. Example highlighting select attributes:"
          ]
        },
        {
          "title": "Sampling Procedures for Inspection and Sampling Plans ...",
          "url": "https://www.powermag.com/sampling-procedures-for-inspection-and-sampling-plans-for-lot-inspection-using-iso-2859/",
          "excerpts": [
            "The AQL defines the percentage of defects at which consumers are willing to accept lots as “good.” The producers would like to design a sampling ..."
          ]
        },
        {
          "title": "ANSI / ASQ Z1.4-2003 (R2018): Sampling Procedures and ...",
          "url": "https://aar.com/standards/pdfs/2023QAConferencePresentations/6%20-%20ANSI%20Z1.4%20Sample%20Methodology%20-%20Steve%20Geneva.pdf",
          "excerpts": [
            "It is an acceptance sampling system used to inspect incoming, in-process, and final products to determine compliance with established acceptance."
          ]
        },
        {
          "title": "What Do the Parts of the ANSI ASQ Z1.4 AQL Table Mean?",
          "url": "https://www.intouch-quality.com/blog/anatomy-of-the-ansi-asq-z1.4-industry-standard-aql-table",
          "excerpts": [
            "Aug 27, 2019 — There are five main parts of the single-sampling ANSI Z1.4 table: lot sizes, inspection levels, sample size code letters, acceptable quality levels and ..."
          ]
        },
        {
          "title": "Understanding Operating Characteristics Curves",
          "url": "https://www.youtube.com/watch?v=n8VeldCFea4",
          "excerpts": [
            "The best way to understand the statistics behind AQL inspections (based on MIL-STD 105E, ANSI Z1.4, ISO 2859) is to draw the OC curve."
          ]
        },
        {
          "title": "Best Practices for Labeling Data for AI",
          "url": "https://www.atltranslate.com/ai/blog/labeling-data-best-practices",
          "excerpts": [
            "Techniques such as Human In The Loop (HITL) annotation and quality assurance processes can further enhance the quality of labeled datasets ..."
          ]
        },
        {
          "title": "Changing Guidelines: Best Practices for Maintaining Data Quality",
          "url": "https://argilla.io/blog/annotation-guidelines-practices/",
          "excerpts": [
            "May 3, 2024 — Well-defined annotation guidelines are crucial for ensuring data quality and consistency in any annotation project."
          ]
        },
        {
          "title": "Guide: Building a Data Labeling Practice for Machine ...",
          "url": "https://humansignal.com/blog/guide-building-a-data-labeling-practice-for-machine-learning-and-data-science/",
          "excerpts": [
            "The HITL process begins by having humans label a dataset sample for training a seed model. The seed model is then used to train machines to identify and label ..."
          ]
        },
        {
          "title": "Inter-rater reliability - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Inter-rater_reliability",
          "excerpts": [
            "In statistics, inter-rater reliability is the degree of agreement among independent observers who rate, code, or assess the same phenomenon."
          ]
        },
        {
          "title": "How to compare the similarity of documents with Simhash ...",
          "url": "https://stackoverflow.com/questions/49820228/how-to-compare-the-similarity-of-documents-with-simhash-algorithm",
          "excerpts": [
            "Simhash is useful as it detects near duplicates. This means that near duplicates will end up with the same hash. · For exact duplicates you can ..."
          ]
        },
        {
          "title": "Validate Data - Great Expectations documentation",
          "url": "https://docs.greatexpectations.io/docs/0.18/oss/guides/validation/validate_data_lp/",
          "excerpts": [
            "Validate Data, save Validation Results, run Actions, and create Data Docs. Data Validation workflow Learn more about the GX Data Validation process."
          ]
        },
        {
          "title": "Great Expectations Tutorial: Validating Data with Python - DataCamp",
          "url": "https://www.datacamp.com/tutorial/great-expectations-tutorial",
          "excerpts": [
            "In this guide, we will walk you through the process of using Great Expectations for data validation, with a practical end-to-end example to help you get ..."
          ]
        },
        {
          "title": "How To Use SimHash — The Ultimate Guide",
          "url": "https://spotintelligence.com/2023/01/02/simhash/",
          "excerpts": [
            "Jan 2, 2023 — Near duplicate detection is a process used to identify and determine the similarity between pieces of text or documents. It involves comparing ..."
          ]
        },
        {
          "title": "Acceptance Sampling - CQE Academy",
          "url": "https://cqeacademy.com/cqe-body-of-knowledge/product-process-control/acceptance-sampling/",
          "excerpts": [
            "A full how to guide for Acceptance Sampling - topics include Single Sampling Plans, OC Curves, Definitions and a free Quiz to test your knowledge.",
            "Acceptance Sampling** originated in the 1930’s at Bell Labs through the work of Harold Dodge and was later popularized during World War II by the U.S. Military for munitions (bullets) productio",
            "Acceptance Sampling became the **compromise** between no inspection and 100% inspection, and allowed the manufacturers to ***infer*** the overall “quality” of an entire lot while only testing a fraction of the entire lot.",
            "**The second part is the sampling standards and plans** – This includes a review of **ANSI/ASQ Z1.4 and Z1.9 standards**, an explanation of **single, double, multiple, sequential, and continuous sampling plans**.",
            "Part 1 – Sampling Concepts",
            "This includes topics such as the operating characteristic (OC) curve, producer and consumer risk, and discussion of common sampling terms like AQL, LTPD, ",
            "ts:**\n\n**The First is sampling concepts** – This includes topics such as the **operating characteristic (OC) curve**, **producer and consumer risk**, and discussion of common sampling terms like **AQL, LTPD. **",
            "ve\n\nLTPD is another common metric in acceptance sampling and it reflects the quality limit, in percent defective, that is the poorest quality that can be tolerated.",
            "In the original military standards for acceptance sampling, **AQL stood for Acceptable Quality Level. ** This connotation of an “**acceptable level**” of quality led to some serious miss-interpretations and has been revised to **Acceptance Quality Lim",
            "If we look at this OC Curve above, we can tell that the AQL associated with this sampling plan is 2%, and the LTPD for this sampling plan is 8%."
          ]
        },
        {
          "title": "Acceptance Sampling Standards and AQL",
          "url": "https://www.asq104.org/app/download/555762704/Acceptance+Sampling+D+OLeary+20110216.pdf",
          "excerpts": [
            "ANSI/ASQ Z1.4 Sampling Procedures and Tables for Inspection By",
            "Attributes",
            "\n",
            "Acceptance Sampling is Common . . . •   The most common place for acceptance sampling is incoming\n\tma",
            "The AQL provides a criterion against which to judge lots.",
            "Z1.4 uses the AQL to index the sampling plans.",
            "The supplier produces product in lots",
            "The input to this process (where I inspect) is defined as:",
            "Attribute Sampling",
            "The type and history get us to the right table. • The Code Letter and AQL get us to the sampling plan.",
            "The AQL concept\n• The AQL is the poorest level of quality (percent\n  nonconforming) that the process can tolerate.",
            "Acceptance Sampling is Common"
          ]
        },
        {
          "title": "What an Automated Data Validation Tool Can Do For Operational and Transactional Data",
          "url": "https://firsteigen.com/blog/what-an-automated-data-validation-tool-can-do-for-operational-and-transactional-data/",
          "excerpts": [
            "Data validation ensures that data is current, conforms to standards, consistent, complete, unique and accurate.",
            "Automated data validation uses AI/ML and other advanced technologies to automatically identify low-quality data.",
            "In Fortune-2000 companies we have found these contribute to a majority of errors the business users experience.",
            "use. The process involves checking many dimensions of data to ensure it is usable.",
            "ccurate. * Automated data validation is faster and more accurate than manual methods.",
            "Automated data validation tools catch more existing errors and eliminate human e",
            "6 Important Benefits of Using an Automated Data Validation Tool",
            "The more data a company uses, the greater the benefits of automated data validation, and most businesses and organizations can realize multiple advantages from automating data validation.",
            "What an Automated Data Validation Tool Can Do For Operational and Transactional Data",
            "DataBuck automates all the essential data validations to e scaled to 1,000’s of data sets in a few clicks.",
            "When you want to automate your organization’s data validation and ensure a constant stream of high-quality data, turn to [DataBuck from FirstEigen"
          ]
        },
        {
          "title": "Twilio Data Validation Techniques",
          "url": "https://www.twilio.com/en-us/resource-center/data-validation-techniques",
          "excerpts": [
            "We walk through 11 indispensable data validation techniques for ensuring accuracy, reliability, and integrity in your datasets.",
            "There are almost a dozen ways to check for data accuracy, consistency, and relevance, and they can be done manually to some degree.",
            "Business rule validation",
            "External data validation",
            "Automate data validation with Segment Protocols",
            "Customize Schema controls",
            "Companies often have proprietary workflows and ways they use data. These methods are unique to the business and meet their internal policies and values. When data is checked to be accurate and consistent according to these internal policies, it’s considered to be a form of business rule validation."
          ]
        },
        {
          "title": "Automate Data Validation - Sigma Computing Blog",
          "url": "https://www.sigmacomputing.com/blog/automate-data-validation",
          "excerpts": [
            "3 best practices for maintaining automated data validation",
            "Setting up automated validation is a big win. But the real value comes from keeping it sharp and ensuring it evolves with your data, systems, and team needs.",
            "Building gradually across layers helps your data stay clean, even as complexity grows.",
            "Data validation checks whether your data is complete, accurate, and structured consistently.",
            "Regularly update your validation rules",
            "2. Test and refine your automation process",
            "3. Combine multiple validation methods. Manual data validation is a repetitive task that quietly drains time and confidence from your analytics process.",
            "Automation offers a better way to catch errors early and keep your data trustworthy without the manual slog.",
            "Auditing each extract, transform, and load step helps isolate where issues occur.",
            "Validation becomes a shared habit, not a hidden task.",
            "Automating checks throughout your pipeline improves data quality, saves time, builds trust, and gives your team confidence that what they see reflects what’s happening in the business."
          ]
        },
        {
          "title": "Best Practices for Managing Data Annotation Projects",
          "url": "https://assets.bbhub.io/company/sites/40/2020/09/Annotation-Best-Practices-091020-FINAL.pdf",
          "excerpts": [
            "by T Tseng · 2020 · Cited by 20 — • Annotation guidelines should include both tool and annotation instructions. • Annotation guidelines should also illustrate each label with examples.See more",
            "by T Tseng · 2020 · Cited by 20 — Annotation is the labeling of data by human effort. Annotation is critical to modern machine learning, and. Bloomberg has developed years of experience of ..."
          ]
        },
        {
          "title": "[PDF] Computing Krippendorff's Alpha-Reliability",
          "url": "https://www.asc.upenn.edu/sites/default/files/2021-03/Computing%20Krippendorff%27s%20Alpha-Reliability.pdf",
          "excerpts": [
            "Krippendorff's alpha (α) is a reliability coefficient developed to measure the agreement among observers, coders, judges, raters, annotators or measuring ..."
          ]
        },
        {
          "title": "Cohen's kappa - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Cohen%27s_kappa",
          "excerpts": [
            "Cohen's kappa coefficient ('κ', lowercase Greek kappa) is a statistic that is used to measure inter-rater reliability for qualitative (categorical) items."
          ]
        },
        {
          "title": "Best practices for deploying human-in-the-loop AI",
          "url": "https://lenovoaiforgood.cio.com/ai-innovation-from-the-pocket-to-the-cloud/best-practices-for-deploying-human-in-the-loop-ai/",
          "excerpts": [
            "Data labeling, where people add tags to raw information like text and photos to contextualize and properly train machine learning algorithms."
          ]
        },
        {
          "title": "Acceptance Sampling and QA Guidelines (Ombu Enterprises document)",
          "url": "http://dev.ombuenterprises.com/wp-content/uploads/2019/03/Attributes_Acceptance_Sampling_Understanding_How_it_Works.pdf",
          "excerpts": [
            "Acceptance Sampling\n",
            "ANSI/ASQ Z1.4 is the classic plan,  \nevolved from MIL-STD-10",
            "The input to this process (where I inspect) is defined\n\nas:\n\n–\n\nThe supplier produces product in lots\n\n–\n\nThe supplier uses essentially the same production process for\n\neach lot\n\n–\n\nThe supplier’s production process should run as well as\n\npossible, _i.e.,_ the process average nonconforming should be\n\nas lo",
            "acceptance sampling",
            "acceptance sampling",
            "The AQL provides a criterion against  \nwhich to judge lots. •\n\nIt does not . . . –\n\nProvide a process or product specificat",
            "AQL = 4.0%",
            "use Z1.4",
            "c=0 plans",
            "e c=0 plans are indexed by AQLs to  \nhelp make them comparable with the  \nZ1.4",
            "The calculations in the c=0 plan book  \nuse the hypergeometric distribution while  \nZ1.4 uses the binomial (and Poisson).",
            "ANSI/ASQ Z1.4",
            "Current version is ANSI/ASQ Z1.4-2003",
            "The AQL provides a criterion against  \nwhich to judge lot"
          ]
        },
        {
          "title": "Zero Acceptance Number Sampling Plans, Sixth Edition",
          "url": "https://asq.org/quality-press/display-item?item=H1607&srsltid=AfmBOop7jTxvkI5nxGfIdJzO9iCxoYoJEZOsdqXjqoaz6X5jQd3pGRtJ",
          "excerpts": [
            "This book covers both the statistical and practical approaches for selecting a particular sampling plan from the c=0 table."
          ]
        },
        {
          "title": "ISO 2859-1",
          "url": "https://chemistry.unt.edu/~tgolden/courses/iso2859-1.pdf",
          "excerpts": [
            "Jul 15, 1993 — ISO 2859-1 is about sampling procedures for inspection by attributes, specifically sampling plans indexed by acceptable quality level (AQL) for ..."
          ]
        },
        {
          "title": "c=0 Sampling Plan Table",
          "url": "https://qscompliance.com/c0-sampling-plan-table/",
          "excerpts": [
            "Aug 30, 2020 — C=0 sampling plans are based on the premise of accepting the lot if zero defects are found during the inspection, and rejecting the lot if one ...",
            "Aug 30, 2020 — Another way to define C=0 is the acceptance number (a) is zero (0); in other words, we accept the lot if zero (0) defects are detected. C=0 ..."
          ]
        },
        {
          "title": "ASQ/ANSI Z1.4-2003 (R2018)",
          "url": "https://mdcpp.com/doc/materialDownload/ANSI_ASQ%20Z1.4%202003(R2018).pdf",
          "excerpts": [
            "The operating characteristic curve for unqualified acceptance under reduced inspection can be found by using the AQL index of the normal plan with the sample ..."
          ]
        },
        {
          "title": "ISO 2859-1 - First edition 1989-08-15 - Cargo Inspection Service",
          "url": "https://cargoinspectionservice.net/wp-content/uploads/2021/04/ISO-Sampling-plans-iso2859-1.pdf",
          "excerpts": [
            "ISO 2859-1 defines sampling procedures for inspection by attributes, indexed by acceptable quality level (AQL) for lot-by-lot inspection."
          ]
        },
        {
          "title": "ANSI/ASQ tables (Z1.4) / OC Curves - Percent accepted ...",
          "url": "https://elsmar.com/elsmarqualityforum/threads/ansi-asq-tables-z1-4-oc-curves-percent-accepted-howd-i-do.29916/",
          "excerpts": [
            "If there are zero defectives, then the chance of acceptance should be 100% If the lot has 100% defectives, then the probability would drop to 0% ..."
          ]
        },
        {
          "title": "Data Type Validation - pandera documentation",
          "url": "https://pandera.readthedocs.io/en/stable/dtype_validation.html",
          "excerpts": [
            "Pandera is primarily a validation library: it only checks the schema metadata or data values of the dataframe without changing anything about the dataframe ..."
          ]
        },
        {
          "title": "Data Validation with Pandera in Python - Medium",
          "url": "https://medium.com/data-science/data-validation-with-pandera-in-python-f07b0f845040",
          "excerpts": [
            "Sometimes it is necessary to add your own custom validations. Pandera allows you to inject column/index checks (custom checks of single columns) ..."
          ]
        },
        {
          "title": "Reliability Test Design",
          "url": "https://help.reliasoft.com/reference/life_data_analysis/lda/reliability_test_design.html",
          "excerpts": [
            "A reliability engineer wants to design a zero-failure demonstration test in ... Number of Failures (r). A, 20, 0. B, 30, 1. C, 100, 4. This data can be used to ..."
          ]
        },
        {
          "title": "Data Validation in a Big Data Environment with Great Expectations",
          "url": "https://sunscrapers.com/blog/data-validation-in-a-big-data-environment-with-great-expectations/",
          "excerpts": [
            "Great Expectations also provides advanced validation features, such as cross-validation and dynamic data profiling, that can help detect data ..."
          ]
        },
        {
          "title": "4.2. Validate Your pandas DataFrame with Pandera",
          "url": "https://khuyentran1401.github.io/reproducible-data-science/testing_data/pandera.html",
          "excerpts": [
            "Create multiple tests for the entire dataset using DataFrameSchema. Create multiple tests for each column using Column. Specify the type of test using Check."
          ]
        },
        {
          "title": "Near Duplicate Detection Using Simhash",
          "url": "https://github.com/sumonbis/NearDuplicateDetection",
          "excerpts": [
            "In this project, we have implemented simhash algorithm to evaluate approximate cosine similarity between two documents from a large collection of files."
          ]
        },
        {
          "title": "MinHash LSH in Milvus: The Secret Weapon for Fighting Duplicates ...",
          "url": "https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md",
          "excerpts": [
            "In short, MinHash + LSH enables scalable approximate deduplication: MinHash compresses documents into compact signatures, and LSH efficiently ..."
          ]
        },
        {
          "title": "ASQ - ANSI/ASQ Z1.4 & Z1.9 Standards",
          "url": "https://asq.org/quality-resources/z14-z19?srsltid=AfmBOoqMohy84wxdIlOt-MeM5LW4etu_e-rlSxvWOFigmFm0gld0lHbs",
          "excerpts": [
            "ANSI/ASQ Z1.9-2003 (R2018): Sampling Procedures and Tables for Inspection by Variables for Percent Nonconforming* is an acceptance sampling system to be used on a continuing stream of lots for Acceptance Quality Limit (AQL) specified",
            "History of Z1.4 & Z1.9",
            "ANSI/ASQ Z1.4-2003 (R2018): Sampling Procedures and Tables for Inspection by Attributes is an acceptance sampling system to be used with switching rules.",
            "ANSI/ASQ Z1.4 and Z1.9 standards provide plans, procedures, and acceptance levels for inspections.",
            "What is the Z1.4 Standard? --------------------------"
          ]
        },
        {
          "title": "Confidence Statements Associated With Sampling Plans",
          "url": "https://variation.com/confidence-statements-associated-with-sampling-plans/",
          "excerpts": [
            "The AQL and LTPD0.10 represent special cases of percentiles of the OC curve. The AQL is the 95th percentile while the LTPD0.10 is the 10th percentile. Other ..."
          ]
        },
        {
          "title": "Selecting Statistically Valid Sampling Plans - Taylor Enterprises",
          "url": "https://variation.com/selecting-statistically-valid-sampling-plans/",
          "excerpts": [
            "OC curves are generally summarized by two numbers: the Acceptable Quality Level (AQL) and Lot Tolerance Percent Defective (LTPD). The AQL is that percent ..."
          ]
        },
        {
          "title": "Zero Acceptance Number Sampling Plans, Fifth Edition",
          "url": "https://www.amazon.com/Acceptance-Number-Sampling-Plans-Fifth/dp/0873897390",
          "excerpts": [
            "C=0 should be a consideration for all companies seeking a simpler alternative to the old Mil Std 105e, current Z 1.4. Its plans eliminate acceptance on anything ..."
          ]
        },
        {
          "title": "confidence interval - Revisiting the Rule of Three - Cross Validated",
          "url": "https://stats.stackexchange.com/questions/497548/revisiting-the-rule-of-three",
          "excerpts": [
            "The rule of three is a method for calculating a 95% confidence interval when estimating p from a set of n IID Bernoulli trials with no successes."
          ]
        },
        {
          "title": "All statistics and graphs for Attributes Acceptance Sampling",
          "url": "https://support.minitab.com/en-us/minitab/help-and-how-to/quality-and-process-improvement/acceptance-sampling/how-to/attributes-acceptance-sampling/interpret-the-results/all-statistics-and-graphs/",
          "excerpts": [
            "This means that approximately 95% of the time, you will correctly accept a lot with a defective rate of 1.5% or less, and 5% of the time, you will incorrectly ..."
          ]
        },
        {
          "title": "Inter annotator agreement",
          "url": "https://www.cs.brandeis.edu/~cs140b/CS140b_slides/CS140_Lect_7_InterAnnotatorAgreement.pdf",
          "excerpts": [
            "Feb 28, 2017 — Inter-annotator agreement measures reliability, ie whether annotators consistently make the same decisions, and is measured by coefficients of  ..."
          ]
        },
        {
          "title": "Building custom NLP tools to annotate discourse-functional ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2772766124000594",
          "excerpts": [
            "by M Eguchi · 2024 · Cited by 5 — When it comes to developing an ML system, the goal of annotation is to construct a “gold-standard” annotation dataset, which provides consistent input-output ..."
          ]
        },
        {
          "title": "The Role of Quality Assurance for Content Moderation - Tremau",
          "url": "https://tremau.com/resources/quality-assurance-for-content-moderation/",
          "excerpts": [
            "Content QA (quality assurance) is essential to ensure that the right balance between safety and freedom of expression is met in a fair and effective manner."
          ]
        },
        {
          "title": "entity hierarchy, corpus annotation, and sequence labeling",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8104034/",
          "excerpts": [
            "by T Thieu · 2020 · Cited by 22 — 2.2.​​ The annotation process: from guidelines and schema development to creation of the gold standard corpus."
          ]
        },
        {
          "title": "Automated Data Validation AI Agents",
          "url": "https://relevanceai.com/agent-templates-tasks/automated-data-validation",
          "excerpts": [
            "AI Agents are transforming data validation from a manual, error-prone process into an intelligent, adaptive system that learns and evolves."
          ]
        },
        {
          "title": "Validating with Checks - pandera documentation",
          "url": "https://pandera.readthedocs.io/en/stable/checks.html",
          "excerpts": [
            "In the example below we define a DataFrameSchema with column checks for height_in_feet using a single column, multiple columns, and a more complex groupby ..."
          ]
        },
        {
          "title": "DataFrame Schemas - pandera documentation",
          "url": "https://pandera.readthedocs.io/en/stable/dataframe_schemas.html",
          "excerpts": [
            "The DataFrameSchema class enables the specification of a schema that verifies the columns and index of a pandas DataFrame object."
          ]
        },
        {
          "title": "understanding content moderation: strategies and best practices for ...",
          "url": "https://www.infosysbpm.com/blogs/trust-safety/digital-content-moderation-strategies-and-best-practices.html",
          "excerpts": [
            "key content moderation best practices and strategies"
          ]
        },
        {
          "title": "Binomial proportion confidence interval - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval",
          "excerpts": [
            "The Clopper–Pearson interval is an early and very common method for calculating binomial confidence intervals. This is often called an 'exact' method, as it ..."
          ]
        },
        {
          "title": "Event Nugget Annotation: Processes and Issues",
          "url": "https://aclanthology.org/W15-0809.pdf",
          "excerpts": [
            "by T Mitamura · 2015 · Cited by 75 — Abstract. This paper describes the processes and issues of annotating event nuggets based on DEFT. ERE Annotation Guidelines v1.3 and TAC."
          ]
        },
        {
          "title": "[PDF] Detecting Near-Duplicates for Web Crawling - Google Research",
          "url": "https://research.google.com/pubs/archive/33026.pdf",
          "excerpts": [
            "simhash is a finger- printing technique that enjoys the property that fingerprints of near-duplicates differ in a small number of bit positions. We ..."
          ]
        },
        {
          "title": "Great expectations -> how to compare 2 dafarames : r/dataengineering",
          "url": "https://www.reddit.com/r/dataengineering/comments/17l89wn/great_expectations_how_to_compare_2_dafarames/",
          "excerpts": [
            "I need to develop some tool with the usage of Greate Expectations framework to do some checks if table was properly migrated. Source table needs to be exactly ..."
          ]
        },
        {
          "title": "java - Make a Sim Hash (Locality Sensitive Hashing) Algorithm more ...",
          "url": "https://stackoverflow.com/questions/8327660/make-a-sim-hash-locality-sensitive-hashing-algorithm-more-accurate",
          "excerpts": [
            "Simhash is not a suitable algorithm for this purpose as it's only useful for near-duplicate detection in which differences are very minor and ..."
          ]
        },
        {
          "title": "ANSI/ASQ Z1.4 & Z1.9 Sampling Plan Standards for Quality ...",
          "url": "https://asq.org/quality-resources/z14-z19?srsltid=AfmBOorNSowdfE70e-aHWY7CrPZnJNPulx-gqemRB3Mrmbets-Vbyyqm",
          "excerpts": [
            "ANSI/ASQ Z1.4-2003 (R2018): Sampling Procedures and Tables for Inspection by Attributes is an acceptance sampling system to be used with switching rules."
          ]
        },
        {
          "title": "ANSI/ASQ Z1.4 and Z1.9 Sampling Standards",
          "url": "https://asq.org/quality-resources/z14-z19?srsltid=AfmBOopy-uXRazS2cMW3DDdt4aObKmnwChaU8o-i-FYyr0XNLsPazFcT",
          "excerpts": [
            "ANSI/ASQ Z1.4-2003 (R2018): Sampling Procedures and Tables for Inspection by Attributes is an acceptance sampling system to be used with switching rules.",
            "ANSI/ASQ Z1.4 and Z1.9 standards provide plans, procedures, and acceptance levels for inspections.",
            "What is the Z1.4 Standard? --------------------------",
            "What is the Z1.9 Standard? --------------------------",
            "g)*ANSI/ASQ Z1.9-2003 (R2018): Sampling Procedures and Tables for Inspection by Variables for Percent Nonconforming* is an acceptance sampling system to be used on a continuing stream of lots for Acceptance Quality Limit (AQL) specified.",
            "It provides tightened, normal, and reduced plans to be used on measurements which are normally distributed."
          ]
        },
        {
          "title": "Developer Policy – X Developers",
          "url": "http://developer.x.com/en/developer-terms/policy",
          "excerpts": [
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties.**If you provide X Content to third parties, including downloadable datasets or via an API, you may only distribute Post IDs, Direct Message IDs, and/or User IDs (except as described below)",
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs).",
            "Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research. Only as many Post IDs or User IDs that is necessary for such research, and has been approved by X may be used.",
            "Your license agreement with X limits your use of the X API and X Content. Among other things, the X API has rate limits which help to ensure fair data usage and to combat spam on the platform."
          ]
        },
        {
          "title": "Terms of Service",
          "url": "https://x.com/en/tos",
          "excerpts": [
            "You must abide by the Services’ acceptable use terms:** You may not access the Services in any way other than through the currently available, published interfaces that we provide. For example, this means that you cannot scrape the Services without X’s express written permission, try to work around any technical limitations we impose, or otherwise attempt to disrupt the operation of the Service",
            " Liquidated Damages\nProtecting our users’ data and our system resources is important to us. You further agree that, to the extent permitted by applicable law, if you violate the Terms, or you induce or facilitate others to do so, in addition to all other legal remedies available to us, you will be jointly and severally liable to us for liquidated damages as follows for requesting, viewing, or accessing more than 1,000,000 posts (including reply posts, video posts, image posts, and any other posts) in any 24-hour period - $15,000 USD per 1,000,000 posts. You agree that these amounts are (i) a reasonable estimate of our damages; (ii) not a penalty; and (iii) not otherwise limiting of our ability to recover from you or others under any legal or equitable theory or claim, including but not limited to statutory damages and/or equitable relief. You further agree that repeated violations of these Terms will irreparably harm and entitle us to injunctive and/or other equitable relief, in addition to monetary damages.",
            "By submitting, posting or displaying Content on or through the Services, you grant us a worldwide, non-exclusive, royalty-free license (with the right to sublicense) to use, copy, reproduce, process, adapt, modify, publish, transmit, display, upload, download, and distribute such Content in any and all media or distribution methods now known or later developed, for any purpose. For clarity, these rights include, for example, curating, transforming, and translating. This license authorizes us to make your Content available to the rest of the world and to let others do the same.",
            "If you want to reproduce, modify, create derivative works, distribute, sell, transfer, publicly display, publicly perform, transmit, or otherwise use the Services or Content on the Services, you must use the interfaces and instructions we provide, except as permitted through the Services, these Terms, or the terms provided on https://developer.x.com/developer-terms . Otherwise, all such actions are strictly prohibited."
          ]
        },
        {
          "title": "Hydration - X API v2 - X Developers",
          "url": "https://devcommunity.x.com/t/hydration/221162",
          "excerpts": [
            "You can use the X API to rehydrate previously distributed data. The access plan necessary for hydration would depend on how fast you want to access the data."
          ]
        },
        {
          "title": "Is Hydrating Tweets from publicly posted tweet ids in github legal?",
          "url": "https://devcommunity.x.com/t/is-hydrating-tweets-from-publicly-posted-tweet-ids-in-github-legal/169663",
          "excerpts": [
            "Yes, this is the recommended way to share datasets - as IDs. under “Content redistribution” Developer Policy – X Developers | Twitter Developer Platform"
          ]
        },
        {
          "title": "X updates its terms to ban crawling and scraping",
          "url": "https://techcrunch.com/2023/09/08/x-updates-its-terms-to-ban-crawling-and-scraping/",
          "excerpts": [
            "Sep 8, 2023 — The new terms, which are effective from September 29, ban any kind of scraping or crawling without “prior written consent.” I",
            "Elon Musk-owned X, formerly Twitter, has updated its terms of service to prohibit scraping and crawling — likely to fend off any AI models ...",
            "”\nNOTE: crawling or scraping the Services in any form, for any purpose without our prior written consent is expressly prohibited."
          ]
        },
        {
          "title": "X Updates its Terms, Bans Data Scraping& Crawling",
          "url": "https://nftnow.com/news/x-updates-terms-of-service-to-ban-unauthorized-data-crawling-scraping/",
          "excerpts": [
            "Sep 8, 2023 — X, formerly known as Twitter, updated its Terms of Service to include the prohibition of unauthorized data crawling and scraping."
          ]
        },
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "If you need to share X content you obtained via the X APIs with another party, the best way to do so is by sharing Post IDs, Direct Message IDs, and/or User IDs, which the end user of the content can then rehydrate (i.e. request the full Post, User, or Direct Message content) using the X APIs.",
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X.",
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from",
            "* Individuals redistributing Post IDs and/or User IDs on behalf of an academic institution for the sole purpose of non-commercial research are permitted to redistribute an unlimited number of Post IDs and/or User IDs.",
            "To request permission to share X content as outlined above, please use the API Policy support form.",
            "If you choose to share hydrated X content with another party in this way, you may only share up to 50,000 hydrated public Post Objects and/or User Objects per recipient, per day, and should not make this data publicly available (for example, as an attachment to a blog post or in a public Github repository)."
          ]
        },
        {
          "title": "Developer Agreement and Policy - Twitter Developers - X",
          "url": "https://developer.x.com/en/developer-terms/agreement-and-policy",
          "excerpts": [
            " If displayed content ceases to be available through the X API, then you must remove it from your service as soon as reasonably possible, or within 24 hours after the receipt of a removal request from X, or the applicable X account owner, or as otherwise required by applicable law.",
            "nfidential Information (as defined below).\n\n**D. Rate Limits.** You will not attempt to exceed or circumvent limitations on access, calls, and use of the X API (\" **Rate Limits** \") or otherwise use the X API in a manner that exceeds reasonable request volume, constitutes excessive or abusive usage, or otherwise does not comply with this Agreement. If you exceed or X reasonably believes that you have attempted to circumvent Rate Limits, controls to limit use of the X APIs, or the terms of this Agreement, then your ability to use the Licensed Material may be temporarily suspended or permanently blocked.",
            "**. Notwithstanding anything to the contrary in this Agreement, to the extent you are provided access to the Licensed Material pursuant to the procedures described in Article 40 of the Digital Services Act (Regulation (EU) 2022/2065) (“DSA”), your access and use of the Licensed Material is limited solely to performing research that contributes to the detection, identification, and understanding of systemic risks in the European Union and only to the extent necessary for X to comply with its obligations under the DSA.",
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs). Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research.",
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties. If you ..."
          ]
        },
        {
          "title": "Terms of Service - X",
          "url": "https://x.com/en/tos/previous/version_19",
          "excerpts": [
            "This license has the sole purpose of enabling you to use and enjoy the benefit of the Services as provided on X, in the manner permitted by these Terms."
          ]
        },
        {
          "title": "X's new Terms of Service enforces that all content can be ...",
          "url": "https://www.reddit.com/r/privacy/comments/1g5cc00/xs_new_terms_of_service_enforces_that_all_content/",
          "excerpts": [
            "X just updated their terms of service as of today (effective on the 15th of November). Obviously, no one actually has time to read it, so I made a color-coded ..."
          ]
        },
        {
          "title": "Twitter Terms of Service - X",
          "url": "https://x.com/en/tos/previous/version_13",
          "excerpts": [
            "These Terms of Service (“Terms”) govern your access to and use of our services, including our various websites, SMS, APIs, email notifications, applications, ..."
          ]
        },
        {
          "title": "X changed its terms of service to let its AI train on ...",
          "url": "https://www.cnn.com/2024/10/21/tech/x-twitter-terms-of-service",
          "excerpts": [
            "Oct 21, 2024 — X can now license all the content on the platform, including using it in its machine learning and artificial intelligence models."
          ]
        },
        {
          "title": "A Guide to DMCA Takedown on Twitter (X)",
          "url": "https://www.enforcity.com/blog/dmca-takedown-on-twitter-x",
          "excerpts": [
            "How to file a DMCA takedown Twitter (X) to protect your copyrighted content. Understand the process, penalties for repeated infringements, ..."
          ]
        },
        {
          "title": "I made a license for my X.com account. Is this legally binding?",
          "url": "https://opensource.stackexchange.com/questions/15070/i-made-a-license-for-my-x-com-account-is-this-legally-binding",
          "excerpts": [
            "As part of having an account on X.com, you have agreed to X.com's Term of Service (TOS). This TOS also contains a license for X.com to use your content."
          ]
        },
        {
          "title": "attribution requirements & exceptions",
          "url": "https://app.dataminr.com/public/attribution-requirements.pdf",
          "excerpts": [
            "If visually displayed, use the Twitter logo next to the username. (except in cases where prohibited by applicable regulations as determined by Company in the."
          ]
        },
        {
          "title": "Getting started with Cards | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/x-for-websites/cards/guides/getting-started",
          "excerpts": [
            "Card and Content Attribution​​ Each card has built-in content attribution, which surfaces appropriate Twitter accounts for the content as specified by you. Users ...",
            "Twitter's crawler respects Google's robots.txt specification when scanning URLs. If a page with card markup is blocked, no card will be shown.",
            "Twitter uses the User-Agent of **Twitterbot** (with version, such as Twitterbot/1.0), which can be used to create an exception in the `robots.txt` f",
            "The server’s `robots.txt` file **must** be saved as plain text with ASCII character encod"
          ]
        },
        {
          "title": "Twitter API Terms of Service differences",
          "url": "https://dracos.co.uk/wrote/twitter-tos-change/",
          "excerpts": [
            "You may not pay, or offer to pay, third parties for distribution of your Client. This includes offering compensation for downloads (other than transactional ..."
          ]
        },
        {
          "title": "What is the deal with “Rate Limit Exceeded” on twitter? - Reddit",
          "url": "https://www.reddit.com/r/OutOfTheLoop/comments/14o0xu1/what_is_the_deal_with_rate_limit_exceeded_on/",
          "excerpts": [
            "The claimed reason is to prevent malicious data scraping of their site, though as per usual there may be financial reasons at play."
          ]
        },
        {
          "title": "To address extreme levels of data scraping & system ...",
          "url": "https://x.com/elonmusk/status/1675187969420828672?lang=en",
          "excerpts": [
            "To address extreme levels of data scraping & system manipulation, we've applied the following temporary limits: - Verified accounts are ..."
          ]
        },
        {
          "title": "X updates its Terms to prohibit crawling/scraping of its data",
          "url": "https://news.ycombinator.com/item?id=37433759",
          "excerpts": [
            "X updates its Terms to prohibit crawling/scraping of its data | Hacker News. \"its data\" - nothing on Twitter, except the pathetic memes from a ..."
          ]
        },
        {
          "title": "Robots.txt that makes sure Facebook and Twitter can crawl ...",
          "url": "https://gist.github.com/peterdalle/302303fb67c2bb73a9a09df78c59ba1d",
          "excerpts": [
            "Robots.txt that makes sure Facebook and Twitter can crawl images on your site. Raw. robots.txt. # Disallow everything. User-agent: *. Disallow ..."
          ]
        },
        {
          "title": "Robots.txt question - Rules and Policies - X Developers",
          "url": "https://devcommunity.x.com/t/robots-txt-question/132501",
          "excerpts": [
            "Hi Robots.txt of twitter has something rules. I want to follow their rules. But I have a problem to understand some rules. Every bot that might possibly read ..."
          ]
        },
        {
          "title": "Twitter Robots.txt",
          "url": "https://twitter.com/robots.txt",
          "excerpts": [
            "u\n# Wait 1 second between successive requests. See ONBOARD-2698 for details. Crawl-delay: 1",
            "\nUser-agent: \\*",
            "Disallow: /search/realtime",
            "Noindex: /i/u",
            "Disallow: /search/users",
            "Disallow: /search/\\*/gri",
            "Sitemap: https://twitter.com/sitemap.xml"
          ]
        },
        {
          "title": "X DevCommunity robots.txt",
          "url": "https://devcommunity.x.com/robots.txt",
          "excerpts": [
            " User-agent: ",
            "Disallow: /",
            "Crawl-delay: 60",
            "Sitemap: https://devcommunity.x.com/sitemap.xml"
          ]
        },
        {
          "title": "x.com robots.txt",
          "url": "http://x.com/robots.txt",
          "excerpts": [
            "\nUser-agent: ",
            "/\nDisallow: /search/realtime",
            "Crawl-delay: 1",
            "Sitemap: https://twitter.com/sitemap.xml",
            "e\nDisallow: /search/users",
            "d\nDisallow: /[\\_0-9a-zA-Z]+/status/[0-9]+/likes"
          ]
        },
        {
          "title": "Knight Institute Says X's New Terms of Service Will Stifle ...",
          "url": "https://knightcolumbia.org/content/knight-institute-says-xs-new-terms-of-service-will-stifle-independent-research",
          "excerpts": [
            "... terms of service that goes into effect on November 15, 2024. The new version includes a provision on “liquidated damages,” which requires ..."
          ]
        },
        {
          "title": "Procedure for authorizing the publication of Post IDs",
          "url": "https://devcommunity.x.com/t/procedure-for-authorizing-the-publication-of-post-ids/215033",
          "excerpts": [
            "Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic ..."
          ]
        },
        {
          "title": "Arjun Sethi",
          "url": "https://x.com/arjunsethi/status/1956131529714729264",
          "excerpts": [
            "APIs were supposed to fix this For the last decade, the industry has pushed for a move away from screen scraping and toward secure, permissioned ..."
          ]
        },
        {
          "title": "Non-programmatic redistribution - Rules and Policies",
          "url": "https://devcommunity.x.com/t/non-programmatic-redistribution/27889",
          "excerpts": [
            "Nov 19, 2014 — According to developer policy 6.b.i., You may, however, provide export via non-automated means (e.g., download of spreadsheets or PDF files, ..."
          ]
        },
        {
          "title": "CNBC article on X terms of service and related policy notes",
          "url": "https://www.cnbc.com/2024/11/22/why-x-new-terms-of-service-driving-some-users-to-leave-elon-musk-platform.html",
          "excerpts": [
            "Another unusual aspect of X's new terms is its \"liquidated damages\" clause. The terms state that if users request, view or access more than 1 ...",
            "The terms state that if users request, view or access more than 1 million posts – including replies, videos, images and others – in any 24-hour period they are liable for damages of $15,000."
          ]
        },
        {
          "title": "Rate limiting | Docs | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/docs/x-ads-api/rate-limiting",
          "excerpts": [
            "Ads API Rate Limits ; Audience Insights, Category, 400 ; Keyword Insights, Category, 500 ; Global Reads (GET endpoints without :account_id), Endpoint, 5."
          ]
        },
        {
          "title": "Country Targeting and Display Requirements - Twitter Developer - X",
          "url": "https://developer.x.com/en/docs/twitter-ads-api/campaign-management/overview/targeting-country-requirements",
          "excerpts": [
            "Country-specific targeting and display requirements are contained on this page. These requirements must be adhered to by all partners."
          ]
        },
        {
          "title": "About X | Our logo, brand guidelines, and tools",
          "url": "https://about.x.com/en/who-we-are/brand-toolkit",
          "excerpts": [
            "Download the X logo, assets, and X Brand Guidelines — and learn how to embed a post on your website."
          ]
        },
        {
          "title": "x-brand-guidelines.pdf",
          "url": "https://cdn.cms-twdigitalassets.com/content/dam/about-twitter/x/brand-toolkit/x-brand-guidelines.pdf",
          "excerpts": [
            "The X logo is black or white. It must be legible and maintain the integrity of its form. Our logo should be large and clear in external communications. Color ..."
          ]
        },
        {
          "title": "Developer terms - Twitter Developer",
          "url": "https://developer.x.com/en/developer-terms",
          "excerpts": [
            "Developer terms – X Developers. Developer use of X materials and content is subject to and governed by our Developer Policy and agreements."
          ]
        },
        {
          "title": "Privacy Policy",
          "url": "https://x.com/en/privacy",
          "excerpts": [
            "Read X's Privacy Policy to learn about the information we collect, how it's used, and the control you have over it."
          ]
        },
        {
          "title": "Use Cases, Tutorials, & Documentation | Twitter Developer ... - X",
          "url": "https://developer.x.com/en",
          "excerpts": [
            "Publish & analyze posts, optimize ads, & create unique customer experiences with the X API, X Ads API, & X Embeds."
          ]
        },
        {
          "title": "Robots.txt Introduction and Guide | Google Search Central",
          "url": "https://developers.google.com/search/docs/crawling-indexing/robots/intro",
          "excerpts": [
            "Robots.txt is used to manage crawler traffic. Explore this robots.txt introduction guide to learn what robot.txt files are and how to use them."
          ]
        },
        {
          "title": "Non-commercial use of the X API | Twitter Developer Platform",
          "url": "https://developer.x.com/en/developer-terms/commercial-terms",
          "excerpts": [
            "Our Developer Agreement includes commercial use terms that govern how the X API can be used with Academic Research access. Updated February 27, 2023: We ..."
          ]
        },
        {
          "title": "More information on X's data processing - Help Center",
          "url": "https://help.x.com/en/rules-and-policies/data-processing-legal-bases",
          "excerpts": [
            "This supplements the X Privacy Policy to provide additional information about the types of data we collect, where we get it, and how we process and share it."
          ]
        },
        {
          "title": "Towards Open-Domain Twitter User Profile Inference",
          "url": "https://aclanthology.org/2023.findings-acl.198.pdf",
          "excerpts": [
            "by H Wen · 2023 · Cited by 6 — According to the Twitter. Developer Agreement and Policy, we will only release IDs instead of actual content for non- commercial research ..."
          ]
        },
        {
          "title": "Are Tweets Copyright-Protected?",
          "url": "https://www.wipo.int/web/wipo-magazine/articles/are-tweets-copyright-protected-36909",
          "excerpts": [
            "In the same vein, titles or short phrases usually cannot be protected since their length contributes to their lack of originality, as defined by copyright law."
          ]
        },
        {
          "title": "Are Tweets Protected by Copyright?",
          "url": "https://copyrightalliance.org/faqs/tweets-protected-copyright/",
          "excerpts": [
            "The tweet must contain something more than simply a name, single word, or short phrase, since these are not protected by copyright law. Although there is a 280 ..."
          ]
        },
        {
          "title": "Fair Use & Copyright in Social Media Sharing - Reddit",
          "url": "https://www.reddit.com/r/COPYRIGHT/comments/1budnq3/fair_use_copyright_in_social_media_sharing/",
          "excerpts": [
            "At risk of going too in detail, in the United States, no copyright is fair use by itself. You can claim it. In the EU, anything that's public ..."
          ]
        },
        {
          "title": "DMCA - Help Center - X",
          "url": "https://help.x.com/en/forms/ipi/dmca",
          "excerpts": [
            "Help with intellectual property issues. What issue are you having? (required). I need to report possible trademark infringement, I'd like to report an issue ..."
          ]
        },
        {
          "title": "“#DisabledOnIndianTwitter” : A Dataset towards ...",
          "url": "https://aclanthology.org/2022.findings-aacl.35.pdf",
          "excerpts": [
            "by I Mondal · 2022 · Cited by 5 — We manually selected Twitter accounts where users disclosed their disability identity, for example, in their Twitter bio, profile picture, ..."
          ]
        },
        {
          "title": "Twitter (X) DMCA Counter notice is broken : r/COPYRIGHT",
          "url": "https://www.reddit.com/r/COPYRIGHT/comments/1h4ikd2/twitter_x_dmca_counter_notice_is_broken/",
          "excerpts": [
            "The problem is, everytime I try to fill out the DMCA counter notice form, they tell me that my information is incomplete and I need to resubmit it."
          ]
        },
        {
          "title": "X Help Center - Copyright Policy",
          "url": "https://help.x.com/en/rules-and-policies/copyright-policy",
          "excerpts": [
            "Your agent will be required to submit the DMCA notice with valid contact information, and identify you as the content owner that they are representing. What ... [Intellectual property](https://help.x.com/en/rules-and-policies#intellectual-property)\n\n# Cop",
            "Copyright policy",
            "#### What types of copyright complaints does X respond to? X responds to copyright complaints submitted under the Digital Millennium Copyright Act (“DMCA”). Section 512 of the DMCA outlines the statutory requirements necessary for formally reporting copyright infringement, as well as providing instructions on how an affected party can appeal a removal by submitting a compliant counter-notice.",
            "#### Am I a copyright holder? How do I know?",
            "If you are reporting the content of a post, please give us a [direct link](/en/using-twitter/tweet-and-moment-url.html) to that post. Or please specify if the alleged infringement is in the header, avatar, etc. A LINK TO A PROFILE PAGE IS INSUFFICIENT FOR X TO IDENTIFY INFRINGING MATERIALS.",
            "ATERIALS. #### How do I file a copyright complaint? You can report alleged copyright infringement by visiting X Help Center and [filing a copyright complaint](https://help.twitter.com/forms/ipi) . If you are logged in to x.com, you can visit the X Help Center directly from your X account by clicking the ‘Help’ link located in the sidebar. Filing a DMCA complaint is the start of a pre-defined legal process. Your complaint will be reviewed for accuracy, validity, and completeness. If your complaint has satisfied these requirements, we will take action on your request - which includes forwarding a full copy of your notice (including your name, address, phone and email address) to the user(s) who posted the allegedly infringing material in question.",
            "If you are concerned about your contact information being forwarded, you may wish to use an agent to report for you.",
            "ort for you. Please be aware that under 17 U.S.C. § 512(f), you may be liable for any damages, including costs and attorneys’ fees incurred by us or our users, if you knowingly materially misrepresent that material or activity is infringing."
          ]
        },
        {
          "title": "Detecting and mitigating bias in natural language processing",
          "url": "https://www.brookings.edu/articles/detecting-and-mitigating-bias-in-natural-language-processing/",
          "excerpts": [
            "May 10, 2021 — A series that explores ways to mitigate possible biases and create a pathway toward greater fairness in AI and emerging technologies."
          ]
        },
        {
          "title": "Multilingual Social Media: Strategies That Actually Work (10 Steps)",
          "url": "https://translatepress.com/multilingual-social-media/",
          "excerpts": [
            "To build a multilingual social media presence, you need to carry out the following steps:"
          ]
        },
        {
          "title": "Top 10 Best Practices for Multilingual Websites - Digital.gov",
          "url": "https://digital.gov/resources/top-10-best-practices-for-multilingual-websites",
          "excerpts": [
            "Ensure that your multilingual website provides a comparable user experience to your English site. MedlinePlus provides the same experience on the English and ..."
          ]
        },
        {
          "title": "Optimal strategies to perform multilingual analysis of social ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S0950705125010469",
          "excerpts": [
            "by M Masson · 2025 · Cited by 1 — In this work, we study different NLP techniques to establish the best ones to obtain competitive performances while keeping the need for training annotated ..."
          ]
        },
        {
          "title": "Optimal Strategies to Perform Multilingual Analysis of ...",
          "url": "https://arxiv.org/html/2311.14727v2",
          "excerpts": [
            "Jul 3, 2025 — Specifically, we investigate which NLP techniques are best to keep manual data annotation to a minimum and to avoid cumbersome and costly rule- ..."
          ]
        },
        {
          "title": "Data and Model Biases in Social Media Analyses",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8861742/",
          "excerpts": [
            "by Y Zhao · 2022 · Cited by 13 — In this study, we examined six different data collection methods and three different machine learning (ML) models-commonly used in social media analysis-to ..."
          ]
        },
        {
          "title": "Comparing methods for creating a national random sample ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11861139/",
          "excerpts": [
            "The following subsections provide a detailed presentation of our results by comparing the four Twitter sampling methods. First, we present essential statistics regarding the outcomes of each sampling method. Second, in accordance with the methodology outlined in Sect. [User pre-processing](), we randomly sample 30K accounts from the output of each sampling method. Subsequently, we apply pre-processing filters and select a random sample of 10K users from the remaining pool. We then report the metrics at both tweet- and user-levels for this subset. Lastly, we generate debiased samples from each 10K random sample by computing inclusion probabilities. We compare their mean absolute percentage errors (MAPE) for the task of estimating the United States population using Twitter data.",
            "he four methods include (1) *1% stream*, in which one uses Twitter Stream API to get 1% of tweets in real-time and then sample from the authors of the tweets, (2) *location query*, in which one uses Twitter Search API and query for a country of interest, (3) *language query*, in which one uses Twitter Search API, query for language(s) representing the country of interest, and filter for the country, and (4) *bounding-box*, in which one uses the ‘bounding-box’ field in the Search API and query for the coordinates enclosing the country of interes",
            "Our results illuminate the positive and negative characteristics of the four main sampling methods used in the literature and help researchers choose the one that best suits their research goals and designs.",
            " Future research should explore the role of large language models in improving the performance of or automating usage of different models used in the process of computing the inclusion probabilities"
          ]
        },
        {
          "title": "Walking Through Twitter: Sampling a Language-Based ...",
          "url": "https://journals.sagepub.com/doi/10.1177/2056305120984475",
          "excerpts": [
            "by FV Münch · 2021 · Cited by 25 — We provide evidence that the method is able to approximate a set of the top 1% to 10% of influential accounts in the German Twittersphere."
          ]
        },
        {
          "title": "A survey of location inference techniques on Twitter",
          "url": "https://arxiv.org/pdf/1701.03639",
          "excerpts": [
            "by O Ajao · 2017 · Cited by 205 — Location inference on Twitter can be used to identify offenders engaging in bullying of other online users via social media also known as 'cyberbullying ...See more"
          ]
        },
        {
          "title": "Quantity or quality? Comparing social media data sampling ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2212420925003553",
          "excerpts": [
            "by J Yu · 2025 · Cited by 1 — The inductive approach has some advantages; for example, the whole data collection process is usually less time consuming.",
            "by J Yu · 2025 · Cited by 1 — In this paper, we use a case study of government crisis communication during Covid-19 in Germany and Italy to compare deductive and inductive sampling ..."
          ]
        },
        {
          "title": "cardiffnlp/tweet_eval · Datasets at Hugging Face",
          "url": "https://huggingface.co/datasets/cardiffnlp/tweet_eval",
          "excerpts": [
            "TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. The tasks include - irony, hate, offensive, stance, ...See more"
          ]
        },
        {
          "title": "Geolocation Prediction in Twitter Using Social Networks",
          "url": "https://ojs.aaai.org/index.php/ICWSM/article/view/14627",
          "excerpts": [
            "by D Jurgens · 2015 · Cited by 305 — We conduct a systematic comparative analysis of nine state-of-the-art network-based methods for performing geolocation inference at the global scale.See more"
          ]
        },
        {
          "title": "TweepFake - Twitter deep Fake text Dataset",
          "url": "https://www.kaggle.com/datasets/mtesconi/twitter-deep-fake-text",
          "excerpts": [
            "This .csv file contains the Twitter test samples (with comma-separated values). Each of the 40 accounts has got at least one tweet in this set."
          ]
        },
        {
          "title": "DSF-GAM: a location inference model in social network ...",
          "url": "https://www.tandfonline.com/doi/full/10.1080/1206212X.2024.2448497",
          "excerpts": [
            "This paper introduces a predictive model called DSF-GAM (DS stand for document similarity, F stands for frequencies and GAM stands for Generalized Additive ...See more"
          ]
        },
        {
          "title": "[PDF] BLEURT Has Universal Translations: An Analysis of Automatic ...",
          "url": "https://aclanthology.org/2023.acl-long.297.pdf",
          "excerpts": [
            "BLEURT has universal adversarial translations, meaning it can achieve high scores with any reference sentence, even if unrelated."
          ]
        },
        {
          "title": "Human Annotations - Social Media Lab",
          "url": "https://social-media-lab.net/evaluation/",
          "excerpts": [
            "by M Achmann-Denkler — The process optimally involves multiple rounds of annotation, each refining the guidelines through discussions of disagreements and revisions.See more"
          ]
        },
        {
          "title": "Celebrity Tweets Dataset (Real vs AI-Generated)",
          "url": "https://www.kaggle.com/datasets/abaghyangor/celebrity-tweets",
          "excerpts": [
            "This dataset contains 120 tweets from 8 popular celebrities. Each tweet is labeled as either real (actually posted by the celebrity) or fake (AI-generated ...See more"
          ]
        },
        {
          "title": "[PDF] BLEU, METEOR, BERTScore: Evaluation of Metrics Performance in ...",
          "url": "https://aclanthology.org/2021.triton-1.6.pdf",
          "excerpts": [
            "The aim of these automatic quality metrics is to evaluate a translation hypothesis (i.e. the automatic translation) against a reference translation, which is ..."
          ]
        },
        {
          "title": "Automatic Evaluation VS. Human Verification of Machine ...",
          "url": "https://www.reddit.com/r/machinetranslation/comments/165avjk/automatic_evaluation_vs_human_verification_of/",
          "excerpts": [
            "My thesis aims at exploring to what extent the results of the automatic machine translation (MT) evaluation metrics, like MT Quality Estimation ..."
          ]
        },
        {
          "title": "An Analysis of Automatic Metrics by Minimum Risk Training - arXiv",
          "url": "https://arxiv.org/html/2307.03131",
          "excerpts": [
            "In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training ..."
          ]
        },
        {
          "title": "How To Choose The Best Social Annotation Tool For Your ...",
          "url": "https://web.hypothes.is/blog/best-social-annotation-tool-strategies-for-every-user-need/",
          "excerpts": [
            "This guide will explain the essential considerations when selecting the best social annotation tool for your requirements.See more"
          ]
        },
        {
          "title": "Twitter metrics complement traditional conference evaluations to ...",
          "url": "https://www.cambridge.org/core/journals/canadian-journal-of-emergency-medicine/article/twitter-metrics-complement-traditional-conference-evaluations-to-evaluate-knowledge-translation-at-a-national-emergency-medicine-conference/D8F2FFA9D7DE38660B9DC1FBBA4EB9B2",
          "excerpts": [
            "Conferences are evaluated by traditional evaluation metrics, but they are inadequate to assess knowledge translation for several reasons."
          ]
        },
        {
          "title": "Target Population and Sampling Frames in Twitter Research",
          "url": "https://link.springer.com/article/10.1007/s11135-023-01615-w",
          "excerpts": [
            "A perfect sampling frame is one in which each statistical unit in the population is separately listed once, only once, and no other irrelevant or extraneous elements are listed.",
            "The sampling frame is ideally a full census of the target population but, in practice, it is typically a subset.",
            " Moreover, it is only recently that researchers have begun to question whether tweet-level sampling—in which the tweet is the sampling unit—should be replaced by user-level sampling—in which the sampling unit is the user (Zhang et al. [2018",
            "The major rationale for this shift is that tweet-level sampling does not consider the fact that some core discussants on Twitter are much more active tweeters than other less active users, which means that discussion trends are dominated by these active users.",
            " There is evidence that the sampling strategy affects not only the representativeness of the data collected but also the substantive conclusions of analyses"
          ]
        },
        {
          "title": "Number Analytics Blog - Creating a Sampling Frame",
          "url": "https://www.numberanalytics.com/blog/sampling-frame-in-research-methodology",
          "excerpts": [
            "Online data sources, such as social media and online forums, can also be used to create a sampling frame for studies on online communities.",
            "Creating a sampling frame requires careful consideration of several factors.",
            "o define the target population and sampling unit.",
            "A well-designed sampling frame is essential for any research study.",
            "gs. The formula for calculating the sampling error is given by:\n\n\\[SE = \\sqrt{\\frac{p(1-p)}{n}}\\] where $p$ is the proportion of the sample and $n$ is the sample size."
          ]
        },
        {
          "title": "MQM and MT Evaluation – Multidimensional Quality Metrics",
          "url": "https://arxiv.org/html/2403.12666v1",
          "excerpts": [
            "s\nMQM, the multidimensional quality metrics framework\nLommel et al. ([2014](https://arxiv.org/html/2403.12666v1.bib30)). It decomposes translation quality\ninto a number of aspects and their subaspects.",
            "MQM is a robust scheme that corresponds well to judges' overall assessment of translation quality and provides nuanced insights into the",
            ".. While recent advancements by neural metrics\n(Zhang et al., [2020](https://arxiv.org/html/2403.12666v1.bib56); Sellam et al., [2020](https://arxiv.org/html/2403.12666v1.bib43); Rei et al., [2020](https://arxiv.org/html/2403.12666v1.bib41)) have significantly brought the evaluation of\nmachine translation forwards, virtually all current work on automatic\nMT evaluation distills the complexity of translation quality into a\nsingle score to be annotated and predicted, respectively. A\nsingle-score approach, while greatly simplifying computational\nmodeling, arguably falls short of capturing the inherent\nmultidimensional concept of translation quality.",
            "\n\n##### Neural Metrics. Neural metrics represent an advanced approach for automatic evaluation of machine-generated translations. It can be categorized as either unsupervised or supervised (Lee et al., [2023](https://arxiv.org/html/2403.12666v1.bib25)).",
            "### 2.3. Quality Estimation\n\nQuality Estimation (QE) is considered an alternative approach to MT evaluation. As a reference-free evaluation, it assesses the quality of a translation by taking only the source sentence and its machine-generated translation without a reference translation (Specia et al., [2018](https://arxiv.org/html/2403.12666v1.bib47))",
            "Table 1: Different types of translation errors",
            "Table 1: Different types of translation errors",
            "Inconsistent use | Terminology is used inconsistently.",
            "Currency format | Wrong format for currency.",
            "Date format | Wrong format for dates.",
            "Name format | Wrong format for names.",
            "\n\nTable 2: MQM hierarchy from Freitag et al. ([2021a](https://arxiv.org/html/2403.12666v1.bib16))",
            "\n\nTable 2: MQM hierarchy from Freitag et al. ([2021a](https://arxiv.org/html/2403.12666v1.bib16))"
          ]
        },
        {
          "title": "ACL Anthology 2023: EAMT-1.26 - ContentQuo / Translation Quality in Social Media NMT",
          "url": "https://aclanthology.org/2023.eamt-1.26.pdf",
          "excerpts": [
            "— The results of their research showed that using a tiny Twitter corpus is useless for NMT train- ing, although the system improved when using back- translation ...",
            "Terminology\t 0\t\t  2\t\t 2\t\t  0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Style\t\t   1\t\t  2\t\t 0\t\t  0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Other\t\t   3\t\t  3\t\t 0\t\t  0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Locale\t\t  0\t\t  0\t\t 0\t\t  0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t convention\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Fluency\t\t 0\t\t  14\t\t2\t\t  0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Design\t\t  0\t\t  0\t\t 0\t\t  0\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t Accuracy\t\t2\t\t  4\t\t 3\t\t  0\n",
            "An intriguing finding was that major errors were\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t mostly distributed in threads",
            "NMT engine for social media",
            "an-\n   The mean overall quality score obtained was\t\t   ",
            "Although this is a good score taking into\naccount that the tweets were not postedited, we were\t 5\t Conclusions"
          ]
        },
        {
          "title": "Amazon releases 51-language dataset for ...",
          "url": "https://www.amazon.science/blog/amazon-releases-51-language-dataset-for-language-understanding",
          "excerpts": [
            "The MASSIVE dataset is a step toward the creation of multilingual natural-language-understanding models that can generalize easily to new languages. One ..."
          ]
        },
        {
          "title": "Demographic Inference and Representative Population ...",
          "url": "https://arxiv.org/abs/1905.05961",
          "excerpts": [
            "by Z Wang · 2019 · Cited by 299 — We address these challenges by combining multilingual demographic inference with post-stratification to create a more representative population sample."
          ]
        },
        {
          "title": "Surveying emigrants worldwide–using Facebook and ...",
          "url": "https://comparativemigrationstudies.springeropen.com/articles/10.1186/s40878-025-00464-w",
          "excerpts": [
            "by S Pötzschke · 2025 — Sampling remains a major challenge when researching minority populations, especially in cross-national settings. While various sampling ..."
          ]
        },
        {
          "title": "Using Google Maps to Generate Organizational Sampling ...",
          "url": "https://journals.sagepub.com/doi/10.1177/00491241241305095",
          "excerpts": [
            "This article presents a new method for generating organizational sampling frames that is cost-effective, uses publicly available data, and can produce sampling ..."
          ]
        },
        {
          "title": "Influencer marketing report 2025: Why nano and micro- ...",
          "url": "https://thesocialcat.com/blog/influencer-marketing-report",
          "excerpts": [
            "Nano-influencers (1K-10K followers) show the highest average engagement rate at 2.71%, followed by micro-influencers (10K-50K followers) at 1.81% . The difference in engagement between micro-influencers and mid-tier influencers (50k-100k) is +46%."
          ]
        },
        {
          "title": "When It Comes to Influencers, Smaller Can Be Better",
          "url": "https://hbr.org/2024/09/when-it-comes-to-influencers-smaller-can-be-better",
          "excerpts": [
            "They discovered that nano influencers, those with fewer than 10,000 followers, yield a remarkable average return of more than $1,000 on a $50 investment—the ..."
          ]
        },
        {
          "title": "Twitter Structure as a Composition of Two Distinct Networks",
          "url": "https://ece.northeastern.edu/fac-ece/dkoutsonikolas/publications/cnc16.pdf",
          "excerpts": [
            "by M Tong · Cited by 1 — Power law distribution (PLD) is often used in the un- derstanding and analysis of complex network structures, and especially in the purview of social networks."
          ]
        },
        {
          "title": "Example Power‐Law Distribution of the Number Followers ...",
          "url": "https://www.researchgate.net/figure/Example-Power-Law-Distribution-of-the-Number-Followers-of-Twitter-Users-Figure_fig1_331617021",
          "excerpts": [
            "Example Power‐Law Distribution of the Number Followers of Twitter Users. Figure differentiates between typical users and network gatekeepers, whose position ...",
            "During the 2014 U.S. gubernatorial election, 74 candidates for State Governor posted 20,580 tweets, of which 10,946 were retweeted almost 140,000 times. By ..."
          ]
        },
        {
          "title": "A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9120935/",
          "excerpts": [
            "This study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF ..."
          ]
        },
        {
          "title": "SoK: Measuring Blockchain Decentralization",
          "url": "https://arxiv.org/html/2501.18279v1",
          "excerpts": [
            "Jan 30, 2025 — A closely related metric to Gini is the Theil index, which intends to capture the lack of diversity, or the redundancy, in a population [14] ."
          ]
        },
        {
          "title": "Detecting malicious activity in Twitter using deep learning ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S1568494621002830",
          "excerpts": [
            "by L Ilias · 2021 · Cited by 105 — In this paper, two methods are introduced targeting this that are mainly based on Natural Language Processing (NLP) to distinguish legitimate users from bots.See more"
          ]
        },
        {
          "title": "Topic Modeling with LDA, NMF, BERTopic, and Top2Vec",
          "url": "https://medium.com/@daphycarol/topic-modeling-with-lda-nmf-bertopic-and-top2vec-model-comparison-part-2-f82787f4404c",
          "excerpts": [
            "LDA and NMF are suitable methods for topic modeling on lengthy textual data, while BERTopic and Top2Vec yield superior results when applied to shorter texts ..."
          ]
        },
        {
          "title": "Categorizing E-cigarette-related tweets using BERT topic modeling",
          "url": "https://www.sciencedirect.com/science/article/pii/S2667118224000199",
          "excerpts": [
            "Our study uniquely provides a method to apply BERTopic to e-cigarette related tweets, and it successfully clusters a large volume of tweets into manually ...",
            "by D Murthy · 2024 · Cited by 6 — We used topic modeling (BERTopic) to iteratively derive vape-related tweet clusters and calculate the importance of particular words to these groupings."
          ]
        },
        {
          "title": "Topic Modeling: A Comparative Overview of BERTopic, ...",
          "url": "https://chamomile.ai/topic-modeling-overview/",
          "excerpts": [
            "Jul 27, 2025 — Compare the strengths, limitations, and tradeoffs of leading topic modeling approaches."
          ]
        },
        {
          "title": "Exploring Twitter discourse with BERTopic: topic modeling of tweets ...",
          "url": "https://link.springer.com/article/10.1007/s10772-024-10142-4",
          "excerpts": [
            "Many previously introduced machine learning approaches for topic modeling are based on Latent Dirichlet Allocation (LDA) (Jelodar et al., 2019).",
            "by NC Hellwig · 2024 · Cited by 5 — When comparing BERTopic with LDA in terms of the common evaluation metrics topic diversity and topic coherence, BERTopic always outperformed LDA ..."
          ]
        },
        {
          "title": "Social Media Sentiments Analysis Dataset - Kaggle",
          "url": "https://www.kaggle.com/datasets/kashishparmar02/social-media-sentiments-analysis-dataset",
          "excerpts": [
            "The dataset file, named sentimentsdataset.csv, encapsulates diverse social media insights. It comprises user-generated content, sentiment labels, timestamps ..."
          ]
        },
        {
          "title": "(PDF) Examining manufacturer concentration metrics in ...",
          "url": "https://www.researchgate.net/publication/340101270_Examining_manufacturer_concentration_metrics_in_consumer_packaged_goods",
          "excerpts": [
            "The research compares three different market concentration metrics (Concentration Ratio, Herfindahl–Hirschman Index, and Gini Coefficient) over the share of ..."
          ]
        },
        {
          "title": "MDPI Algorithms (Mondal, 2024)",
          "url": "https://www.mdpi.com/2073-431X/13/6/141",
          "excerpts": [
            "novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers.",
            "It would be beneficial to see larger LLMs (70 billion parameters or more) being fine-tuned on datasets augmented through our bias-reducing approach and their mb-index performance.",
            "it would be interesting to explore bias reduction in other non-English languages using this framework, given the correct embeddings.",
            "we outline a mechanism to tackle bias caused by training and fine-tuning data within large language models through an automated augmentation algorithm based on bias producers.",
            "We also provide ways to quantify bias inherent to datasets and large language models through the db-index and mb-index accordingly.",
            "It is important to note that our mechanism should be able to extrapolate to other languages and not just be restricted to English. The only requirement is that the correct embeddings are provided."
          ]
        },
        {
          "title": "Bias and Fairness in Large Language Models: A Survey",
          "url": "https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A",
          "excerpts": [
            "We present a comprehensive survey of bias evaluation and mitigation techniques for LLMs.",
            "We then developed three intuitive taxonomies: metrics and datasets for bias evaluation, and techniques for bias mitigation.",
            "A more holistic approach includes problem formulation, data collection, and deployment and integration into real-world contexts."
          ]
        },
        {
          "title": "Diversity in ML Datasets: Measure Dataset Diversity, Don't Just Claim It",
          "url": "https://arxiv.org/html/2407.08188v1",
          "excerpts": [
            "Our research explores the implications of this issue by analyzing “diversity” across 135 image and text datasets. Position: Measure Dataset Diversity, Don’t Just Claim It",
            "Diversity can also refer to part of the collection process, such as recruiting annotators with ",
            "We encourage researchers to explicitly define the empirical indicators they use to measure diversity, specifying the scale, inclusion/exclusion criteria, and other relevant parameters.",
            "Table 4: Taxonomy of the definitions of diversity identified through our literature review."
          ]
        },
        {
          "title": "Using Quotas Constructed from Auxiliary Data to Guide the ...",
          "url": "https://www.tandfonline.com/doi/abs/10.1080/19345747.2024.2342852",
          "excerpts": [
            "This paper describes how to establish quotas using auxiliary data to avoid including “too many” schools of a particular type."
          ]
        },
        {
          "title": "Hate Speech Dataset Catalogue | hatespeechdata",
          "url": "https://hatespeechdata.com/",
          "excerpts": [
            "This page catalogues datasets annotated for hate speech, online abuse, and offensive language. They may be useful for eg training a natural language processing ..."
          ]
        },
        {
          "title": "A Platform Problem: Hate Speech and Bots Still Thriving on X",
          "url": "https://www.isi.edu/news/73786/a-platform-problem-hate-speech-and-bots-still-thriving-on-x/",
          "excerpts": [
            "A study published February 12, 2024 in PLOS ONE sheds light on ongoing challenges with hate speech and inauthentic accounts on X (formerly Twitter)."
          ]
        },
        {
          "title": "Disinformation most active on X, formerly known as Twitter, EU says",
          "url": "https://www.bbc.com/news/technology-66926080",
          "excerpts": [
            "X, formerly Twitter, has the biggest proportion of disinformation of six big social networks, a European Commission study has suggested."
          ]
        },
        {
          "title": "Comparing Methods for Creating a National Random Sample ... - arXiv",
          "url": "https://arxiv.org/abs/2402.04879",
          "excerpts": [
            "In this paper, we implement four common methods to collect a random sample of Twitter users in the US: 1% Stream, Bounding Box, Location Query, and Language ..."
          ]
        },
        {
          "title": "Comparing methods for creating a national random sample of twitter ...",
          "url": "https://link.springer.com/article/10.1007/s13278-024-01327-5",
          "excerpts": [
            "The results show that the *bounding box* (BB) and *location query* (Loc) sampling methods produce a significantly higher number of tweets compared to the *language query* (Lang) and *1% steam* methods. Among the four Twitter sampling methods, BB and Loc methods produce more than 18 million tweets, whereas the Lang and 1% stream methods generate 4.5 million and 174,000 tweets, respectively, withi",
            "A common aim when working with Twitter data is the construction of a random sample of users from a given country.",
            "The *1% Stream* method achieves the minimum error, compared to the other three methods, in the prediction task of estimating the population of the US from Twitter users.",
            "To address this fact, researchers often try to create a random sample of Twitter users from a country. However, at least four widely-used sampling methods exist in the literature, and the extent to which their outputs are similar or different has not been explored systematically so far.",
            "r results highlight the *1% Stream* Twitter sampling method, which exhibits different characteristics compared to the other three sampling methods and fits as the top candidate in most use cases."
          ]
        },
        {
          "title": "(PDF) Comparing methods for creating a national random sample of ...",
          "url": "https://www.researchgate.net/publication/383119495_Comparing_methods_for_creating_a_national_random_sample_of_twitter_users",
          "excerpts": [
            "Our results show that users collected by the 1% Stream method tend to have more tweets, tweets per day, followers, and friends, a fewer number ...",
            "PI. This is the simplest\n\nmethod that returns 1 % of all tweets during the listening\n\nperiod, but it generates the most amount of noise as well."
          ]
        },
        {
          "title": "Comparing Methods of Creating a Random Sample of Twitter Users ...",
          "url": "https://laura.alessandretti.com/public/pdf_accepted/paper714.pdf",
          "excerpts": [
            "Moreover, the bounding-box method locates a majority of tweets at the centroid of a given country . This probably hap- pens for those users which their self-reported location only include their country, and therefore, Twitter locates their coordinates at the centroid of that country."
          ]
        },
        {
          "title": "(PDF) A Survey of Location Inference Techniques on Twitter",
          "url": "https://www.researchgate.net/publication/280566718_A_Survey_of_Location_Inference_Techniques_on_Twitter",
          "excerpts": [
            "In this paper, we survey a range of techniques applied to infer the location of Twitter users from inception to state-of-the-art. We find ..."
          ]
        },
        {
          "title": "We need to talk more about follower retention on social",
          "url": "https://sproutsocial.com/insights/social-follower-retention/",
          "excerpts": [
            "Aug 26, 2024 — Follower count tells your potential audience size. But follower retention—an overlooked metric—tells how well your strategy is working."
          ]
        },
        {
          "title": "Analyze an account's Twitter followers (Any public account!)",
          "url": "https://circleboom.com/blog/analyze-an-accounts-twitter-followers/",
          "excerpts": [
            "Circleboom offers a detailed analysis of any account's followers, including demographics, genuinity, and many other details."
          ]
        },
        {
          "title": "How to Find X (Twitter) Influencers in 3 Steps +Benchmarks",
          "url": "https://brand24.com/blog/how-to-find-twitter-influencers/",
          "excerpts": [
            "Use Twitter's search function: Input relevant hashtags, keywords, or phrases associated with your niche to find actively engaged Twitter users. Engage in a ..."
          ]
        },
        {
          "title": "The Myth of Follower Counts: Finding the Right Influencers",
          "url": "https://www.ama.org/2022/10/11/finding-goldilocks-influencers-why-high-follower-count-may-not-be-the-best-driver-of-engagement-on-social-media/",
          "excerpts": [
            "Nov 10, 2022 — In a new Journal of Marketing study, we find that influencers with an intermediate follower count represent the engagement sweet spot."
          ]
        },
        {
          "title": "NeurIPS Poster The PRISM Alignment Dataset",
          "url": "https://neurips.cc/virtual/2024/poster/97804",
          "excerpts": [
            "The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of ..."
          ]
        },
        {
          "title": "How Big Tech platforms are neglecting their non-English language users",
          "url": "https://www.globalwitness.org/en/campaigns/digital-threats/how-big-tech-platforms-are-neglecting-their-non-english-language-users/",
          "excerpts": [
            "th X and Pinterest, only 8% of the content moderators are proficient in an official EU language that is not English.",
            " French, Spanish and Portuguese are underrepresented, especially on X and YouTube"
          ]
        },
        {
          "title": "Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict",
          "url": "https://dl.acm.org/doi/10.1145/3652583.3657622",
          "excerpts": [
            "Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict",
            "Jun 7, 2024 — We collected tweets from February 2022 to May 2023 to acquire approximately 1.5 million tweets in 60 different languages along with their images ... Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict | Proceedings of the 2024 International Conference on Multimedia Retrie"
          ]
        },
        {
          "title": "Large Scale Multilingual Dataset of Tweets (arXiv:2304.00913)",
          "url": "https://arxiv.org/pdf/2304.00913",
          "excerpts": [
            "Twitter supports content in 34 languages and user can use any one of them to express\nviews."
          ]
        },
        {
          "title": "Sampling Twitter users for social science research: evidence from a systematic review of the literature",
          "url": "https://www.researchgate.net/publication/367497892_Sampling_Twitter_users_for_social_science_research_evidence_from_a_systematic_review_of_the_literature",
          "excerpts": [
            "Twitter datasets were subject to sampling biases, as only 1-3% of posts contain geolocation data (Qazi et al. 2020), and the geotagging behaviour varies by region (Huang and Carley 2019).",
            "Overall, most of the studies reviewed (63%) were conducted without a sampling frame",
            "and the remaining resorted to sampling frames that are external sources of the Twitterverse,\n\neither panels of research companies or recruitment from crowdsourcing platforms",
            "In studies with customized sampling frames,\n\nthe sample of Twitter users is obtained by random selection (e.g., Bouteca etal.\n .",
            "This chapter provides a foundational overview of trace data in communication research. It systematizes trace data by distinguishing traces as either 1) usage logs or 2) sensor readings."
          ]
        },
        {
          "title": "Data Statements for Natural Language Processing: Toward ...",
          "url": "https://aclanthology.org/Q18-1041/",
          "excerpts": [
            "by EM Bender · 2018 · Cited by 1195 — Emily M. Bender and Batya Friedman. 2018. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science."
          ]
        },
        {
          "title": "[1803.09010] Datasheets for Datasets - arXiv",
          "url": "https://arxiv.org/abs/1803.09010",
          "excerpts": [
            "We propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on."
          ]
        },
        {
          "title": "Data Statements for Natural Language Processing",
          "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00041/43452/Data-Statements-for-Natural-Language-Processing",
          "excerpts": [
            "Dec 1, 2018 — In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and ..."
          ]
        },
        {
          "title": "Datasheets for datasets | Communications of the ACM",
          "url": "https://dl.acm.org/doi/10.1145/3458723",
          "excerpts": [
            "Datasheets for datasets. Authors: Timnit Gebru ... for Computational Linguistics 6 (2018), 587--604. Crossref · Google Scholar. [3]. Bhardwaj, A."
          ]
        },
        {
          "title": "Data Statements for Natural Language Processing - CIS UPenn",
          "url": "https://www.cis.upenn.edu/~mkearns/teaching/ScienceDataEthics/files/lecture/l22/reading1.pdf",
          "excerpts": [
            "by EM Bender · Cited by 1195 — In this paper, we propose data statements as a design solution and professional practice for natural language processing technolo-."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2019.00013/full",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Social Data: Biases, Methodological Pitfalls, and Ethical ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7931947/",
          "excerpts": [
            "by A Olteanu · 2019 · Cited by 929 — There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked."
          ]
        },
        {
          "title": "Towards Evaluating Cultural Bias in Hate Speech Datasets",
          "url": "https://arxiv.org/html/2404.17874v1",
          "excerpts": [
            "Apr 27, 2024 — In this work, we evaluate cultural bias in HS datasets by leveraging two interrelated cultural proxies: language and geography."
          ]
        },
        {
          "title": "A survey of location inference techniques on Twitter - Sage Journals",
          "url": "https://journals.sagepub.com/doi/abs/10.1177/0165551515602847",
          "excerpts": [
            "In this paper, we survey a range of techniques applied to infer the location of Twitter users from inception to state of the art. We find ..."
          ]
        },
        {
          "title": "Large-scale Near-deduplication Behind BigCode",
          "url": "https://huggingface.co/blog/dedup",
          "excerpts": [
            "May 16, 2023 — The typical workflow of MinHash is as follows: Shingling (tokenization) and fingerprinting (MinHashing), where we map each document into a set ..."
          ]
        },
        {
          "title": "[PDF] Twitter Bot Detection using Diversity Measures - ACL Anthology",
          "url": "https://aclanthology.org/W19-7401.pdf",
          "excerpts": [
            "The dataset was crawled on January 5th, 2019 and it contains 5,261,940 tweets. Number of tweets per user ranges from 20 to 3,250 (we fil- tered out accounts ..."
          ]
        },
        {
          "title": "Twitter-Dataset - Kaggle",
          "url": "https://www.kaggle.com/datasets/goyaladi/twitter-dataset",
          "excerpts": [
            "A curated collection of tweets carefully gathered from the vast Twitter ecosystem. This comprehensive dataset offers a valuable resource for researchers."
          ]
        },
        {
          "title": "Twitter follower distribution - Applied Mathematics Consulting",
          "url": "https://www.johndcook.com/blog/2022/08/09/twitter-follower-distribution/",
          "excerpts": [
            "A conversation this morning prompted the question of how many Twitter accounts have between 10,000 and 20,000 followers."
          ]
        },
        {
          "title": "FED: Fast and Efficient Dataset Deduplication Framework ...",
          "url": "https://arxiv.org/html/2501.01046v2",
          "excerpts": [
            "Feb 16, 2025 — This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally ..."
          ]
        },
        {
          "title": "Top 25 Twitter Training Datasets for Data Scientists (Free)",
          "url": "https://smartone.ai/blog/top-25-twitter-training-datasets-free/",
          "excerpts": [
            "Let's explore the top 25 Twitter datasets that are invaluable for sentiment analysis, content moderation, and various other AI applications."
          ]
        },
        {
          "title": "Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition",
          "url": "https://aclanthology.org/2020.lrec-1.180/",
          "excerpts": [
            "Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition",
            "Existing research on fairness evaluation of document classification models mainly uses synthetic monolingual data without ground truth for author demographic attributes. In this work, we assemble and publish a multilingual Twitter corpus for the task of hate speech detection with inferred four author demographic factors: age, country, gender and race/ethnicity. The corpus covers five languages: English, Italian, Polish, Portuguese and Spanish.",
            "We evaluate the inferred demographic labels with a crowdsourcing platform, Figure Eight. To examine factors that can cause biases, we take an empirical analysis of demographic predictability on the English corpus.",
            "We measure the performance of four popular document classifiers and evaluate the fairness and bias of the baseline classifiers on the author-level demographic attributes."
          ]
        },
        {
          "title": "Bias in Abusive Language Datasets (Wich, 2021)",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8288848/",
          "excerpts": [
            " — We developed a bias and comparison framework for abusive language datasets for their in-depth analysis and to provide a comparison of five English and six",
            "[[9]()] reported problems with the association of minority group language with hate in their data, while [[47]()] have done work on the influence of different biases in the sampling of popular abusive language datasets (e.g., topic and author bias).",
            "[[46]()] analyzed how political bias influence hate speech classification models.",
            "[[37]()] proposed social bias frames, which is a formalism that “aims to model the pragmatic frames in which people project social biases and stereotypes onto others",
            "Another form of bias in abusive language datasets that researchers have addressed is annotator bias.",
            "[[36]()] investigated annotator bias concerning the Afro-American English (AAE) dialect.",
            "They showed that a classifier trained on a standard abusive language dataset discriminates documents in AAE.",
            "[[1]()] identified annotator bias by splitting annotators according to their demographic characteristics.",
            "[[45]()] addressed this problem by identifying annotator bias purely on similarities in the annotation behavior.",
            "Lastly, [[15]()] compared six popular datasets according to their differences in class labeling via similarity in a common word embedding space and further classifying them with the Perspective API framework."
          ]
        },
        {
          "title": "The Moonlight Review: Comparing Methods for Creating a National Random Sample of Twitter Users",
          "url": "https://www.themoonlight.io/en/review/comparing-methods-for-creating-a-national-random-sample-of-twitter-users",
          "excerpts": [
            "This paper compares four common methods for creating a random sample of Twitter users in the United States: the 1% Stream, Bounding Box, Location Query, and Language Query methods.",
            "Data collection spanned from September 7, 2022, to October 8, 2022.",
            "The researchers implemented the four sampling methods using the Twitter V2 API.",
            "A crucial step involved pre-processing the collected user accounts to remove bots, organizations, and inactive users, thereby improving the quality of the sample.",
            "From each sampling method, an initial random sample of 30,000 users was drawn to equalize the dataset sizes for comparison."
          ]
        },
        {
          "title": "Virality Prediction on Twitter Using Combined CNN and BERT Models",
          "url": "https://ieeexplore.ieee.org/document/10913355/",
          "excerpts": [
            "Specifically, it shows a 7.1% increase in precision, a 9.9% increase in recall, a 3.8% increase in accuracy, and an 8.1% increase in F1-Score compared to the ..."
          ]
        },
        {
          "title": "Context-Based Tweet Engagement Prediction",
          "url": "https://repositum.tuwien.at/bitstream/20.500.12708/177668/1/Jeromela%20Jovan%20-%202023%20-%20Context-Based%20Tweet%20Engagement%20Prediction.pdf",
          "excerpts": [
            "by J Jeromela · 2023 · Cited by 1 — The submitted models predicting like, reply, retweet, and quote engagements were evaluated based on two metrics: area under the precision-recall curve (PRAUC) ..."
          ]
        },
        {
          "title": "Enhancing Tweet Virality Prediction through Clustering-Driven ...",
          "url": "https://medium.com/@mkim1171/enhancing-tweet-virality-prediction-through-clustering-driven-classification-a7b2b93fe94d",
          "excerpts": [
            "The results were clear: adding cluster information improved model performance. The baseline model achieved approximately 59% accuracy, while the ..."
          ]
        },
        {
          "title": "Predicting Information Diffusion on Twitter a Deep Learning Neural ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7134238/",
          "excerpts": [
            "Through this paper, we propose a Custom Weighted Word Embedding (CWWE) to study the degree of diffusion of content (retweet on Twitter).",
            "The primary contribution of our research paper is CWWE with a combination of CNN in using the linguistic features.",
            "For experimentation, we created a corpus of size 230,000 tweets posted by more than 45,000 users in 6 months.",
            "Research experimentations reveal that using the proposed framework of Custom Weighted Word Embedding (CWWE) from the tweet there is a significant improvement in the overall accuracy of Deep Learning framework model in predicting information diffusion through tweets.",
            "\nHence we can state that our initial hypothesis of a writer’s deliberate choice of placing the more influential words (like nouns, adjectives) at the first half of the tweet puts more emphasis on the intent interpretation by the readers and helps to cascade the same at a faster rate on social media platform as compared to sentences/tweets where these words are used at the latter half is correct."
          ]
        },
        {
          "title": "Task 05 - Viral Tweets Prediction Challenge",
          "url": "https://www.kaggle.com/competitions/task-05-viral-tweets-prediction-challenge",
          "excerpts": [
            "Angelo Menezes, Daniela Lopes Freire, and Robson Parmezan Bonidia. Task 05 - Viral Tweets Prediction Challenge. https://kaggle.com/competitions/task-05-viral- ..."
          ]
        },
        {
          "title": "Tweet question classification for enhancing ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2949719125000068",
          "excerpts": [
            "by C Mallikarjuna · 2025 · Cited by 1 — In this study, we annotated questions in the Tweet QA dataset, performed tweet question classification, and developed the TweetQC dataset."
          ]
        },
        {
          "title": "Prediction of Likes and Retweets Using Text Information ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S1877050920304129",
          "excerpts": [
            "by I Daga · 2020 · Cited by 43 — In this paper we employed different machine learning classifiers like SVM, Naïve Bayes, Logistic Regression, Random Forest, and Neural Network."
          ]
        },
        {
          "title": "The Grim Conclusions of the Largest-Ever Study of Fake News",
          "url": "https://www.theatlantic.com/technology/archive/2018/03/largest-study-ever-fake-news-mit-twitter/555104/",
          "excerpts": [
            "Fake tweets tended to elicit words associated with surprise and disgust, while accurate tweets summoned words associated with sadness and trust, ..."
          ]
        },
        {
          "title": "Emotions and virality: Social transmission of political messages on ...",
          "url": "https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.931921/full",
          "excerpts": [
            "In particular, we focus on how emotional content influences the virality of politicians' tweets. ... emotional features of a tweet (Casarin et al."
          ]
        },
        {
          "title": "In-Depth Twitter Retweet Analysis Dataset - Kaggle",
          "url": "https://www.kaggle.com/datasets/mulengakawimbe89/in-depth-twitter-retweet-analysis-dataset",
          "excerpts": [
            "This dataset provides an extensive analysis of Twitter retweet activities, focusing on various attributes that can influence and describe the nature of retweets ..."
          ]
        },
        {
          "title": "25 Essential Twitter Datasets for Advanced NLP and ML",
          "url": "https://imerit.net/resources/blog/top-25-twitter-datasets-for-natural-language-processing-and-machine-learning-all-pbm/",
          "excerpts": [
            "In this article, we'll list the top 25 Twitter datasets that can be used for models across sentiment analysis and content moderation."
          ]
        },
        {
          "title": "X (Twitter) Engagement Rate Benchmark - Social Status",
          "url": "https://www.socialstatus.io/insights/social-media-benchmarks/twitter-engagement-rate-benchmark/",
          "excerpts": [
            "We publish the average X (Twitter) Engagement Rate Benchmark every month so you can compare your X (Twitter) profiles and improve performance over time."
          ]
        },
        {
          "title": "Prediction of Number of Likes and Retweets based on the ...",
          "url": "https://dl.acm.org/doi/fullHtml/10.1145/3508230.3508244",
          "excerpts": [
            "by R Amitani · 2021 · Cited by 2 — The results revealed that the average vectors of BERT and inceptionresnetv2 served as predictors of the number of likes and RTs. We also found that tweet text ..."
          ]
        },
        {
          "title": "ViralTweet Metrics and Predictors for Tweet Virality (OpenReview)",
          "url": "https://openreview.net/forum?id=FYvZCwdb6F",
          "excerpts": [
            "Previous studies have utilized various measures to forecast the virality of tweets or Facebook posts, but these approaches exhibit limitations, particularly in the absence of a virality metric that specifically considers social biases.",
            "In this paper, we test existing metrics and introduce a new metric, $\\textbf{ViralTweet Score (VTS)}$, inspired by principles of momentum from physics to better predict a tweet's virality given that it consists of social biases.",
            "We test our hypothesis that VTS is a better metric using two methodologies and we show how VTS achieves an F1 score of 0.87 based on pairwise evaluation methodology and an overall F1 score of 0.58 based on our clustering-based verification methodology.",
            "We release the $\\textbf{ViralTweets Dataset}$ with $\\mathbf{88.8k}$ Hindi tweets and corresponding virality labels based on our VTS metric."
          ]
        },
        {
          "title": "ViralBERT: A User Focused BERT-Based Approach to Virality Prediction",
          "url": "https://arxiv.org/pdf/2206.10298",
          "excerpts": [
            "The numerical features chosen for all models are hashtags, mentions, followers,\nfollowing, verified status, and text length.",
            " ‘Hashtags’ and ‘mentions’ are the amount of these elements in the tweet\ntext, so is a feature derived from tweet text.",
            "the sentiment of tweet text is added as a feature for baselines.",
            "494. We find that text sentiment and follower counts,\nand to a lesser extent mentions and following counts, are the most important features in determ",
            "that hashtag counts negatively affect the model performance.",
            "Table 1 shows the results for our different model architectures.",
            " experiments show that our approach outperforms these baselines,\n\t\t\t\t\t\t\t\t\t\t with a 13% increase in both F1 Score and Accuracy compared to the best performing baseline"
          ]
        },
        {
          "title": "Twitter Data: Tweets and User Interactions",
          "url": "https://www.kaggle.com/datasets/thedevastator/tweets-and-user-engagement",
          "excerpts": [
            "By utilizing this extensive dataset, researchers can gain valuable insights into patterns of online communication within Twitter's vast network.",
            "The dataset Twitter Data: Tweets and User Interactions provides comprehensive information about tweets and user interactions on the popular social media platform Twitter. The dataset includes various attributes that shed light on the characteristics and engagement metrics of tweets, allowing for in-depth analysis of user behavior and content performance."
          ]
        },
        {
          "title": "RSOS paper on sentiment and virality (as cited in the provided document)",
          "url": "https://royalsocietypublishing.org/doi/10.1098/rsos.201756",
          "excerpts": [
            "In this section, we present the procedure followed to collect the data of study, the features used to represent each tweet and the statistical model employed to assess the effect of the use of sentiment words on the virality of a tweet.",
            "s, we consider that the integration of iSOL semantic resource can be used as a significant predictor related to the virality for the number of retweets, once the effect of other tweets features related to its diffusion are isolated.",
            "the use of negativity contributes to the retweeting average in the political domain, while positivity reduces the probability of retweeting.",
            "In order to demonstrate the hypotheses, we statistically assess the effect of some relevant tweet features and the use of sentiment words on the spreading of a tweet.",
            "the more negative words you use in a tweet, the higher is the probability of retweet; on the contrary, the more positive words the tweet contains, the lower is that probability of retweet.",
            "model indicates that tweets from verified accounts are retweeted 23.51% less than those from non-verified accounts and similar other features.",
            "Finally, we find that models fitted with NRC and ML-Senticon provide quite similar results about the effect of these features on the number of retweets.",
            "Apr 14, 2021 — Both, diffusion and virality are observed by the same outcome, the number of retweets. So the way of writing something influences its virality. ..."
          ]
        },
        {
          "title": "ArXiv: 2303.06120 — Predicting Viral Tweets with Language Models",
          "url": "https://arxiv.org/pdf/2303.06120",
          "excerpts": [
            "We mainly employ text content as the feature and use transformer-\n\nbased language models to represent it. We also use additional\n\nfeatures that the language models may not model. In our exper-\n\niment, we observe that such additional features slightly increase\n\nour model’s performance. Those features are the boolean features\n\nthat whether the tweet contains media, hashtags, mentions, has\n\npositive sentiment, negative sentiment, and if it is sourced from\n\na verified a",
            "We used distilbert-base-uncased-finetuned-sst-  \n2\\-english\n\nmodel to compute sentiment, which yields if the tweet\n\nis positive and negative with a confidence score. We assign the\n\nsentiment of the tweet if the confidence score was higher th",
            "Table 3: Results using the plain model and the model with\n\nextra features marked with",
            "This study improves virality understanding on social media by\n\ntesting existing metrics on reliable data, proposing a new metric,\n\nand predicting viral tweets. Our analysis predates Twitter’s view\n\ncount disclosure policy. However, our virality metrics may benefit\n\nthe works on other social media platforms which do not disclose\n\nsuch data. Additionally, our work on predicting viral tweets using\n\nlanguage models may inspire future work which automatically\n\ngenerates content that is likely to go viral, which may help experts\n\nand fact-checkers to create content that better resonates with the"
          ]
        },
        {
          "title": "Dartmouth Senior Thesis on Predicting Influencer Virality",
          "url": "https://digitalcommons.dartmouth.edu/cgi/viewcontent.cgi?article=1154&context=senior_theses",
          "excerpts": [
            "by DK Han · 2020 · Cited by 2 — We first approach virality prediction using traditional methods, examining the pre- dictive capability of individual feature categories ...",
            "Feature Extraction and Analysis"
          ]
        },
        {
          "title": "Emotion shapes the diffusion of moralized content in social ...",
          "url": "https://www.pnas.org/doi/pdf/10.1073/pnas.1618923114",
          "excerpts": [
            "by WJ Brady · 2017 · Cited by 1534 — Moral-emotional language predicts the greatest number of retweets. The graph depicts the number of retweets, at the mean level of continuous and."
          ]
        },
        {
          "title": "How do sentiments affect virality on Twitter? - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8059576/",
          "excerpts": [
            "We have found that the use of negativity in a tweet increases the probability of retweeting and that iSOL lexicon is the one that better ..."
          ]
        },
        {
          "title": "How It Works - LIWC-22",
          "url": "https://www.liwc.app/help/howitworks",
          "excerpts": [
            "LIWC reads a given text and compares each word in the text to the list of dictionary words and calculates the percentage of total words in the text that match ..."
          ]
        },
        {
          "title": "Concreteness ratings for 40 thousand generally known ...",
          "url": "https://link.springer.com/article/10.3758/s13428-013-0403-5",
          "excerpts": [
            "by M Brysbaert · 2014 · Cited by 2332 — Concreteness ratings are presented for 37,058 English words and 2,896 two-word expressions (such as zebra crossing and zoom in), ..."
          ]
        },
        {
          "title": "English Abstractness / Concreteness Ratings - Universität Stuttgart",
          "url": "https://www.ims.uni-stuttgart.de/en/research/resources/lexica/abst-ratings-en/",
          "excerpts": [
            "A collection of English concreteness / abstractness norms for 3 million English words and Multiword expression."
          ]
        },
        {
          "title": "Automatic Detection of Irony and Humour in Twitter",
          "url": "https://computationalcreativity.net/iccc2014/wp-content/uploads/2014/06/9.2_Barbieri.pdf",
          "excerpts": [
            "by F Barbieri · Cited by 110 — In this article we have proposed a novel linguistically moti- vated set of features to detect irony and humour in the social network Twitter. The features take ...",
            "by F Barbieri · Cited by 110 — Looking at the figures obtained in our irony detection exper- iments, it appears that our model is more balanced in terms of precision and recall and that our ..."
          ]
        },
        {
          "title": "[PDF] A Lexicon-Based Approach for Detecting Hedges in Informal Text",
          "url": "https://aclanthology.org/2020.lrec-1.380.pdf",
          "excerpts": [
            "Abstract. Hedging is a commonly used strategy in conversational management to show the speaker's lack of commitment to what they com-."
          ]
        },
        {
          "title": "#EngagementBait - Search / X",
          "url": "https://twitter.com/search?q=%23EngagementBait&src=hashtag_click&f=user",
          "excerpts": [
            "Engagement Bait Detector · @baitdetectir. Follow. Click to Follow baitdetectir ... Engagement Bait Tweets · @BaitTweet. Follow. Click to Follow BaitTweet."
          ]
        },
        {
          "title": "Surprises: low probabilities or high contrasts? - ScienceDirect.com",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0010027702002019",
          "excerpts": [
            "Surprises are generally created by low-probability outcomes, yet, as shown by several experiments, not all low-probability outcomes are equally surprising."
          ]
        },
        {
          "title": "Concreteness ratings for 62000 English multiword ...",
          "url": "https://link.springer.com/article/10.3758/s13428-022-01912-6",
          "excerpts": [
            "by EJ Muraki · 2023 · Cited by 29 — Brysbaert and Biemiller (2017) validated test-based age-of-acquisition norms collected by Dale and O'Rouke (1981) for 3100 multiword expressions ..."
          ]
        },
        {
          "title": "Concreteness and Abstractness as Causes and Effects of ...",
          "url": "https://www.tandfonline.com/doi/abs/10.1080/15213269.2023.2256214",
          "excerpts": [
            "by I Razpurker-Apfeld · 2024 · Cited by 1 — We explored new antecedents and consequences of identification with media characters that are related to concreteness or abstractness of representations."
          ]
        },
        {
          "title": "Moral-emotional language and contagion on Twitter (Brady et al. 2017, PNAS)",
          "url": "https://www.pnas.org/doi/10.1073/pnas.1618923114",
          "excerpts": [
            "Adding a single moral-emotional word to a given tweet increased its expected retweet rate by 17%.",
            "Importantly, there was a significant main effect of moral-emotional language (IRR = 1.19, _P_ < 0.001, 95% CI = 1.14, 1.23); adding a single moral-emotional word to a given tweet increased its expected retweet rate by 19% ( [Fig. 1](#fig01) )."
          ]
        },
        {
          "title": "LIWC Dictionary (Linguistic Inquiry and Word Count)",
          "url": "https://lit.eecs.umich.edu/geoliwc/liwc_dictionary.html",
          "excerpts": [
            "LIWC Dictionary",
            "The LIWC dictionary used in this demonstration is composed of 5,690 words and word stems. Each word or word stem defines one or more word categories. For example, the word 'cried' is part of four word categories: sadness, \n negative emotion, overall affect, and a past tense verb. Hence, \n if it is found in the target text, each of these four category \n scale scores will be incremented. As in this example, many of the \n LIWC categories are arranged hierarchically. All anger words, \n by definition, will be categorized as negative emotion and overall \n emotion words. Each of the 69 preset LIWC categories used in this demo is composed of a list \n of dictionary words that define that scale. The table below provides \n a comprehensive list of these LIWC categories with \n sample scale words. | ## LIWC Dimensions and Sample Words |  |  |\n| **DIMENSION** | **EXAMPLES** |  |"
          ]
        },
        {
          "title": "LIWC-22 Development Manual",
          "url": "https://www.liwc.app/static/documents/LIWC-22%20Manual%20-%20Development%20and%20Psychometrics.pdf",
          "excerpts": [
            "The LIWC-22 Dictionary is the heart of the text analysis strategy. The internal dictionary is\n\ncomposed of over 12,000 words, word stems, phrases, and select emoticons. Each dictionary\n\nentry is part of one or more categories, or subdictionaries, designed to assess various\n\npsychosocial construc",
            "Most, but not all, of the LIWC-22 categories are arranged\n\nhierarchically. All sadness words, by definition, belong to the broader “emo\\_neg”, “emotion”,\n\n“tone\\_neg”, as well as the overall “affect” category. Note too that word stems can be captured by\n\nthe LIWC-22 sy",
            "For example, the word _cried_ is part of 10-word categories: affect,\n\ntone\\_pos, emotion, emo\\_neg, emo\\_sad, verbs, focuspast, communication, linguistic, and\n\ncognition. Hence, if the word _cried_ is found in the target text, each of these 10 subdictionary\n\nscale scores will be in",
            "LIWC-22 is designed to accept written or transcribed verbal text which has been stored as a\n\ndigital, machine-readable file in one of multiple formats, including plain text (.txt), PDF, RTF, or\n\nstandard Microsoft Word files (.docx",
            "he LIWC-22 processing module accesses each text in your dataset, compares\n\nthe language within each text against the LIWC-22 dictionary.",
            "The LIWC-22 Dictionary and its Development**\n\nThe LIWC-22 Dictionary is the heart of the text analysis strategy. The internal dictionary is\n\ncomposed of over 12,000 words, word stems, phrases, and select emotico",
            "LIWC-22 Development Manual"
          ]
        },
        {
          "title": "arXiv:2504.16723v1 - Multimodal heuristics for insightful content and meme detection",
          "url": "https://arxiv.org/html/2504.16723v1",
          "excerpts": [
            "This paper introduces a framework that integrates optical character recognition (OCR), caption generation, retrieval-augmented classification, and a visual ...",
            "Recent studies [ 13 , 11 , 23 ] have attempted to bridge the gap between language and vision representations, revealing that combined multimodal strategies can achieve promising results for specific domains such as misogynistic memes [ 34 ] or harmful COVID-19 memes [ 29 ] .",
            "While these approaches have shown promise, they exhibit several key limitations that hinder their ability to comprehensively detect nuanced hateful content.",
            "First, many existing methods [ 35 , 4 , 1 ] rely on fixed multimodal representations, where text and image features are extracted independently and fused statically. This rigid approach fails to capture the dynamic interplay between textual and visual cues, making it difficult to detect contextually embedded hate signals, such as sarcasm, coded symbols, or ambiguous imagery [ 18 ] .",
            "Second, these methods typically lack real-time adaptive reasoning, instead relying on predefined classification heuristics [ 19 , 33 ] . As a result, they struggle with detecting veiled or evolving hate speech that requires contextual reasoning beyond surface-level analysis.",
            "Third, existing models often categorize hateful content using coarse-grained labels, such as simply hateful or non-hateful, without distinguishing between different forms of hate speech. This lack of specificity reduces interpretability and makes it harder to apply targeted moderation strategies.",
            "The proposed framework demonstrates a robust approach to addressing hateful content detection within memes by integrating multiple modules for text extraction, captioning, retrieval, and visual question answering. The integrated pipeline achieves significant accuracy and AUC-ROC compared to existing methods on established benchmarks.",
            "These results underline the importance of uniting refined language strategies with methods that analyze images more deeply.",
            "Future work could extend the framework by incorporating culturally nuanced knowledge graphs, refining VQA prompts to reduce false positives, or integrating dynamic feedback loops for real-time detection."
          ]
        },
        {
          "title": "Meme Detection Thesis (Frangidis, 2021)",
          "url": "https://ikee.lib.auth.gr/record/329871/files/GRI-2021-30317.pdf",
          "excerpts": [
            "Meme Detection Modeling",
            "the images\nthat accompany the original post as input, determines whether that image is meme or\nnot. Moreover, the second module takes as input the tweets ’s text and analyzes it in\norder to extract features that point to either of the two classes. The final module makes\nthe classification of the Twitter post based on contextual metadata like the number of\nfollowers that a user has or the number of likes on a particular tweet.",
            "Twitter data\nare utilized in the task of rumour detection [24]. Moreover, in YouTube platform it\nhas been researched the way that popular meme videos affect other creators [59].",
            "Moreover, texts from the Twitter posts are\nproved to be incapable of learning any difference between posts that contain meme im-\nages and ones that do not. However, when they are used in combination with the images\nthey improve the results of image-based model",
            "the proposed combi-\nnation of all three models (ICM, MCM and TCM) also edges out the image-only model\non the task of meme detection, although it is not superior to the image-text and image-\nmetadata variatio",
            "They use Tesseract to extract the em-\nbedded text inputing it in an LSTM network to extract textual features and ResNet 18\nto extract image features. They pass all this features along with a face encoding, which\nthey created utilizing an open source package, to a dense layer that classifies the image\nas a meme or no",
            "They extract different features from tweets (such as adjacency matrices, follower\ndistributions and sentiment scores) and utilize a KNN with Dynamic Time Wrapping\n[40] to classify each post in the appropriate cate",
            "They extract different features from tweets (such as adjacency matrices, follower\ndistributions and sentiment scores) and utilize a KNN with Dynamic Time Wrapping\n[40] to classify each post in the appropriate cate",
            "e of them. The three pretrained models, VGG19, ResNet50 and InceptionV3, are also used in\nthis dissertation’s model for detecting memes using images."
          ]
        },
        {
          "title": "Causal language and strength of inference in academic and ...",
          "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196346",
          "excerpts": [
            "by N Haber · 2018 · Cited by 104 — We investigated the state of causal inference in health research as it appears at the end of the pathway, at the point of social media consumption."
          ]
        },
        {
          "title": "The Power of Discourse Markers in Effective Communication",
          "url": "https://www.numberanalytics.com/blog/the-power-of-discourse-markers-in-effective-communication",
          "excerpts": [
            "Causal discourse markers are used to indicate cause-and-effect relationships between ideas. Examples include: therefore; thus; consequently; as ..."
          ]
        },
        {
          "title": "Twitter Sentiment and Engagement Analysis - Kaggle",
          "url": "https://www.kaggle.com/code/jocelyndumlao/twitter-sentiment-and-engagement-analysis",
          "excerpts": [
            "Tweets with positive sentiments tend to have higher average likes and retweets compared to neutral and negative tweets. Negative tweets receive the lowest ..."
          ]
        },
        {
          "title": "The Why We Retweet scale | PLOS One - Research journals",
          "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0206076",
          "excerpts": [
            "by A Majmundar · 2018 · Cited by 78 — The Why We Retweet Scale offers a useful conceptualization and assessment of motivations for retweeting. In the future, communication ..."
          ]
        },
        {
          "title": "(PDF) Linguistic Features of English in Twitter",
          "url": "https://www.researchgate.net/publication/284786037_Linguistic_Features_of_English_in_Twitter",
          "excerpts": [
            "The dominant grammatical features of English used in Twitter are prepositions; indefinite, personal, reflexive and wh-pronouns; auxiliary verbs."
          ]
        },
        {
          "title": "Responding to Causal Uncertainty in the Twitterverse",
          "url": "https://journals.sagepub.com/doi/10.1016/j.intmar.2018.11.002",
          "excerpts": [
            "Jan 31, 2022 — Events that capture the public's attention are saturated with causal uncertainty (i.e., uncertainty about why things happen)."
          ]
        },
        {
          "title": "Who Sees Retweets? -",
          "url": "https://themarketingheaven.com/who-sees-retweets/",
          "excerpts": [
            "Feb 16, 2025 — High retweet rates serve as a key performance indicator, signaling how well your content resonates and influences brand perception. By ..."
          ]
        },
        {
          "title": "Why are some social-media contents more popular than ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S0957417422001609",
          "excerpts": [
            "by E Saquete · 2022 · Cited by 30 — A novel approach to extract virality patterns from social media Twitter is presented. Opinion mining extracts subjective content transforming it into ..."
          ]
        },
        {
          "title": "Measuring engagement on twitter using a composite index",
          "url": "https://www.sciencedirect.com/science/article/pii/S175115772200075X",
          "excerpts": [
            "The publications by SMIs that were retweets of other users were discarded. Ultimately, 144,989 tweets were analyzed, which is an average of 2416 tweets per SMI."
          ]
        },
        {
          "title": "Twitter giveaways can be hacked to win every time",
          "url": "https://levels.io/giveaway/",
          "excerpts": [
            "Jan 11, 2020 — It's relatively easy to hack giveaways to win them by being one of the, or all of the, last 100 retweeters."
          ]
        },
        {
          "title": "Ultimate Guide to Running a Successful Twitter Giveaway ...",
          "url": "https://uplup.com/blog/ultimate-guide-to-x-twitter-giveaways",
          "excerpts": [
            "Jun 25, 2025 — Boost engagement and grow followers with a Twitter giveaway. Learn benefits, rules, and strategies for a successful giveaway in this guide."
          ]
        },
        {
          "title": "Automatic Detection of Irony and Humour in Twitter",
          "url": "https://www.semanticscholar.org/paper/Automatic-Detection-of-Irony-and-Humour-in-Twitter-Barbieri-Saggion/79d3c23b6f763a6d0a0a5538a46b0e1f97959209",
          "excerpts": [
            "This study investigates the automatic detection of irony and humour in social networks such as Twitter casting it as a classification problem and proposes a ..."
          ]
        },
        {
          "title": "Fake News Detection on X (Twitter)",
          "url": "https://repository.rit.edu/cgi/viewcontent.cgi?article=13188&context=theses",
          "excerpts": [
            "by SM Almutaiwei · 2025 — As shown in Equation (5), F1 evaluates precision and recall using their harmonic mean,. F1 Score = 2* (Precision * Recall) / (Precision + Recall)."
          ]
        },
        {
          "title": "arXiv:2002.01462v1 - Semantic Search of Memes on Twitter",
          "url": "https://arxiv.org/vc/arxiv/papers/2002/2002.01462v1.pdf",
          "excerpts": [
            "ResNet + LinearSVM \n0.7",
            "(mAP)\n​score. This score summarizes a precision-recall curve as the\n \n \n \n \n \n \n \n   \n \n \n \n \nweighted mean of all precision values achieved at each threshold, with the increase in recall\n \n   \n \n \n \n   \n \n \n \n \n   \n \nfrom the"
          ]
        },
        {
          "title": "Welcome to LIWC-22",
          "url": "https://www.liwc.app/",
          "excerpts": [
            "LIWC is the gold standard in software for analyzing word use. It can be used to study a single individual, groups of people over time, or all of social media."
          ]
        },
        {
          "title": "A hedging annotation scheme focused on epistemic ...",
          "url": "https://aclanthology.org/W15-0302.pdf",
          "excerpts": [
            "by LM Sanchez · 2015 · Cited by 13 — This paper presents an annotation scheme for hedging focused on epistemic modality expressions in informal language. Hedges are used by a speaker to modulate ..."
          ]
        },
        {
          "title": "Epistemic Modality",
          "url": "https://iep.utm.edu/ep-moda/",
          "excerpts": [
            "Hedging views can offer some account of the oddness of embedded epistemic modals, since on these views the speaker is using “may” or “might” to express ..."
          ]
        },
        {
          "title": "Implicative verbs and their presuppositions - Prerna Nadathur",
          "url": "https://pnadathur.github.io/pdfs/implicatives_final.pdf",
          "excerpts": [
            "by P Nadathur · 2015 · Cited by 8 — Mary dared to open the door. b. ; Mary opened the door. Implicatives may be distinguished from factive verbs like know and regret (Kiparsky and."
          ]
        },
        {
          "title": "Implicative-Verbs.pdf",
          "url": "https://www.researchgate.net/profile/Lauri-Karttunen/publication/271696228_Implicative_Verbs/links/54f9eca60cf21ee4fdedfd2b/Implicative-Verbs.pdf",
          "excerpts": [
            "There are also verbs that are not inherently factive or non-factive, e.g. report, announce, remember. They can be used with or without presupposition of the ..."
          ]
        },
        {
          "title": "Evidence for the Concreteness of Abstract Language",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8773921/",
          "excerpts": [
            "by N Del Maschio · 2021 · Cited by 30 — Abstract concepts have been defined as mental representations referring to entities that are neither purely physical nor spatially constrained [ ..."
          ]
        },
        {
          "title": "Concreteness vs. Abstractness: A Selectional Preference ...",
          "url": "https://aclanthology.org/2022.aacl-srw.13/",
          "excerpts": [
            "by T Tater · 2022 · Cited by 4 — Concrete words refer to concepts that are strongly experienced through human senses (banana, chair, salt, etc.), whereas abstract concepts are less perceptually ..."
          ]
        },
        {
          "title": "Measuring and Detecting Virality on Social Media",
          "url": "https://arxiv.org/abs/2303.06120",
          "excerpts": [
            "by T Elmas · 2023 · Cited by 16 — We find that a tweet is more likely to be classified as viral by Twitter if the ratio of retweets to its author's followers exceeds some threshold."
          ]
        },
        {
          "title": "(PDF) Improved Twitter Virality Prediction using Text and ...",
          "url": "https://www.researchgate.net/publication/355379921_Improved_Twitter_Virality_Prediction_using_Text_and_RNN-LSTM",
          "excerpts": [
            "PDF | The matter of influence and virality in social media has been studied since the popularity explosion of these platforms. A gargantuan amount of."
          ]
        },
        {
          "title": "Contrast Pattern-Based Classification for Bot Detection on ...",
          "url": "https://ieeexplore.ieee.org/iel7/6287639/8600701/08685085.pdf",
          "excerpts": [
            "by O Loyola-González · 2019 · Cited by 99 — Our representation is based on sentiment analysis extracted from the text posted in a tweet and the frequency data coming from tweets and ..."
          ]
        },
        {
          "title": "Discourse, Sentiment, and Credibility in Public Health ...",
          "url": "https://arxiv.org/html/2505.22032v1",
          "excerpts": [
            "May 28, 2025 — This study analyzes two years of tweets from, to, and about the CDC using a mixed-methods approach to examine discourse characteristics, ..."
          ]
        },
        {
          "title": "Humor detection using deep learning in 10 years: A survey",
          "url": "https://www.scipedia.com/public/Ren_et_al_2023b",
          "excerpts": [
            "Jan 31, 2024 — The model obtained 77.36% and 79.41% precision in detecting humorous punchlines on UR-FUNNY and MUStaRD datasets. The study of Yang et al. [30] ..."
          ]
        },
        {
          "title": "MemeTector: enforcing deep focus for meme detection",
          "url": "https://link.springer.com/article/10.1007/s13735-023-00277-6",
          "excerpts": [
            "by C Koutlis · 2023 · Cited by 14 — We propose a methodology, called visual part utilization, that utilizes the visual part of image memes as instances of the regular image class.See more"
          ]
        },
        {
          "title": "Evaluating Twitter's algorithmic amplification of low-credibility ...",
          "url": "https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00456-3",
          "excerpts": [
            "by G Corsi · 2024 · Cited by 12 — This study presents a measurement approach that uses observed digital traces to infer the status of algorithmic amplification of low-credibility content on ..."
          ]
        },
        {
          "title": "I cue you liking me: Causal and spillover effects of ...",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0747563223002157",
          "excerpts": [
            "by WJ Zhang · 2023 · Cited by 1 — Technological engagement bait has a spillover effect to increase engagement with the non-baiting videos of the same users."
          ]
        },
        {
          "title": "#EngagementBait - Search / X",
          "url": "https://twitter.com/search?q=%23EngagementBait&src=hashtag_click",
          "excerpts": [
            "The latest posts on #EngagementBait. Read what people are saying and join the conversation."
          ]
        },
        {
          "title": "Rant: The amount of engagement bait on vtuber twitter is ...",
          "url": "https://www.reddit.com/r/VirtualYoutubers/comments/1du8r41/rant_the_amount_of_engagement_bait_on_vtuber/",
          "excerpts": [
            "It's now mostly rage bait and engagement farming, with regular replies getting hidden, and personally, that's led me to mostly stop tweeting."
          ]
        },
        {
          "title": "Curation: How to Beat Negative Network Effects",
          "url": "https://breadcrumb.vc/curation-how-to-beat-negative-network-effects-229dffff166b",
          "excerpts": [
            "Jan 11, 2021 — There are two types of negative network effects in the software realm — networks can choose between five curation mechanisms to overcome them."
          ]
        },
        {
          "title": "Virality Prediction and Community Structure in Social Networks",
          "url": "https://arxiv.org/pdf/1306.0158",
          "excerpts": [
            "ur\n\ncommunity-based prediction excels in both precision and recall, indicating that communities are helpful in\n\ncapturing viral memes (Fig. 5)",
            "For example, when detecting the most viral memes by users (θU = 90),\n\nour method is about seven times as precise as random guess and over three times as precise as prediction\n\nwithout community features. We achieve a recall over 350% better than random guess and over 200% better\n\nthan community-blind predicti",
            "statistics based on early popularity and three types of community-based features in the prediction model,\n\nlisted below. 1. Basic features based on early popularity. Two basic statistical features are included in the\n\n\t prediction model. The number of early adopters is the number of distinct users who generated the\n\n\t earliest tweets. The number of uninfected neighbors of early adopters characterizes the set of users who\n\n\t can adopt the meme during the next step. 2. Infected communities.\nThe simplest feature related to communities is the number of infected com-\n\n\t munities, i.e., the number of communities containing early adopters. 3. Usage and adoption entropy. H t (h) and H u (h) are good indicators of the strength of meme\n\n\t concentration, as shown in Fig. 3. 5\n\f  4. Fraction of intra-community user interactions. We count pair-wise user interactions about any\n\n\t  given meme, and calculate the proportion that occur between people in the same",
            "Two baselines are set up for comparison. Random guess selects nviral memes at random,\n\nwhere nviral is the number of viral memes in the actual data. Community-blind prediction employs the\n\nsame learning algorithm as ours but without the community-based features. We compute both precision\n\nand recall for evaluation; the former measures the proportion of predicted viral memes that are actually\n\nviral in the real data, and the latter quantifies how many of the viral memes are correctly predic",
            "Our\n\ncommunity-based prediction excels in both precision and recall, indicating that communities are helpful in\n\ncapturing viral memes (Fig. 5",
            "We show that (i) communities allow us\n\nto estimate how much the spreading pattern of a meme deviates from that of infectious diseases; (ii) viral\n\nmemes tend to spread like epidemics; and finally (iii) we can predict the virality of memes based on early\n\nspreading patterns in terms of community structu"
          ]
        },
        {
          "title": "Twitter:domain - Cards - X Developers",
          "url": "https://devcommunity.x.com/t/twitter-domain/64463",
          "excerpts": [
            "Apr 4, 2016 — UWhen you SHARE through a tool like AddThis, it is pulling the FULL url (canonical URL) - not the shortened URL that is in the meta tags. It ...See more"
          ]
        },
        {
          "title": "Why does Twitter.com not change its URL to X.com?",
          "url": "https://www.reddit.com/r/Twitter/comments/18nufog/why_does_twittercom_not_change_its_url_to_xcom/",
          "excerpts": [
            "Most of their codebases uses “twitter.com” as it was seen as a stable, static URL. No way they change the URL without a multi day outage.See more"
          ]
        },
        {
          "title": "Twitter: canonical URLs and Protocols",
          "url": "https://pigsonthewing.org.uk/twitter-canonical-urls-protocols/",
          "excerpts": [
            "May 17, 2009 — We can again regard the first of them, on twitter.com, as canonical. Anyone using one of those services, and who wants to link to my profile or ..."
          ]
        },
        {
          "title": "Sharing a URL with a query string on Twitter - Stack Overflow",
          "url": "https://stackoverflow.com/questions/6208363/sharing-a-url-with-a-query-string-on-twitter",
          "excerpts": [
            "This can be solved by using https://twitter.com/intent/tweet instead of http://www.twitter.com/share. Using the intent/tweet function, you simply URL encode ..."
          ]
        },
        {
          "title": "Conversation ID - X - Twitter Developer",
          "url": "https://developer.x.com/en/docs/x-api/conversation-id",
          "excerpts": [
            "When Posts are posted in response to a Post (known as a reply), or in response to a reply, there is now a defined conversation_id on each reply, which matches ...",
            "Using the X API v2, you can retrieve and reconstruct an entire conversation thread, allowing you to understand what is being said and how conversations and ..."
          ]
        },
        {
          "title": "[Help] Referenced_tweets returning none when there is clearly a ...",
          "url": "https://devcommunity.x.com/t/help-referenced-tweets-returning-none-when-there-is-clearly-a-parent-tweet/175721",
          "excerpts": [
            "I've read the docs and learnt that it is stored in referenced tweets as a list. But everytime I run, it's returning me “None”, even tho the tweet I'm running ..."
          ]
        },
        {
          "title": "Presence of both retweeted_status and quoted_status on ...",
          "url": "https://devcommunity.x.com/t/presence-of-both-retweeted-status-and-quoted-status-on-root-level/129366",
          "excerpts": [
            "In the root level of some tweet objects, I see valid payload in both retweeted_status and quoted_status. As per my understanding, if a tweet t2 ...See more"
          ]
        },
        {
          "title": "Twitter API: Check if a tweet is a retweet",
          "url": "https://stackoverflow.com/questions/18869688/twitter-api-check-if-a-tweet-is-a-retweet",
          "excerpts": [
            "Make sure your \"re-tweet\" is not actually a quote of an another tweet. In this case, I had to use the quoted_status field to get the original ...See more",
            "I've noticed that retweets are distinguished from regular tweets by the line \"RT @...\" The Python code following can be used to identify them:",
            "You can check if its a retweet by checking the tweet and then getting what you need from the retweeted_status hash."
          ]
        },
        {
          "title": "Edit Posts",
          "url": "https://docs.x.com/x-api/fundamentals/edit-posts",
          "excerpts": [
            "This information is specified by the edit_history_tweet_ids , which is a default field in the Post payload. This array will contain at least one ID, the ID of ..."
          ]
        },
        {
          "title": "SimHash for Efficient IR",
          "url": "https://www.numberanalytics.com/blog/simhash-for-efficient-information-retrieval",
          "excerpts": [
            "Threshold Selection: Establishing an appropriate Hamming distance threshold is crucial for distinguishing between duplicates and non-duplicates."
          ]
        },
        {
          "title": "Locality Sensitive Hashing: An Short Intro Based on Minhash",
          "url": "https://benjamin-wang.medium.com/locality-sensitive-hashing-6b859d164ab1",
          "excerpts": [
            "There are different LSH functions associated with different distance measures. LSH functions are used to group similar items into the same ..."
          ]
        },
        {
          "title": "Locality Sensitive Hashing (LSH): The Illustrated Guide - Pinecone",
          "url": "https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/",
          "excerpts": [
            "First, we convert text to sparse vectors using k-shingling (and one-hot encoding), then use minhashing to create 'signatures' — which are passed onto our LSH ...",
            "In this article, we'll be covering the traditional approach — which consists of multiple steps — shingling, MinHashing, and the final banded LSH function. At ..."
          ]
        },
        {
          "title": "sentence-transformers/paraphrase-multilingual-MiniLM- ...",
          "url": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
          "excerpts": [
            "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or ..."
          ]
        },
        {
          "title": "Near-Duplicate Detection. or How to Clean Up All Those ...",
          "url": "https://medium.com/@jonathankoren/near-duplicate-detection-b6694e807f7a",
          "excerpts": [
            "Finding the near-duplicates to an article is straightforward. We take the article, shingle it, calculate its signature, and then find its ..."
          ]
        },
        {
          "title": "Searching strings where Hamming Distance is less than a ...",
          "url": "https://stackoverflow.com/questions/27049419/searching-strings-where-hamming-distance-is-less-than-a-threshold",
          "excerpts": [
            "if Hamming Distance is less than the threshold, then add the hash string to the results list. repeat step 1 and 2 for all hash strings."
          ]
        },
        {
          "title": "Simhash and solving the hamming distance problem: explained",
          "url": "http://ben-whitmore.com/simhash-and-solving-the-hamming-distance-problem-explained/",
          "excerpts": [
            "You could readily increase the possible Hamming distance up to k = 6, by dividing the hash into 8 blocks (28 tables)."
          ]
        },
        {
          "title": "Getting to the canonical URL for a Tweet",
          "url": "https://developer.x.com/en/blog/community/2020/getting-to-the-canonical-url-for-a-post",
          "excerpts": [
            "Getting to the canonical url for a tweet",
            "Here's the secret... Each Tweet ID is already unique, so you don't need to know the user that posted it! It turns out, you can add any username to the URL.",
            "URL = \"https://twitter.com/[screen name]/status/[Tweet ID]\"_",
            "Each Tweet ID is already unique, so you don't need to know the user that posted it! It turns out, you can add any username to the URL, and Twitter will work it out for you.",
            "_URL = \"https://twitter.com/twitter/status/1234903157580296192\"_",
            "A surprisingly-common question I hear from developers, bloggers and researchers using Twitter is, _\"how can I get the original URL for a Tweet\"?_"
          ]
        },
        {
          "title": "Near-Duplicate Tweet Detection (Fagbemi 2014)",
          "url": "https://csserver.ucd.ie/~meloc/MScASE/resources/Damilare_Fagbemi.pdf",
          "excerpts": [
            "Similarity Technique\t\t  Similarity Threshold",
            "The clustering system incorporates 4 similarity measurement techniques namely\nLevenshtein distance, Jaccards coefficient, Minhashing, and Hamming distance.",
            "\t\t\t\t  Minhashing\t\t\t\t\t\t 0.10",
            "\t\t\t\tHamming Distance\t\t\t\t\t 0.65",
            ". The experiments\nwere conducted using different representations of such tweets.\nThis helped us\nto identify what similarity threshold was necessary to correctly identify near-\nduplicates given a specific combination of tweet representation and similarity\nmeasure",
            ". Hence, it is not possible to use one threshold for all tweet comparisons. In order to determine the appropriate thresholds for different combinations of\ntweet representations and similarity techniques, we ran a series of similarity\nscoring experiments on existing near-duplicate tweet pairs.",
            "Twitter has become a very popular micro blogging tool used for the expression of\nviews and to broadcast news or events.",
            "Near-duplicate detection in Twitter is of increasing importance\ndue to the primary role it plays in first story detection, spam detection, and\nmany other clustering processes.",
            "We decided to treat\ntweets as more conventional text pieces. Our idea being the clustering of tweets\nbased on pure text analytics.",
            "We also designed and/or implemented 3 clustering strategies: Naive Pairwise\n(NPW), Exact Duplicate Filtering (EDF), and Multi-Index Locality Sensitive Hash-\ning (LSH",
            "o discover the optimal text representation for detecting near-\nduplicates in the context of Twitter, we utilized 8 tweet representations namely\n\n\t\t\t\t\t\t\t\t\t\t51\n\fFull text, Filtered punctuation text, Stopped and stemmed text, Full shingles,\nFiltered punctuation shingles, Stopped and stemmed shingles, minhash signa-\ntures,"
          ]
        },
        {
          "title": "Using Postman (Twitter API v2) to get retweets and replies for ...",
          "url": "https://devcommunity.x.com/t/using-postman-twitter-api-v2-to-get-retweets-and-replies-for-thousands-of-tweets/151988",
          "excerpts": [
            "Mar 30, 2021 — I already used referenced_tweets which includes the type (retweeted, replied_to, quoted). My question is if a tweet is a retweet of a ..."
          ]
        },
        {
          "title": "Tweet urls some have status some others do not - X API v2",
          "url": "https://devcommunity.x.com/t/tweet-urls-some-have-status-some-others-do-not/177275",
          "excerpts": [
            "Sep 20, 2022 — I see in the documentation that the tweet object also have the status field as well as title, description and unwound_url. The second url ..."
          ]
        },
        {
          "title": "Entities object | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/x-api/v1/data-dictionary/object-model/entities",
          "excerpts": [
            "The entities section provides arrays of common things included in Tweets: hashtags, user mentions, links, stock tickers (symbols), Twitter polls, and attached ..."
          ]
        },
        {
          "title": "Retweets lookup: Standard v1.1 compared to Twitter API v2",
          "url": "https://developer.x.com/en/docs/twitter-api/tweets/retweets/migrate/retweets-lookup-standard-to-twitter-api-v2.html",
          "excerpts": [
            "This guide is to help you understand the similarities and differences between the standard v1.1 and X API v2 Retweets lookup endpoints."
          ]
        },
        {
          "title": "How to find near duplicate text documents?",
          "url": "https://www.reddit.com/r/LanguageTechnology/comments/i4bli4/how_to_find_near_duplicate_text_documents/",
          "excerpts": [
            "Generate a \"fingerprint\" of 64bits for each document in your dataset. Consider duplicate fingerprints that have a small (2~3) hamming distance."
          ]
        },
        {
          "title": "When to use Unicode Normalization Forms NFC and NFD?",
          "url": "https://stackoverflow.com/questions/15985888/when-to-use-unicode-normalization-forms-nfc-and-nfd",
          "excerpts": [
            "NFC is the best form for general text, since it is more compatible with strings converted from legacy encodings. NFD and NFKD are most useful for internal ..."
          ]
        },
        {
          "title": "Identifying and Filtering Near-Duplicate Documents",
          "url": "https://cs.brown.edu/courses/cs253/papers/nearduplicate.pdf",
          "excerpts": [
            "by AZ Broder · Cited by 682 — In other words, it suffices to determine whether the resemblance is above a certain threshold. In this talk we show how this determination can be made using a ” ..."
          ]
        },
        {
          "title": "In which cases normalize('NFKC') method work?",
          "url": "https://stackoverflow.com/questions/69058397/in-which-cases-normalizenfkc-method-work",
          "excerpts": [
            "NFKC makes compatible and canonically equivalent normalizations in different ways. Canonically equivalent normalization by NFKC is produced the same way as NFC."
          ]
        },
        {
          "title": "Tweet canonical URL - Publisher Tools / Cards - X Developers",
          "url": "https://devcommunity.x.com/t/tweet-canonical-url/141760",
          "excerpts": [
            " friendly URL. This canonical URL contains all twitter card tags and open graph tags. I’m seeking confirmation that Twitter will not use a canonical URL to pull Twitter card details when users Tweet the link. E.g. `<link rel=\"canonical\" href=\"https://www.myexamplesite.com/twittercard.html\" />`",
            "Tweet canonical URL - Publisher Tools / Cards - X Developers"
          ]
        },
        {
          "title": "Noisy Text: Proceedings of the ACL 2015 Workshop on Noisy User-generated Text",
          "url": "https://noisy-text.github.io/2015/pdf/WNUT12.pdf",
          "excerpts": [
            "Due to Twitter's fre- quently informal tone, text normalization can be a crucial element for exploiting that infor- mati"
          ]
        },
        {
          "title": "How to expand t.co URLs in tweets with python",
          "url": "https://stackoverflow.com/questions/45268715/how-to-expand-t-co-urls-in-tweets-with-python",
          "excerpts": [
            "Get the expanded_url from the Tweet object instead of the url - that will automatically give you the full URL."
          ]
        },
        {
          "title": "POST statuses/retweet/:id | Docs | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/docs/x-api/v1/tweets/post-and-engage/api-reference/post-statuses-retweet-id",
          "excerpts": [
            "Retweets a tweet. Returns the original Tweet with Retweet details embedded. Usage Notes: This method is subject to update limits."
          ]
        },
        {
          "title": "How do I use the \"referenced_tweets.type\" response field ...",
          "url": "https://stackoverflow.com/questions/67671790/how-do-i-use-the-referenced-tweets-type-response-field-in-the-twitter-api-in-p",
          "excerpts": [
            "I now want to add referenced_tweets.type in order to get if the Tweet is a Retweet or not but I'm not sure how to do it. Can someone help?"
          ]
        },
        {
          "title": "DashReza7/sentence-transformers_paraphrase- ...",
          "url": "https://huggingface.co/DashReza7/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2_FINETUNED_on_torob_data_v5",
          "excerpts": [
            "This is a sentence-transformers model finetuned from sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2. ... Cosine Accuracy Threshold on Unknown.See more"
          ]
        },
        {
          "title": "Hamming distance (Simhash python) giving out unexpected value",
          "url": "https://stackoverflow.com/questions/38570476/hamming-distance-simhash-python-giving-out-unexpected-value",
          "excerpts": [
            "The Simhash(\"String\").distance(Simhash(\"Another string\")) is the hamming distance between the two strings. Now, I am not sure I understand this get_features( ..."
          ]
        },
        {
          "title": "Simhashing (hopefully) made simple - ferd.ca",
          "url": "https://ferd.ca/simhashing-hopefully-made-simple.html",
          "excerpts": [
            "Our new pattern has a hamming distance of 1 compared to signatures 1 and 3, and a distance of 3 compared to signatures 2 and 4. As such, we can ..."
          ]
        },
        {
          "title": "MinHash LSH — datasketch 1.6.5 documentation",
          "url": "http://ekzhu.com/datasketch/lsh.html",
          "excerpts": [
            "The Jaccard similarity threshold must be set at initialization, and cannot be changed. So does the number of permutation functions ( num_perm ) parameter."
          ]
        },
        {
          "title": "Twitter API Changes: Navigating the End of Free Access",
          "url": "https://medium.com/@asaan/twitter-api-changes-navigating-the-end-of-free-access-your-2024-guide-b9f9cf47ea79",
          "excerpts": [
            "Basic Tier: $100/month for access to 10,000 tweets per month. Pro Tier: $5,000/month for access to 2 million tweets per month. Enterprise Tier: ..."
          ]
        },
        {
          "title": "Twitter is opening up its full archive to the broader ...",
          "url": "https://techcrunch.com/2018/02/01/twitter-is-opening-up-its-full-archive-to-the-broader-developer-community/",
          "excerpts": [
            "Existing customers of the Premium (30-day) API can immediately try out the new Full-archive Search free sandbox (it's under “Subscriptions” in ..."
          ]
        },
        {
          "title": "Full archive search (non academic) - X API - X Developers",
          "url": "https://devcommunity.x.com/t/full-archive-search-non-academic/173845",
          "excerpts": [
            "If that link is correct, does the content there mean that a premium package option costing $774 per month would allow for 1000 requests per ..."
          ]
        },
        {
          "title": "Label Studio Enterprise Pricing",
          "url": "https://humansignal.com/pricing/",
          "excerpts": [
            "Something for every human. · Open Source · $149/month · Custom Pricing · Talk to Sales · Trusted by 350,000+ users across all industries · Why Label Studio ..."
          ]
        },
        {
          "title": "Label Studio Community and Enterprise Features",
          "url": "https://labelstud.io/guide/label_studio_compare",
          "excerpts": [
            "Label Studio is available to everyone as open source software (Label Studio Community Edition). There are also two paid editions: Starter Cloud and Enterprise."
          ]
        },
        {
          "title": "Label Studio: Open Source Data Labeling",
          "url": "https://labelstud.io/",
          "excerpts": [
            "A flexible data labeling tool for all data types. Prepare training data for computer vision, natural language processing, speech, voice, and video models."
          ]
        },
        {
          "title": "Get Prodigy · Prodigy · An annotation tool for AI, Machine Learning ...",
          "url": "https://prodi.gy/buy",
          "excerpts": [
            "Pricing · Lifetime license, pay once, use forever · Flexible options for individuals and teams · Full privacy, no data leaves your servers · Download and install ..."
          ]
        },
        {
          "title": "Prodigy · An annotation tool for AI, Machine Learning & NLP",
          "url": "https://prodi.gy/",
          "excerpts": [
            "Pricing · Lifetime license, pay once, use forever · Flexible options for individuals and teams · Full privacy, no data leaves your servers · Download and install ..."
          ]
        },
        {
          "title": "Pricing - Anthropic",
          "url": "https://www.anthropic.com/pricing",
          "excerpts": [
            "Claude Haiku 3.5. Fastest, most cost-effective model. Input. $0.80 / MTok. Output. $4 / MTok. Prompt caching. Write $1 / MTok. Read $0.08 / MTok. Prompt caching ..."
          ]
        },
        {
          "title": "Pricing",
          "url": "https://docs.anthropic.com/en/docs/about-claude/pricing",
          "excerpts": [
            "​. Model pricing ; Claude Opus 3 (deprecated), $15 / MTok, $18.75 / MTok, $30 / MTok, $1.50 / MTok ; Claude Haiku 3, $0.25 / MTok, $0.30 / MTok, $0.50 / MTok ..."
          ]
        },
        {
          "title": "Azure OpenAI Service - Pricing",
          "url": "https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/",
          "excerpts": [
            "GPT-4.1 series ; GPT-4.1-2025-04-14 Global, Input: $2. Cached Input: $0.50. Output: $8, Input: $1. Output: $4 ; GPT-4.1-2025-04-14 Data Zone, Input: $2.20. Cached ..."
          ]
        },
        {
          "title": "Introducing Claude 3.5 Sonnet - Anthropic",
          "url": "https://www.anthropic.com/news/claude-3-5-sonnet",
          "excerpts": [
            "The model costs $3 per million input tokens and $15 per million output tokens, with a 200K token context window. Claude model family. Frontier ..."
          ]
        },
        {
          "title": "GPT-5 vs GPT-4o API Pricing: 2025 Cost Guide",
          "url": "https://www.creolestudios.com/gpt-5-vs-gpt-4o-api-pricing-comparison/",
          "excerpts": [
            "TL;DR. GPT-5 API is up to 55–90% cheaper than GPT-4o across common use cases, thanks to lower per-token pricing and a 90% caching discount."
          ]
        },
        {
          "title": "Updated Anthropic model comparison table",
          "url": "https://simonwillison.net/2025/May/22/updated-anthropic-models/",
          "excerpts": [
            "May 22, 2025 — Opus 4 matches the pricing of the older Opus 3 - $15/million for input and $75/million for output. I've updated llm-prices.com with the new ..."
          ]
        },
        {
          "title": "Claude Pricing: In-Depth Guide [2025]",
          "url": "https://team-gpt.com/blog/claude-pricing/",
          "excerpts": [
            "Dec 11, 2024 — Claude offers a free forever plan for individuals who want to test the AI waters without any strings attached. It's a good option for beginners."
          ]
        },
        {
          "title": "Labelbox Pricing",
          "url": "https://labelbox.com/pricing/",
          "excerpts": [
            "Labelbox is free to use for those who attend and teach at qualified educational institutions to use for non-commercial research.",
            "Flexible services and subscriptions Choose the Labelbox offering that’s right for you and your team. We offer advanced services and flexible software subscriptions to accelerate your AI initiatives with model evaluations, data generation services, and industry-leading tooling.",
            "Labelbox services Built for AI labs and model builders, enjoy fast, high-quality model evaluations and data generation and labeling services with the world’s best AI trainers.",
            "Labelbox Platform features",
            "free tier",
            "Subscription Tier",
            "Platform features",
            "Core catalog, annotate, & model features",
            "Users",
            "Up to 30",
            "projects",
            "Up to 50",
            "Ontologies",
            "Up to 25",
            "Workspaces",
            "1",
            "Labelbox Monitor",
            "SSO",
            "Custom embeddings",
            "Platform add-ons",
            "Additional LBUs",
            "Additional workspaces",
            "Security & HIPAA add-ons",
            "Available for purchase",
            "Available for purchase",
            "Available for purchase",
            "AI engine capabilities",
            "Data curation with natural language search",
            "Model-assisted labeling",
            "✓",
            "✓",
            "Foundry models for model-assisted labeling",
            "Live, multimodal chat editor for model evaluations",
            "Auto-labeling tools",
            "Frontier and custom models",
            "AI critic",
            "Services & support",
            "labeling services",
            "Self-serve add-on",
            "Alignerrs, Alignerr Connect, and other services available as add-ons",
            "Labeling quality guarantee",
            "Proactive platform alerts",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "—",
            "support",
            "Community support",
            "Access to premium support"
          ]
        },
        {
          "title": "Labelbox Pricing (Vendr Marketplace)",
          "url": "https://www.vendr.com/marketplace/labelbox",
          "excerpts": [
            "Labelbox",
            "$7,200–$51,200 per year",
            "View Pricing",
            "Median contract value\n$12,000\nper year Based on data from 3 purchases.",
            "How much does Labelbox cost in 2025? Access Labelbox pricing insights and get price estimates. Compare alternatives, explore Vendr's community insights, ..."
          ]
        },
        {
          "title": "OpenAI Pricing Documentation",
          "url": "https://platform.openai.com/docs/pricing",
          "excerpts": [
            "Prices per 1M tokens.",
            "| Model | Input | Cached input | Output |",
            "For gpt-4o and gpt-4.1 models, these tokens are included in the $25/1K calls cost."
          ]
        },
        {
          "title": "ND-H100-v5 size series - Azure Virtual Machines",
          "url": "https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/ndh100v5-series",
          "excerpts": [
            "Nov 7, 2024 — The ND H100 v5 series virtual machine (VM) is a new flagship addition to the Azure GPU family. This series is designed for high-end Deep Learning training."
          ]
        },
        {
          "title": "ND96isr H100 v5 pricing and specs - Vantage",
          "url": "https://instances.vantage.sh/azure/vm/nd96isrh100-v5",
          "excerpts": [
            "The ND96isr H100 v5 is in the Ndsrv5 series with 96 vCPUs and 1900 GiB of memory starting at $98.32 per hour on-demand or $28.994568 per hour with spot ..."
          ]
        },
        {
          "title": "NVIDIA H100 Pricing (August 2025): Cheapest On- ...",
          "url": "https://www.thundercompute.com/blog/nvidia-h100-pricing",
          "excerpts": [
            "May 19, 2025 — The table below compares current on-demand, hourly rental prices for a single NVIDIA H100 80GB GPU across major U.S. cloud providers."
          ]
        },
        {
          "title": "AWS P5 vs Thunder Compute: Pricing, Specs, and the Cheapest ...",
          "url": "https://www.thundercompute.com/blog/aws-p5-vs-thunder-compute",
          "excerpts": [
            "As of August 2025, AWS P5 H100 is roughly 55 to 66 dollars per hour per node, while Thunder Compute's A100 80 GB is $0.78 per hour with ..."
          ]
        },
        {
          "title": "Pricing | Compute Engine: Virtual Machines (VMs)",
          "url": "https://cloud.google.com/compute/all-pricing",
          "excerpts": [
            "Spot prices also provide smaller discounts for local SSDs and A3 machine types. ... $1.514787123 / 1 hour, $1.182701523 / 1 hour, $0.969217923 / 1 hour. c4 ..."
          ]
        },
        {
          "title": "Amazon EC2 Capacity Blocks for ML Pricing - AWS",
          "url": "https://aws.amazon.com/ec2/capacityblocks/pricing/",
          "excerpts": [
            "EC2 Capacity Blocks pricing consists of a reservation fee and an operating system fee. Prices and examples of these fees are provided below."
          ]
        },
        {
          "title": "What is the highest hourly rate you make on Data Annotation and ...",
          "url": "https://www.reddit.com/r/DataAnnotationTech/comments/18x0dm1/what_is_the_highest_hourly_rate_you_make_on_data/",
          "excerpts": [
            "Missing: MTurk Prolific Upwork"
          ]
        },
        {
          "title": "Amazon Mechanical Turk Pricing",
          "url": "https://requestersandbox.mturk.com/pricing",
          "excerpts": [
            "The minimum fee is $0.01 per assignment or bonus payment. Additional Fee for using the. Masters Qualification. (What are Masters?) 5% of the reward you ..."
          ]
        },
        {
          "title": "Estimated vs average reward per hour - Researcher Help Center",
          "url": "https://researcher-help.prolific.com/en/article/56b197",
          "excerpts": [
            "The reward per hour shown on the study creation page before you collect any data is based on this estimate. We have a minimum reward rate of £6 / $8 per hour."
          ]
        },
        {
          "title": "I Spent 30 Days Doing Amazon Mechanical Turk",
          "url": "https://dev.to/bzturk_extenstion/i-spent-30-days-doing-amazon-mechanical-turk-how-much-did-i-make-2024-edition-408p",
          "excerpts": [
            "The average MTurk worker earns around $2 to $6 per hour depending on the tasks, according to recent industry studies. However, those who ..."
          ]
        },
        {
          "title": "Heavy-tailed distribution",
          "url": "https://en.wikipedia.org/wiki/Heavy-tailed_distribution",
          "excerpts": [
            "In probability theory, heavy-tailed distributions are probability distributions whose tails are not exponentially bounded."
          ]
        },
        {
          "title": "Maximizing the Tweet Engagement Rate in Academia",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7963637/",
          "excerpts": [
            "by V Wadhwa · 2017 · Cited by 75 — The mean engagement rate of all tweets was 4.75%, and the median engagement rate was 3.4%. Table 1: Frequency distribution of tweet characteristics ...See more"
          ]
        },
        {
          "title": "Distribution of retweet counts in the dataset",
          "url": "https://www.researchgate.net/figure/Distribution-of-retweet-counts-in-the-dataset_fig4_268980781",
          "excerpts": [
            "The threshold for the number of retweets was identified through distributional analysis of retweet counts per tweet (see Figure 4) and empirical tests with ..."
          ]
        },
        {
          "title": "Engagement, User Satisfaction, and the Amplification of ...",
          "url": "https://knightcolumbia.org/content/engagement-user-satisfaction-and-the-amplification-of-divisive-content-on-social-media",
          "excerpts": [
            "by S Milli — Our study reveals that the engagement- based algorithm tends to amplify emotionally charged content, particularly that which expresses anger and ...See more"
          ]
        },
        {
          "title": "How Many Retweets per Day Can I Tweet? - - The Marketing Heaven",
          "url": "https://themarketingheaven.com/how-many-retweets-per-day-can-i-tweet/",
          "excerpts": [
            "You can tweet and retweet up to 2,400 times a day, and if your account is verified, that number jumps to a whopping 10,000. Understanding how ..."
          ]
        },
        {
          "title": "Throughput Formula: How to Calculate Throughput Rate",
          "url": "https://www.worximity.com/blog/apply-these-3-proven-techniques-to-improve-throughput-rate",
          "excerpts": [
            "Consider a factory that produces 10,000 widgets over 8 hours. Using the throughput formula: Throughput=10,000 widgets/8 hours=1,250 widgets per hour. This ..."
          ]
        },
        {
          "title": "How Throughput is calculate and display in Sec,Minute ...",
          "url": "https://stackoverflow.com/questions/41933739/how-throughput-is-calculate-and-display-in-sec-minute-and-hours-in-jmeter",
          "excerpts": [
            "The formula is: Throughput = (number of requests) / (total time). unit time varies based on the throughput values. examples: In 10 seconds, 10 ..."
          ]
        },
        {
          "title": "Bye Bye Velocity. Hello Throughput.",
          "url": "https://www.scrum.org/resources/blog/bye-bye-velocity-hello-throughput",
          "excerpts": [
            "Jan 14, 2019 — As for velocity, it is measured in terms of Story Points per sprint or iteration. So throughput is a number of items finished per unit of time ..."
          ]
        },
        {
          "title": "On the Frequency Distribution of Retweets",
          "url": "https://www.sciencedirect.com/science/article/pii/S1877050914005006",
          "excerpts": [
            "by Y Lu · 2014 · Cited by 49 — In this paper, we present a hypothesis that the frequency distribution of retweets follows a power law distribution asymptotically by analyzing the retweets ..."
          ]
        },
        {
          "title": "An overview of heavy-tail extensions of multivariate ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9542722/",
          "excerpts": [
            "by S Park · 2022 · Cited by 5 — The family of heavy-tailed distributions covers distributions having tail-decay slower than that of the Gaussian; for example, multivariate t distribution."
          ]
        },
        {
          "title": "Power Laws & Heavy Tail Distributions",
          "url": "https://www.di.fc.ul.pt/~jpn/r/powerlaw/powerlaw.html",
          "excerpts": [
            "In many applications it is the right tail of the distribution that is of interest, but a distribution may have a heavy left tail, or both tails may be heavy."
          ]
        },
        {
          "title": "A comparative analysis of user and text features in Twitter",
          "url": "https://link.springer.com/article/10.1007/s13278-022-00872-1",
          "excerpts": [
            "by C Toraman · 2022 · Cited by 35 — In this study, we aim to understand the driving factors behind four engagement types in Twitter, namely like, reply, retweet, and quote.See more"
          ]
        },
        {
          "title": "Impact of Tweet Content on the Number of Retweets",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10329898/",
          "excerpts": [
            "by T Suzuki · 2023 — The study revealed that official meeting-designated Twitter ambassadors disseminate more educational content than non-ambassadors, and generated more retweets."
          ]
        },
        {
          "title": "Cloud GPU pricing",
          "url": "https://cloud.google.com/compute/gpus-pricing",
          "excerpts": [
            "NVIDIA T4 · 16 GB GDDR6. $0.35 / 1 hour ",
            " NVIDIA P4 · 8 GB GDDR5. $0.60 / 1 hour",
            " NVIDIA V100 · 16 GB HBM2. $2.48 / 1 hour",
            " NVIDIA P100 · 16 GB HBM2. $1.46 / 1 hour"
          ]
        },
        {
          "title": "Paperspace GPU Cloud Comparison",
          "url": "https://www.paperspace.com/gpu-cloud-comparison",
          "excerpts": [
            "1x, 2x, 4x, 8x, 16",
            "85",
            "$3.67",
            "$2,682",
            "48",
            "336",
            "$1.79",
            "$900",
            "Linode",
            "Linode",
            "RTX A6000",
            "$1.45",
            "[A100 40 GB](#)",
            "[A100 40 GB](#)",
            "40",
            "40",
            "432",
            "432",
            "$2.39",
            "$1,200",
            "[Lambda Labs](https://lambdalabs.com/service/gpu-cloud/pricing)",
            "[Lambda Labs](https://lambdalabs.com/service/gpu-cloud/pricing)",
            "V100 16 GB",
            "V100 16 GB",
            "V100 16 GB",
            "12",
            "12",
            "56",
            "$6.80",
            "--",
            "Google Cloud",
            "P100",
            "1x, 2x, 4x",
            "0",
            "$1.86",
            "$950",
            "[Google Cloud](https://cloud.google.com/compute/gpus-pricing)",
            "[Google Cloud](https://cloud.google.com/compute/gpus-pricing)",
            "[Google Cloud](https://cloud.google.com/compute/gpus-pricing)",
            "[V100 16 GB](#)",
            "1x, 2x, 4x, 8x",
            "1x, 2x, 4x, 8x",
            "16",
            "16",
            "16",
            "16",
            "640",
            "640",
            "640",
            "8",
            "8",
            "8",
            "8",
            "30",
            "30",
            "30",
            "$2.88",
            "$2.88",
            "$1,471",
            "$1,471",
            "[Jarvis Labs](https://jarvislabs.ai/pricing/)",
            "[Jarvis Labs](https://jarvislabs.ai/pricing/)",
            "[RTX A6000](#)",
            "[RTX A6000](#)",
            "1x",
            "1x",
            "1x",
            "1x",
            "24",
            "24",
            "256",
            "256",
            "7",
            "7",
            "7",
            "7",
            "32",
            "32",
            "32",
            "32",
            "$1.29",
            "$650",
            "$650"
          ]
        },
        {
          "title": "Upwork Data Annotation Specialists",
          "url": "https://www.upwork.com/hire/data-annotation-specialists/",
          "excerpts": [
            "Last updated: Jul 6, 2025",
            "Need an EXPERT DATA ANNOTATOR for the long haul? I am well-versed in Data Annotation methodologies, ensuring precision and quality in labeling datasets for machine learning algorithm training.",
            "\n  $10/hr $10 hourly",
            " [Mary Ann L.](/freelancers/~015bf0a38b71580034)",
            " [Anthony A. ](/freelancers/~01737d6a5c1289e881)",
            "\n  $12/hr $12 hourly",
            "\n  $12/hr $12 hourly",
            " [Lenny Rose L.](/freelancers/~01d78020016f2ede2c)",
            "\n  $15/hr $15 hourly",
            " [Caitlyn V.](/freelancers/~01d71a47051fdaba24)"
          ]
        },
        {
          "title": "Inter-Annotator Agreement: Building Datasets",
          "url": "https://keymakr.com/blog/measuring-inter-annotator-agreement-building-trustworthy-datasets/",
          "excerpts": [
            "Inter-annotators determine how similar the results obtained by different people are when annotating the same data. This is done using statistical methods."
          ]
        },
        {
          "title": "Ensuring Quality in Data Annotation - Keymakr",
          "url": "https://keymakr.com/blog/ensuring-quality-in-data-annotation/",
          "excerpts": [
            "Quality assurance techniques for data annotation include subsampling, which involves observing a subset of annotated data to check for errors, setting a gold standard as a benchmark for annotator performance, utilizing annotator consensus to ensure accurate annotations, and applying scientific methods like Cronbach ..."
          ]
        },
        {
          "title": "Text Annotation for NLP: A Comprehensive Guide [2025 ...",
          "url": "https://www.habiledata.com/blog/text-annotation-for-nlp/",
          "excerpts": [
            "May 17, 2025 — Essential for training NLP models, text annotation involves tasks like named entity recognition, sentiment analysis, and part-of-speech tagging."
          ]
        },
        {
          "title": "How Many Likes Is Viral? [Real Numbers Explained]",
          "url": "https://www.shortsgenerator.ai/blog/how-many-likes-is-viral/",
          "excerpts": [
            "A tweet with 50,000 likes and 2,000 comments often has deeper impact than one with higher vanity metrics. Social proof matters—users trust ...",
            " act as digital applause. They signal approval and nudge algorithms to push **posts** further. But they’re just one piece of the puzzle. A tweet with 50,000 _likes_ and 2,000 **comments** often has deeper impact than one with higher vanity metrics. Social proof matters—users trust content others valid",
            ": virality isn’t a formula. It’s a mix of relevance, relatability, and a dash of luck."
          ]
        },
        {
          "title": "Twitter Limits: A Guide On X's Threshold - Tweet Binder",
          "url": "https://www.tweetbinder.com/blog/twitter-limit/",
          "excerpts": [
            "How many tweets can be read per day? The daily rate limit for unverified accounts is 1,000 tweets. Verified X users can view up to 10,000 tweets per day."
          ]
        },
        {
          "title": "What's the rate limit for searchPosts API? #2820",
          "url": "https://github.com/bluesky-social/atproto/discussions/2820",
          "excerpts": [
            "Sep 18, 2024 — searchPosts is only limited by Global limit? Global limit (aggregated across all routes) Rate limited by IP 3000/5 minSee more"
          ]
        },
        {
          "title": "Rate Limits",
          "url": "https://docs.bsky.app/docs/advanced-guides/rate-limits",
          "excerpts": [
            "Rate limits help service providers keep the network secure. For example, by limiting the number of requests a user or bot can make in a given time period.See more"
          ]
        },
        {
          "title": "Annotation and API considerations for research on viral content (PMC 2018)",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6371268/",
          "excerpts": [
            " Annotation speed varied across different users (40.47 – 92.22 words per minute) leading to wide ranges in the quantity of sentences annotated (435 -920) and ...",
            "| User ID | Number of sentences | Length of sentence | Number of entities | Time (seconds / sentence) | Speed (words / minutes) | Annotation quality (%) |",
            "user 7 | 435 | 11.11 ± 8.94[\\*]() | 1.30 ± 1.55 | 16.48 ± 18.25 | 40.47 | 74.16 |",
            "user 2 | 553 | 11.48 ± 10.46 | 1.39 ± 1.85 | 12.96 ± 13.28 | 53.18 | 78.39 |",
            "user 9 | 617 | 11.45 ± 9.90 | 1.37 ± 1.67 | 11.69 ± 11.11 | 58.77 | 85.61 |",
            "user 3 | 766 | 10.73 ± 8.88 | 1.42 ± 1.64 | 9.39 ± 9.17 | 68.58 | 76.75 |",
            "user 6 | 745 | 11.51 ± 10.73 | 1.32 ± 1.82 | 9.67 ± 10.99 | 71.44 | 82.72 |",
            "user 8 | 875 | 10.95 ± 9.11 | 1.36 ± 1.58 | 8.23 ± 8.18 | 79.89 | 79.43 |",
            "user 4 | 842 | 11.64 ± 9.63 | 1.33 ± 1.59 | 8.52 ± 8.59 | 81.97 | 85.24 |",
            "user 1 | 920 | 11.37 ± 9.58 | 1.40 ± 1.57 | 7.82 ± 6.90 | 87.16 | 79.16 |",
            "user 5 | 910 | 12.15 ± 10.35 | 1.40 ± 1.68 | 7.90 ± 8.32 | 92.22 | 83.66 |",
            "All | 6663 | 11.40 ± 9.76 | 1.37 ± 1.66 | - | - | 80.89 |",
            "[Open in a new tab](table/t2-2974888/)",
            "\n\\* The value represents mean ± standard deviation. In order to discover the relation between annotation time and proposed factors, we built a linear regression model for each user. [Table 3]() shows the coefficients of the linear model for each user, where the bold and star represented the significant coefficients (p value < 0.05). The basic and lexicon factors were significant for almost all users."
          ]
        },
        {
          "title": "X API - Search Recent Posts and Related Limits",
          "url": "https://docs.x.com/x-api/posts/recent-search",
          "excerpts": [
            "Retrieves Posts from the last 7 days matching a search query.",
            "Search recent Posts - X",
            "The maximum number of search results to be returned by a request. Required range: `10 <= x <= 100`",
            "Retrieves Posts from the last 7 days matching a search quer",
            "The oldest UTC timestamp from which the Posts will be provided. Timestamp is in second granularity and is inclusive (i.e. 12:00:01 includes the first second of the minute)."
          ]
        },
        {
          "title": "The Distribution and Structure of Retweet Cascades",
          "url": "https://blogs.cornell.edu/info2040/2019/11/20/the-distribution-and-structure-of-retweet-cascades/",
          "excerpts": [
            "We found that the distribution of popularities often follows a power law distribution. In fact, Figure 4 from Paper (1) shows that both cascade ..."
          ]
        },
        {
          "title": "What it's really like to go viral on Twitter",
          "url": "https://www.metromag.co.nz/society/society-society/what-its-really-like-to-go-viral-on-twitter-social-media",
          "excerpts": [
            "Apr 12, 2019 — Gloria had about 150 followers when a joke tweet she thought of “while taking a dump” went viral last month, receiving more than 50,000 likes ..."
          ]
        },
        {
          "title": "TrackMyHashtag: Hashtag Tracking tool for X (Twitter)",
          "url": "https://www.trackmyhashtag.com/",
          "excerpts": [
            "Our hashtag reach tracker measures the reach, impressions, and engagement level of your X (Twitter) Hashtag campaign to understand the impact of your hashtag."
          ]
        },
        {
          "title": "17 Hilarious Tweets From July That Made My Month 10,000 ... - Yahoo",
          "url": "https://www.yahoo.com/entertainment/articles/17-funniest-july-tweets-internet-201919206.html",
          "excerpts": [
            "As we are reaching the end of July, let's look back at the funniest tweets people have posted this month so far."
          ]
        },
        {
          "title": "The Ultimate Guide to AWS S3 Pricing in 2025 - nOps",
          "url": "https://www.nops.io/blog/aws-s3-pricing/",
          "excerpts": [
            "Costs are higher for S3 Standard than for other S3 storage classes. Pricing is tiered; you pay $0.023 per GB per month for the first 50 TB per month . The next tier is $0.022 per GB, then storage above 500 TB per month is priced at $0.021."
          ]
        },
        {
          "title": "S3 pricing for LIST requests - AWS re:Post",
          "url": "https://repost.aws/questions/QUS6QtxAWiSwaJxAybghbS6A/s3-pricing-for-list-requests",
          "excerpts": [
            "In the US East (N. Virginia) Region, for example, the price for LIST requests is $0.005 per 1,000 requests. This rate applies whether you're ..."
          ]
        },
        {
          "title": "AWS S3 Cost Calculator 2025: Hidden Fees Most Companies Miss",
          "url": "https://costq.ai/blog/aws-s3-cost-calculator-2025/",
          "excerpts": [
            "The AWS S3 Cost Calculator shows that S3 Standard costs $0.023/GB, Standard-IA costs $0.0125/GB, Glacier Instant Retrieval costs $0.004/GB, and ..."
          ]
        },
        {
          "title": "Announcing up to 85% price reductions for Amazon S3 ...",
          "url": "https://aws.amazon.com/blogs/aws/up-to-85-price-reductions-for-amazon-s3-express-one-zone/",
          "excerpts": [
            "Apr 10, 2025 — Effective April 10, 2025, S3 Express One Zone has reduced storage prices by 31 percent, PUT request prices by 55 percent, and GET request prices by 85 percent."
          ]
        },
        {
          "title": "Understanding AWS Costs & 8 Ways to Cut Your Costs in ...",
          "url": "https://n2ws.com/blog/aws-cost-optimization/understanding-aws-costs",
          "excerpts": [
            "Mar 31, 2025 — Amazon S3 (Standard storage): $0.023 per GB for the first 50 TB per month. Amazon S3 (Infrequent access): $0.0125 per GB for all storage. EBS ..."
          ]
        },
        {
          "title": "Pricing",
          "url": "https://www.superannotate.com/pricing",
          "excerpts": [
            "SuperAnnotate offers Starter (free trial), Pro (request demo), and Enterprise (contact sales) pricing options."
          ]
        },
        {
          "title": "SuperAnnotate Pricing: Cost and Pricing plans",
          "url": "https://www.saasworthy.com/product/superannotate/pricing",
          "excerpts": [
            "Check out the detailed pricing information for SuperAnnotate. Explore pricing tiers and compare pricing against other New SaaS Software."
          ]
        },
        {
          "title": "Cloud and External Storage Integration",
          "url": "https://labelstud.io/guide/storage.html",
          "excerpts": [
            "Integrate popular cloud and external storage systems with Label Studio to collect new items uploaded to the buckets, containers, databases, or directories."
          ]
        },
        {
          "title": "Top Tweets (@toptweets) / X",
          "url": "https://x.com/toptweets?lang=en",
          "excerpts": [
            "Top Tweets's posts ; Elon Musk. @elonmusk · Image. 46K. 98K ; DC_Draino · @DC_Draino · Embedded video. 0:06. 5K. 33K ; Jan Leike · @janleike · Stepping away from this ..."
          ]
        },
        {
          "title": "70 Twitter (X) Statistics You Should Know",
          "url": "https://thunderbit.com/blog/x-stats",
          "excerpts": [
            "Jun 5, 2025 — As of 2024, X boasts about 415 million users worldwide, with forecasts nudging that number to 440 million by the end of 2025 (Oberlo Pricing)."
          ]
        },
        {
          "title": "24 Tweets That Were So Good They Got 10,000 Retweets In Like ...",
          "url": "https://www.buzzfeed.com/benhenry/tweets-that-got-10k-retweets-in-less-than-a-week",
          "excerpts": [
            "24 Tweets That Were So Good They Got 10,000 Retweets In Like, No Time At All ; 1. 𓃵 @lordflaconegro. Girl: mom can I be a princess for Halloween?"
          ]
        },
        {
          "title": "How many likes on twitter are botten?",
          "url": "https://www.reddit.com/r/Twitter/comments/1gwpba6/how_many_likes_on_twitter_are_botten/",
          "excerpts": [
            "I see posts that have 1-10 million views and they already have 100-500k likes. Is it possible twitter users even engage this much? It feels like there are more ..."
          ]
        },
        {
          "title": "SuperAnnotate Pro - AWS Marketplace",
          "url": "https://aws.amazon.com/marketplace/pp/prodview-rwm2qheoon66g",
          "excerpts": [
            "Pricing is based on the duration and terms of your contract with the vendor, and additional usage. You pay upfront or in installments according to your contract ..."
          ]
        },
        {
          "title": "Start Collecting: Twarc Command Basics",
          "url": "https://scholarslab.lib.virginia.edu/learn-twarc/06-twarc-command-basics",
          "excerpts": [
            "This process is called “rehydration”, and will return a JSON file containing the data for every tweet that it was able to locate. It's important to note that ...",
            "A dehydrated data set can be “rehydrated” using Twarc - Twitter will take each ID and search the current Twitterverse for a corresponding tweet. If it locates a ..."
          ]
        },
        {
          "title": "Docnow – Docnow",
          "url": "https://www.docnow.io/",
          "excerpts": [
            "Hydrator is a desktop application for turning Tweet ID datasets back into tweet data to use in your research. It has been designed to be a reliable option for ..."
          ]
        },
        {
          "title": "Docnow App",
          "url": "https://www.docnow.io/docnow-app/",
          "excerpts": [
            "DocNow is an open-source app for appraising, collecting, and gathering consent for social media archives. It collects content, downloads archives, and hydrates ..."
          ]
        },
        {
          "title": "Metrics",
          "url": "https://developer.x.com/en/docs/x-api/metrics",
          "excerpts": [
            "Metrics include the total count of impressions, Retweets, Quote Tweets, likes, replies, video views, video view quartiles, URL and profile link clicks for each ...See more"
          ]
        },
        {
          "title": "Tweet's public metrics - X API v2",
          "url": "https://devcommunity.x.com/t/tweets-public-metrics/180920",
          "excerpts": [
            "Nov 30, 2022 — I'm specifically interested in the public ones, but if the answer is similar for paid or organic metrics, it would be great. Thanks.See more"
          ]
        },
        {
          "title": "Twitter Data Collection: Tweet Rehydration - Communalytic",
          "url": "https://communalytic.org/tutorials/twitter-data-collection-tweet-rehydration/",
          "excerpts": [
            "Tweet Rehydration allows you to collect tweets from Twitter using their unique IDs. To collect data using this function, you need a list of tweet IDs in a CSV ..."
          ]
        },
        {
          "title": "DocNow/hydrator: Turn Tweet IDs into Twitter JSON & CSV ...",
          "url": "https://github.com/DocNow/hydrator",
          "excerpts": [
            "However they do allow datasets of tweet IDs to be shared. Hydrator helps you turn these tweet IDs back into JSON and also CSV from the comfort of your desktop.",
            "Twitter's changes to their API which greatly reduce the amount of read-only access means that the Hydrator is no longer a useful application. The application keys, which functioned for the last 7 years, have been rescinded by Twitter.",
            "Hydrator is an Electron based desktop application for hydrating Twitter ID datasets. Twitter's Terms of Service do not allow the full JSON for datasets of ..."
          ]
        },
        {
          "title": "Learn how to easily hydrate tweets",
          "url": "https://towardsdatascience.com/learn-how-to-easily-hydrate-tweets-a0f393ed340e/",
          "excerpts": [
            "Mar 8, 2022 — Two tools provided by Documenting the Now (DocNow) for this purpose: a desktop application Hydrator and a command line tool twarc."
          ]
        },
        {
          "title": "Guide to rehydrating tweets with the Twitter API V2",
          "url": "https://developer.talkwalker.com/guides/rehydrating-tweet",
          "excerpts": [
            "Guide to rehydrating tweets with the Twitter API V2 · Getting started with Twitter API · Retrieve tweet details · Retrieve author details."
          ]
        },
        {
          "title": "New ways of sharing Tweet data & Tweet Hydration!",
          "url": "https://medium.com/@rajkaransinh/new-ways-of-sharing-tweet-data-tweet-hydration-bf06545022d6",
          "excerpts": [
            "While converting these tweet-ids to tweet data, I ... Hydrator is an Electron based desktop application for hydrating Twitter ID datasets."
          ]
        },
        {
          "title": "How to use the Twitter data — #metoo project Schlesinger Library",
          "url": "https://www.schlesinger-metooproject-radcliffe.org/how-to-use-the-twitter-data",
          "excerpts": [
            "This automated action is called “rehydration”. Tweets can be “rehydrated” using a relatively simple computer program with a graphical user interface. We have ..."
          ]
        },
        {
          "title": "A longitudinal assessment of the persistence of twitter ...",
          "url": "https://www.bohrium.com/paper-details/a-longitudinal-assessment-of-the-persistence-of-twitter-datasets/813035398098845700-7147",
          "excerpts": [
            "May 14, 2018 — This leads to subsets of the data no longer being available, as content can be deleted or user accounts deactivated. To quantify the long‐term ..."
          ]
        },
        {
          "title": "A longitudinal assessment of the persistence of twitter datasets",
          "url": "https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.24026",
          "excerpts": [
            "by A Zubiaga · 2018 · Cited by 111 — Social media datasets are not always completely replicable. Having to adhere to requirements of platforms such as Twitter, researchers can ..."
          ]
        },
        {
          "title": "What's the difference between ISO 8601 and RFC 3339 ...",
          "url": "https://stackoverflow.com/questions/522251/whats-the-difference-between-iso-8601-and-rfc-3339-date-formats",
          "excerpts": [
            "RFC 3339 requires 4-digit years, and the RFC only allows a period character to be used as the decimal point for fractional seconds."
          ]
        },
        {
          "title": "A Longitudinal Assessment of the Persistence of Twitter ...",
          "url": "https://arxiv.org/abs/1709.09186",
          "excerpts": [
            "by A Zubiaga · 2017 · Cited by 111 — We perform a longitudinal analysis of the persistence of 30 Twitter datasets, which include over 147 million tweets."
          ]
        },
        {
          "title": "A Longitudinal Assessment of the Persistence of Twitter ...",
          "url": "https://www.researchgate.net/publication/320078025_A_Longitudinal_Assessment_of_the_Persistence_of_Twitter_Datasets",
          "excerpts": [
            "PDF | Sharing of social media datasets presents the caveat that they are not always completely replicable. Having to adhere to requirements of platforms."
          ]
        },
        {
          "title": "A Longitudinal Assessment of the Persistence of Twitter ...",
          "url": "http://www.zubiaga.org/publications/a-longitudinal-assessment-of-the-persistence-of-twitter-datasets/",
          "excerpts": [
            "Social media datasets are not always completely replicable. Having to adhere to requirements of platforms such as Twitter, researchers can only release a ..."
          ]
        },
        {
          "title": "PROV-Overview",
          "url": "https://www.w3.org/TR/prov-overview/",
          "excerpts": [
            "Apr 30, 2013 — Provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form ..."
          ]
        },
        {
          "title": "W3C PROV to describe provenance at the dataset, feature ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S0198971517300558",
          "excerpts": [
            "by G Closa · 2017 · Cited by 47 — This paper presents the application of W3C PROV, which is a generic specification to express provenance records, for representing geospatial data provenance at ..."
          ]
        },
        {
          "title": "A Persistent Twitter Dataset for Learning Social Meaning",
          "url": "https://workshop-proceedings.icwsm.org/pdf/2022_92.pdf",
          "excerpts": [
            "by CZMAM El · 2021 · Cited by 1 — To the best of our knowledge, this is the first work to employ SOTA paraphrase methods to tackle occasional data decay for social media research. (2) We develop ..."
          ]
        },
        {
          "title": "RFC 3339: Date and Time on the Internet: Timestamps",
          "url": "https://www.rfc-editor.org/rfc/rfc3339.html",
          "excerpts": [
            "This document defines a date and time format for use in Internet\n   protocols that is a profile of the ISO 8601 standard for\n   representation of dates and times using the Gregorian cal",
            "The date format defined in section 5 of this document. Timestamp   This term is used in this document to refer to an\n                  unambiguous representation of some instant in time.",
            " A suffix which, when applied to a time, denotes a UTC\n                  offset of 00:00; often spoken \"Zulu\" from the ICAO\n                  phonetic alphabet representation",
            " Timestamps can express times that occurred before the introduction\n      of UTC. Such timestamps are expressed relative to universal time,\n      using the best available practice at the",
            "All dates and times are assumed to be in the \"current era\",\n      somewhere between 0000AD and 9999AD."
          ]
        },
        {
          "title": "Tweet counts enterprise to Twitter API v2 migration guide | Docs",
          "url": "https://developer.x.com/en/docs/twitter-api/tweets/counts/migrate/enterprise-to-twitter-api-v2",
          "excerpts": [
            "The enterprise version of this endpoint uses the following date/time format in both the pagination parameters and the `timePeriod` response field: `YYYYMMDDHHmm`",
            "The v2 endpoint uses ISO 8601/RFC 3339 date/time format in both the pagination parameters and the `start` and `end` response fields: `YYYY-MM-DDTHH:mm:ssZ`",
            "The v2 endpoint uses ISO 8601/RFC 3339 date/time format in both the pagination parameters and the `start` and `end` response fields: `YYYY-MM-DDTHH:mm:ssZ`",
            "#### Request parameters",
            "The following is a table of the request parameters for enterprise and X API v2:",
            "| Enterprise | Search Posts v2 |",
            "| --- | --- |",
            "| query | query |",
            "| bucket | granularity |",
            "| fromDate (YYMMDDHHmm) | start\\_time (YYYY-MM-DDTHH:mm:ssZ) ",
            "| toDate (YYMMDDHHmm) | end\\_time (YYYY-MM-DDTHH:mm:ssZ) ",
            "|  | since\\_id ",
            "|  | until\\_id ",
            "| next | next\\_token and pagination\\_token"
          ]
        },
        {
          "title": "Validity and Inter-Rater Reliability Testing of Quality Assessment Instruments",
          "url": "https://www.ncbi.nlm.nih.gov/books/NBK92287/table/executivesummary.t2/",
          "excerpts": [
            "| --- | --- |"
          ]
        },
        {
          "title": "Twitter developer policy update | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/blog/industry-team-news/2020/x-developer-policy-update",
          "excerpts": [
            "Mar 10, 2020 — Researchers can now share an unlimited number of Tweet IDs and/or User IDs if they are doing so on behalf of an academic institution and for ..."
          ]
        },
        {
          "title": "Versioning, Provenance, and Reproducibility in Production Machine ...",
          "url": "https://ckaestne.medium.com/versioning-provenance-and-reproducibility-in-production-machine-learning-355c48665005",
          "excerpts": [
            "Versioning, provenance tracking, and reproducibility are essential tools for responsible engineers that provide a technical foundation for debugging."
          ]
        },
        {
          "title": "13 Most Important Social Media Metrics To Track in 2025",
          "url": "https://agencyanalytics.com/blog/social-media-metrics",
          "excerpts": [
            "QUICK SUMMARY: Social media metrics are used to measure performance, understand audience behavior, and optimize campaign strategy."
          ]
        },
        {
          "title": "[PDF] The Provenance of a Tweet - gwu-libraries.github.io",
          "url": "https://gwu-libraries.github.io/sfm-ui/resources/provenance-of-tweet.pdf",
          "excerpts": [
            "Even in those domains where reproducibility is not required or even desired, it needs to be made transparent how results were generated. . . ."
          ]
        },
        {
          "title": "A Longitudinal Assessment of the Persistence of Twitter ...",
          "url": "https://arxiv.org/pdf/1709.09186",
          "excerpts": [
            "by A Zubiaga · 2017 · Cited by 111 — This study assesses the persistence of 30 Twitter datasets, analyzing completeness, representativity, similarity, and changingness of ..."
          ]
        },
        {
          "title": "Data Authenticity, Consent, and Provenance for AI Are All Broken",
          "url": "https://mit-genai.pubpub.org/pub/uk7op8zs",
          "excerpts": [
            "We explain why existing tools for data authenticity, consent, and documentation alone are unable to solve the core problems facing the AI community."
          ]
        },
        {
          "title": "11 Best Practices For Tracking Social Media With Google Analytics",
          "url": "https://www.lovesdata.com/blog/tracking-social-media/",
          "excerpts": [
            "Comparing your organic (free) and paid social media efforts; Using the social reports in Google Analytics; Interpreting social media data in ..."
          ]
        },
        {
          "title": "(PDF) A longitudinal study of topic classification on Twitter",
          "url": "https://www.researchgate.net/publication/361149588_A_longitudinal_study_of_topic_classification_on_Twitter",
          "excerpts": [
            "In summary, this work provides a long-term study of topic classifiers on Twitter that further justifies classification-based topical filtering approaches while ..."
          ]
        },
        {
          "title": "Best practices for timestamps and time zones in databases",
          "url": "https://www.tinybird.co/blog-posts/database-timestamps-timezones",
          "excerpts": [
            "## 1\\. Thou shalt default to UTC",
            "Regardless of the time zone of the server where an event is created, you should first convert the timestamp into UTC when you store it. You may also keep A) the relevant time zone offset in seconds, B) the name of the time zone, and C) the local timestamp as a string. But always, always, **make your primary storage of a historical event timestamp in UTC. **",
            "UTC is an absolute point in time. It is based on a [highly accurate and stable standard](https://en.wikipedia.org/wiki/Coordinated_Universal_Time) recognized worldwide. It is not affected by changes in time zones or other adjustments at the whim of the latest local government. UTC is always constant - every other modern time is relative.",
            "## 2\\. Thou shalt only do that which is necessary",
            "It can be tempting to process timestamp data in a way that seems comprehensive or exhaustive, but this can lead to wasted effort and unnecessary complexity. This is why **you should only do what is needed to serve the use case in front of you. **"
          ]
        },
        {
          "title": "Notes on Downloading Conversations through Twitter's V2 ...",
          "url": "https://cborchers.com/2021/03/23/notes-on-downloading-conversations-through-twitters-v2-api/",
          "excerpts": [
            "Mar 23, 2021 — The variable created_at might be a good proxy for filtering tweets in the sense of a forward search through only including tweets posted after ..."
          ]
        },
        {
          "title": "Whats new with the Twitter API v2 in 2022",
          "url": "https://dev.to/suhemparack/whats-new-with-the-twitter-api-v2-in-2022-plb",
          "excerpts": [
            "Oct 11, 2022 — This tells us that the Tweet with ID 1265324480517361664 was deleted. This guide showcases how to work with the batch compliance solution in ..."
          ]
        },
        {
          "title": "Response Codes | Docs | Twitter Developer Platform - X",
          "url": "https://developer.x.com/ja/docs/basics/response-codes",
          "excerpts": [
            "The standard Twitter API returns HTTP status codes in addition to JSON-based error codes and messages."
          ]
        },
        {
          "title": "Data Collection with Twitter API v2",
          "url": "https://www.kaggle.com/code/andrewedward37/data-collection-with-twitter-api-v2",
          "excerpts": [
            "In this article, I will go through a step-by-step process from setting up, accessing endpoints, to saving tweets collected in CSV format to use for analysis in ..."
          ]
        },
        {
          "title": "[PDF] An archive and corpus of Twitter/X's policies for Tweet redistribution ...",
          "url": "https://access.gesis.org/sharing/2847/6568",
          "excerpts": [
            "When researchers publish research based on the analysis of Tweets, good practice requires sharing the Tweets. (or Tweet IDs) to enable ..."
          ]
        },
        {
          "title": "Recommended date format for REST GET API - Stack Overflow",
          "url": "https://stackoverflow.com/questions/9581692/recommended-date-format-for-rest-get-api",
          "excerpts": [
            "Missing: social reproducibility provenance"
          ]
        },
        {
          "title": "Table format comparisons - Append-only tables and incremental reads",
          "url": "https://jack-vanlightly.com/blog/2024/8/13/table-format-comparisons-append-only-tables-and-incremental-reads",
          "excerpts": [
            "Missing: metrics practices"
          ]
        },
        {
          "title": "GET statuses/home_timeline | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/x-api/v1/tweets/timelines/api-reference/get-statuses-home_timeline",
          "excerpts": [
            "X API v2 ... The value of count is best thought of as a limit to the number of tweets to return because suspended or deleted content is removed after the count ..."
          ]
        },
        {
          "title": "HTTP Error code: 403 (no access to Twitter API v2)",
          "url": "https://devcommunity.x.com/t/http-error-code-403-no-access-to-twitter-api-v2/155377",
          "excerpts": [
            "Jun 10, 2021 — When authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project."
          ]
        },
        {
          "title": "Best Practices for Time-Series Data Modeling: Single or Multiple ...",
          "url": "https://www.tigerdata.com/learn/best-practices-time-series-data-modeling-single-or-multiple-partitioned-tables-aka-hypertables",
          "excerpts": [
            "With time-series data being (normally) append-only, removing parts of the data (this specific user's data) may be tricky. Schema changes."
          ]
        },
        {
          "title": "Date and Time Formatting - Amazon-Services-API",
          "url": "https://developer-docs.amazon.com/sp-api/docs/iso-8601",
          "excerpts": [
            "Missing: social media reproducibility provenance"
          ]
        },
        {
          "title": "on Data Persistence for Manipulative Social Media Content",
          "url": "https://aclanthology.org/2024.clicit-1.115.pdf",
          "excerpts": [
            "by O Uryupina · 2024 — This work presents an in-depth investigation of the data decay for publicly fact-checked online content. We monitor compromised posts on major ..."
          ]
        },
        {
          "title": "The 21 essential social media metrics you must track for ...",
          "url": "https://blog.hootsuite.com/social-media-metrics/",
          "excerpts": [
            "We've broken down the 21 key social media metrics you need to keep an eye on into six different categories to help keep you organized."
          ]
        },
        {
          "title": "Nonrandom Tweet Mortality and Data Access Restrictions Compromising the Replication of Sensitive Twitter Studies",
          "url": "https://www.cambridge.org/core/journals/political-analysis/article/nonrandom-tweet-mortality-and-data-access-restrictions-compromising-the-replication-of-sensitive-twitter-studies/17E618F1D395CBBB2360AA424DA5533A",
          "excerpts": [
            "As proposed in Davidson _et al._ ( [Reference Davidson 2023]() ), automatic crawlers could update these archives without needing an API.",
            "However, one must carefully evaluate this step, as crawling social media platforms might be a legal gray zone.",
            "zone. The library intends to make its collected data available “within the German National Library’s infrastructure.",
            "Twitter’s policy still prohibits researchers from directly sharing the raw content of tweets.",
            ") ). The one-way aspect of this well-established computer science technique prevents rehydrating tweets’ raw content.",
            "hat these Twitter studies and their findings are considerably affected by nonrandom tweet mortality and data access restrictions imposed by ... As pr"
          ]
        },
        {
          "title": "Provenance Data in Social Media",
          "url": "https://www.odbms.org/wp-content/uploads/2014/03/Provenance-Data-in-Social-Media.pdf",
          "excerpts": [
            "Provenance data associated\n with a social media statement can help dispel rumors, clarify opinions, and conﬁrm facts.",
            "However, provenance data about social media statements is not readily available to users\ntoday.",
            "The search for provenance attribute values can continue using\nthe profile with the highest probability.",
            "The W3C incubator group provided a list of provenance dimensions that are applicable to\nprovenance data in social media.",
            "imeliness\n of social media data can be defined more simply as [8]:\n\t\t\t\t Qcurrency = (current_time − time_provenance_data_created)/retrie",
            "A provenance system\n that takes longer to gather provenance attribute values than the frequency at which the provenance\n attribute values are, or are likely to be updated, does not provide accurate or valuable provenance\n attribute valu"
          ]
        },
        {
          "title": "python - How to query Twitter API public metrics for multiple ...",
          "url": "https://stackoverflow.com/questions/72640093/how-to-query-twitter-api-public-metrics-for-multiple-tweet-ids-using-tweepy",
          "excerpts": [
            "Use Tweepy to to call the Twitter API to return the public_metrics ( likes , retweets , quotes , replies ) for each of multiple tweet_id s."
          ]
        },
        {
          "title": "Twitter API v2 in Python - issues with datetime conversion",
          "url": "https://stackoverflow.com/questions/66629933/twitter-api-v2-in-python-issues-with-datetime-conversion",
          "excerpts": [
            "I upgraded my Python-Twitter API access from v1 to v2. But as it seems, Twitter does not any longer accept my timestamp-formats for the updated ..."
          ]
        },
        {
          "title": "Possible to check API rate limit headers without burning a ...",
          "url": "https://community.openai.com/t/possible-to-check-api-rate-limit-headers-without-burning-a-request/510041",
          "excerpts": [
            "Nov 17, 2023 — I'd like to tell my user how many requests they have left before they start running a bulk API task. The best information on remaining rate ..."
          ]
        },
        {
          "title": "Identifying time zones in ISO 8601",
          "url": "https://stackoverflow.com/questions/42194571/identifying-time-zones-in-iso-8601",
          "excerpts": [
            "ISO 8601 supports zone offsets, but not timezones with variable offsets (typically timezones with summer time, which goes for many of the IANA timezones).See more"
          ]
        },
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/datasets/hsankesara/the-tweets-of-wisdom",
          "excerpts": [
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author. Also, every retweet with ... A collection of wise words straight from the Twitter",
            "The Tweets of Wisdom\n====================\n\nA collection of wise words straight from the Twitte",
            "e scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "d to scrap the tweets so that you can explore the words of these \"self-help\" tweets and understand them much better.",
            "tweets.csv(6.52 MB)",
            "License\n-------\n\n[CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author."
          ]
        },
        {
          "title": "The Tweets of Wisdom",
          "url": "https://www.kaggle.com/hsankesara/the-tweets-of-wisdom/code",
          "excerpts": [
            "The Tweets of Wisdom. A collection of wise words straight from the Twitter. arrow_drop_up 14. file_downloadDownload. The Tweets of Wisdom."
          ]
        },
        {
          "title": "Starter: The Tweets of Wisdom b542b708-9",
          "url": "https://www.kaggle.com/code/kerneler/starter-the-tweets-of-wisdom-b542b708-9",
          "excerpts": [
            "Explore and run machine learning code with Kaggle Notebooks | Using data from The Tweets of Wisdom."
          ]
        },
        {
          "title": "The-Tweets-of-Wisdom (Hsankesara)",
          "url": "https://github.com/Hsankesara/The-Tweets-of-Wisdom",
          "excerpts": [
            "A dataset which contains 30k+ so called \"self-help\" tweets from 100+ authors.",
            "I scraped the data using **Tweepy** API. I have scraped all the tweets, retweets and retweets with a comment of 40 authors.",
            "The data contains more than 40 authors because every retweet from any of the 40 authors is stored as a tweet from the original author.",
            "Here is the [link](https://www.kaggle.com/hsankesara/the-tweets-of-wisdom) to the dataset there.",
            "The-Tweets-of-Wisdom",
            "Public"
          ]
        },
        {
          "title": "Sahil Bloom (@SahilBloom) / X",
          "url": "https://x.com/sahilbloom?lang=en",
          "excerpts": [
            "Underrated life advice: You can reinvent yourself as many times as you need. New habits. New standards. New people. New career. You're never stuck. You're ...See more"
          ]
        },
        {
          "title": "Tweets To Templates: Train ChatGPT To Reverse Engineer ...",
          "url": "https://writewithai.substack.com/p/tweets-to-templates-train-chatgpt",
          "excerpts": [
            "Today we want to help you create your own library of viral Twitter templates. Going viral on Twitter always comes down to the hook you use."
          ]
        },
        {
          "title": "Reddit - r/indiehackers discussion by erik-grielenberger",
          "url": "http://reddit.com/r/indiehackers/comments/1m2ugls/i_reverseengineered_500_viral_tweets_heres",
          "excerpts": [
            "1. Lead with one outcome. [r/indiehackers - I reverse‑engineered 500+ VIRAL TWEETS - Here’s everything I've learned. Steal it ",
            "TL;DR**: Hook hard. Prove fast. Ask once. Media every two tweets. Follow this blueprint and turn that viral ceiling into your floor. Share if it help",
            "**4. Six hook patterns you can copy today:**",
            "**5. Pick a Narrative Skeleton (lock one). **",
            "**6. Media matters:** Drop a data card, GIF or 5‑sec demo every two or three tweets. Alternating text and media increases retweets by 50%. Add a three‑word headline and ALT text for each visual.",
            "5. Pick a Narrative Skeleton (lock one).",
            "4. Six hook patterns you can copy today:",
            "Copy my data-backed AI Templates: bettrprompts.com/category/viral-social-media",
            "*Hi, I’m Erik** - massive data nerd. I’ve primarily created these datasets because I want to increase my social media reach and understand how some posts go viral and most of them don",
            "This data analysis shows you how only **39 Threads** reached **118M+ impressions** and **450k+ likes** *(save for later)*."
          ]
        },
        {
          "title": "The anatomy of a viral tweet: The \"rehashing old news\" variant",
          "url": "https://weaponizedspaces.substack.com/p/the-anatomy-of-a-viral-tweet-the",
          "excerpts": [
            "A tweet that is said to “go viral” is generally one that has garnered a certain number of interactions within a specified time frame."
          ]
        },
        {
          "title": "What does a non-celebrity have to do to get famous on ...",
          "url": "https://www.quora.com/What-does-a-non-celebrity-have-to-do-to-get-famous-on-Twitter",
          "excerpts": [
            "I have seen many non-celebs who became prominent on Twitter. You need to show off the value in your tweets to become famous."
          ]
        },
        {
          "title": "Initial tweet valence, abuse volume, and observer Dark ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11106073/",
          "excerpts": [
            "by CJ Hand · 2024 · Cited by 1 — The current study. Participants processed six Twitter threads consisting of an initial tweet by a female celebrity followed by six replies."
          ]
        },
        {
          "title": "Studying Celebrity Practices on Twitter Using a Framework ...",
          "url": "https://journals.sagepub.com/doi/10.1177/2056305118763365",
          "excerpts": [
            "by S Tanupabrungsun · 2018 · Cited by 28 — We constructed three datasets by employing a toolkit (Hemsley, Ceskavich, & Tanupabrungsun, 2014) that collects tweets using a streaming API."
          ]
        },
        {
          "title": "As a YouTuber, is it really that vital to have a Twitter ...",
          "url": "https://www.reddit.com/r/socialmedia/comments/lqa4o1/as_a_youtuber_is_it_really_that_vital_to_have_a/",
          "excerpts": [
            "There are two things I see creators do on Twitter to their benefit. They make connections with other creators so they can work together, and ..."
          ]
        },
        {
          "title": "I followed a lot of indie hackers and “build in public” accounts on ...",
          "url": "https://news.ycombinator.com/item?id=43407164",
          "excerpts": [
            "I followed a lot of indie hackers and “build in public” accounts on Twitter over the years. Most of them struggled for a while and then pivoted into some ..."
          ]
        },
        {
          "title": "21 Extremely Relatable Tweets That Will Speak To Every ...",
          "url": "https://www.buzzfeed.com/ishabassi/funny-writer-tweets",
          "excerpts": [
            "May 30, 2018 — 21 Extremely Relatable Tweets That Will Speak To Every Writer's Soul ; 1. Hannah Giorgis @ethiopienne ; 2. anna s-r @annaspargoryan ; 3. f thot ..."
          ]
        },
        {
          "title": "32 Funny Tweets That Are Completely Underrated",
          "url": "https://www.buzzfeed.com/erinchack/underrated-tweets",
          "excerpts": [
            "Mar 10, 2017 — We asked the BuzzFeed Community what their favorite underrated tweet was. Here are the best responses. Note: Not all submissions are from Community users."
          ]
        },
        {
          "title": "Wise life advice from a 5-year-old is going viral on Twitter | Mashable",
          "url": "https://mashable.com/article/twitter-thread-wise-life-advice-from-5-year-old-is-going-viral",
          "excerpts": [
            "A 5-year-old boy dispensed some powerful life advice to his mom on his way to school. And Twitter is loving it."
          ]
        },
        {
          "title": "Twitter users share their best advice in four words or less - Daily Mail",
          "url": "https://www.dailymail.co.uk/femail/article-7124905/Thousands-Twitter-users-share-best-advice-just-four-words-less.html",
          "excerpts": [
            "'Just be bloody nice!' Thousands of people sum up their VERY best advice in just four words or less as part of an inspiring viral Twitter thread."
          ]
        },
        {
          "title": "The Best Book Tweets Of 2021 - BuzzFeed",
          "url": "https://www.buzzfeed.com/farrahpenn/best-funniest-viral-book-tweets-2021",
          "excerpts": [
            "Missing: underrated thinkers"
          ]
        },
        {
          "title": "11 Twitter accounts that will make you a better writer",
          "url": "https://forthwrite.substack.com/p/11-twitter-accounts-that-will-make",
          "excerpts": [
            "1. Stephen King | @StephenKing ... I'm usually too much of a wuss for horror stories, but even I've read Stephen King – and there is no doubt he's ..."
          ]
        },
        {
          "title": "5 Million Subscribers in 4 Years, The Most Underrated Writing ...",
          "url": "https://johnbardos.medium.com/5-million-subscribers-in-4-years-the-most-underrated-writing-platform-how-i-grew-my-twitter-9k-4f72b489943a?source=user_profile---------8----------------------------",
          "excerpts": [
            "YouTuber Peter Mckinnon got 1.8 million subs in his first year and more than 5 million subscribers in his first four years."
          ]
        },
        {
          "title": "The belief that artists must create content on social media ...",
          "url": "https://www.reddit.com/r/musicmarketing/comments/1cjf6k3/the_belief_that_artists_must_create_content_on/",
          "excerpts": [
            "Promoting your music or creating content on social media and building a following seems to be a huge waste in effort if you want to people to listen to your ...See more"
          ]
        },
        {
          "title": "What an excellent, insightful post. This really resonates",
          "url": "https://news.ycombinator.com/item?id=32125130",
          "excerpts": [
            "What an excellent, insightful post. This really resonates: \"when self-promotion is done right, it never looks like self-promotion\"."
          ]
        },
        {
          "title": "21 Tweets By Non-Celebrities That Got Over 500000 Likes ...",
          "url": "https://www.buzzfeed.com/mikespohr/tweets-by-non-celebrities-that-got-over-500000-likes",
          "excerpts": [
            "Feb 15, 2020 — 1. This perfect halftime show joke with a twist: · 2. This kid who thinks on a whole other level: · 3. And this adorable kid who goes his own way, ..."
          ]
        },
        {
          "title": "Collect posts from Twitter and send to Airtable",
          "url": "https://n8n.io/workflows/1403-collect-posts-from-twitter-and-send-to-airtable/",
          "excerpts": [
            "This workflow collects tweets (100 but adjustable) and add them to AirTable. Starting the 2nd execution, the workflow will add only the new tweets."
          ]
        },
        {
          "title": "CASOS-IDeaS-CMU/twitter_conversation_collection",
          "url": "https://github.com/CASOS-IDeaS-CMU/twitter_conversation_collection",
          "excerpts": [
            "Repository for collecting Twitter conversations using the V2 API - CASOS-IDeaS-CMU/twitter_conversation_collection."
          ]
        },
        {
          "title": "How do you get started from scratch on Twitter(or Threads) ...",
          "url": "https://www.reddit.com/r/socialmedia/comments/1dx5fha/how_do_you_get_started_from_scratch_on_twitteror/",
          "excerpts": [
            "My question revolves around just getting anyone random to simply view my posts, not getting followers, not getting a huge amount of likes/views."
          ]
        },
        {
          "title": "Reddit GrowthHacking Analysis and Related Datasets",
          "url": "https://www.reddit.com/r/GrowthHacking/comments/1m2u9tp/i_reverseengineered_500_viral_tweets_this_is/",
          "excerpts": [
            "This data analysis shows you how only 39 Threads reached 118M+ impressions and 450k+ likes (save for later)."
          ]
        },
        {
          "title": "A comprehensive study of wisdom",
          "url": "https://www.kaggle.com/code/hsankesara/a-comprehensive-study-of-wisdom",
          "excerpts": [
            "Understanding the Author, Likes and Retweets¶. How much likes and retweets are related. Value count of tweets of each author. Do more tweets increase the mean ... Explore and run machine learning code with Kaggle Notebooks | Using data from The Tweets of Wisdom\n",
            "A comprehensive study of wisdom",
            "the-tweets-of-wisdom"
          ]
        },
        {
          "title": "Hsankesara",
          "url": "https://www.kaggle.com/datasets/hsankesara/",
          "excerpts": [
            "Flickr Image dataset. Updated 7 years ago Usability 7.1 · 9 GB ... The Tweets of Wisdom. Updated 6 years ago Usability 10.0 · 1 File (CSV) · 2 ..."
          ]
        },
        {
          "title": "Trending Humor Tweets",
          "url": "https://www.kaggle.com/datasets/jashjasani/trending-humor-tweets",
          "excerpts": [
            "Twitter New Dataset 2024 March Data · Ayush Kumar Singh · a year ago · 8.1 · 3 MB · 351 downloads ; The Tweets of Wisdom · Hsankesara · 6 years ago · 10.0 · 2 MB · 379 ..."
          ]
        },
        {
          "title": "Sentiment & Emotions Labelled Tweets",
          "url": "https://www.kaggle.com/datasets/ankitkumar2635/sentiment-and-emotions-of-tweets",
          "excerpts": [
            "The dataset contains about 25k tweets along with date, timestamp, username, tweet Id, sentiment, sentiment score, emotion and emotion score. grid_3x3sort"
          ]
        },
        {
          "title": "Twitter Tweets Sentiment Dataset",
          "url": "https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset",
          "excerpts": [
            "Twitter is an online Social Media Platform where people share their their though as tweets. It is observed that some people misuse it to tweet hateful content."
          ]
        }
      ],
      "reasoning": "Direct evidence linking to the requested finegrained field value appears in passages that explicitly describe abstractive/extreme summarization and methods to ensure accuracy. For example, one excerpt explicitly names extreme summarization in the XSum sense and frames the task as generating a single, self-contained sentence that captures the article's core message rather than extracting phrases. This aligns with the core_insight_summarization_methodology's emphasis on abstractive, single-sentence summarization. Other excerpts discuss grounding techniques such as retrieval-augmented generation (RAG) and decoding strategies designed to reduce hallucinations, which directly support the requested hallucination_prevention_techniques. Additional excerpts enumerate automated evaluation metrics and human quality-control frameworks (SummaC, QAFactEval, FactCC, SBERTScore, SummEval, FRANK), which map to the automated_quality_metrics and human_quality_control_framework components of the field value. Related prompts and prompting strategies are also covered, illustrating a structured, instruction-heavy approach to producing reliable summaries, which supports the prompt_engineering_strategy portion of the value. Taken together, the strongest support comes from passes that explicitly discuss extreme/abstractive summarization and explicit grounding/verification mechanisms; secondary support comes from mentions of evaluation frameworks and human-in-the-loop quality checks. The excerpts collectively corroborate the claimed components of the summarization methodology, though some passages discuss adjacent areas (general summarization evaluation or related metrics) rather than the exact combination, hence the moderate-to-high relevance. ",
      "confidence": "high"
    },
    {
      "field": "insightful_content_annotation_guidelines",
      "citations": [
        {
          "title": "Description of a Twitter Corpus and Guidelines (PMCID: PMC7066507)",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7066507/",
          "excerpts": [
            "The key objective behind creating detailed annotation guidelines and making them publicly available is to ensure the reproducibility of the annotation experiments.",
            "The annotation task was started as an iterative process both for training purposes and to test the efficacy and clarity of the guidelines over a small initial dataset.",
            "We executed a total of 4 such iterations over the same dataset, refining the guidelines at each iteration and expanding them to make distinctions between the different categories more explicit.",
            "The guidelines were improved over a series of iterations of annotation and reviewed until we reached an agreeable level of consistency in our annotations.",
            "The annotators were instructed to code each tweet into only one category and were asked to create brief notes stating their thought process for instances in which coding was difficult or where they felt that the reason for their decision was not obvious.",
            "\nThe full annotation guidelines used by the annotators, with details and examples of each subcategory within the 4 classes, are made available with this publication",
            "The same is true if tweets mentioning medications not included in our sample are to be annotated for the same purpose in the future. Having a thorough, standardized annotation guideline may guide future annotation efforts.",
            "In total, a sample of 16,443 tweets were selected for annotation from more than 1 million posts collected from April 2013 to July 2018. This rather arbitrary number of tweets resulted from the various filtering methods (eg, removing short tweets and undersampling tweets with stimulants) that we applied on a much larger random sample of about 50,000 tweets."
          ]
        },
        {
          "title": "Annotation Guidelines for Twitter Part-of-Speech Tagging ...",
          "url": "https://www.cs.cmu.edu/~ark/TweetNLP/annot_guidelines.pdf",
          "excerpts": [
            "Here we describe part-of-speech (POS) annotation guidelines for online conversational text, using the Twitter POS tagset from Gimpel et al. (2011). That paper ..."
          ]
        },
        {
          "title": "Data annotation guidelines and best practices - Snorkel AI",
          "url": "https://snorkel.ai/blog/data-annotation/",
          "excerpts": [
            "Annotation guidelines are the guideposts that annotators, domain experts, and data scientists follow when labeling data."
          ]
        },
        {
          "title": "Decoding Toxic Memes with Rich Tag Annotations",
          "url": "https://arxiv.org/html/2508.04166v1",
          "excerpts": [
            " During the whole annotation process, annotators were asked to adhere to the definitions provided in the subsection [D]",
            "Annotation codebook: We also provide detailed annotation instructions, accompanied by 25 samples that have been manually selected and annotated by the two supervising researchers.",
            "upervising researchers. As outlined in Appendix [D](https://arxiv.org/html/2508.04166v1 \"Appendix D Details of annotation ‣ ToxicTags: Decoding Toxic Memes with Rich Tag Annotations\"), our annotation codebook has been constructed based on standard guidelines from which we have derived our definitions.",
            "In particular, we asked all the participating annotators to sign an agreement before starting the annotation, wherein we specifically focus on their mental well-being, given the nature of the work.",
            "Two staged annotation: We adopt a two stage annotation process – (i) classification of each meme into toxic or normal, and (ii) further bifurcation of toxic memes into offensive, dangerous or hateful. The two-stage process is specifically adopted to mitigate annotation errors as suggested in (Rizwan et al. [2025]",
            "Two staged annotation: We adopt a two stage annotation process – (i) classification of each meme into toxic or normal, and (ii) further bifurcation of toxic memes into offensive, dangerous or hateful.",
            ". \nThe overall dataset statistics are noted in Table [2](https://arxiv.org/html/2508.04166v1.T2 \"Table 2 ‣ Stage II: Fine-grained labelling ‣ 4 Annotation ‣ ToxicTags: Decoding Toxic Memes with Rich Tag Annotations\")."
          ]
        },
        {
          "title": "SemEval 2021 Task 7 HaHackathon: Detecting and Rating Humor and Offense",
          "url": "https://aclanthology.org/2021.semeval-1.9.pdf",
          "excerpts": [
            "Our annotation procedure asks raters if\ntexts",
            "If an annotator did not label at least 60%",
            "\n\tfirst shared task to combine the previously sep-\n\tarate domains of humor detection and offense\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t "
          ]
        },
        {
          "title": "SemEval-2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense",
          "url": "https://www.research.ed.ac.uk/files/244675909/SemEval_2021_MEANEY_DOA29032021_VOR_CC_BY.pdf",
          "excerpts": [
            "Our annotation procedure asks raters if the intention of the text is to be humorous (as evidenced by the the setup/punchline struc- ture, or absurd content",
            "annotators aged 18-70."
          ]
        },
        {
          "title": "SemEval 2021 Task 7: HaHackathon, Detecting and Rating Humor and Offense",
          "url": "https://aclanthology.org/2021.semeval-1.9/",
          "excerpts": [
            "We collected 10,000 texts from Twitter and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70.",
            "SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection.",
            "Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task: to predict if the variance in the humor ratings was higher than a specific threshold.",
            "The results suggest that the participating systems are well suited to humor detection, but that humor controversy is a more challenging task."
          ]
        },
        {
          "title": "Humor Analysis based on Human Annotation at IberLEF ...",
          "url": "https://www.aclweb.org/portal/content/haha-humor-analysis-based-human-annotation-iberlef-2021",
          "excerpts": [
            "May 7, 2021 — These are the four subtasks organized this year: * Humor Detection: determining if a tweet is a joke or not (intended humor by the author or not) ..."
          ]
        },
        {
          "title": "Automatic Detection of Irony and Humour in Twitter",
          "url": "https://www.semanticscholar.org/paper/Automatic-Detection-of-Irony-and-Humour-in-Twitter-Barbieri-Saggion/79d3c23b6f763a6d0a0a5538a46b0e1f97959209",
          "excerpts": [
            "This study investigates the automatic detection of irony and humour in social networks such as Twitter casting it as a classification problem and proposes a ..."
          ]
        },
        {
          "title": "Humor detection using deep learning in 10 years: A survey",
          "url": "https://www.scipedia.com/public/Ren_et_al_2023b",
          "excerpts": [
            "Jan 31, 2024 — The model obtained 77.36% and 79.41% precision in detecting humorous punchlines on UR-FUNNY and MUStaRD datasets. The study of Yang et al. [30] ..."
          ]
        },
        {
          "title": "Felicitta Annotation Guidelines",
          "url": "http://www.di.unito.it/~tutreeb/AnnotationGuidelines.pdf",
          "excerpts": [
            "2.4\tHUM\nThe tweets labeled as HUM are the ones with a clear ironic, sarcastic or other-\nwise humorous intent:",
            "HUM\t\tIronic",
            "[3,93303E+17] @SimoneLodi andrenfkdksjjsjfj non riesco nemmeno a\nscriverlo! [3,9188E+17] Oggi pranzo da nonna. Tornerò rotolando",
            "[3,93307E+17] Ho la risposta a tutte le vostre domande:\t\t\t Si",
            "\n[3,91965E+17] GIUSTO DOBBIAMO FARE LORO LA DANZA DELLA PIOGGIA COSI’\nSONO CONTENTI..............!!!!!!!!!!!! http://t.co/hQZ4qWGbVu",
            "Each tweet must be self-contained, there should not be a logical link be- tween a tweet and the next one"
          ]
        },
        {
          "title": "Memotion Dataset 7k - Kaggle",
          "url": "https://www.kaggle.com/datasets/williamscott701/memotion-dataset-7k",
          "excerpts": [
            "The objective of this proposal is to bring the attention of the research community towards the automatic processing of Internet memes. The task Memotion ..."
          ]
        },
        {
          "title": "Memotion Dataset: Comprehensive Analysis of Multimodal Internet",
          "url": "https://gts.ai/dataset-download/memotion-dataset/",
          "excerpts": [
            "Explore the Memotion Dataset, a groundbreaking collection of 8K annotated memes designed for advanced sentiment and humor classification."
          ]
        },
        {
          "title": "Memotion 3.0 - Artificial Intelligence Institute of South Carolina (AIISC)",
          "url": "https://aiisc.ai/defactify2/Memotion.html",
          "excerpts": [
            "Please register here to access the dataset. Memotion 3 is the 2nd iteration of the Memotion task which was conducted at Semeval 2020."
          ]
        },
        {
          "title": "Benchmark dataset of memes with text transcriptions for ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S235234092200720X",
          "excerpts": [
            "by F Gasparini · 2022 · Cited by 48 — The benchmark here described is composed of 800 memes collected from the most popular social media platforms, such as Facebook, Twitter, Instagram and Reddit."
          ]
        },
        {
          "title": "Deciphering Hate: Identifying Hateful Memes and Their Targets - arXiv",
          "url": "https://arxiv.org/html/2403.10829v1",
          "excerpts": [
            "The dataset includes two sets of labels for (i) detecting hateful memes and (ii) identifying targeted entities (individuals, organizations, ...",
            "We develop BHM focusing on two tasks: (i) detecting whether a meme is hateful or not and (ii) identifying the targeted entity of a hateful meme."
          ]
        },
        {
          "title": "MUTE: A Multimodal Dataset for Detecting Hateful Memes - ACL ...",
          "url": "https://aclanthology.org/2022.aacl-srw.5/",
          "excerpts": [
            "This paper introduces a multimodal hate speech dataset (named MUTE) consisting of 4158 memes having Bengali and code-mixed captions."
          ]
        },
        {
          "title": "Explainable Detection of Propagandistic and Hateful Memes",
          "url": "https://arxiv.org/html/2502.16612v1",
          "excerpts": [
            "We introduce MemeXplain, an explanation-enhanced dataset for propaganda memes in Arabic and hateful memes in English, making it the first large-scale resource ..."
          ]
        },
        {
          "title": "Meme Detection Thesis (Frangidis, 2021)",
          "url": "https://ikee.lib.auth.gr/record/329871/files/GRI-2021-30317.pdf",
          "excerpts": [
            "Meme Detection Modeling",
            "the images\nthat accompany the original post as input, determines whether that image is meme or\nnot. Moreover, the second module takes as input the tweets ’s text and analyzes it in\norder to extract features that point to either of the two classes. The final module makes\nthe classification of the Twitter post based on contextual metadata like the number of\nfollowers that a user has or the number of likes on a particular tweet.",
            "Twitter data\nare utilized in the task of rumour detection [24]. Moreover, in YouTube platform it\nhas been researched the way that popular meme videos affect other creators [59].",
            "Moreover, texts from the Twitter posts are\nproved to be incapable of learning any difference between posts that contain meme im-\nages and ones that do not. However, when they are used in combination with the images\nthey improve the results of image-based model",
            "the proposed combi-\nnation of all three models (ICM, MCM and TCM) also edges out the image-only model\non the task of meme detection, although it is not superior to the image-text and image-\nmetadata variatio",
            "They use Tesseract to extract the em-\nbedded text inputing it in an LSTM network to extract textual features and ResNet 18\nto extract image features. They pass all this features along with a face encoding, which\nthey created utilizing an open source package, to a dense layer that classifies the image\nas a meme or no",
            "They extract different features from tweets (such as adjacency matrices, follower\ndistributions and sentiment scores) and utilize a KNN with Dynamic Time Wrapping\n[40] to classify each post in the appropriate cate",
            "They extract different features from tweets (such as adjacency matrices, follower\ndistributions and sentiment scores) and utilize a KNN with Dynamic Time Wrapping\n[40] to classify each post in the appropriate cate",
            "e of them. The three pretrained models, VGG19, ResNet50 and InceptionV3, are also used in\nthis dissertation’s model for detecting memes using images."
          ]
        },
        {
          "title": "arXiv:2504.16723v1 - Multimodal heuristics for insightful content and meme detection",
          "url": "https://arxiv.org/html/2504.16723v1",
          "excerpts": [
            "This paper introduces a framework that integrates optical character recognition (OCR), caption generation, retrieval-augmented classification, and a visual ...",
            "Recent studies [ 13 , 11 , 23 ] have attempted to bridge the gap between language and vision representations, revealing that combined multimodal strategies can achieve promising results for specific domains such as misogynistic memes [ 34 ] or harmful COVID-19 memes [ 29 ] .",
            "While these approaches have shown promise, they exhibit several key limitations that hinder their ability to comprehensively detect nuanced hateful content.",
            "First, many existing methods [ 35 , 4 , 1 ] rely on fixed multimodal representations, where text and image features are extracted independently and fused statically. This rigid approach fails to capture the dynamic interplay between textual and visual cues, making it difficult to detect contextually embedded hate signals, such as sarcasm, coded symbols, or ambiguous imagery [ 18 ] .",
            "Second, these methods typically lack real-time adaptive reasoning, instead relying on predefined classification heuristics [ 19 , 33 ] . As a result, they struggle with detecting veiled or evolving hate speech that requires contextual reasoning beyond surface-level analysis.",
            "Third, existing models often categorize hateful content using coarse-grained labels, such as simply hateful or non-hateful, without distinguishing between different forms of hate speech. This lack of specificity reduces interpretability and makes it harder to apply targeted moderation strategies.",
            "The proposed framework demonstrates a robust approach to addressing hateful content detection within memes by integrating multiple modules for text extraction, captioning, retrieval, and visual question answering. The integrated pipeline achieves significant accuracy and AUC-ROC compared to existing methods on established benchmarks.",
            "These results underline the importance of uniting refined language strategies with methods that analyze images more deeply.",
            "Future work could extend the framework by incorporating culturally nuanced knowledge graphs, refining VQA prompts to reduce false positives, or integrating dynamic feedback loops for real-time detection."
          ]
        },
        {
          "title": "Propaganda Detection 101",
          "url": "https://www.numberanalytics.com/blog/propaganda-detection-101",
          "excerpts": [
            "May 27, 2025 — Several techniques have been developed for propaganda detection, including machine learning, deep learning, and rule-based approaches. Machine ..."
          ]
        },
        {
          "title": "SemEval-2020 Task 11: Detection of Propaganda ...",
          "url": "https://aclanthology.org/2020.semeval-1.186/",
          "excerpts": [
            "by G Da San Martino · 2020 · Cited by 280 — We present the results and the main findings of SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. The task featured two subtasks."
          ]
        },
        {
          "title": "Factoring Hate Speech: A New Annotation Framework to ...",
          "url": "https://aclanthology.org/2023.woah-1.21.pdf",
          "excerpts": [
            "by G Ron · 2023 · Cited by 5 — In this work we propose a novel annotation scheme which factors hate speech into five sep- arate discursive categories. To evaluate our."
          ]
        },
        {
          "title": "Disinformation most active on X, formerly known as Twitter, EU says",
          "url": "https://www.bbc.com/news/technology-66926080",
          "excerpts": [
            "X, formerly Twitter, has the biggest proportion of disinformation of six big social networks, a European Commission study has suggested."
          ]
        },
        {
          "title": "A Platform Problem: Hate Speech and Bots Still Thriving on X",
          "url": "https://www.isi.edu/news/73786/a-platform-problem-hate-speech-and-bots-still-thriving-on-x/",
          "excerpts": [
            "A study published February 12, 2024 in PLOS ONE sheds light on ongoing challenges with hate speech and inauthentic accounts on X (formerly Twitter)."
          ]
        },
        {
          "title": "Evaluating Twitter's algorithmic amplification of low-credibility ...",
          "url": "https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-024-00456-3",
          "excerpts": [
            "by G Corsi · 2024 · Cited by 12 — This study presents a measurement approach that uses observed digital traces to infer the status of algorithmic amplification of low-credibility content on ..."
          ]
        },
        {
          "title": "I cue you liking me: Causal and spillover effects of ...",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0747563223002157",
          "excerpts": [
            "by WJ Zhang · 2023 · Cited by 1 — Technological engagement bait has a spillover effect to increase engagement with the non-baiting videos of the same users."
          ]
        },
        {
          "title": "#EngagementBait - Search / X",
          "url": "https://twitter.com/search?q=%23EngagementBait&src=hashtag_click",
          "excerpts": [
            "The latest posts on #EngagementBait. Read what people are saying and join the conversation."
          ]
        },
        {
          "title": "Rant: The amount of engagement bait on vtuber twitter is ...",
          "url": "https://www.reddit.com/r/VirtualYoutubers/comments/1du8r41/rant_the_amount_of_engagement_bait_on_vtuber/",
          "excerpts": [
            "It's now mostly rage bait and engagement farming, with regular replies getting hidden, and personally, that's led me to mostly stop tweeting."
          ]
        },
        {
          "title": "#EngagementBait - Search / X",
          "url": "https://twitter.com/search?q=%23EngagementBait&src=hashtag_click&f=user",
          "excerpts": [
            "Engagement Bait Detector · @baitdetectir. Follow. Click to Follow baitdetectir ... Engagement Bait Tweets · @BaitTweet. Follow. Click to Follow BaitTweet."
          ]
        },
        {
          "title": "Determining Tweet Types | Docs | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/docs/tutorials/determining-tweet-types",
          "excerpts": [
            "### Twitter API v2 payload\n\nFortunately, the new [v2 data format](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/introduction) streamlines the Tweet identification process. There's a single field, \" referenced\\_tweets.type \", that allows for easy identification of the Tweet type. | Tweet type | How to identify |\n| --- | --- |\n| Reply Tweet | * \n  The \" data.referenced\\_tweets.type \" field will have a value of \" replied\\_to",
            "### v2 Tweets\n\nReply Tweet Quote Tweet Retweet Original Tweet",
            "There are four different types of Tweets that can be created using the Twitter platform. 1. **Original Tweet**",
            "A few notes:\n\n* [Guides about the different Tweet types](https://help.twitter.com/en/using-twitter/types-of-tweets) will sometimes call out \"Mentions\" as another type of Tweet. A mention is when a person includes the username (@handle) of another person in the body of the Tweet. For our purposes, this is not a unique type of Tweet because original Tweets, Replies, Retweets, and Quote Tweets can all contain a mention. This metadata is structured for you in the [entities object](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/entities#mentions) in a Tweet payload. * You can have a Retweet of a Reply Tweet or of a Quote Tweet. But this guide will focus on classifying the outermost type of Tweet. So, a Retweet of a Quote Tweet will be classified as a \"Retweet.\"",
            "Quote Tweet. The \"data.referenced_tweets.type\" field will have a value of \"quoted\". Key payload object: data.referenced_tweets.type. Retweet. The \"data ..."
          ]
        },
        {
          "title": "Models — tweepy 4.14.0 documentation",
          "url": "https://docs.tweepy.org/en/stable/v2_models.html",
          "excerpts": [
            "For example, if the parent Tweet is a Retweet, a Retweet with comment (also known as Quoted Tweet) or a Reply, it will include the related Tweet referenced to ..."
          ]
        },
        {
          "title": "How do I use the \"referenced_tweets.type\" response field ...",
          "url": "https://stackoverflow.com/questions/67671790/how-do-i-use-the-referenced-tweets-type-response-field-in-the-twitter-api-in-p",
          "excerpts": [
            "I now want to add referenced_tweets.type in order to get if the Tweet is a Retweet or not but I'm not sure how to do it. Can someone help?"
          ]
        },
        {
          "title": "Using Postman (Twitter API v2) to get retweets and replies for ...",
          "url": "https://devcommunity.x.com/t/using-postman-twitter-api-v2-to-get-retweets-and-replies-for-thousands-of-tweets/151988",
          "excerpts": [
            "Mar 30, 2021 — I already used referenced_tweets which includes the type (retweeted, replied_to, quoted). My question is if a tweet is a retweet of a ..."
          ]
        },
        {
          "title": "Conversation ID - X - Twitter Developer",
          "url": "https://developer.x.com/en/docs/x-api/conversation-id",
          "excerpts": [
            "When Posts are posted in response to a Post (known as a reply), or in response to a reply, there is now a defined conversation_id on each reply, which matches ...",
            "Using the X API v2, you can retrieve and reconstruct an entire conversation thread, allowing you to understand what is being said and how conversations and ..."
          ]
        },
        {
          "title": "[Help] Referenced_tweets returning none when there is clearly a ...",
          "url": "https://devcommunity.x.com/t/help-referenced-tweets-returning-none-when-there-is-clearly-a-parent-tweet/175721",
          "excerpts": [
            "I've read the docs and learnt that it is stored in referenced tweets as a list. But everytime I run, it's returning me “None”, even tho the tweet I'm running ..."
          ]
        },
        {
          "title": "Presence of both retweeted_status and quoted_status on ...",
          "url": "https://devcommunity.x.com/t/presence-of-both-retweeted-status-and-quoted-status-on-root-level/129366",
          "excerpts": [
            "In the root level of some tweet objects, I see valid payload in both retweeted_status and quoted_status. As per my understanding, if a tweet t2 ...See more"
          ]
        },
        {
          "title": "Twitter API: Check if a tweet is a retweet",
          "url": "https://stackoverflow.com/questions/18869688/twitter-api-check-if-a-tweet-is-a-retweet",
          "excerpts": [
            "Make sure your \"re-tweet\" is not actually a quote of an another tweet. In this case, I had to use the quoted_status field to get the original ...See more",
            "I've noticed that retweets are distinguished from regular tweets by the line \"RT @...\" The Python code following can be used to identify them:",
            "You can check if its a retweet by checking the tweet and then getting what you need from the retweeted_status hash."
          ]
        },
        {
          "title": "How to Exclude retweets and replies in a search api?",
          "url": "https://stackoverflow.com/questions/27941940/how-to-exclude-retweets-and-replies-in-a-search-api",
          "excerpts": [
            "In the new Search Tweets API, including the following parameters will remove different flavors of retweets: -is:retweet Excludes retweets -is:quote Excludes ..."
          ]
        },
        {
          "title": "Fields - X",
          "url": "https://docs.x.com/x-api/fundamentals/fields",
          "excerpts": [
            " {\n       \"id\": \"1263150595717730305\",\n       \"public_metrics\": {\n           \"retweet_count\": 12,\n           \"reply_count\": 14,\n           \"like_count\": 49,\n           \"quote_count\": 7\n ",
            "\"public_metrics\": {",
            "\"retweet_count\": 12,",
            "\"reply_count\": 13,",
            "\"like_count\": 51,",
            "\"quote_count\": 7",
            "}",
            "}",
            "}",
            "}",
            "Bear in mind that you cannot request specific subfields (for example, `public_metrics.retweet_count` ). All subfields will be returned when the top-level field ( `public_metrics` ) is specified.",
            "If you are interested in receiving the public metrics of the Posts that are returned in the response, you will want to include the `tweet.fields` parameter in your request, with `public_metrics` set as the value.",
            "If you need the Post’s created date or public metrics, you will need to use the `tweet.fields` parameters to request them."
          ]
        },
        {
          "title": "X API v2 Expansions - Fundamentals",
          "url": "https://docs.x.com/x-api/fundamentals/expansions",
          "excerpts": [
            "}"
          ]
        },
        {
          "title": "X API v2 data dictionary",
          "url": "https://docs.x.com/x-api/fundamentals/data-dictionary",
          "excerpts": [
            "_metrics** | object | Public engagement metrics for the Tweet at the time of the request. | Measure Tweet engageme",
            "\"public_metrics\": {\n              \"retweet_count\": 5219,\n              \"reply_count\": 1828,\n              \"like_count\": 17141,\n              \"quote_count\": 3255\n   "
          ]
        },
        {
          "title": "X API v2 data dictionary",
          "url": "http://developer.x.com/en/docs/twitter-api/data-dictionary/object-model/tweet",
          "excerpts": [
            "**public\\_metrics** | object | Public engagement metrics for the Tweet at the time of the request. | Measure Tweet engagement"
          ]
        },
        {
          "title": "GET /labs/2/tweets/:id | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/labs/tweets-and-users/api-reference/get-tweets-id",
          "excerpts": [
            "To return this field, add tweet.fields=public_metrics in the request's query parameter. public_metrics.retweet_count, integer, Number of times this Tweet has ...",
            "To return this field, add tweet.fields=public_metrics in the request's query parameter.",
            "public_metrics.retweet_count, integer, Number of times this Tweet has ... https://t.co/Hg8nkfoizN\""
          ]
        },
        {
          "title": "Twitter Data: Tweets and User Interactions",
          "url": "https://www.kaggle.com/datasets/thedevastator/tweets-and-user-engagement",
          "excerpts": [
            "The dataset Twitter Data: Tweets and User Interactions provides comprehensive information about tweets and user interactions on the popular social media platform Twitter. The dataset includes various attributes that shed light on the characteristics and engagement metrics of tweets, allowing for in-depth analysis of user behavior and content performance."
          ]
        },
        {
          "title": "Metrics",
          "url": "https://developer.x.com/en/docs/x-api/metrics",
          "excerpts": [
            "Metrics include the total count of impressions, Retweets, Quote Tweets, likes, replies, video views, video view quartiles, URL and profile link clicks for each ...See more"
          ]
        },
        {
          "title": "Tweet's public metrics - X API v2",
          "url": "https://devcommunity.x.com/t/tweets-public-metrics/180920",
          "excerpts": [
            "Nov 30, 2022 — I'm specifically interested in the public ones, but if the answer is similar for paid or organic metrics, it would be great. Thanks.See more"
          ]
        },
        {
          "title": "Whats new with the Twitter API v2 in 2022",
          "url": "https://dev.to/suhemparack/whats-new-with-the-twitter-api-v2-in-2022-plb",
          "excerpts": [
            "Oct 11, 2022 — This tells us that the Tweet with ID 1265324480517361664 was deleted. This guide showcases how to work with the batch compliance solution in ..."
          ]
        },
        {
          "title": "GET statuses/home_timeline | Docs | Twitter Developer Platform",
          "url": "https://developer.x.com/en/docs/x-api/v1/tweets/timelines/api-reference/get-statuses-home_timeline",
          "excerpts": [
            "X API v2 ... The value of count is best thought of as a limit to the number of tweets to return because suspended or deleted content is removed after the count ..."
          ]
        },
        {
          "title": "The 21 essential social media metrics you must track for ...",
          "url": "https://blog.hootsuite.com/social-media-metrics/",
          "excerpts": [
            "We've broken down the 21 key social media metrics you need to keep an eye on into six different categories to help keep you organized."
          ]
        },
        {
          "title": "Nonrandom Tweet Mortality and Data Access Restrictions Compromising the Replication of Sensitive Twitter Studies",
          "url": "https://www.cambridge.org/core/journals/political-analysis/article/nonrandom-tweet-mortality-and-data-access-restrictions-compromising-the-replication-of-sensitive-twitter-studies/17E618F1D395CBBB2360AA424DA5533A",
          "excerpts": [
            "As proposed in Davidson _et al._ ( [Reference Davidson 2023]() ), automatic crawlers could update these archives without needing an API.",
            "Twitter’s policy still prohibits researchers from directly sharing the raw content of tweets.",
            ") ). The one-way aspect of this well-established computer science technique prevents rehydrating tweets’ raw content.",
            "hat these Twitter studies and their findings are considerably affected by nonrandom tweet mortality and data access restrictions imposed by ... As pr"
          ]
        },
        {
          "title": "Implications of changes in Twitter's Developer Policy",
          "url": "https://gwu-libraries.github.io/sfm-ui/posts/2017-05-18-twitter-policy-change",
          "excerpts": [
            "You may not distribute Tweet IDs for the purposes of (a) enabling any entity to store and analyze Tweets for a period exceeding 30 days without ...",
            "May 18, 2017 — You may not distribute Tweet IDs for the purposes of (a) enabling any entity to store and analyze Tweets for a period exceeding 30 days without ..."
          ]
        },
        {
          "title": "Twitter's Developer Policies for Researchers, Archivists, and Librarians",
          "url": "https://medium.com/on-archivy/twitters-developer-policies-for-researchers-archivists-and-librarians-63e9ba0433b2",
          "excerpts": [
            "I will unpack some key portions of Twitter's Developer Policies that are relevant to research and archiving and offer my interpretation.",
            "You may not distribute Tweet IDs for the purposes of (a) enabling any entity to store and analyze Tweets for a period exceeding 30 days unless ..."
          ]
        },
        {
          "title": "Developer policy support | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/support/x-api/policy",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 ...",
            "If you provide X Content to third parties, including downloadable datasets or via an API, you may only distribute X IDs, Direct Message IDs, and/or User IDs ( ... Support > X API > Developer policy\n",
            " X Content, you must comply with ALL X policies. These include this Developer Policy , the Automation Rules , the Display Requirements , the API Restricted Uses Rules , the X Rules , the X Brand Resources , the Periscope Community Guidelines , and the Periscope Trademark Guidelines , as well as any other agreements you enter into with X relating to your use of the X API or X Content, including but not limited to the Developer Agreement or a Master Licensing Agreement or Order (as applicable)."
          ]
        },
        {
          "title": "Does showing a lot of tweets by a site violates the TOS?",
          "url": "https://devcommunity.x.com/t/does-showing-a-lot-of-tweets-by-a-site-violates-the-tos/175606",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 Tweet IDs ... In addition, any content shared remains subject to the Twitter Developer Policy ...",
            "Aug 17, 2022 — In total, you may not distribute more than 1,500,000 Tweet IDs to ... non-automated means (e.g., download of spreadsheets or PDFs). In ..."
          ]
        },
        {
          "title": "Question about 6)b)i) of the developer policy",
          "url": "https://devcommunity.x.com/t/question-about-6-b-i-of-the-developer-policy/27542",
          "excerpts": [
            "Would 6) b) i) allow us to redistribute the raw tweet text alongside the annotations, provided we limited the total number of downloads? Related ..."
          ]
        },
        {
          "title": "Developer Policy – X Developers",
          "url": "http://developer.x.com/en/developer-terms/policy",
          "excerpts": [
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties.**If you provide X Content to third parties, including downloadable datasets or via an API, you may only distribute Post IDs, Direct Message IDs, and/or User IDs (except as described below)",
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs).",
            "Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research. Only as many Post IDs or User IDs that is necessary for such research, and has been approved by X may be used."
          ]
        },
        {
          "title": "More on restricted use cases - Twitter Developer - X",
          "url": "https://developer.x.com/en/developer-terms/more-on-restricted-use-cases",
          "excerpts": [
            "If you need to share X content you obtained via the X APIs with another party, the best way to do so is by sharing Post IDs, Direct Message IDs, and/or User IDs, which the end user of the content can then rehydrate (i.e. request the full Post, User, or Direct Message content) using the X APIs.",
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from X.",
            "There are a few other points to keep in mind about redistributing X content:\n\n* You may only distribute up to a total of 1,500,000 Post IDs to a single entity within a 30 day period unless you’ve received prior express written permission from",
            "* Individuals redistributing Post IDs and/or User IDs on behalf of an academic institution for the sole purpose of non-commercial research are permitted to redistribute an unlimited number of Post IDs and/or User IDs.",
            "To request permission to share X content as outlined above, please use the API Policy support form.",
            "If you choose to share hydrated X content with another party in this way, you may only share up to 50,000 hydrated public Post Objects and/or User Objects per recipient, per day, and should not make this data publicly available (for example, as an attachment to a blog post or in a public Github repository)."
          ]
        },
        {
          "title": "Developer Agreement and Policy - Twitter Developers - X",
          "url": "https://developer.x.com/en/developer-terms/agreement-and-policy",
          "excerpts": [
            "In total, you may not distribute more than 1,500,000 Post IDs to any entity (inclusive of multiple individuals associated with a single entity) within any 30 day period unless you have received written permission from X. In addition, developers may provide up to 500 public Posts Objects and/or User Objects to each person who uses your service on a daily basis if this is done via non-automated means (e.g., download of spreadsheets or PDFs). Academic researchers are permitted to distribute Post IDs and/or User IDs solely for the purposes of non-commercial research on behalf of an academic institution, and that has been approved by X in writing, or peer review or validation of such research.",
            "The best place to get X Content is directly from X. Consequently, we restrict the redistribution of X Content to third parties. If you ..."
          ]
        },
        {
          "title": "Non-commercial use of the X API | Twitter Developer Platform",
          "url": "https://developer.x.com/en/developer-terms/commercial-terms",
          "excerpts": [
            "Our Developer Agreement includes commercial use terms that govern how the X API can be used with Academic Research access. Updated February 27, 2023: We ..."
          ]
        },
        {
          "title": "Towards Open-Domain Twitter User Profile Inference",
          "url": "https://aclanthology.org/2023.findings-acl.198.pdf",
          "excerpts": [
            "by H Wen · 2023 · Cited by 6 — According to the Twitter. Developer Agreement and Policy, we will only release IDs instead of actual content for non- commercial research ..."
          ]
        },
        {
          "title": "Twitter developer policy update | Twitter Developer Platform - X",
          "url": "https://developer.x.com/en/blog/industry-team-news/2020/x-developer-policy-update",
          "excerpts": [
            "Mar 10, 2020 — Researchers can now share an unlimited number of Tweet IDs and/or User IDs if they are doing so on behalf of an academic institution and for ..."
          ]
        },
        {
          "title": "[PDF] An archive and corpus of Twitter/X's policies for Tweet redistribution ...",
          "url": "https://access.gesis.org/sharing/2847/6568",
          "excerpts": [
            "When researchers publish research based on the analysis of Tweets, good practice requires sharing the Tweets. (or Tweet IDs) to enable ..."
          ]
        },
        {
          "title": "Automating Annotation Guideline Improvements using LLMs",
          "url": "https://aclanthology.org/2025.comedi-1.13.pdf",
          "excerpts": [
            "by A Bibal · 2025 · Cited by 2 — Designing guidelines making every annotator agree on their annotations is a difficult and te- dious process. Such a task can be even more."
          ]
        },
        {
          "title": "An empirical evaluation of electronic annotation tools for ...",
          "url": "https://genominfo.org/upload/pdf/gi-2020-18-2-e24.pdf",
          "excerpts": [
            "by D Weissenbacher · 2020 · Cited by 2 — Multi-annotator support It should calculate the inter-annotator agreement and provide an interface to help adjudication. Export. It should ..."
          ]
        },
        {
          "title": "Identifying and Resolving Annotation Changes for Natural ...",
          "url": "https://aclanthology.org/2021.naacl-industry.2.pdf",
          "excerpts": [
            "por JG Ramas · 2021 · Mencionado por 4 — Given an NLU dataset with utterances having multiple, possibly conflicting annotations (IC and SL), our goal is to find the right annotation for each such ..."
          ]
        },
        {
          "title": "Data Annotation for ML: Guide to Label AI Training Data",
          "url": "https://www.innovatiana.com/en/post/data-annotation-101-our-guide",
          "excerpts": [
            "Data annotation refers to the process of assigning labels to raw data. Labeling data is a core part of the annotation process across all data types."
          ]
        },
        {
          "title": "GPT-4 as a Twitter Data Annotator",
          "url": "https://www.techrxiv.org/users/693749/articles/683242/master/file/data/GPT4%20as%20a%20Twitter%20stance%20annotator_Ravihari_finalrevised/GPT4%20as%20a%20Twitter%20stance%20annotator_Ravihari_finalrevised.pdf",
          "excerpts": [
            "With the emergence of GPT models and their proficiency in various NLP tasks, this study aims to estab- lish a performance baseline for GPT-4 as a social media ..."
          ]
        },
        {
          "title": "(PDF) Semi-Automatic Dataset Annotation Applied to ...",
          "url": "https://www.researchgate.net/publication/377919409_Semi-automatic_dataset_annotation_applied_to_automatic_violent_message_detection",
          "excerpts": [
            "The methodology consists of annotating the dataset incrementally, which delivers an increase in annotator efficiency, thereby validating the suitability of the ..."
          ]
        },
        {
          "title": "Annotated Data for Semantic Role Labeling of Crisis ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2352340925004184",
          "excerpts": [
            "by ADP Ariyanto · 2025 — This data was manually labeled for two main tasks: entities (Named Entity Recognition) and semantic role arguments (Semantic Role Labeling). The ..."
          ]
        },
        {
          "title": "Discussing best practices for the annotation of Twitter ...",
          "url": "https://d-nb.info/1136959270/34",
          "excerpts": [
            "by I Rehbein · 2013 · Cited by 5 — Abstract. This paper contributes to the discussion on best practices for the syntac- tic analysis of non-canonical language, focusing on Twitter microtext."
          ]
        },
        {
          "title": "[PDF] An exploration of Italian Twitter data using sentiment analysis",
          "url": "https://www.demographic-research.org/volumes/vol40/25/40-25.pdf",
          "excerpts": [
            "We trained a binary Support Vector Machine with the labelled tweets of the first annotation layer derived from the manual annotation process."
          ]
        },
        {
          "title": "SD Williams 2023 — Comparative Adjudication of Noisy and Subjective Data Annotation Disagreements for Deep Learning",
          "url": "https://etd.ohiolink.edu/acprod/odb_etd/ws/send_file/send?accession=wright1682700129672253&disposition=inline",
          "excerpts": [
            "Disagreements can be addressed in training by using the label a majority of annotators assigned\n\n(majority vote), training only with unanimously annotated cases (clean filtering), and\n\nrepresenting training labels as probabilities (soft labeling",
            "This study shows clean filtering\n\noccasionally outperforming majority voting, and soft labeling outperforming both",
            "The majority vote approach to addressing inter-annotator disagreements is the simplest\n\napproach, and it treats annotations that agree as correct and excludes annotations that disagree\n\nfrom trainin",
            "On the\n\nother hand, it has the effect of treating instances where annotators might have had equally valid\n\nbut differing judgments as equivalent to instances where all annotators agreed, which disguises\n\ndisagreements in the datas",
            "In conventional machine learning with deep neural networks,\n\nwe configure the final layer to return the most likely class for each instance of input. The model's\n\ndecision about the class of the input can be the impetus for some real-world actio",
            "Although we typically want a classifier to be decisive, humans may be\n\nindecisive when presented with the same input"
          ]
        },
        {
          "title": "Dealing with Annotator Disagreement in Hate Speech Classification (arXiv 2502.08266v1)",
          "url": "https://arxiv.org/html/2502.08266v1",
          "excerpts": [
            "The issue is often addressed with one of two approaches:\n(1) keeping only the samples where there is agreement among annotators, (2) selecting the samples where the majority opinion is clear.",
            "However, for cases of disagreement where the majority is not obvious, researchers must decide whether to use methods like expert adjudication, averaging annotations, or considering the context in more detail to determine the most accurate label.",
            "Recognizing the challenge of subjective disagreements among annotators, especially in tasks such as hate speech detection, we shed light on this issue and propose a novel method for deriving the true label from such annotations.",
            "disagreement rates are approximately 10%, 12%, and 13% for the 2-class, 4-class, and 6-class problems, respectively.",
            "rthermore, as annotators in general agree about the no hate speech class, there are almost 4-fold samples in that category",
            "In the regression experiments, we first trained the model using the mean of the annotated strength scores, which range from 0 to 10, with the Mean Squared Error (MSE) loss function.",
            "We evaluated the two-class classification with the binary class labels obtained by converting the mean strength scores using a threshold of 0.5, as described in Section [6",
            "We also trained a classification-based model using the Cross Entropy Loss function using these binary class labels."
          ]
        },
        {
          "title": "Measuring Annotator Agreement Generally across ...",
          "url": "https://dl.acm.org/doi/fullHtml/10.1145/3485447.3512242",
          "excerpts": [
            "by A Braylan · 2022 · Cited by 26 — We investigate the design and evaluation of IAA measures for complex annotation tasks, with evaluation spanning seven diverse tasks.",
            "Data annotations are often collected from human experts or crowdsourcing [ [2]() ] as part of the process for training and evaluati",
            "When annotators label data, a key metric for quality assurance is inter-annotator agreement (IAA): the extent to which annotators agree on their labels.",
            "A more comprehensive approach for this problem is to understand the complexity of the labeling task, identify an agreement metric that is explainable and fits the specific project requirements, in combination with other quality control mechanisms that can quantify the quality of a dataset.",
            " To support reproducibility, we share our code and data"
          ]
        },
        {
          "title": "Assessing Inter-Annotator Agreement for Medical Image ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10062409/",
          "excerpts": [
            "by F YANG · 2023 · Cited by 52 — This study aims to assess, illustrate and interpret the inter-annotator agreement among multiple expert annotators when segmenting the same lesion(s)/ ..."
          ]
        },
        {
          "title": "Cohen's Kappa: Measuring Inter-Rater Agreement",
          "url": "https://datatab.net/tutorial/cohens-kappa",
          "excerpts": [
            "The table of Landis & Koch (1977) can be used as a guide. Therefore, the calculated Cohen's Kappa coefficient of 0.44 indicates moderate reliability or ..."
          ]
        },
        {
          "title": "Criteria for the Interpretation of Kappa values by Landis & ...",
          "url": "https://www.researchgate.net/figure/Criteria-for-the-Interpretation-of-Kappa-values-by-Landis-Koch-1977_tbl1_259976499",
          "excerpts": [
            "Cohen's Kappa was used to test the inter-rater reliability of the movement analysts and non-parametric Mann–Whitney U tests were undertaken to assess the ...",
            "While there are no hard and fast rules for what constitutes a \"good\" or \"poor\" value of kappa, Landis & Koch (1977) provided some criteria for the ..."
          ]
        },
        {
          "title": "python - Interrater reliability with multi-rater, multi-label dataset",
          "url": "https://stats.stackexchange.com/questions/511927/interrater-reliability-with-multi-rater-multi-label-dataset",
          "excerpts": [
            "I'm trying to calculate interrater reliability with 3 coders, and each question is multiple choice. An example tagging would be:"
          ]
        },
        {
          "title": "Krippendorff's Alpha for Annotation Agreement - Label Studio",
          "url": "https://labelstud.io/blog/how-to-use-krippendorff-s-alpha-to-measure-annotation-agreement/",
          "excerpts": [
            "Missing: thresholds .80 .667"
          ]
        },
        {
          "title": "(PDF) Inter-annotator Agreement Using the Conversation Analysis ...",
          "url": "https://www.researchgate.net/publication/357895917_Inter-annotator_Agreement_Using_the_Conversation_Analysis_Modelling_Schema_for_Dialogue",
          "excerpts": [
            "Inter-annotator agreement is shown to be higher for task-oriented dialogs than non-task-oriented, though the structure of the dialogue itself ..."
          ]
        },
        {
          "title": "Krippendorff's Alpha Reliability Estimate: Simple Definition",
          "url": "https://www.statisticshowto.com/krippendorffs-alpha/",
          "excerpts": [
            "Missing: .80",
            "Krippendorff suggests: “[I]t is customary to require α ≥ .800. Where tentative conclusions are still acceptable, α ≥ .667 is the lowest conceivable limit (2004, ...See more",
            "Krippendorff suggests: “[I]t is customary to require α ≥ .800. Where tentative conclusions are still acceptable, α ≥ .667 is the lowest conceivable limit ... Krippendorff’s alpha:\n\n* Ignores missing data entirely. * Can handle various sample sizes, categories, and numbers of raters. * Applies to any [measurement level](https://www.statisticshowto.com/probability-and-statistics/descriptive-statistics/scales-of-measurement/) (i.e. ( [nominal, ordinal, interval, ratio](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/nominal-ordinal-interval-ratio/) ). Commonly used in [content analysis](https://www.statisticshowto.com/content-analysis/) to quantify the extent of agreement between raters, it differs from most other measures of inter-rater reliability because it calculates _dis_ agreement (as opposed to agreement). This is one reason why the statistic is arguably more reliable, but some researchers report that in practice, the results from both alpha and kappa are similar (Dooley). ## Computation of Krippendorff’s Alpha\n\nThe basic formula for al\n (Dooley). ## Computation of Krippendorff’s Alpha\n\nThe basic formula for alpha is a [ratio](https://www.statisticshowto.com/ratios-and-rates/) (observed disagreement/expected disagreement). The ratio is deceptively simple, because the method is actually computationally complex, involving [resampling](https://www.statisticshowto.com/resampling-techniques/) methods like the [bootstrap](https://www.statisticshowto.com/bootstrap-sample/) . This is a major disadvantage (Osborne). You can get an idea of the computations involved from the following formula.\n"
          ]
        },
        {
          "title": "Prodigy Metrics",
          "url": "https://prodi.gy/docs/metrics",
          "excerpts": [
            "To help make this easy, Prodigy implements a few of inter-annotator agreement (IAA) metrics that you can apply to your annotated datasets.",
            "Measuring inter-annotator agreement",
            "Measuring inter-annotator agreement. When multiple users annotate a dataset, you should check that the annotators agree before training a machine learning model ...",
            "The annotators should follow the annotation guidelines to make sure their\noutput is consistent and reproducible.",
            "The annotators should work independently as groupthink will likely obfuscate\nany potential issues with the annotation schema or the interpretation of the\ndata leading to unfairly high agreement scores.",
            "The subset of the data used for IAA calculation should be representative of\nthe corpus to be annotated in terms of data types and categories.",
            "\nVarious quantitative methods exist to assess inter annotator agreement (IAA),\nwith the most commonly reported metrics in the literature being the Kappa\nmetrics, such as Cohen’s Kappa and Fleiss’ Kappa , and Alpha\nmetrics, notably Krippendorf’s Alpha .",
            "All\nof these metrics share the general form in that they take into account both the\nobserved agreement ( pₐ ) - the proportion of instances where annotators agree\nand the expected agreement ( pₑ ) the agreement that would occur by chance:"
          ]
        },
        {
          "title": "Inter-Annotator Agreement – Innovatiana",
          "url": "https://www.innovatiana.com/en/post/inter-annotator-agreement",
          "excerpts": [
            "The Inter-Annotator Agreement can help to clarify annotation criteria by identifying areas of disagreement between annotators.",
            "May 10, 2024 — An Inter-Annotator Agreement (IAA) is a measure of the agreement or consistency between each annotation produced by different annotators working on the same ...",
            "By regularly checking the Inter Annotator Agreement, it is possible to identify trends and recurring problems in the evaluations, in the data sets under construction. This makes it possible to optimize the annotation process, whether it is images or [**videos**](https://en.innovatiana.com/post/video-annotation-common-mistakes) in particular, by implementing corrective measures over time to improve the reliability of dataset evaluations over the long term.",
            "Cohen's Kappa coefficient",
            "The Cohen's Kappa coefficient is a statistical measure that assesses the agreement between two annotators corrected by the possibility of random agreement. It is calculated by comparing the observed frequency of agreement between annotators to the expected frequency of agreement by chance.",
            "It is calculated by comparing the observed frequency of agreement between annotators to the expected frequency of agreement by chance.",
            "Annotate data",
            "Krippendorff alpha coefficient"
          ]
        },
        {
          "title": "Methodological Notes",
          "url": "https://www.k-alpha.org/methodological-notes",
          "excerpts": [
            " a Krippendorff's Alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions based on the rated data.",
            "_ _:_ This range is often considered the lower bound for tentative conclusions. A Krippendorff's Alpha in this range suggests moderate agreement; thus, outcomes should be interpreted with concern, questioning the roots of such diverging ratings.",
            "Alpha = 1_ _:_ Indicates perfect agreement among raters. It is the scenario where all raters have provided the exact same ratings for each item evalua",
            "Alpha ≥ 0.80: This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's ...",
            "Data with a Krippendorff's Alpha below this threshold are often deemed unreliable for drawing triangulated conclusions. It suggests that the raters are not ..."
          ]
        },
        {
          "title": "The measurement of observer agreement for categorical data",
          "url": "https://pubmed.ncbi.nlm.nih.gov/843571/",
          "excerpts": [
            "by JR Landis · 1977 · Cited by 93933 — This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies."
          ]
        },
        {
          "title": "Inter-Annotator Agreement: An Introduction to Cohen's ...",
          "url": "https://surge-ai.medium.com/inter-annotator-agreement-an-introduction-to-cohens-kappa-statistic-dcc15ffa5ac4",
          "excerpts": [
            "It's calculated as (TP+TN)/N: TP is the number of true positives, i.e. the number of students Alix and Bob both passed. TN is the number of true ...",
            "Cohen's kappa statistic, which measures how often two raters agree with each other after accounting for the likelihood they'd agree by chance.",
            "In order to build a training dataset, you use a workforce of data labelers to read and grade your essays. Of course, you want to ensure that every rater applies the grading rubric consistently: if two raters read the same 10 essays, for example, you want to ensure that each essay receives a similar grade from both of them.",
            "Cohen’s kappa ranges from 1, representing perfect agreement between raters, to -1, meaning the raters choose different labels for every sample. A value of 0 means the raters agreed exactly as often as if they were both randomly guessing.",
            "s important to remember that there’s a difference between _consistent_ raters and _high-quality_ raters. Alix and Bob could easily achieve a Cohen’s kappa of 1 if they were both so lazy they decided to just auto-pass every single essay",
            "A high Cohen’s kappa might also reflect shared inappropriate biases — imagine if Alix and Bob both resented being forced to read Shakespeare in high school and automatically failed anyone who mentioned him. Similarly, negative values for Cohen’s kappa don’t automatically mean that one of the raters is low-quality. They might just have different values or interpretations of the labeling criteria."
          ]
        },
        {
          "title": "Fleiss' Kappa: Measuring Agreement Among Multiple Raters",
          "url": "https://datatab.net/tutorial/fleiss-kappa",
          "excerpts": [
            "Fleiss Kappa is a measure of inter-rater reliability. Definition: The Fleiss Kappa is a measure of how reliably three or more raters measure the same thing."
          ]
        },
        {
          "title": "Fleiss's kappa",
          "url": "https://en.wikipedia.org/wiki/Fleiss%27s_kappa",
          "excerpts": [
            "Whereas Scott's pi and Cohen's kappa work for only two raters, Fleiss's kappa works for any number of raters giving categorical ratings, to a fixed number ..."
          ]
        },
        {
          "title": "Statistical Methods for Annotation Analysis",
          "url": "https://direct.mit.edu/coli/article/49/3/763/116159/Statistical-Methods-for-Annotation-Analysis",
          "excerpts": [
            "Sep 1, 2023 — In 2022, Silviu Paun, Ron Artstein, and Massimo Poesio published a book addressing primarily the NLP community but also including other ..."
          ]
        },
        {
          "title": "Statistical Methods for Annotation Analysis",
          "url": "https://link.springer.com/book/10.1007/978-3-031-03763-4",
          "excerpts": [
            "by S Paun · Cited by 37 — This book is meant to provide a survey of the most widely used among these statistical methods supporting annotation practice."
          ]
        },
        {
          "title": "The adjudication process in collaborative annotation",
          "url": "https://medium.com/@jorgecp/the-adjudication-process-in-collaborative-annotation-61623c46b700",
          "excerpts": [
            "We define adjudication as the process to resolve inconsistencies among these versions before a version is promoted to the gold standard."
          ]
        },
        {
          "title": "Automatic adjudication based on the inter-annotator agreement",
          "url": "https://tagtog.medium.com/automatic-adjudication-based-on-the-inter-annotator-agreement-a62f49be7bcf",
          "excerpts": [
            "When annotators label the same data, adjudication is the process to resolve inconsistencies among the different versions and to promote a final ..."
          ]
        },
        {
          "title": "Natural Language Annotation for Machine Learning",
          "url": "https://www.oreilly.com/library/view/natural-language-annotation/9781449332693/ch06.html",
          "excerpts": [
            "A critical part of this stage is adjudication—where you take your annotators' work and use it to create the gold standard corpus that you will use for machine ..."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "by ML McHugh · 2012 · Cited by 21858 — As a general heuristic, sample sizes should not consist of less than 30 comparisons. Sample sizes of 1,000 or more are mathematically most likely to produce ...",
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            " However, this interpretation allows for very little agreement among raters to be described as “substantial”.",
            "For percent agreement, 61% agreement can immediately be seen as problematic.",
            "Almost 40% of the data in the dataset represent faulty data.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        },
        {
          "title": "What is the minimum sample size for using statistical test ...",
          "url": "https://www.researchgate.net/post/What_is_the_minimum_sample_size_for_using_statistical_test_Krippendorffs_alpha",
          "excerpts": [
            "Dec 12, 2021 — Hello everyone,. What is the minimum sample size for using statistical test Krippendorff's alpha? Thanks."
          ]
        },
        {
          "title": "What is Kappa and How Does It Measure Inter-rater ...",
          "url": "https://www.theanalysisfactor.com/kappa-measures-inter-rater-reliability/",
          "excerpts": [
            "The Kappa statistic is a measure of inter-rater reliability. There is no absolute value for good agreement and depends on the nature of the study."
          ]
        },
        {
          "title": "Krippendorff's alpha",
          "url": "https://en.wikipedia.org/wiki/Krippendorff%27s_alpha",
          "excerpts": [
            "Krippendorff's alpha is applicable to any number of coders, each assigning one value to one unit of analysis, to incomplete (missing) data, to any number of values available for coding a variable, to binary, nominal, ordinal, interval, ratio, polar, and circular metrics (note that this is not a metric in the mathematical sense, but often the square of a mathematical metric, see levels of measurement), and it adjusts itself to small sample sizes of the reliability data.",
            "The minimum acceptable alpha coefficient should be chosen according to the importance of the conclusions to be drawn from imperfect data.",
            "In the absence of knowledge of the risks of drawing false conclusions from unreliable data, social scientists commonly rely on data with reliabilities α ≥ 0.800, consider data with 0.800 > α ≥ 0.667 only to draw tentative conclusions, and discard data whose agreement measures α < 0.667.",
            "Cohen's kappa, by contrast, defines expected agreement in terms of contingencies, as the agreement that would be expected if coders were statistically independent of each other. Cohen's conception of chance fails to include disagreements between coders’ individual predilections for particular categories, punishes coders who agree on their use of categories, and rewards those who do not agree with higher kappa-values."
          ]
        },
        {
          "title": "Cohen's Kappa Sample Size",
          "url": "https://real-statistics.com/reliability/interrater-reliability/cohens-kappa/cohens-kappa-sample-size/",
          "excerpts": [
            "Describes how to calculate the power and minimum sample size required for Cohen's kappa in the case where there are two categories."
          ]
        },
        {
          "title": "Validity and Inter-Rater Reliability Testing of Quality Assessment Instruments",
          "url": "https://www.ncbi.nlm.nih.gov/books/NBK92287/table/executivesummary.t2/",
          "excerpts": [
            "Table BInterpretation of Fleiss' kappa (κ) (from Landis and Koch 1977) ; 0.0-0.20, Slight agreement ; 0.21-0.40, Fair agreement ; 0.41-0.60, Moderate agreement.",
            "| Κ | Interpretation |",
            "| --- | --- |",
            "| <0 | Poor agreement |",
            "| 0.0-0.20 | Slight agreement |",
            "| 0.21-0.40 | Fair agreement |",
            "| 0.41-0.60 | Moderate agreement |",
            "| 0.61-0.80 | Substantial agreement |",
            "| 0.81-1.0 | Almost perfect agreement |",
            "From: [Executive Summar"
          ]
        },
        {
          "title": "Tweet counts enterprise to Twitter API v2 migration guide | Docs",
          "url": "https://developer.x.com/en/docs/twitter-api/tweets/counts/migrate/enterprise-to-twitter-api-v2",
          "excerpts": [
            "| --- | --- |"
          ]
        },
        {
          "title": "PMC article on pilot feasibility studies and reliability",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8849521/",
          "excerpts": [
            "Interrater reliability should be established for interviewer-administered\nmeasures, and a certification form developed and tested in the pilot study."
          ]
        },
        {
          "title": "Leveraging Annotator Disagreement for Text Classification",
          "url": "https://arxiv.org/html/2409.17577v1",
          "excerpts": [
            "Sep 26, 2024 — In this article, we propose three different strategies to leverage annotator disagreement during the training of text classification models: a ..."
          ]
        },
        {
          "title": "Krippendorff's alpha in R for multi-label annotation",
          "url": "https://stats.stackexchange.com/questions/407453/krippendorffs-alpha-in-r-for-multi-label-annotation",
          "excerpts": [
            "Here is explained how to calculate Krippendorff's alpha for multi-label annotation using MASI distance using Python."
          ]
        },
        {
          "title": "Interrater agreement and interrater reliability: Key concepts ...",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S1551741112000642",
          "excerpts": [
            "Interrater reliability indices assess the extent to which raters consistently distinguish between different responses. A number of indices exist, and some ..."
          ]
        },
        {
          "title": "Krippendorff's Alpha",
          "url": "https://www.benchmarksixsigma.com/forum/topic/39347-krippendorffs-alpha/",
          "excerpts": [
            "Fleiss Kappa can be used only for nominal data whereas Krippendorff's Alpha can be used for continuous data too. Secondly, Krippendorff's Alpha ..."
          ]
        },
        {
          "title": "Establishing Annotation Quality in Multi-Label Annotations",
          "url": "https://aclanthology.org/2022.coling-1.322.pdf",
          "excerpts": [
            "Multi-label annotations, where multiple interpretations are possible, make inter-coder agreement challenging."
          ]
        },
        {
          "title": "A user-friendly tool for computing Krippendorff's Alpha inter-rater ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2215016123005411",
          "excerpts": [
            "Alpha ≥ 0.80 : This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's Alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions based on the rated data."
          ]
        },
        {
          "title": "Inter-annotator Agreement - DTIC",
          "url": "https://apps.dtic.mil/sti/trecms/pdf/AD1158943.pdf",
          "excerpts": [
            "by R Artstein · 2017 · Cited by 344 — This chapter touches upon several issues in the calculation and assessment of inter- annotator agreement. It gives an introduction to the theory behind ..."
          ]
        },
        {
          "title": "Establishing Annotation Quality in Multi-label Annotations",
          "url": "https://aclanthology.org/2022.coling-1.322/",
          "excerpts": [
            "We evaluate different metrics for calculating agreement on multi-label annotations: agreement on the intersection of annotated labels, an augmented version of ..."
          ]
        },
        {
          "title": "Any good metric for measuring multi-annotator agreement on ...",
          "url": "https://stats.stackexchange.com/questions/617547/any-good-metric-for-measuring-multi-annotator-agreement-on-an-imbalanced-dataset",
          "excerpts": [
            "Jun 1, 2023 — The idea behind Cohen's κ is to give context to the agreement rate by considering the agreement rate for a random annotater."
          ]
        },
        {
          "title": "Calculating multi-label inter-annotator agreement in Python",
          "url": "https://stackoverflow.com/questions/70708403/calculating-multi-label-inter-annotator-agreement-in-python",
          "excerpts": [
            "Can anyone recommend a particular metric/python library for assessing the agreement between 3 annotators when the data can be assigned a combination of labels?"
          ]
        },
        {
          "title": "Stack Overflow: Multi-label annotator agreement with Cohen's Kappa",
          "url": "https://stackoverflow.com/questions/52272901/multi-label-annotator-agreement-with-cohen-kappa",
          "excerpts": [
            "Cohen's Kappa does not support multi-label input. Instead of using Cohen's Kappa, one could use Krippendorff's Alpha.",
            "This measure supports inter-rater agreement, missing values and non-exclusive topics."
          ]
        },
        {
          "title": "Active Learning for BERT: An Empirical Study",
          "url": "https://aclanthology.org/2020.emnlp-main.638.pdf",
          "excerpts": [
            "by LE Dor · 2020 · Cited by 280 — Seven selection strategies are exam- ined over the 10 fully labeled binary classification datasets described above. The use of fully labeled."
          ]
        },
        {
          "title": "Active Learning for NLP with Large Language Models",
          "url": "https://arxiv.org/html/2401.07367v1",
          "excerpts": [
            "Jan 14, 2024 — This work investigates the accuracy and cost of using LLMs (GPT-3.5 and GPT-4) to label samples on 3 different datasets. A consistency-based ..."
          ]
        },
        {
          "title": "Platt scaling",
          "url": "https://en.wikipedia.org/wiki/Platt_scaling",
          "excerpts": [
            "In machine learning, Platt scaling or Platt calibration is a way of transforming the outputs of a classification model into a probability distribution over ..."
          ]
        },
        {
          "title": "Precision-Recall",
          "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html",
          "excerpts": [
            "The precision-recall curve shows the tradeoff between precision and recall for different thresholds. A high area under the curve represents both high recall ..."
          ]
        },
        {
          "title": "A Gentle Introduction to Threshold-Moving for Imbalanced ...",
          "url": "https://www.machinelearningmastery.com/threshold-moving-for-imbalanced-classification/",
          "excerpts": [
            "Jan 5, 2021 — In this tutorial, you will discover how to tune the optimal threshold when converting probabilities to crisp class labels for imbalanced classification."
          ]
        },
        {
          "title": "Choosing the Right Metrics: Recall, Precision, PR Curve ...",
          "url": "https://medium.com/@juanc.olamendy/choosing-the-right-metrics-recall-precision-pr-curve-and-roc-curve-explained-682259961cbe",
          "excerpts": [
            "By examining the curve, you can find the point where precision begins to drop significantly and set the threshold just before this drop."
          ]
        },
        {
          "title": "Improved Threshold Selection by Using Calibrated ...",
          "url": "https://ieeexplore.ieee.org/document/7158334",
          "excerpts": [
            "by F Baumann · 2015 · Cited by 1 — In this paper, we propose to introduce an additional condition for selecting the best variable leading to an improvement of the recognition accuracy."
          ]
        },
        {
          "title": "Instance-wise Temperature Scaling for Vision Transformers",
          "url": "https://arxiv.org/html/2508.08547v1",
          "excerpts": [
            "We propose Calibration Attention (CalAttn), a novel drop-in module designed to dynamically learn an adaptive, per-instance temperature directly ..."
          ]
        },
        {
          "title": "Model calibration for classification tasks using Python",
          "url": "https://medium.com/data-science-at-microsoft/model-calibration-for-classification-tasks-using-python-1a7093b57a46",
          "excerpts": [
            "We demonstrated isotonic regression which can be used when there is enough training data available and model is not pre-calibrated. We also ..."
          ]
        },
        {
          "title": "LR Threshold Calibration: Step-by-Step Guide (NumberAnalytics)",
          "url": "https://www.numberanalytics.com/blog/lr-threshold-calibration-step-guide",
          "excerpts": [
            "Threshold Selection?",
            "Defining Decision Thresholds",
            "In logistic regression, the model outputs a probability value between 0 and 1 for each data instance, representing the likelihood of the instance belonging to the positive class.",
            "To convert these probabilities into binary classifications, a decision threshold is applied.",
            "The choice of decision threshold can have a significant impact on the model's performance metrics, such as accuracy, precision, recall, and F1-score.",
            "There are several common methods for selecting the decision threshold in logistic regression:",
            "*Fixed Threshold**: A common default is to use a threshold of 0.5, which assumes that instances with a probability greater than or equal to 0.5 are classified as positive.",
            "*Optimal Threshold**: The threshold can be optimized based on a specific performance metric, such as maximizing the F1-score or minimizing the overall classification error."
          ]
        },
        {
          "title": "Confidence Statements Associated With Sampling Plans",
          "url": "https://variation.com/confidence-statements-associated-with-sampling-plans/",
          "excerpts": [
            "The AQL and LTPD0.10 represent special cases of percentiles of the OC curve. The AQL is the 95th percentile while the LTPD0.10 is the 10th percentile. Other ..."
          ]
        },
        {
          "title": "Selecting Statistically Valid Sampling Plans - Taylor Enterprises",
          "url": "https://variation.com/selecting-statistically-valid-sampling-plans/",
          "excerpts": [
            "OC curves are generally summarized by two numbers: the Acceptable Quality Level (AQL) and Lot Tolerance Percent Defective (LTPD). The AQL is that percent ..."
          ]
        },
        {
          "title": "confidence interval - Revisiting the Rule of Three - Cross Validated",
          "url": "https://stats.stackexchange.com/questions/497548/revisiting-the-rule-of-three",
          "excerpts": [
            "The rule of three is a method for calculating a 95% confidence interval when estimating p from a set of n IID Bernoulli trials with no successes."
          ]
        },
        {
          "title": "All statistics and graphs for Attributes Acceptance Sampling",
          "url": "https://support.minitab.com/en-us/minitab/help-and-how-to/quality-and-process-improvement/acceptance-sampling/how-to/attributes-acceptance-sampling/interpret-the-results/all-statistics-and-graphs/",
          "excerpts": [
            "This means that approximately 95% of the time, you will correctly accept a lot with a defective rate of 1.5% or less, and 5% of the time, you will incorrectly ..."
          ]
        },
        {
          "title": "Heavy-tailed distribution",
          "url": "https://en.wikipedia.org/wiki/Heavy-tailed_distribution",
          "excerpts": [
            "In probability theory, heavy-tailed distributions are probability distributions whose tails are not exponentially bounded."
          ]
        },
        {
          "title": "Distribution of retweet counts in the dataset",
          "url": "https://www.researchgate.net/figure/Distribution-of-retweet-counts-in-the-dataset_fig4_268980781",
          "excerpts": [
            "The threshold for the number of retweets was identified through distributional analysis of retweet counts per tweet (see Figure 4) and empirical tests with ..."
          ]
        },
        {
          "title": "On the Frequency Distribution of Retweets",
          "url": "https://www.sciencedirect.com/science/article/pii/S1877050914005006",
          "excerpts": [
            "by Y Lu · 2014 · Cited by 49 — In this paper, we present a hypothesis that the frequency distribution of retweets follows a power law distribution asymptotically by analyzing the retweets ..."
          ]
        },
        {
          "title": "An overview of heavy-tail extensions of multivariate ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9542722/",
          "excerpts": [
            "by S Park · 2022 · Cited by 5 — The family of heavy-tailed distributions covers distributions having tail-decay slower than that of the Gaussian; for example, multivariate t distribution."
          ]
        },
        {
          "title": "Power Laws & Heavy Tail Distributions",
          "url": "https://www.di.fc.ul.pt/~jpn/r/powerlaw/powerlaw.html",
          "excerpts": [
            "In many applications it is the right tail of the distribution that is of interest, but a distribution may have a heavy left tail, or both tails may be heavy."
          ]
        },
        {
          "title": "Measuring and Detecting Virality on Social Media",
          "url": "https://arxiv.org/abs/2303.06120",
          "excerpts": [
            "by T Elmas · 2023 · Cited by 16 — We find that a tweet is more likely to be classified as viral by Twitter if the ratio of retweets to its author's followers exceeds some threshold."
          ]
        },
        {
          "title": "(PDF) Improved Twitter Virality Prediction using Text and ...",
          "url": "https://www.researchgate.net/publication/355379921_Improved_Twitter_Virality_Prediction_using_Text_and_RNN-LSTM",
          "excerpts": [
            "PDF | The matter of influence and virality in social media has been studied since the popularity explosion of these platforms. A gargantuan amount of."
          ]
        },
        {
          "title": "Virality Prediction and Community Structure in Social Networks",
          "url": "https://arxiv.org/pdf/1306.0158",
          "excerpts": [
            "ur\n\ncommunity-based prediction excels in both precision and recall, indicating that communities are helpful in\n\ncapturing viral memes (Fig. 5)",
            "For example, when detecting the most viral memes by users (θU = 90),\n\nour method is about seven times as precise as random guess and over three times as precise as prediction\n\nwithout community features. We achieve a recall over 350% better than random guess and over 200% better\n\nthan community-blind predicti",
            "statistics based on early popularity and three types of community-based features in the prediction model,\n\nlisted below. 1. Basic features based on early popularity. Two basic statistical features are included in the\n\n\t prediction model. The number of early adopters is the number of distinct users who generated the\n\n\t earliest tweets. The number of uninfected neighbors of early adopters characterizes the set of users who\n\n\t can adopt the meme during the next step. 2. Infected communities.\nThe simplest feature related to communities is the number of infected com-\n\n\t munities, i.e., the number of communities containing early adopters. 3. Usage and adoption entropy. H t (h) and H u (h) are good indicators of the strength of meme\n\n\t concentration, as shown in Fig. 3. 5\n\f  4. Fraction of intra-community user interactions. We count pair-wise user interactions about any\n\n\t  given meme, and calculate the proportion that occur between people in the same",
            "Two baselines are set up for comparison. Random guess selects nviral memes at random,\n\nwhere nviral is the number of viral memes in the actual data. Community-blind prediction employs the\n\nsame learning algorithm as ours but without the community-based features. We compute both precision\n\nand recall for evaluation; the former measures the proportion of predicted viral memes that are actually\n\nviral in the real data, and the latter quantifies how many of the viral memes are correctly predic",
            "Our\n\ncommunity-based prediction excels in both precision and recall, indicating that communities are helpful in\n\ncapturing viral memes (Fig. 5",
            "We show that (i) communities allow us\n\nto estimate how much the spreading pattern of a meme deviates from that of infectious diseases; (ii) viral\n\nmemes tend to spread like epidemics; and finally (iii) we can predict the virality of memes based on early\n\nspreading patterns in terms of community structu"
          ]
        },
        {
          "title": "Near-Duplicate Tweet Detection (Fagbemi 2014)",
          "url": "https://csserver.ucd.ie/~meloc/MScASE/resources/Damilare_Fagbemi.pdf",
          "excerpts": [
            "Similarity Technique\t\t  Similarity Threshold",
            "The clustering system incorporates 4 similarity measurement techniques namely\nLevenshtein distance, Jaccards coefficient, Minhashing, and Hamming distance.",
            "\t\t\t\t  Minhashing\t\t\t\t\t\t 0.10",
            "\t\t\t\tHamming Distance\t\t\t\t\t 0.65",
            ". The experiments\nwere conducted using different representations of such tweets.\nThis helped us\nto identify what similarity threshold was necessary to correctly identify near-\nduplicates given a specific combination of tweet representation and similarity\nmeasure",
            ". Hence, it is not possible to use one threshold for all tweet comparisons. In order to determine the appropriate thresholds for different combinations of\ntweet representations and similarity techniques, we ran a series of similarity\nscoring experiments on existing near-duplicate tweet pairs.",
            "Twitter has become a very popular micro blogging tool used for the expression of\nviews and to broadcast news or events.",
            "Near-duplicate detection in Twitter is of increasing importance\ndue to the primary role it plays in first story detection, spam detection, and\nmany other clustering processes.",
            "We decided to treat\ntweets as more conventional text pieces. Our idea being the clustering of tweets\nbased on pure text analytics.",
            "We also designed and/or implemented 3 clustering strategies: Naive Pairwise\n(NPW), Exact Duplicate Filtering (EDF), and Multi-Index Locality Sensitive Hash-\ning (LSH",
            "o discover the optimal text representation for detecting near-\nduplicates in the context of Twitter, we utilized 8 tweet representations namely\n\n\t\t\t\t\t\t\t\t\t\t51\n\fFull text, Filtered punctuation text, Stopped and stemmed text, Full shingles,\nFiltered punctuation shingles, Stopped and stemmed shingles, minhash signa-\ntures,"
          ]
        },
        {
          "title": "Noisy Text: Proceedings of the ACL 2015 Workshop on Noisy User-generated Text",
          "url": "https://noisy-text.github.io/2015/pdf/WNUT12.pdf",
          "excerpts": [
            "Due to Twitter's fre- quently informal tone, text normalization can be a crucial element for exploiting that infor- mati"
          ]
        },
        {
          "title": "DashReza7/sentence-transformers_paraphrase- ...",
          "url": "https://huggingface.co/DashReza7/sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2_FINETUNED_on_torob_data_v5",
          "excerpts": [
            "This is a sentence-transformers model finetuned from sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2. ... Cosine Accuracy Threshold on Unknown.See more"
          ]
        },
        {
          "title": "MinHash LSH — datasketch 1.6.5 documentation",
          "url": "http://ekzhu.com/datasketch/lsh.html",
          "excerpts": [
            "The Jaccard similarity threshold must be set at initialization, and cannot be changed. So does the number of permutation functions ( num_perm ) parameter."
          ]
        },
        {
          "title": "How can you perform paraphrase mining using Sentence ...",
          "url": "https://milvus.io/ai-quick-reference/how-can-you-perform-paraphrase-mining-using-sentence-transformers-to-find-duplicate-or-semantically-similar-sentences-in-a-large-corpus",
          "excerpts": [
            "Thresholds depend on your use case: a lower threshold (0.7) captures more paraphrases but risks false positives, while a higher threshold (0.9) ensures ...",
            "To perform paraphrase mining using Sentence Transformers, you first encode sentences ... threshold (e.g., 0.85 cosine similarity) to flag potential paraphrases."
          ]
        },
        {
          "title": "Semantic Textual Similarity",
          "url": "https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html",
          "excerpts": [
            "For Semantic Textual Similarity (STS), we want to produce embeddings for all texts involved and calculate the similarities between them.",
            "Dot product on normalized embeddings is equivalent to cosine similarity, but “cosine” will re-normalize the embeddings again. As a result, the “dot” metric ..."
          ]
        },
        {
          "title": "A Comprehensive Benchmark for Evaluating Paraphrase Detection ...",
          "url": "https://arxiv.org/abs/2409.12060",
          "excerpts": [
            "We create PARAPHRASUS, a benchmark designed for multi-dimensional assessment, benchmarking and selection of paraphrase detection models."
          ]
        },
        {
          "title": "[PDF] A Comprehensive Benchmark for Evaluating Paraphrase Detection ...",
          "url": "https://aclanthology.org/2025.coling-main.585.pdf",
          "excerpts": [
            "We present PARAPHRASUS, a multi-faceted evaluation benchmark for paraphrase detec- tion, including datasets of human-written sen- tence pairs of ..."
          ]
        },
        {
          "title": "Quoting, Paraphrasing, and Summarizing - Purdue OWL",
          "url": "https://owl.purdue.edu/owl/research_and_citation/using_research/quoting_paraphrasing_and_summarizing/index.html",
          "excerpts": [
            "This handout is intended to help you become more comfortable with the uses of and distinctions among quotations, paraphrases, and summaries."
          ]
        },
        {
          "title": "How does software that checks for plagiarism work? - Quora",
          "url": "https://www.quora.com/How-does-software-that-checks-for-plagiarism-work",
          "excerpts": [
            "Plagiarism checking software works by looking for structural patterns or unique identifiers. Well built systems usually have two or more phases."
          ]
        },
        {
          "title": "In-Text Citations: Quotations vs. Paraphrasing",
          "url": "https://www.utep.edu/uwc/the-writing-mine/articles/in-text-citations-quotations-vs-paraphrasing.html",
          "excerpts": [
            "In this post, we aim to shed light on the differences between the two methods in order to demonstrate their respective strengths along with their optimal use."
          ]
        },
        {
          "title": "Quoting and Paraphrasing - The Writing Center",
          "url": "https://writing.wisc.edu/handbook/quotingsources/",
          "excerpts": [
            "Begin longer quotations (for instance, in the APA system, 40 words or more) on a new line and indent the entire quotation (i.e., put in block form), with no ..."
          ]
        },
        {
          "title": "Stylometry recognizes human and LLM-generated texts in ...",
          "url": "https://arxiv.org/html/2507.00838v1",
          "excerpts": [
            "Jul 1, 2025 — The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans."
          ]
        },
        {
          "title": "Stylometry and forensic science: A literature review - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11707938/",
          "excerpts": [
            "by V Cammarota · 2024 · Cited by 3 — The article focuses on a careful description of literature on stylometry and on its potential use in forensic science.",
            "by V Cammarota · 2024 · Cited by 3 — What must also be tested is the minimum length that the questioned text should have in order to guarantee robust results, as well as the minimum amount of ..."
          ]
        },
        {
          "title": "Abirate/english_quotes · Datasets at Hugging Face",
          "url": "https://huggingface.co/datasets/Abirate/english_quotes",
          "excerpts": [
            "english_quotes is a dataset of all the quotes retrieved from goodreads quotes. This dataset can be used for multi-label text classification and text generation."
          ]
        },
        {
          "title": "Stylometry recognizes human and LLM-generated texts in ...",
          "url": "https://arxiv.org/abs/2507.00838",
          "excerpts": [
            "by K Przystalski · 2025 · Cited by 2 — The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans.See more"
          ]
        },
        {
          "title": "Privacy Issues in Stylometric Methods",
          "url": "https://www.mdpi.com/2410-387X/6/2/17",
          "excerpts": [
            "by A Patergianakis · 2022 · Cited by 3 — Stylometric techniques offer several benefits in fields such as reliable authorship attribution as well as in copyright investigation or in detecting harmful ..."
          ]
        },
        {
          "title": "Explainable Authorship Verification in Social Media via Attention ...",
          "url": "https://arxiv.org/abs/1910.08144",
          "excerpts": [
            "Authorship verification is the task of analyzing the linguistic patterns of two or more texts to determine whether they were written by the same author or not."
          ]
        },
        {
          "title": "PAN at CLEF 2023 - Authorship Verification - Webis Group",
          "url": "https://pan.webis.de/clef23/pan23-web/author-identification.html",
          "excerpts": [
            "Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles."
          ]
        },
        {
          "title": "PAN at CLEF 2022 - Authorship Verification - Webis Group",
          "url": "https://pan.webis.de/clef22/pan22-web/author-identification.html",
          "excerpts": [
            "Authorship verification is the task of deciding whether two texts have been written by the same author based on comparing the texts' writing styles."
          ]
        },
        {
          "title": "Shingling for Similarity and Plagiarism Detection",
          "url": "https://dzone.com/articles/shingling-for-similarity-and-plagiarism-detection",
          "excerpts": [
            "Shingling is a widely used technique in detecting and mitigating textual similarities.",
            "This article introduces you to the concept of shingling, the basics of shingling technique, Jaccard similarity, advanced techniques, and optimizations",
            "Documents are first converted into sets of k-shingles, which are contiguous sequences of k tokens (words or characters) extracted from the text.",
            "Minhashing then compresses these shingle sets into compact signatures, preserving similarity between documents.",
            "banding splits the minhash signatures into multiple bands, and bucketing hashes each band into buckets, grouping similar documents together.",
            "This process generates candidate pairs, which are pairs of documents that share at least one bucket across all bands, significantly reducing the number of document pairs that need to be compared for similarity.",
            "The actual similarity computation is then performed only on the candidate pairs, using the original minhash signatures to estimate the Jaccard similarity.",
            " search. Overall, the combination of shingling, minhashing, banding, and LSH offers a powerful and efficient solution for plagiarism detection and near-duplicate identification, with applications across academia, publishing, and content management systems"
          ]
        },
        {
          "title": "Minhash and LSH: Large scale document similarity search",
          "url": "https://mrhasankthse.github.io/riz/2020/03/19/Minhash-and-LSH.html",
          "excerpts": [
            "Minhash and LSH are such algorithms that can compare and search similar documents in large corpus.",
            "These kinds of applications are usually useful to the News Agencies where they need to recognize that a group of articles are really all based on the same article about one particular story. Other usage could be Plagiarism detection, mirror page identification etc.",
            "Let’s start a completely different approach (fig:1) to make this whole process more efficient - instead of comparing the raw document we can compare a compressed representation of the documents.",
            "LSH - locality sensitive hashing is just for this purpose."
          ]
        },
        {
          "title": "DH2017 Abstract 341",
          "url": "https://dh2017.adho.org/abstracts/341/341.pdf",
          "excerpts": [
            "The question of minimal sample size is one of the most important issues in stylometry and non- traditional authorship attribution."
          ]
        },
        {
          "title": "Authorship Verification for Short Messages Using Stylometry",
          "url": "https://www.researchgate.net/publication/261045998_Authorship_verification_for_short_messages_using_stylometry",
          "excerpts": [
            "Our evaluation yields an EER of 14.35%, which is very\n\nencouraging considering the existing works on authorship ve-\n\nriﬁcation using stylomet",
            "The rest of the paper is structured as follows. Section II\n\nsummarizes and discusses related works. Section III introduces\n\nour proposed approac",
            "Our technique is based on a combination of super-\n\nvised learning and n-gram analys",
            "These hurdles must be addressed for stylometry to be usable in checking authorship of online messages such as emails, text messages, or twitter feeds.",
            "Authorship verification can be checked using stylometric techniques through the analysis of linguistic styles and writing characteristics of the authors.",
            "Stylometry is a behavioral feature that a person exhibits during writing and can be extracted and used potentially to check the identity of the author of online documents.",
            "Although stylometric techniques can achieve high accuracy rates for long documents, it is still challenging to identify an author for short documents, in particular when dealing with large authors populations."
          ]
        },
        {
          "title": "Winnowing: Local Algorithms for Document Fingerprinting",
          "url": "https://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf",
          "excerpts": [
            "by S Schleimer · 2003 · Cited by 1938 — Winnowing is an efficient local document fingerprinting algorithm that uses a window of hashes to detect at least one k-gram in shared substrings.",
            "by S Schleimer · 2003 · Cited by 1938 — We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33% of the lower bound. Finally, we also ...",
            "by S Schleimer · 2003 · Cited by 1938 — In this section we describe and analyze the winnowing algorithm for selecting fingerprints from hashes of k-grams. We give an upper bound on the performance of ..."
          ]
        },
        {
          "title": "Testing of support tools for plagiarism detection",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00192-4",
          "excerpts": [
            "This paper reports on a collaborative test of 15 web-based text-matching systems that can be used when plagiarism is suspected."
          ]
        },
        {
          "title": "[PDF] A Survey of Plagiarism Detection Systems - arXiv",
          "url": "https://arxiv.org/pdf/2201.03423",
          "excerpts": [
            "We define a plagiarism detection system as software that offers the necessary data and information to support a human reviewer in asserting whether a document, ..."
          ]
        },
        {
          "title": "Faculty Members' Perceptions and Attitudes Towards Anti ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9992686/",
          "excerpts": [
            "To combat plagiarisms, many universities are currently using anti-plagiarism detection tools (APTs). Recent studies have shown that more than 50% of American ..."
          ]
        },
        {
          "title": "Notes of MOSS and Followups | Programmer's Musings",
          "url": "https://alexkassil.github.io/2020/08/23/Notes-on-Moss-and-Followups.html",
          "excerpts": [
            "Aug 23, 2020 — A MOSS Tool Addressing Plagiarism at Scale. The original Moss project works great for single documents you want to check for plagiarism against ..."
          ]
        },
        {
          "title": "Winnowing: Local Algorithms for Document Fingerprinting",
          "url": "https://www.researchgate.net/publication/2840981_Winnowing_Local_Algorithms_for_Document_Fingerprinting",
          "excerpts": [
            "ArticlePDF Available. Winnowing: Local Algorithms for Document Fingerprinting. April 2003. DOI:10.1145/872757.872770. Authors: Saul David Schleimer at ...See more"
          ]
        },
        {
          "title": "A Plagiarism Detection Algorithm based on Extended ...",
          "url": "https://www.matec-conferences.org/articles/matecconf/pdf/2017/42/matecconf_eitce2017_02019.pdf",
          "excerpts": [
            "by X Duan · 2017 · Cited by 13 — This paper introduces the method of extending classic Winnowing plagiarism detection algorithm, expands the algorithm in functionality. The extended algorithm ..."
          ]
        },
        {
          "title": "Content similarity detection - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Content_similarity_detection",
          "excerpts": [
            "Citation-based plagiarism detection using citation pattern analysis is capable of identifying stronger paraphrases and translations with higher success rates ..."
          ]
        },
        {
          "title": "CrossCheck Plagiarism Screening: Understanding the ...",
          "url": "https://www.ithenticate.com/plagiarism-detection-blog/bid/63534/CrossCheck-Plagiarism-Screening-Understanding-the-Similarity-Score",
          "excerpts": [
            "CrossCheck members use the iThenticate plagiarism detection system to screen submitted papers for originality and can quickly tell whether a paper contains ..."
          ]
        },
        {
          "title": "How Is AI (Really) Changing Peer Review?",
          "url": "https://paeditorial.co.uk/post/how-is-ai-really-changing-peer-review-and-do-humans-still-matter/",
          "excerpts": [
            "Plagiarism detection is perhaps the best known. Tools like Turnitin, iThenticate, and Crossref Similarity Check scan new manuscripts against vast databases ..."
          ]
        },
        {
          "title": "Plagiarism detection | Editors",
          "url": "https://www.elsevier.com/editor/perk/plagiarism-complaints/plagiarism-detection",
          "excerpts": [
            "Detecting plagiarism with the help of CrossRef Similarity Check and Editorial Manager's Duplicate Submission Check."
          ]
        },
        {
          "title": "Plagiarism detection and paraphrase plagiarism identification (Educational Technology Journal)",
          "url": "https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00277-8",
          "excerpts": [
            "In this research we propose methods to identify two important paraphrase types – synonymous substitution and word reordering in paraphrased, plagiarised sentence pairs.",
            "Plagiarism has been on the rise with the widespread availability of digital information and the ease with which it can be copied. Recent and past surveys suggest an increase in cases of plagiarism in both academic work and scientific literature",
            ") ). _Paraphrase plagiarism_ (Carmona et al. [2018](/articles/10.1186/s41239-021-00277-8 \"Carmona, M. Á. Á., Franco-Salvador, M., Villatoro-Tello, E., Montes-y-Gómez, M., Rosso, P., & Pineda, L. V. \\\\(2018\\\\). Semantically-informed distance and similarity measures for paraphrase plagiarism identification. Journal of Intelligent and Fuzzy Systems, 34\\\\(5\\\\), 2983–2990.\") )",
            " synonymous substitution and word reordering in paraphrased, plagiarised sentence pairs. We propose a three staged approach that uses context matching and pretrained word embeddings for identifying synonymous",
            "Synonymous substitution, word reordering and insertion/deletion have been identified as some of the common paraphrasing strategies used by plagiarists.",
            "Plagiarism detection refers to techniques, tools and methods used for automated detection of plagiarism, since manual detection becomes infeasible with large amounts of information.",
            "In this section we provide a brief overview of various approaches proposed for the detection of plagiarism and paraphrase plagiarism. In particular, approaches based on character and word _n_ \\-gram similarity (Bensalem et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Bensalem, I., Rosso, P., & Chikhi, S. \\\\(2019\\\\). On the use of character n-grams as the only intrinsic evidence of plagiarism.\n ... \nIn: Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, \\\\(pp 182–190\\\\).\") ) and alignment algorithms (Nichols et al. [2019](/articles/10.1186/s41239-021-00277-8 \"Nichols, L., Dewey, K., Emre, M., Chen, S., & Hardekopf, B. \\\\(2019\\\\). Syntax-based improvements to plagiarism detectors and their evaluations. In Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education, Association of Computing Machinery.\") ) have been successfully applied towards plagiaris",
            "The output of the plagiarism detection module (matching sections of text) can be sent as input to the paraphrase type identification module."
          ]
        },
        {
          "title": "Plagiarism in a submitted manuscript - COPE Flowchart",
          "url": "https://publicationethics.org/guidance/flowchart/plagiarism-submitted-manuscript",
          "excerpts": [
            "A reviewer suspects plagiarism in a submitted manuscript. This flowchart provides a step by step process to help editors handle this problem. [Plagiarism](/guidance?f%5B0%5D=topics%3A25)",
            "\n\nThe journal should ask the reviewer for full documentary evidence and investigate the concerns. Authors should be contacted where there is clear plagiarism or copying, explaining the journal’s process and next steps. The author's institution should be informed, where necessary.",
            "Authors should be contacted where there is clear plagiarism or copying, explaining the journal’s process and next steps.",
            "The author's institution should be informed, where necessary.",
            "If the author has copied from their own work, refer to the flowchart redundant (duplicate) publication in a submitted manuscript."
          ]
        },
        {
          "title": "Crossref Similarity Check",
          "url": "https://www.crossref.org/services/similarity-check/",
          "excerpts": [
            "A service provided by Crossref and powered by iThenticate—Similarity Check provides editors with a user-friendly tool to help detect plagiarism.",
            "Similarity Check allows editors to upload a paper, and instantly produces a report highlighting potential matches and indicating if and how the paper overlaps with other work.",
            "This report enables editors to assess the originality of the work before they publish it, providing confidence for publishers and authors, and evidence of trust for readers.",
            "If you participate in Similarity Check, not only do you get reduced rate access to iThenticate, but you also have the peace of mind of knowing that any similarity between your published content and manuscripts checked by other publishers will be flagged as a potential issue too.",
            "as the iThenticate database contains over 78 million full-text scholarly content items, editors can be confident that Similarity Check will provide a comprehensive and reliable addition to their workflow."
          ]
        },
        {
          "title": "Text recycling guidelines for editors",
          "url": "https://publicationethics.org/guidance/endorsed-guidance/text-recycling-guidelines-editors",
          "excerpts": [
            "\n\nA common issue encountered by editors is overlap of text with an author’s own previously published work, particularly with the increasing use of plagiarism detection software. This practice is known as ‘text recycling’ (also sometimes referred to as ‘self-plagiarism’).",
            ". These guidelines were developed by BioMed Central.",
            ". The guidelines cover how to deal with text recycling both in a submitted manuscript and a published article and include situations where text recycling may be acceptable as well as those where it is unlikely to be."
          ]
        },
        {
          "title": "iThenticate Crossref Similarity Check FAQS",
          "url": "https://www.ithenticate.com/training-crosscheck-faqs",
          "excerpts": [
            "A similarity report is a tool that helps editors identify potential scholarly misconduct in author manuscripts.",
            "The Similarity Report identifies and highlights every section of text within the manuscript that was found to match the iThenticate content repositories and produces an overall similarity percentage.",
            "the available exclusion options in order to remove matches to the bibliography, quoted text, abstract and methods sections, and small matches in order to display more relevant matches and display a more accurate similarity percentage.",
            "The biblliography exclusion excludes all text after the Bibliography keyword heading.",
            "Editors should contact their publishers if further clarification is needed on how to proceed with a manuscript determined to be plagiarized."
          ]
        },
        {
          "title": "util — Sentence Transformers documentation",
          "url": "https://sbert.net/docs/package_reference/util.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ( ..."
          ]
        },
        {
          "title": "A Comparative Analysis of Plagiarism Detection Algorithms",
          "url": "https://irojournals.com/tcsst/article/pdf/5/3/1",
          "excerpts": [
            "The winnowing algorithm can only calculate the similarity rate between documents, whereas the extended algorithm can mark the plagiarized text ...",
            "Jul 5, 2023 — The winnowing algorithm process includes first text preprocessing followed by forming k- grams of length k. After that, those k-gram hash values ..."
          ]
        },
        {
          "title": "What percentage of plagiarism is acceptable on Turnitin?",
          "url": "https://www.quora.com/What-percentage-of-plagiarism-is-acceptable-on-Turnitin",
          "excerpts": [
            "Regardless, plagiarism may exist at any score, 0 through 100 percent. Those who check Turnitin scores are concerned about (a) originality, and ( ..."
          ]
        },
        {
          "title": "Evaluation — Sentence Transformers documentation",
          "url": "https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html",
          "excerpts": [
            "Given a large set of sentences, this evaluator performs paraphrase (duplicate) mining and identifies the pairs with the highest similarity. It compare the ...",
            "... Threshold: 0.8352 ... query_chunk_size (int, optional) – To identify the paraphrases, the cosine-similarity between all sentence-pairs will be computed.See more"
          ]
        },
        {
          "title": "PAN at CLEF 2016 - Webis Group",
          "url": "https://pan.webis.de/clef16/pan16-web/",
          "excerpts": [
            "This session presents illustrative cases of translingual plagiarism and discusses some of the approaches adopted by forensic linguists.See more"
          ]
        },
        {
          "title": "A Semantic Similarity Approach to Paraphrase Detection",
          "url": "https://www.researchgate.net/publication/228616213_A_Semantic_Similarity_Approach_to_Paraphrase_Detection",
          "excerpts": [
            "This paper presents a novel approach to the problem of paraphrase identification. Al-though paraphrases often make use of syn-onymous or near synonymous ..."
          ]
        },
        {
          "title": "An Improved Impostors Method for Authorship Verification",
          "url": "https://www.researchgate.net/publication/319138439_An_Improved_Impostors_Method_for_Authorship_Verification",
          "excerpts": [
            "In this paper, we propose a modification of the Impostors method that focuses on both appropriate selection of impostor documents and enhanced comparison of ..."
          ]
        },
        {
          "title": "[PDF] An Improved Impostors Method for Authorship Verification",
          "url": "https://icsdweb.aegean.gr/stamatatos/papers/CLEF-Potha-2017.pdf",
          "excerpts": [
            "In this paper, we propose a modification of the Impostors method that focuses on both appropriate selection of impostor documents and enhanced comparison of ..."
          ]
        },
        {
          "title": "Authorship attribution using author profiling classifiers",
          "url": "https://www.cambridge.org/core/journals/natural-language-engineering/article/authorship-attribution-using-author-profiling-classifiers/9ADB32F9096C1212E8099BF016B0B218",
          "excerpts": [
            "by C Deutsch · 2023 · Cited by 16 — Authorship attribution has been a popular research topic in Natural Language Processing and the subject of several shared tasks in the PAN-CLEF ...See more"
          ]
        },
        {
          "title": "Analysis of Stylometric Variables in Long and Short Texts",
          "url": "https://www.sciencedirect.com/science/article/pii/S1877042813042080/pdf?md5=47d0c5e358cae45dbb22d87c76b7626e&pid=1-s2.0-S1877042813042080-main.pdf",
          "excerpts": [
            "by F López-Escobedo · 2013 · Cited by 28 — We conclude that the length of texts is a factor that affects the discriminatory capacity of the stylometric variables. We also found that there are certain ..."
          ]
        },
        {
          "title": "Understanding the Similarity Report",
          "url": "https://guides.ithenticate.com/hc/en-us/articles/27842305774605-Understanding-the-Similarity-Report",
          "excerpts": [
            "Jun 27, 2025 — Instructors can opt to exclude quotes from the Similarity Report to lower similarity scores where applicable. Example 4: A student has submitted ..."
          ]
        },
        {
          "title": "Audit Trail Checklist for 2025 (With Examples) - Sprinto",
          "url": "https://sprinto.com/blog/audit-trail/",
          "excerpts": [
            "An audit trail is a detailed series of records of human activities, application processes, data flows, system snapshots, transactions, administrative changes, ..."
          ]
        },
        {
          "title": "Plagiarism - Editorial Guidance Resource - Confluence",
          "url": "https://documentation.cochrane.org/egr/plagiarism-318472381.html",
          "excerpts": [
            "Editors are encouraged to, at minimum, check at least a portion of text for all protocols and reviews (including updates) when initially submitted."
          ]
        },
        {
          "title": "Plagiarism - Editorial Policies - Author Services - Taylor & Francis",
          "url": "https://authorservices.taylorandfrancis.com/editorial-policies/plagiarism/",
          "excerpts": [
            "Taylor & Francis takes the issue of plagiarism very seriously. Find out what plagiarism is (and isn't) and how you can avoid it."
          ]
        },
        {
          "title": "How to interpret the Turnitin Similarity Report (Instructors)",
          "url": "https://extensionhelpcenter.ucsd.edu/hc/en-us/articles/27459870154893-How-to-interpret-the-Turnitin-Similarity-Report-Instructors",
          "excerpts": [
            " Above all, it is important to understand that a high similarity score does not always suggest that a piece of writing has been plagiarized, and neither does a low similarity score always indicate that no plagiarism has occurred—in short, there is no ideal cutoff percentage or threshold . Therefore, the Turnitin Similarity Report is not a replacement for instructor expertise and manual review.",
            "Manual Review of Each Report is Essential: A thorough review of all flagged areas is crucial."
          ]
        },
        {
          "title": "Authorship verification for short messages using stylometry",
          "url": "https://ieeexplore.ieee.org/document/6705711",
          "excerpts": [
            "Authorship verification can be checked using stylometric techniques through the analysis of linguistic styles and writing characteristics of the authors.",
            "Stylometry is a behavioral feature that a person exhibits during writing and can be extracted and used potentially to check the identity of the author of online documents.",
            "Although stylometric techniques can achieve high accuracy rates for long documents, it is still challenging to identify an author for short documents, in particular when dealing with large authors populations.",
            "In this paper, we pose some steps toward achieving that goal by proposing a supervised learning technique combined with n-gram analysis for authorship verification in short texts.",
            "Experimental evaluation based on the Enron email dataset involving 87 authors yields very promising results consisting of an Equal Error Rate (EER) of 14.35% for message blocks of 500 characters."
          ]
        },
        {
          "title": "Plagiarism in a published article",
          "url": "https://publicationethics.org/guidance/flowchart/plagiarism-published-article",
          "excerpts": [
            "A reader suspects plagiarism in a published article. This flowchart provides a step by step process to help editors handle this problem.",
            "The journal's instructions to authors should include a definition of plagiarism and state the journal’s policy on plagiarism. Authors should be contacted explaining the journal’s process and next steps. The journal should consider whether a retraction or correction is required, depending on the degree of copying. Where necessary, editors of other journals involved should be informed.",
            "COPE Flowcharts and infographics — Plagiarism in a published article — English."
          ]
        },
        {
          "title": "Finding near-duplicates with Jaccard similarity and MinHash",
          "url": "https://blog.nelhage.com/post/fuzzy-dedup/",
          "excerpts": [
            "Jul 3, 2024 — In this post I want to explore the method of approximate deduplication via Jaccard similarity and the MinHash approximation trick."
          ]
        },
        {
          "title": "Searching for Near Duplicates with Minhash",
          "url": "https://skeptric.com/minhash-lsh/",
          "excerpts": [
            "May 9, 2020 — LSH can work really well as an online algorithm to efficiently check for near-duplicates in a large corpus, by storing and adding to these band hash tables."
          ]
        },
        {
          "title": "Near-duplicate Detection with Locality-Sensitive Hashing and ...",
          "url": "https://yorko.github.io/2023/practical-near-dup-detection/",
          "excerpts": [
            "Jun 27, 2023 — In this post, I review Locality-Sensitive Hashing for near-duplicate detection. I demonstrate the principle and provide a quick intro to Datasketch."
          ]
        },
        {
          "title": "Paraphrase Mining — Sentence Transformers documentation",
          "url": "https://sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html",
          "excerpts": [
            "It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score. Parameters: model ..."
          ]
        },
        {
          "title": "Implementation of Winnowing Algorithm with Dictionary ...",
          "url": "https://thesai.org/Downloads/Volume9No5/Paper_23-Implementation_of_Winnowing_Algorithm.pdf",
          "excerpts": [
            "Detection of plagiarism in this study will use a winnowing algorithm that has a function to check every character in two samples by hashing method that can ..."
          ]
        },
        {
          "title": "Authorship Verification Using the Impostors Method - CEUR-WS.org",
          "url": "https://ceur-ws.org/Vol-1179/CLEF2013wn-PAN-Seidman2013.pdf",
          "excerpts": [
            "The approach is based on comparing the similarity between the given documents and a number of external (impostor) documents, so that documents can be classified as having been written by the same author, if they are shown to be more similar to each other than to the impostors, in a number of trials."
          ]
        },
        {
          "title": "[PDF] Not All Character N-grams Are Created Equal: A Study in Authorship ...",
          "url": "https://aclanthology.org/N15-1010.pdf",
          "excerpts": [
            "Character n-grams have been identified as the most successful feature in both single- domain and cross-domain Authorship Attribu-."
          ]
        },
        {
          "title": "Authorship Verification Based on SimCSE",
          "url": "https://ceur-ws.org/Vol-3497/paper-228.pdf",
          "excerpts": [
            "by Y Qiu · Cited by 2 — The goal of the PAN@CLEF 2023 Authorship verification [1][2] task is to determine whether the two texts are written by the same author by ..."
          ]
        },
        {
          "title": "Paraphrase detection and text reuse detection methods",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S1568494614005316",
          "excerpts": [
            "# Text reuse detection using a composition of text similarity measures",
            "A number of metrics have been proposed in the literature to measure text re-use between pairs of sentences or short passages.",
            "Basic lexical similarity measures have been used, e.g., Levenshtein edit distance, longest common subsequence (LCS), and several word *n*-gram (i.e., sequence of *n* contiguous words) overlap functions.",
            "Since individual measures of text similarity have their own strengths and limitations, it is expected that better results can be attained by combining multiple measures into one score which is then used to detect text similarity or reuse.",
            "Paraphrase detection is an important task in text analytics with numerous applications such as plagiarism detection, duplicate question identification, and enhanced customer support helpdesks."
          ]
        },
        {
          "title": "Large-scale Near-deduplication Behind BigCode",
          "url": "https://huggingface.co/blog/dedup",
          "excerpts": [
            "May 16, 2023 — The typical workflow of MinHash is as follows: Shingling (tokenization) and fingerprinting (MinHashing), where we map each document into a set ..."
          ]
        },
        {
          "title": "FED: Fast and Efficient Dataset Deduplication Framework ...",
          "url": "https://arxiv.org/html/2501.01046v2",
          "excerpts": [
            "Feb 16, 2025 — This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally ..."
          ]
        },
        {
          "title": "SimHash for Efficient IR",
          "url": "https://www.numberanalytics.com/blog/simhash-for-efficient-information-retrieval",
          "excerpts": [
            "Threshold Selection: Establishing an appropriate Hamming distance threshold is crucial for distinguishing between duplicates and non-duplicates."
          ]
        },
        {
          "title": "Locality Sensitive Hashing: An Short Intro Based on Minhash",
          "url": "https://benjamin-wang.medium.com/locality-sensitive-hashing-6b859d164ab1",
          "excerpts": [
            "There are different LSH functions associated with different distance measures. LSH functions are used to group similar items into the same ..."
          ]
        },
        {
          "title": "Locality Sensitive Hashing (LSH): The Illustrated Guide - Pinecone",
          "url": "https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/",
          "excerpts": [
            "First, we convert text to sparse vectors using k-shingling (and one-hot encoding), then use minhashing to create 'signatures' — which are passed onto our LSH ...",
            "In this article, we'll be covering the traditional approach — which consists of multiple steps — shingling, MinHashing, and the final banded LSH function. At ..."
          ]
        },
        {
          "title": "Near-Duplicate Detection. or How to Clean Up All Those ...",
          "url": "https://medium.com/@jonathankoren/near-duplicate-detection-b6694e807f7a",
          "excerpts": [
            "Finding the near-duplicates to an article is straightforward. We take the article, shingle it, calculate its signature, and then find its ..."
          ]
        },
        {
          "title": "Searching strings where Hamming Distance is less than a ...",
          "url": "https://stackoverflow.com/questions/27049419/searching-strings-where-hamming-distance-is-less-than-a-threshold",
          "excerpts": [
            "if Hamming Distance is less than the threshold, then add the hash string to the results list. repeat step 1 and 2 for all hash strings."
          ]
        },
        {
          "title": "Simhash and solving the hamming distance problem: explained",
          "url": "http://ben-whitmore.com/simhash-and-solving-the-hamming-distance-problem-explained/",
          "excerpts": [
            "You could readily increase the possible Hamming distance up to k = 6, by dividing the hash into 8 blocks (28 tables)."
          ]
        },
        {
          "title": "How to find near duplicate text documents?",
          "url": "https://www.reddit.com/r/LanguageTechnology/comments/i4bli4/how_to_find_near_duplicate_text_documents/",
          "excerpts": [
            "Generate a \"fingerprint\" of 64bits for each document in your dataset. Consider duplicate fingerprints that have a small (2~3) hamming distance."
          ]
        },
        {
          "title": "When to use Unicode Normalization Forms NFC and NFD?",
          "url": "https://stackoverflow.com/questions/15985888/when-to-use-unicode-normalization-forms-nfc-and-nfd",
          "excerpts": [
            "NFC is the best form for general text, since it is more compatible with strings converted from legacy encodings. NFD and NFKD are most useful for internal ..."
          ]
        },
        {
          "title": "Identifying and Filtering Near-Duplicate Documents",
          "url": "https://cs.brown.edu/courses/cs253/papers/nearduplicate.pdf",
          "excerpts": [
            "by AZ Broder · Cited by 682 — In other words, it suffices to determine whether the resemblance is above a certain threshold. In this talk we show how this determination can be made using a ” ..."
          ]
        },
        {
          "title": "In which cases normalize('NFKC') method work?",
          "url": "https://stackoverflow.com/questions/69058397/in-which-cases-normalizenfkc-method-work",
          "excerpts": [
            "NFKC makes compatible and canonically equivalent normalizations in different ways. Canonically equivalent normalization by NFKC is produced the same way as NFC."
          ]
        },
        {
          "title": "Hamming distance (Simhash python) giving out unexpected value",
          "url": "https://stackoverflow.com/questions/38570476/hamming-distance-simhash-python-giving-out-unexpected-value",
          "excerpts": [
            "The Simhash(\"String\").distance(Simhash(\"Another string\")) is the hamming distance between the two strings. Now, I am not sure I understand this get_features( ..."
          ]
        },
        {
          "title": "Simhashing (hopefully) made simple - ferd.ca",
          "url": "https://ferd.ca/simhashing-hopefully-made-simple.html",
          "excerpts": [
            "Our new pattern has a hamming distance of 1 compared to signatures 1 and 3, and a distance of 3 compared to signatures 2 and 4. As such, we can ..."
          ]
        },
        {
          "title": "How to compare the similarity of documents with Simhash ...",
          "url": "https://stackoverflow.com/questions/49820228/how-to-compare-the-similarity-of-documents-with-simhash-algorithm",
          "excerpts": [
            "Simhash is useful as it detects near duplicates. This means that near duplicates will end up with the same hash. · For exact duplicates you can ..."
          ]
        },
        {
          "title": "How To Use SimHash — The Ultimate Guide",
          "url": "https://spotintelligence.com/2023/01/02/simhash/",
          "excerpts": [
            "Jan 2, 2023 — Near duplicate detection is a process used to identify and determine the similarity between pieces of text or documents. It involves comparing ..."
          ]
        },
        {
          "title": "Near Duplicate Detection Using Simhash",
          "url": "https://github.com/sumonbis/NearDuplicateDetection",
          "excerpts": [
            "In this project, we have implemented simhash algorithm to evaluate approximate cosine similarity between two documents from a large collection of files."
          ]
        },
        {
          "title": "MinHash LSH in Milvus: The Secret Weapon for Fighting Duplicates ...",
          "url": "https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md",
          "excerpts": [
            "In short, MinHash + LSH enables scalable approximate deduplication: MinHash compresses documents into compact signatures, and LSH efficiently ..."
          ]
        },
        {
          "title": "[PDF] Detecting Near-Duplicates for Web Crawling - Google Research",
          "url": "https://research.google.com/pubs/archive/33026.pdf",
          "excerpts": [
            "simhash is a finger- printing technique that enjoys the property that fingerprints of near-duplicates differ in a small number of bit positions. We ..."
          ]
        },
        {
          "title": "java - Make a Sim Hash (Locality Sensitive Hashing) Algorithm more ...",
          "url": "https://stackoverflow.com/questions/8327660/make-a-sim-hash-locality-sensitive-hashing-algorithm-more-accurate",
          "excerpts": [
            "Simhash is not a suitable algorithm for this purpose as it's only useful for near-duplicate detection in which differences are very minor and ..."
          ]
        },
        {
          "title": "The WikiquoteDumper downloads Wikiquote dumps in any ... - GitHub",
          "url": "https://github.com/sgottsch/WikiquoteDumper",
          "excerpts": [
            "The WikiquoteDumper downloads Wikiquote dumps in any languages and converts them into JSON format. Extraction pipeline. First, download and pre-process the dump ...",
            "The WikiquoteDumper downloads Wikiquote dumps in any languages and converts them into JSON format. ### Resources"
          ]
        },
        {
          "title": "Wikiquote",
          "url": "https://en.wikipedia.org/wiki/Wikiquote",
          "excerpts": [
            "Wikiquote is part of a family of wiki-based projects run by the Wikimedia Foundation using MediaWiki software. The project's objective is to collaboratively ..."
          ]
        },
        {
          "title": "Mirroring How To | Project Gutenberg",
          "url": "https://www.gutenberg.org/help/mirroring.html",
          "excerpts": [
            "The book directories are the only part we offer for mirror. The Project Gutenberg catalog in XML/RDF is in the root directory of the generated content, if ...",
            "We recommend that you use rsync. The wget and cURL tools are not suitable, because they need to look at all files just to get the ones that were updated ..."
          ]
        },
        {
          "title": "Offline Catalogs and Feeds - Project Gutenberg",
          "url": "https://www.gutenberg.org/ebooks/offline_catalogs.html",
          "excerpts": [
            "The Project Gutenberg collection is available from dozens of sites offering access via http/https, ftp, rsync, and a few other methods."
          ]
        },
        {
          "title": "API:REST API/Reference",
          "url": "https://www.mediawiki.org/wiki/API:REST_API/Reference",
          "excerpts": [
            "The REST API lets you interact with MediaWiki by sending HTTP requests to unique URLs. You can use the API to build apps and scripts that search wiki pages and ..."
          ]
        },
        {
          "title": "PROMETHEUS: A Corpus of Proverbs Annotated with Metaphors",
          "url": "https://aclanthology.org/L16-1600/",
          "excerpts": [
            "In this paper, we introduce PROMETHEUS, a dataset consisting of English proverbs and their equivalents in Italian."
          ]
        },
        {
          "title": "Parser that converts Wikiquote dumps into structured JSON - GitHub",
          "url": "https://github.com/heyseth/wickedQuotes",
          "excerpts": [
            "There aren't many large, public datasets of quotes online, so I created my own by parsing and cleaning up a Wikiquote data dump."
          ]
        },
        {
          "title": "Retrieve quotes from any Wikiquote article - GitHub",
          "url": "https://github.com/federicotdn/wikiquote",
          "excerpts": [
            "The wikiquote package for Python (>=3.8) allows you to search and retrieve quotes from any Wikiquote article, as well as retrieve the quote of the day."
          ]
        },
        {
          "title": "Wikipedia:Database download",
          "url": "https://en.wikipedia.org/wiki/Wikipedia:Database_download",
          "excerpts": [
            "Wikipedia offers free copies of all available content to interested users. These databases can be used for mirroring, personal use, informal backups, offline ..."
          ]
        },
        {
          "title": "API:Get the contents of a page",
          "url": "https://www.mediawiki.org/wiki/API:Get_the_contents_of_a_page",
          "excerpts": [
            "The TextExtracts extension provides an API which allows you to retrieve plain-text or limited HTML extracts of page content. See Extension:TextExtracts for ..."
          ]
        },
        {
          "title": "Help:Export",
          "url": "https://en.wikiquote.org/wiki/Help:Export",
          "excerpts": [
            "Wiki pages can be exported in a special XML format to import into another MediaWiki installation or use it elsewise for instance for analysing the content. See ..."
          ]
        },
        {
          "title": "hewikiquote dump progress on 20250720 - Wikimedia Downloads",
          "url": "https://dumps.wikimedia.org/hewikiquote/20250720/",
          "excerpts": [
            "Missing: parser wickedQuotes"
          ]
        },
        {
          "title": "How to get WikiQuote Source by Wikidata API?",
          "url": "https://stackoverflow.com/questions/37765344/how-to-get-wikiquote-source-by-wikidata-api",
          "excerpts": [
            "Which claims represent the quotes on Wikiquote? For example, for the Knowledge page. And the corresponding WikiData JSON: https://www.wikidata."
          ]
        },
        {
          "title": "Quick implementation of character n-grams for word - Stack Overflow",
          "url": "https://stackoverflow.com/questions/18658106/quick-implementation-of-character-n-grams-for-word",
          "excerpts": [
            "My question is, how do I get an output that excludes the last character (ie t)? and is there a quicker and more efficient method for computing ..."
          ]
        },
        {
          "title": "How to Write a Summary | Guide & Examples",
          "url": "https://www.scribbr.com/working-with-sources/how-to-summarize/",
          "excerpts": [
            "Summarizing means giving a short overview of a source's main points in your own words. There are five key steps to writing a summary.",
            "Nov 23, 2020 — Step 1: Read the text · Step 2: Break the text down into sections · Step 3: Identify the key points in each section · Step 4: Write the summary."
          ]
        },
        {
          "title": "Summarizing Sources: Definition and Examples of Summary",
          "url": "https://academicguides.waldenu.edu/c.php?g=465757&p=4226514",
          "excerpts": [
            "Summary, in its simplest form, is an articulation of a source's basic argument and main points. What this means is that it's broad in nature."
          ]
        },
        {
          "title": "Article Summaries, Reviews & Critiques - RCC Library",
          "url": "https://libguides.randolph.edu/summaries",
          "excerpts": [
            "Writing an article SUMMARY · State the main ideas. · Identify the most important details that support the main ideas. · Summarize in your own words. · Do not copy ..."
          ]
        },
        {
          "title": "5.4: Writing a One-Sentence Summary - Humanities LibreTexts",
          "url": "https://human.libretexts.org/Courses/Nashville_State_Community_College/Academic_Writing_for_ESL_Students/05%3A_Summarizing/5.04%3A_Writing_a_One-sentence_Summary",
          "excerpts": [
            "A one-sentence summary provides information about the source that is being summarized and presents the thesis or argument of that source.",
            "Think of it as a very brief introduction to your reader of the article, chapter, or book you’ve read; all that they need to know is who wrote it, what its title is, and what its main argument is.",
            "Include the author’s full name.",
            "Include the title of the source.",
            " For articles like the ones we are using in this class, put the title in quotation marks (\"\").",
            "If you need to summarize a book or movie, put the title in italics.",
            "Include the thesis or argument of the article you are summarizing.",
            "State the argument or main idea concisely and clearly.",
            "Do not include details in the summary sentence.",
            "Use a strong reporting verb that accurately describes what the article is doing. (However, if you use \"according to\" in your summary sentence you will not need a reporting verb.)",
            "in ",
            "Coping with Procrastination,",
            "Angela Moore suggests that in order to stop procrastinating, we need to analyze the real reasons we procrastinate.",
            " In order to stop procrastinating, we need to analyze the real reasons we procrastinate, according to Angela Moore in the article “Coping with Procrastination.",
            "For a one-sentence summary or the first sentence of a summary-response essay, it might be most helpful to focus on what the author says, or the main argument of the article.",
            "These signal verbs are typically followed by that and a complete statement of the main idea.",
            "In the article \"Coping with Procrastination,\" Angela Moore suggests that in order to change the habit of procrastination, it is essential to look below the surface for the real reasons why one puts off doing things. (main argument of the article)",
            "Moore argues that procrastination is one of the most pervasive bad habits among college students. (statement of a point from the author’s argument)",
            "For a paragraph summary, consider focusing on what the author does, or how he or she constructs the argument or organizes the ideas in the article.",
            "These signal verbs are generally followed by a noun or noun phrase.",
            "Throughout the article, Moore enumerates the fundamental reasons why people procrastinate and how to overcome these issues. (reference to the organizational patterns used in the article)",
            "Moore presents three solutions to overcoming procrastination.\n(one of the moves the author makes to support her main point)",
            "[SIGNAL VERBS FOR “SAYING\nacknowledges (that)\nagrees (that)\nargues (that)\nbelieves (that)\nclaims (that)\ncomplains (that)\ndeclares (that)\nexplains (that)\nfinds (that)\ninsists (that)\nmaintains (that)\nmakes the case (that)\nnotes (that)\nobserves (that)\npoints out (that)\nposits (that)\nshows (that)\nspeculates (that)\nstates (that)\nstresses (that)\nsubmits (that)\nsuggests (that)\ntheorizes (that)\nthinks (that)\nwarns (that)\nwrites (that)\n",
            "[SIGNAL VERBS FOR “DOING\nacknowledges + noun\naddresses + noun\naffirms + noun\nanalyzes + noun\nasks + noun\ncategorizes + noun\ncompares + noun\ncritiques + noun\ndefines + noun\ndemonstrates + noun\ndiscusses + noun\ndisproves + noun\nenumerates + noun\nexamines + noun\nfurnishes + noun\nidentifies + noun\nillustrates + noun\ninterprets + noun\ninvestigates + noun\nlists + noun\noutlines + noun\npresents + noun\nquestions + noun\nsupports + noun\nsurveys + noun\ntraces + noun\n"
          ]
        },
        {
          "title": "A One-Sentence Summary Clinic",
          "url": "https://www.advancedfictionwriting.com/blog/2010/03/10/a-one-sentence-summary-clinic/",
          "excerpts": [
            "Mar 10, 2010 — It's one sentence that defines the “story question” for your novel. It should be as short as possible, but no shorter. that I do on this blog is to periodically hold a clinic in writing a one-sentence summary. It’s time to do it again. I think we’ll have a lot of fun. **Simply put**, the one-sentence summary is one of the most effective marketing tools you’ll ever find for your novel. Not to mention, it’s one of the most powerful ways of keeping you on track as you write or edit your novel. **What’s a one-sentence summary? ** It’s one sentence that defines the “story question” for your novel. It should be as short as possible, but no shorter. **Here are a couple of examples** which I’m going to steal from my book [WRITING FICTION FOR DUMMIES](http://www.AdvancedFictionWriting.com/blinks/wffd.php). (The contract for the bo",
            "*OUTLANDER**, by Diana Gabaldon: “A young English nurse searches for the way back home after time-traveling from 1945 to 1743 Scotland.",
            "*THE KITE RUNNER**, by Khaled Hosseini: “A boy raised in Afghanistan grows up with the shame of having failed to fight the gang of boys who raped his closest friend."
          ]
        },
        {
          "title": "How to write a killer one sentence pitch (or logline) for ...",
          "url": "https://nathanbransford.com/blog/2023/05/how-to-write-killer-one-sentence-pitch-logline-novels-memoirs",
          "excerpts": [
            "May 22, 2023 — A good pitch is a specific description of what actually happens in your novel. It's a one sentence description of the plot, not the theme."
          ]
        },
        {
          "title": "Highly Efficient Prompt for Summarizing — GPT-4 : r/ChatGPTPro",
          "url": "https://www.reddit.com/r/ChatGPTPro/comments/13n55w7/highly_efficient_prompt_for_summarizing_gpt4/",
          "excerpts": [
            "Summarize TEXT by producing a series of summaries, starting with a one-sentence summary and then creating subsequent summaries that are each ..."
          ]
        },
        {
          "title": "Shockingly good super-intelligent summarization prompt",
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ftjbz3/shockingly_good_superintelligent_summarization/",
          "excerpts": [
            "1.) Analyze the input text and generate 5 essential questions that, when answered, capture the main points and core meaning of the text."
          ]
        },
        {
          "title": "Crafting Single-Sentence Summaries with LLMs | CodeSignal Learn",
          "url": "https://codesignal.com/learn/courses/engineering-output-size-with-llms-1/lessons/crafting-single-sentence-summaries-with-llms",
          "excerpts": [
            "This lesson focuses on crafting prompts to obtain single-sentence summaries, specifically in the context of financial market trends."
          ]
        },
        {
          "title": "Writing a One-Sentence Summary - Rachelle Gardner",
          "url": "https://rachellegardner.com/writing-a-one-sentence-summary/",
          "excerpts": [
            "Writing a One-Sentence Summary",
            "Let’s discuss the\n**one-sentence summary** , also known as a logline, a hook, or a one-sentence pitch.",
            "**What:** About 25 words that capture your novel, memoir, or non-fiction book.",
            "**Why:** To get someone interested in reading your book.",
            " **When to use it:** The start of a query, book proposal, or anytime someone asks you, “What’s your book about?",
            "What it does:** A one-sentence summary takes your complex book with multiple characters and plotlines and boils it down into a simple statement that can be quickly conveyed and understood, and generates interest in the boo",
            "What it should include:**  \n→ A character or two  \n→ Their choice, conflict, or goal  \n→ What’s at stake (may be implied)  \n→ Action that will get them to the goal  \n→ Setting (if imp",
            "Tips:** → Keep it simple. One plotline, 1 or 2 characters. → Use the strongest nouns, verbs and adjectives. → Make the conflict clear but you don’t have to hint at the solution. In your one-sentence summary, try not to pitch a _theme_ . Pitch what _happens_",
            "There are always numerous ways to express your book in a single sentence, so I recommend you create 10 or 20 different ones, before settling on the best angle and combination of words."
          ]
        },
        {
          "title": "The Simple Prompt I Use to Get Meaningful Responses ...",
          "url": "https://medium.com/@fadiboulos/the-simple-prompt-i-use-to-get-meaningful-responses-from-llms-1aa6ec0c8e7a",
          "excerpts": [
            "Give the LLM a persona. · Describe your request context. · Explain the task in detail. · Make the expected outcome clear.See more"
          ]
        },
        {
          "title": "Unlocking Twitter Insights with Prompt Engineering Using ...",
          "url": "https://www.lbsocial.net/post/unlocking-twitter-insights-with-prompt-engineering-using-openai-gpt",
          "excerpts": [
            "Oct 20, 2024 — Summarizing Tweets. GPT can quickly summarize large datasets of tweets, condensing hundreds of tweets into concise statements ..."
          ]
        },
        {
          "title": "Best Prompts Asking for a Summary: Top AI Techniques - Blog",
          "url": "https://blog.promptlayer.com/best-prompts-for-asking-a-summary-a-guide-to-effective-ai-summarization/",
          "excerpts": [
            "Dec 27, 2024 — Prompt: \"Condense this article into a 25-word summary that captures the core message and most important takeaways. Avoid technical jargon ..."
          ]
        },
        {
          "title": "Little-Known ChatGPT Prompts for Summarization",
          "url": "https://medium.com/@kay.herklotz/little-known-chatgpt-prompts-for-summarization-ca48b60157b7",
          "excerpts": [
            "— Please summarize the following article in 300 words. — Summarize this text in 3 bullet points. — Create a summary of this article within 2 to ..."
          ]
        },
        {
          "title": "Analyzing and Evaluating Faithfulness in Dialogue Summarization",
          "url": "https://arxiv.org/abs/2210.11777",
          "excerpts": [
            "In this work, we first perform the fine-grained human analysis on the faithfulness of dialogue summaries and observe that over 35% of generated summaries are faithfully inconsistent respective the source dialogues.",
            "The human-annotated faithfulness samples and the evaluation toolkit are released to facilitate future research toward faithful dialogue summarization.",
            "Dialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications."
          ]
        },
        {
          "title": "Single-Document Abstractive Text Summarization: A Systematic ...",
          "url": "https://dl.acm.org/doi/full/10.1145/3700639",
          "excerpts": [
            "The XSum dataset contained 226,711 news articles with one-sentence summaries . This dataset is primarily used for abstractive text summarization. The XSum dataset contains a one-sentence news summary that describes the details of an article."
          ]
        },
        {
          "title": "Don't Give Me the Details, Just the Summary! Topic-Aware ...",
          "url": "https://aclanthology.org/D18-1206/",
          "excerpts": [
            "by S Narayan · 2018 · Cited by 2036 — We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling ..."
          ]
        },
        {
          "title": "Evaluating the Factual Consistency of Abstractive Text ...",
          "url": "https://arxiv.org/abs/1910.12840",
          "excerpts": [
            "by W Kryściński · 2019 · Cited by 882 — We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated ...See more"
          ]
        },
        {
          "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection ...",
          "url": "https://aclanthology.org/2022.tacl-1.10/",
          "excerpts": [
            "Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization.",
            "by P Laban · 2022 · Cited by 459 — In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets ( ..."
          ]
        },
        {
          "title": "Multilingual Summarization with Factual Consistency ...",
          "url": "https://aclanthology.org/2023.findings-acl.220/",
          "excerpts": [
            "by R Aharoni · 2023 · Cited by 59 — In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate ..."
          ]
        },
        {
          "title": "mFACE: Multilingual Summarization with Factual ...",
          "url": "https://arxiv.org/abs/2212.10622",
          "excerpts": [
            "by R Aharoni · 2022 · Cited by 59 — In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate ..."
          ]
        },
        {
          "title": "Evaluating the Factual Consistency of Abstractive Text ...",
          "url": "https://aclanthology.org/2020.emnlp-main.750/",
          "excerpts": [
            "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries.",
            "Training data is generated by applying a series of rule-based transformations to the sentences of source documents.",
            "The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it.",
            "We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC."
          ]
        },
        {
          "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection ...",
          "url": "https://arxiv.org/abs/2111.09525",
          "excerpts": [
            "by P Laban · 2021 · Cited by 459 — We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task."
          ]
        },
        {
          "title": "[PDF] Mitigating Hallucination in Abstractive Summarization with Domain ...",
          "url": "https://aclanthology.org/2024.findings-naacl.117.pdf",
          "excerpts": [
            "A primary challenge in abstractive summariza- tion is hallucination—the phenomenon where a model generates plausible text that is absent."
          ]
        },
        {
          "title": "Factual consistency evaluation of summarization in the Era ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S0957417424013228",
          "excerpts": [
            "by Z Luo · 2024 · Cited by 21 — To encourage better FC in summarization, a number of automated evaluation methods have been developed including natural language inference(NLI) ...",
            "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks."
          ]
        },
        {
          "title": "What is This Article About? Extreme Summarization with ...",
          "url": "https://www.jair.org/index.php/jair/article/download/11315/26523/22081",
          "excerpts": [
            "by S Narayan · 2019 · Cited by 24 — Abstract. We introduce extreme summarization, a new single-document summarization task which aims at creating a short, one-sentence news summary answering ..."
          ]
        },
        {
          "title": "[2410.23609] On Positional Bias of Faithfulness for Long- ...",
          "url": "https://arxiv.org/abs/2410.23609",
          "excerpts": [
            "by D Wan · 2024 · Cited by 8 — We investigate the presence of this bias in long-form summarization, its impact on faithfulness, and various techniques to mitigate this bias."
          ]
        },
        {
          "title": "Improving Faithfulness of Large Language Models in ...",
          "url": "https://arxiv.org/html/2407.21443v1",
          "excerpts": [
            "Jul 31, 2024 — In this paper, we propose SliSum, a novel summary generation strategy that improves the faithfulness of LLMs in both short and long text ..."
          ]
        },
        {
          "title": "Text Summarization with LLMs",
          "url": "https://www.promptingguide.ai/prompts/text-summarization",
          "excerpts": [
            "This section contains a collection of prompts for exploring text summarization capabilities of LLMs. Physical ReasoningExplain A Concept."
          ]
        },
        {
          "title": "Best Prompt Techniques for Best LLM Responses",
          "url": "https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca",
          "excerpts": [
            "In this article, we explore what is prompt engineering, what constitutes best techniques to engineer a well-structured prompt, and what prompt types steer an ..."
          ]
        },
        {
          "title": "HaDeS: Retrieving semantic similarity to improve factuality in abstractive summarisation (Authorea/HaDeS study)",
          "url": "https://www.authorea.com/doi/pdf/10.22541/au.174222554.47389246",
          "excerpts": [
            " However, one significant challenge associated with abstractive summarisation by LLMs is the\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t",
            "Retrieval-Augmented Generation (RAG) has been hailed as a key technique in minimizing hallucinations in LLMs across various Natural Language Processing (NLP) downstream tasks, including text summarisation (Wu et al., 2023).",
            "Retrieval-Augmented Generation (RAG) has been hailed as a key technique in minimizing hallucinations in LLMs across various Natural Language Processing (NLP) downstream tasks, including text summarisation (Wu et al., 2023).",
            "This study aims to create a system that effectively mitigates hallucinations in abstractive text summarisation."
          ]
        },
        {
          "title": "Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information",
          "url": "https://arxiv.org/html/2404.09480v1",
          "excerpts": [
            "\nA primary challenge in abstractive summarization is hallucination—the phenomenon where a model generates plausible text that is absent in the source text.",
            "we introduce a decoding strategy based on domain-conditional pointwise mutual information.",
            "According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance.",
            "The code is publicly available at <https://github.com/qqplot/dcpmi"
          ]
        },
        {
          "title": "Prompt Engineering Guide - Introduction to Examples and Techniques",
          "url": "https://www.promptingguide.ai/introduction/examples",
          "excerpts": [
            "One of the standard tasks in natural language generation is text summarization. Text summarization can include many different flavors and domains.",
            " [Retrieval Augmented Generation",
            "Jun 7, 2025 — This section will provide more examples of how to use prompts to achieve different tasks and introduce key concepts along the way."
          ]
        },
        {
          "title": "QAFactEval: Improved QA-Based Factual Consistency ...",
          "url": "https://aclanthology.org/2022.naacl-main.187/",
          "excerpts": [
            "by AR Fabbri · 2022 · Cited by 233 — We propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual ...",
            "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization",
            "Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric.",
            "Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost."
          ]
        },
        {
          "title": "[2105.04623] Improving Factual Consistency of Abstractive ...",
          "url": "https://arxiv.org/abs/2105.04623",
          "excerpts": [
            "by F Nan · 2021 · Cited by 108 — In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure ..."
          ]
        },
        {
          "title": "SUMMAC: Re-Visiting NLI-based Models for Inconsistency ...",
          "url": "https://aclanthology.org/2022.tacl-1.10.pdf",
          "excerpts": [
            "by P Laban · 2022 · Cited by 459 — We provide a highly effective and light-weight method called SUMMACCONV that enables NLI models to be successfully used for this task by segmenting documents ..."
          ]
        },
        {
          "title": "Don't Give Me the Details, Just the Summary! Topic-Aware ...",
          "url": "https://arxiv.org/abs/1808.08745",
          "excerpts": [
            "by S Narayan · 2018 · Cited by 2036 — Abstract:We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls ..."
          ]
        },
        {
          "title": "GEM/xsum · Datasets at Hugging Face",
          "url": "https://huggingface.co/datasets/GEM/xsum",
          "excerpts": [
            "XSum is an English news summarization dataset where the task is to predict the first sentence of an article from the rest of it."
          ]
        },
        {
          "title": "GEM <!-- -->xsum",
          "url": "https://gem-benchmark.com/data_cards/xsum",
          "excerpts": [
            "XSum is an English news summarization dataset where the task is to predict the first sentence of an article from the rest of it."
          ]
        },
        {
          "title": "Improving Factual Consistency in Abstractive ...",
          "url": "https://openreview.net/forum?id=BcB7tPQOT9",
          "excerpts": [
            "by D Hu · 2024 · Cited by 3 — State-of-the-art abstractive summarization models still suffer from the content contradiction between the summaries and the input text, ..."
          ]
        },
        {
          "title": "Transformer-Based Abstractive Summarization for Reddit ...",
          "url": "https://www.mdpi.com/1999-5903/14/3/69",
          "excerpts": [
            "by IS Blekanov · 2022 · Cited by 26 — Abstractive summarization is a technique that allows for extracting condensed meanings from long texts, with a variety of potential practical applications."
          ]
        },
        {
          "title": "Social Media Posts: Make It More Appealing with Content ...",
          "url": "https://groupboss.io/blog/social-media-posts/",
          "excerpts": [
            "A content summary should be 150-200 words in length, contain no more than one sentence per paragraph, and include an introduction, three main ..."
          ]
        },
        {
          "title": "TWEETSUMM - A Dialog Summarization Dataset for Customer Service",
          "url": "https://aclanthology.org/2021.findings-emnlp.24/",
          "excerpts": [
            "We introduce the first large scale, high quality, customer care dialog summarization dataset with close to 6500 human annotated summaries."
          ]
        },
        {
          "title": "Improving Factual Consistency of Abstractive Summarization via Question Answering",
          "url": "https://aclanthology.org/2021.acl-long.536/",
          "excerpts": [
            "A commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure factual consistency; next, we propose a novel learning algorithm that maximizes the proposed metric during model training. Through extensive experiments, we confirm that our method is effective in improving factual consistency and even overall quality of the summaries, as judged by both automatic metrics and human evaluation."
          ]
        },
        {
          "title": "Kumar 2024 Multimodal Crisis Microblog Summarization",
          "url": "https://ieeexplore.ieee.org/document/10636755",
          "excerpts": [
            "Our model is designed and evaluated on a newly curated Twitter dataset featuring 12 494 tweets and 3090 images across eight crisis events, each accompanied by gold-standard summaries.",
            "In our digitally connected world, the influx of microblog data poses a formidable challenge in extracting relevant information amid a continuous stream of updates. This challenge intensifies during crises, where the demand for timely and relevant information is crucial.",
            "To address this, our research explores crisis-related microblogs, recognizing the crucial role of multimedia content, such as images, in offering a comprehensive perspective.",
            "In response to these challenges, we introduce a multimodal extractive-abstractive summarization model.",
            "Leveraging a fusion of TF-IDF scoring and bigram filtering, coupled with the effectiveness of three distinct models—BIGBIRD, CLIP, and bootstrapping language-image pre-training (BLIP)—we aim to overcome the limitations of traditional extractive and text-only approaches.",
            "The experimental findings showcase the remarkable efficacy of our model, surpassing current benchmarks by a notable margin of 16% and 17%.",
            "This confirms our model's strength and its relevance in crisis scenarios with the crucial interplay of text and multimedia.",
            "Notably, our research contributes to multimodal, abstractive microblog summarization, addressing a key gap in the literature."
          ]
        },
        {
          "title": "Abstractive Text Summarization: State of the Art, Challenges ... - arXiv",
          "url": "https://arxiv.org/html/2409.02413v1",
          "excerpts": [
            "The task of creating a condensed version of an input sentence while maintaining its core meaning is called a single-sentence summary, which was ..."
          ]
        },
        {
          "title": "xsum - Datasets - TensorFlow",
          "url": "https://www.tensorflow.org/datasets/catalog/xsum",
          "excerpts": [
            "Extreme Summarization (XSum) Dataset. There are two features: - document: Input news article. - summary: One sentence summary of the article."
          ]
        },
        {
          "title": "Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations",
          "url": "https://arxiv.org/html/2410.18860v1",
          "excerpts": [
            "DeCoRe operates by masking specific retrieval heads to trigger hallucinations and then employs a contrastive mechanism that penalises outputs ..."
          ]
        },
        {
          "title": "Prompt Engineering Guide to Summarization - Blog",
          "url": "https://blog.promptlayer.com/prompt-engineering-guide-to-summarization/",
          "excerpts": [
            "Oct 14, 2024 — In this guide, we'll dive into advanced prompt engineering techniques that will turn summarization agents into robust tools capable of handling ..."
          ]
        },
        {
          "title": "Multilingual Summarization with Factual Consistency ...",
          "url": "https://arxiv.org/html/2212.10622v2",
          "excerpts": [
            "Jan 5, 2024 — Using a multilingual entailment model during training (via data filtering or controlled generation) improves summary quality over a baseline ..."
          ]
        },
        {
          "title": "Factual Instance Tweet Summarization and Opinion ...",
          "url": "https://link.springer.com/chapter/10.1007/978-981-13-3393-4_16",
          "excerpts": [
            "by N Vijay Kumar · 2019 · Cited by 15 — We endorse a scheme aimed at factual instance summarization of arranged sub-events aimed at sporting race by means of tweet information.See more"
          ]
        },
        {
          "title": "Single-Document Abstractive Text Summarization: A Systematic ...",
          "url": "https://dl.acm.org/doi/10.1145/3700639",
          "excerpts": [
            "This study provides a broad systematic literature review of abstractive text summarization on single-document summarization to gain insights into the ..."
          ]
        },
        {
          "title": "Tweet Based Tweet Summarization",
          "url": "https://rjpn.org/ijcspub/papers/IJCSP23C1090.pdf",
          "excerpts": [
            "This paper offers a comprehensive overview of the most prominent recent approaches for automatic Twitter topic summarization. It explores various techniques ..."
          ]
        },
        {
          "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in ...",
          "url": "https://arxiv.org/html/2401.01313v1",
          "excerpts": [
            "This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, customer support ..."
          ]
        },
        {
          "title": "Abstractive Text Summarization for Tweets - SJSU ScholarWorks",
          "url": "https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=2205&context=etd_projects",
          "excerpts": [
            "by S Chen · 2022 · Cited by 1 — The primary goal of text summarization is to shorten the text while including as much vital information as possible in the original text so ..."
          ]
        },
        {
          "title": "Graph Neural Network and NER-Based Text Summarization - arXiv",
          "url": "https://arxiv.org/html/2402.05126v1",
          "excerpts": [
            "Missing: entailment check"
          ]
        },
        {
          "title": "Leveraging Entailment Judgements in Cross-Lingual ...",
          "url": "https://arxiv.org/abs/2408.00675",
          "excerpts": [
            "by H Zhang · 2024 · Cited by 2 — Our results show that it is possible to train CLS models that yield more faithful summaries while maintaining comparable or better informativess."
          ]
        },
        {
          "title": "Cross-Lingual Summarization in the Age of Large Language ...",
          "url": "https://www.rohan-paul.com/p/cross-lingual-summarization-in-the",
          "excerpts": [
            "Apr 20, 2025 — Cross-lingual summarization (CLS) uses AI to condense content from one language into a summary in another language. This challenging task ..."
          ]
        },
        {
          "title": "QAFACTEVAL: A QA-based metric for factual consistency (Fabbri et al., NAACL 2022)",
          "url": "https://aclanthology.org/2022.naacl-main.187.pdf",
          "excerpts": [
            "Factual consistency is an essential quality of text summarization models in practical settings.",
            "In this work, we conduct an extensive\n\ncomparison of entailment and QA-based met-\n\nrics, demonstrating that carefully choosing the\n\ncomponents of a QA-based metric, especially\n\nquestion generation and answerability classi-\n\nfication, is critical to perform",
            "we propose an optimized\n\nmetric, which we call QAFACTEVAL, that\n\nleads to a 14% average improvement over pre-\n\nvious QA-based metrics on the SummaC fac-\n\ntual consistency benchmark, and also outper-\n\nforms the best-performing entailment-based\n\nm",
            "Moreover, we find that QA-based and\n\nentailment-based metrics can offer complemen-\n\ntary signals and be combined into a single met-\n\nric for a further performance bo"
          ]
        },
        {
          "title": "Using Similarity to Evaluate Factual Consistency in Summaries",
          "url": "https://arxiv.org/html/2409.15090v1",
          "excerpts": [
            "Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed.",
            "Therefore, many techniques for detecting factual inconsistencies build pipelines around natural language inference (NLI) or question-answering (QA) models with additional supervised learning steps.",
            "In this paper, we revisit similarity-based metrics,\nshowing that this failure stems from the comparison text selection and its granularity.",
            "We propose a new zero-shot factuality evaluation metric,\nSentence-BERT Score (SBERTScore), which compares sentences between the summary and the source document.",
            "It outperforms widely-used word-word metrics including BERTScore and can compete with existing NLI and QA-based factuality metrics on the benchmark without needing any fine-tuning.",
            "SBERTScore only comes after BERTScore in processing speed, and is 3 times faster than the rival NLI-based method SummaC{ZS,Conv} and 30 times faster than the QA-based metric QuestEval.",
            "In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 5055–5070, Online. Association for Computational Linguistics.",
            "The lower triangular matrix indicates that logical AND can improve the balanced accuracy, while the upper triangular matrix suggest opposite to logical OR."
          ]
        },
        {
          "title": "QAFactEval: Improved QA-Based Factual Consistency ...",
          "url": "https://arxiv.org/abs/2112.08542",
          "excerpts": [
            "by AR Fabbri · 2021 · Cited by 233 — We propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual ..."
          ]
        },
        {
          "title": "Understanding Factuality in Abstractive Summarization ...",
          "url": "https://arxiv.org/abs/2104.13346",
          "excerpts": [
            "by A Pagnoni · 2021 · Cited by 351 — Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. Modern summarization models generate ..."
          ]
        },
        {
          "title": "REALSumm: Re-evaluating Evaluation in Text ...",
          "url": "https://github.com/neulab/REALSumm",
          "excerpts": [
            "All ouputs used for human evaluation · Semantic Content Units (SCUs) and manual annotations of outputs · All outputs with human scores. Please read our ..."
          ]
        },
        {
          "title": "Factual Consistency Evaluation for Text Summarization via ...",
          "url": "https://aclanthology.org/2021.findings-emnlp.10/",
          "excerpts": [
            "We propose a novel metric to evaluate the factual consistency in text summarization via counterfactual estimation, which formulates the causal relationship."
          ]
        },
        {
          "title": "SummaCoz: A Dataset for Improving the Interpretability of Factual ...",
          "url": "https://aclanthology.org/2024.findings-emnlp.210/",
          "excerpts": [
            "Summarization is an important application of Large Language Models (LLMs). When judging the quality of a summary, factual consistency holds a significant weight ..."
          ]
        },
        {
          "title": "SummaCoz: A Dataset for Improving the Interpretability of Factual...",
          "url": "https://openreview.net/forum?id=1zJrDjKTWg",
          "excerpts": [
            "We build our explanation-augmented dataset on top of the widely used SummaC summarization consistency benchmark. Additionally, we develop an ..."
          ]
        },
        {
          "title": "salesforce/QAFactEval",
          "url": "https://github.com/salesforce/QAFactEval",
          "excerpts": [
            "May 2, 2022 — This is the official code repository for the NAACL 2022 paper QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization."
          ]
        },
        {
          "title": "A simple prompting technique to reduce hallucinations by ...",
          "url": "https://www.reddit.com/r/ChatGPTPromptGenius/comments/15nhlo1/a_simple_prompting_technique_to_reduce/",
          "excerpts": [
            "A new prompting method that reduces hallucinations, and it's really simple to use. It involves adding some text to a prompt that instructs the model to source ..."
          ]
        },
        {
          "title": "How to prevent ChatGPT-4 from answering questions that are ...",
          "url": "https://community.openai.com/t/how-to-prevent-chatgpt-4-from-answering-questions-that-are-outside-our-context/910021",
          "excerpts": [
            "I'm developing an assistant with ChatGPT-4 and I want it to respond only using the context I provide, which in my case is a single PDF file."
          ]
        },
        {
          "title": "How to Reduce Hallucinations in ChatGPT Responses to Data ...",
          "url": "https://community.openai.com/t/how-to-reduce-hallucinations-in-chatgpt-responses-to-data-queries/900796",
          "excerpts": [
            "Any tips or techniques to reduce these hallucinations, especially when dealing with extensive tables and detailed prompts, would be greatly appreciated."
          ]
        },
        {
          "title": "What are current best practices for avoiding prompt injection attacks ...",
          "url": "https://www.reddit.com/r/googlecloud/comments/1df7lhn/what_are_current_best_practices_for_avoiding/",
          "excerpts": [
            "Although there are external APIs, I generally prefer to stop prompt injection using various classifiers or training my own classifier for input ..."
          ]
        },
        {
          "title": "Prompt Migration Guide | OpenAI Cookbook",
          "url": "https://cookbook.openai.com/examples/prompt_migration_guide",
          "excerpts": [
            "This interactive notebook helps you improve an existing prompt (written for another model) into one that is clear, unambiguous and optimised for GPT-4.1 ..."
          ]
        },
        {
          "title": "[2007.12626] SummEval: Re-evaluating Summarization Evaluation",
          "url": "https://arxiv.org/abs/2007.12626",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "SummEval: Re-evaluating Summarization Evaluation - ACL Anthology",
          "url": "https://aclanthology.org/2021.tacl-1.24/",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "SummEval: Re-evaluating Summarization Evaluation",
          "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "artidoro/frank: FRANK: Factuality Evaluation Benchmark",
          "url": "https://github.com/artidoro/frank",
          "excerpts": [
            "It has a Factuality field which is the total human judgement assigned to the summary. This is a score between 0 and 1 as we collected judgements on each ..."
          ]
        },
        {
          "title": "Fine-grained, Multi-dimensional Summarization Evaluation ...",
          "url": "https://arxiv.org/html/2407.00908v2",
          "excerpts": [
            "Jul 9, 2024 — It includes a list of human keyfacts, along with corresponding annotations indicating their presence in the summary. FRANK and REALSumm obtain ..."
          ]
        },
        {
          "title": "[2109.09209] CLIFF: Contrastive Learning for Improving Faithfulness ...",
          "url": "https://arxiv.org/abs/2109.09209",
          "excerpts": [
            "We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is ..."
          ]
        },
        {
          "title": "Stop AI Hallucinations: A Developer\u0019s Guide to Prompt Engineering",
          "url": "https://shelf.io/blog/stop-ai-hallucinations-a-developers-guide-to-prompt-engineering/",
          "excerpts": [
            "Stop AI Hallucinations: A Developer’s Guide to Prompt Engineering",
            "Output Format Enforcement",
            "Structured outputs help prevent hallucinations by enforcing predefined formats.",
            "JSON schema implementation lets developers specify exact output structures:",
            " The console shows “Mixed results” when it finds inconsistencies. Regular policy reviews lead to better results through continuous improvements.",
            "\nGrounding scores show factual accuracy based on source data, with scores below 0.6 flagged as potential hallucinations"
          ]
        }
      ],
      "reasoning": "The fine-grained field `insightful_content_annotation_guidelines` requires information on how to define, categorize, and ensure the quality of annotations for insightful tweets. The provided value breaks this down into several sub-components: development principles, definition of insightful content, exclusion criteria, adjudication process, and quality metrics. I will address each part.\n\n**Development Principles:**\nSeveral excerpts discuss the iterative nature of developing annotation guidelines and the importance of annotator training and consistency. It is stated that annotation guidelines are crucial for data quality and consistency, and that they should be refined through iterative processes, with annotators providing feedback to address ambiguities. The process involves inspecting data and identifying types of tweets of interest, with guidelines being improved over a series of iterations to reach an agreeable level of consistency. The goal is to create a \"gold-standard\" dataset that provides consistent input-output. Annotators should follow guidelines for consistent and reproducible output, and work independently to avoid groupthink. The importance of public availability for transparency and reproducibility is also mentioned, particularly for academic research.\n\n**Definition of Insightful:**\nInsightful content is defined as a self-contained, profound thought, observation, mental model, poignant short story, or counter-intuitive idea. The core principle is 'self-containment,' meaning the insight must be fully understandable from the tweet's text alone without external context. This includes original aphorisms and micro-stories but excludes well-known clichés that lack a novel perspective. The concept of a one-sentence summary is directly related to the idea of a self-contained, concise thought. Several excerpts discuss how to write a one-sentence summary, emphasizing that it should capture the main idea or argument without including excessive details. This aligns with the requirement for a self-contained and profound thought. The task of extreme summarization, which aims to create a short, one-sentence news summary, further supports the idea of distilling content into a concise, self-contained insight. The ability of LLMs to craft single-sentence summaries also reinforces the feasibility of this definition.\n\n**Exclusion Criteria:**\nExclusion criteria are provided for jokes/humor, memes, political attacks, and context-dependent replies. For jokes and humor, excerpts discuss the automatic detection of irony and humor in tweets, often framed as a classification problem. Research on detecting and rating humor and offense, including shared tasks, indicates that humor can be identified based on intent and structure. For memes, several excerpts describe meme detection, often involving multimodal analysis (image and text) and the classification of memes as hateful or not. Some studies focus on identifying hateful memes and their targets, and the challenges of detecting nuanced hateful content. For political attacks, the concept of propaganda detection is relevant, with research exploring machine learning and rule-based approaches to identify propaganda techniques in news articles. The user query also specifies excluding tweets from major global celebrities, top-tier politicians, or large media corporations, which aligns with the idea of filtering out content from mainstream sources. For context-dependent replies, the Twitter API v2 provides a `referenced_tweets.type` field that indicates if a tweet is a reply, retweet, or quoted tweet, allowing for the exclusion of replies that require external context for understanding. The `conversation_id` field also helps in reconstructing conversation threads, which can be used to identify and filter out replies that are not self-contained.\n\n**Adjudication Process:**\nThe adjudication process is described as a formal method to resolve disagreements between annotators. If annotators disagree, the item is flagged and escalated to a senior third-party adjudicator who makes a final, binding decision. These disagreements are then analyzed to refine guidelines. This process is crucial for maintaining data quality and consistency, especially in subjective annotation tasks. The importance of inter-annotator agreement (IAA) in identifying areas of disagreement and optimizing the annotation process is highlighted. Methods for dealing with annotator disagreement, such as majority voting or expert adjudication, are also discussed.\n\n**Quality Metric:**\nInter-Annotator Agreement (IAA) is identified as the primary quality metric. Krippendorff's Alpha (α) is recommended due to its flexibility in handling multiple annotators, missing data, and various measurement levels. A target score of α ≥ 0.800 is set for reliable agreement, with values between 0.667 and 0.800 considered acceptable for tentative conclusions. Other metrics like Cohen's Kappa and Fleiss' Kappa are also mentioned for measuring inter-rater reliability. The importance of establishing a reliable rating and the interpretation of Kappa values are discussed, with a value of 0.81-1.0 indicating almost perfect agreement. The concept of a ",
      "confidence": "high"
    },
    {
      "field": "core_insight_summarization_methodology.hallucination_prevention_techniques",
      "citations": [
        {
          "title": "Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information",
          "url": "https://arxiv.org/html/2404.09480v1",
          "excerpts": [
            "we introduce a decoding strategy based on domain-conditional pointwise mutual information.",
            "\nA primary challenge in abstractive summarization is hallucination—the phenomenon where a model generates plausible text that is absent in the source text."
          ]
        },
        {
          "title": "SUMMAC: Re-Visiting NLI-based Models for Inconsistency ...",
          "url": "https://aclanthology.org/2022.tacl-1.10.pdf",
          "excerpts": [
            "by P Laban · 2022 · Cited by 459 — We provide a highly effective and light-weight method called SUMMACCONV that enables NLI models to be successfully used for this task by segmenting documents ..."
          ]
        },
        {
          "title": "QAFACTEVAL: A QA-based metric for factual consistency (Fabbri et al., NAACL 2022)",
          "url": "https://aclanthology.org/2022.naacl-main.187.pdf",
          "excerpts": [
            "Factual consistency is an essential quality of text summarization models in practical settings.",
            "In this work, we conduct an extensive\n\ncomparison of entailment and QA-based met-\n\nrics, demonstrating that carefully choosing the\n\ncomponents of a QA-based metric, especially\n\nquestion generation and answerability classi-\n\nfication, is critical to perform",
            "we propose an optimized\n\nmetric, which we call QAFACTEVAL, that\n\nleads to a 14% average improvement over pre-\n\nvious QA-based metrics on the SummaC fac-\n\ntual consistency benchmark, and also outper-\n\nforms the best-performing entailment-based\n\nm",
            "Moreover, we find that QA-based and\n\nentailment-based metrics can offer complemen-\n\ntary signals and be combined into a single met-\n\nric for a further performance bo"
          ]
        },
        {
          "title": "Using Similarity to Evaluate Factual Consistency in Summaries",
          "url": "https://arxiv.org/html/2409.15090v1",
          "excerpts": [
            "Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpt explicitly names a decoding approach that uses retrieval-informed contrast (DeCoRe) to penalize outputs that diverge from retrieved context, which aligns with the third component of the field value. It also discusses reducing hallucinations via a retrieval-based decoding strategy. The closely related excerpt introduces the broader idea of mitigating hallucinations in abstractive summarization through information-theoretic or domain-aware mechanisms, supporting the first two components when interpreted as aiming to improve factual grounding during generation. Other excerpts discuss SummaC and QA-based factuality evaluation (QAFactEval), which provide conceptual support for evaluating and improving factual consistency through grounding and verification, consistent with the overarching objective of reducing hallucinations. The collection together sketches a landscape in which factual grounding (via retrieval), verification (factual checks), and constrained decoding cohere to minimize hallucinations in summaries. The remaining excerpts offer broader context on summarization evaluation or related techniques but do not directly name the specific mechanisms cited in the fine-grained field value, hence are secondary in relevance.",
      "confidence": "high"
    },
    {
      "field": "core_insight_summarization_methodology.summarization_type",
      "citations": [
        {
          "title": "What is This Article About? Extreme Summarization with ...",
          "url": "https://www.jair.org/index.php/jair/article/download/11315/26523/22081",
          "excerpts": [
            "by S Narayan · 2019 · Cited by 24 — Abstract. We introduce extreme summarization, a new single-document summarization task which aims at creating a short, one-sentence news summary answering ..."
          ]
        },
        {
          "title": "Don't Give Me the Details, Just the Summary! Topic-Aware ...",
          "url": "https://aclanthology.org/D18-1206/",
          "excerpts": [
            "by S Narayan · 2018 · Cited by 2036 — We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling ..."
          ]
        },
        {
          "title": "Don't Give Me the Details, Just the Summary! Topic-Aware ...",
          "url": "https://arxiv.org/abs/1808.08745",
          "excerpts": [
            "by S Narayan · 2018 · Cited by 2036 — Abstract:We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls ..."
          ]
        },
        {
          "title": "How to write a killer one sentence pitch (or logline) for ...",
          "url": "https://nathanbransford.com/blog/2023/05/how-to-write-killer-one-sentence-pitch-logline-novels-memoirs",
          "excerpts": [
            "May 22, 2023 — A good pitch is a specific description of what actually happens in your novel. It's a one sentence description of the plot, not the theme."
          ]
        },
        {
          "title": "How to Write a Summary | Guide & Examples",
          "url": "https://www.scribbr.com/working-with-sources/how-to-summarize/",
          "excerpts": [
            "Nov 23, 2020 — Step 1: Read the text · Step 2: Break the text down into sections · Step 3: Identify the key points in each section · Step 4: Write the summary."
          ]
        },
        {
          "title": "A One-Sentence Summary Clinic",
          "url": "https://www.advancedfictionwriting.com/blog/2010/03/10/a-one-sentence-summary-clinic/",
          "excerpts": [
            "Mar 10, 2010 — It's one sentence that defines the “story question” for your novel. It should be as short as possible, but no shorter. that I do on this blog is to periodically hold a clinic in writing a one-sentence summary. It’s time to do it again. I think we’ll have a lot of fun. **Simply put**, the one-sentence summary is one of the most effective marketing tools you’ll ever find for your novel. Not to mention, it’s one of the most powerful ways of keeping you on track as you write or edit your novel. **What’s a one-sentence summary? ** It’s one sentence that defines the “story question” for your novel. It should be as short as possible, but no shorter. **Here are a couple of examples** which I’m going to steal from my book [WRITING FICTION FOR DUMMIES](http://www.AdvancedFictionWriting.com/blinks/wffd.php). (The contract for the bo"
          ]
        }
      ],
      "reasoning": "The target field value specifies an abstractive, extreme summarization method that produces a novel, single, self-contained sentence capturing the core idea of the tweet, similar to tasks like XSum. The most relevant excerpts explicitly discuss extreme summarization and one-sentence summaries as a core task: one excerpt introduces extreme summarization as a task aimed at creating a one-sentence summary that answers the article's essence, which directly matches the field value's description. A closely related excerpt mentions the framework of \"extreme summarization\" and references a one-sentence description of a plot or story, reinforcing the single-sentence, abstractive goal. Another excerpt discusses a concise, one-sentence clinic/pitch for summarization, which aligns with the single-sentence, high-level summarization aim even though it may be pitched toward a different context. Additional excerpts provide practical guidance on writing or summarizing in one sentence, and the classic one-sentence summary practice, which supports the interpretation of a tight, self-contained summary approach. Collectively, these excerpts directly support the idea of using abstractive, extreme, one-sentence summarization akin to XSum and the core insight that the summary should stand alone as a single, novel sentence describing the source's thesis. The least direct items are those offering general summary-writing tips or examples that are not explicitly framed as extreme abstractive single-sentence outputs, but they still provide contextual grounding for the one-sentence summary concept. Overall, the strongest evidence confirms the methodological stance described in the field value, with additional corroboration from related guidance on single-sentence summaries.",
      "confidence": "high"
    },
    {
      "field": "core_insight_summarization_methodology.prompt_engineering_strategy",
      "citations": [
        {
          "title": "Prompt Engineering Guide - Introduction to Examples and Techniques",
          "url": "https://www.promptingguide.ai/introduction/examples",
          "excerpts": [
            "One of the standard tasks in natural language generation is text summarization. Text summarization can include many different flavors and domains."
          ]
        },
        {
          "title": "The Simple Prompt I Use to Get Meaningful Responses ...",
          "url": "https://medium.com/@fadiboulos/the-simple-prompt-i-use-to-get-meaningful-responses-from-llms-1aa6ec0c8e7a",
          "excerpts": [
            "Give the LLM a persona. · Describe your request context. · Explain the task in detail. · Make the expected outcome clear.See more"
          ]
        },
        {
          "title": "Don't Give Me the Details, Just the Summary! Topic-Aware ...",
          "url": "https://arxiv.org/abs/1808.08745",
          "excerpts": [
            "by S Narayan · 2018 · Cited by 2036 — Abstract:We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls ..."
          ]
        }
      ],
      "reasoning": "The field value demands a concrete, multi-part prompt engineering strategy that yields reliable, structured outputs from LLMs. An excerpt titled 'Prompt Engineering Guide - Introduction to Examples and Techniques' directly addresses the availability and use of prompting techniques, which is foundational to the requested strategy. A second excerpt known as 'The Simple Prompt I Use to Get Meaningful Responses' exemplifies a concrete prompt approach, illustrating how a practitioner can craft prompts to elicit useful, controllable results. A third excerpt discussing extreme or targeted summarization patterns ('Don't Give Me the Details, Just the Summary! Topic-Aware ...') demonstrates an application of prompting strategy toward a specific summarization goal, aligning with the objective of steering outputs toward concise, high-signal content. Taken together, these excerpts support a finegrained field value that centers on designing prompts, constraining outputs, and shaping model behavior for reliable JSON-structured results.",
      "confidence": "high"
    },
    {
      "field": "core_insight_summarization_methodology.human_quality_control_framework",
      "citations": [
        {
          "title": "[2007.12626] SummEval: Re-evaluating Summarization Evaluation",
          "url": "https://arxiv.org/abs/2007.12626",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "SummEval: Re-evaluating Summarization Evaluation - ACL Anthology",
          "url": "https://aclanthology.org/2021.tacl-1.24/",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "SummEval: Re-evaluating Summarization Evaluation",
          "url": "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation",
          "excerpts": [
            "We re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd- ..."
          ]
        },
        {
          "title": "Fine-grained, Multi-dimensional Summarization Evaluation ...",
          "url": "https://arxiv.org/html/2407.00908v2",
          "excerpts": [
            "Jul 9, 2024 — It includes a list of human keyfacts, along with corresponding annotations indicating their presence in the summary. FRANK and REALSumm obtain ..."
          ]
        },
        {
          "title": "QAFACTEVAL: A QA-based metric for factual consistency (Fabbri et al., NAACL 2022)",
          "url": "https://aclanthology.org/2022.naacl-main.187.pdf",
          "excerpts": [
            "we propose an optimized\n\nmetric, which we call QAFACTEVAL, that\n\nleads to a 14% average improvement over pre-\n\nvious QA-based metrics on the SummaC fac-\n\ntual consistency benchmark, and also outper-\n\nforms the best-performing entailment-based\n\nm",
            "Moreover, we find that QA-based and\n\nentailment-based metrics can offer complemen-\n\ntary signals and be combined into a single met-\n\nric for a further performance bo"
          ]
        },
        {
          "title": "Using Similarity to Evaluate Factual Consistency in Summaries",
          "url": "https://arxiv.org/html/2409.15090v1",
          "excerpts": [
            "Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed."
          ]
        },
        {
          "title": "SUMMAC: Re-Visiting NLI-based Models for Inconsistency ...",
          "url": "https://aclanthology.org/2022.tacl-1.10.pdf",
          "excerpts": [
            "by P Laban · 2022 · Cited by 459 — We provide a highly effective and light-weight method called SUMMACCONV that enables NLI models to be successfully used for this task by segmenting documents ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt explicitly centers on the SummEval framework and its role in evaluation, which directly underpins a human quality control framework for summarization. It provides a key reference point for how human evaluation has been conceived and benchmarked in the field. The next excerpt introduces a fine-grained, multi-dimensional approach to summarization evaluation, signaling a move toward richer human-centric assessment beyond single-score metrics; this aligns with the proposed framework's emphasis on multiple dimensions (Factual Faithfulness, Relevance, Conciseness, Clarity) and annotator training. Additional excerpts discuss QA-based factuality metrics (QA-FactEval) and other factuality-focused evaluation work (SummaC variants, FRANK, REALSumm, etc.), which offer complementary methods and validation signals that a human-quality-control pipeline could triangulate with. Collectively these excerpts outline both the anchor benchmarks (SummEval) and the broader ecosystem of evaluation approaches that inform a manual review framework. The inclusion of a QA-based metric and the SummaC family underscores the importance of factuality checks within a human-in-the-loop setting, while the SummaC-focused work reinforces the relevance of model-agnostic evaluation signals alongside human annotations. The FRANK-related dimension arises indirectly through Fine-grained evaluation literature that foregrounds multiple facets of factuality and interpretability, which supports implementing annotator-guided checks for identified spans of unfaithfulness. Overall, the combination of direct SummEval benchmarking, multi-dimensional evaluation framing, and factuality-oriented metrics provides a coherent basis for a human quality control framework as described in the finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "core_insight_summarization_methodology.automated_quality_metrics",
      "citations": [
        {
          "title": "Using Similarity to Evaluate Factual Consistency in Summaries",
          "url": "https://arxiv.org/html/2409.15090v1",
          "excerpts": [
            "We propose a new zero-shot factuality evaluation metric,\nSentence-BERT Score (SBERTScore), which compares sentences between the summary and the source document.",
            "SBERTScore only comes after BERTScore in processing speed, and is 3 times faster than the rival NLI-based method SummaC{ZS,Conv} and 30 times faster than the QA-based metric QuestEval.",
            "Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed.",
            "Therefore, many techniques for detecting factual inconsistencies build pipelines around natural language inference (NLI) or question-answering (QA) models with additional supervised learning steps.",
            "In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 5055–5070, Online. Association for Computational Linguistics."
          ]
        },
        {
          "title": "QAFACTEVAL: A QA-based metric for factual consistency (Fabbri et al., NAACL 2022)",
          "url": "https://aclanthology.org/2022.naacl-main.187.pdf",
          "excerpts": [
            "Factual consistency is an essential quality of text summarization models in practical settings.",
            "we propose an optimized\n\nmetric, which we call QAFACTEVAL, that\n\nleads to a 14% average improvement over pre-\n\nvious QA-based metrics on the SummaC fac-\n\ntual consistency benchmark, and also outper-\n\nforms the best-performing entailment-based\n\nm",
            "Moreover, we find that QA-based and\n\nentailment-based metrics can offer complemen-\n\ntary signals and be combined into a single met-\n\nric for a further performance bo"
          ]
        },
        {
          "title": "SUMMAC: Re-Visiting NLI-based Models for Inconsistency ...",
          "url": "https://aclanthology.org/2022.tacl-1.10.pdf",
          "excerpts": [
            "by P Laban · 2022 · Cited by 459 — We provide a highly effective and light-weight method called SUMMACCONV that enables NLI models to be successfully used for this task by segmenting documents ..."
          ]
        },
        {
          "title": "Understanding Factuality in Abstractive Summarization ...",
          "url": "https://arxiv.org/abs/2104.13346",
          "excerpts": [
            "by A Pagnoni · 2021 · Cited by 351 — Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics. Modern summarization models generate ..."
          ]
        }
      ],
      "reasoning": "The central idea describes a suite of automated metrics for factuality evaluation, including NL I-based methods (SummaC), QA-based metrics (QAFactEval), dedicated factuality evaluators (FactCC), and embedding-based measures (SBERTScore). Multiple excerpts directly cite these components and explain their roles in assessing factual consistency at scale: one excerpt explicitly names SBERTScore as an embedding-based metric for factual checks, establishing its place in the automated evaluation toolkit. Another excerpt lists SBERTScore as part of the broader category of factuality metrics, reinforcing its inclusion in a suite of tools for evaluating summaries. QA-focused metrics are covered by excerpts describing QA-based evaluations like QAFactEval and its improvements, demonstrating the QA-based track within automated factuality assessment. An excerpt on SummaC underscores the use of NLI-based approaches to detect inconsistencies, situating SummaC as a core NL I-based metric in the evaluation landscape. Additional excerpts tie QA-based metrics and SummaC together, illustrating how multiple automated metrics operate in concert to judge factuality at scale. Collectively, these excerpts support the field value by enumerating and characterizing the automated metrics that constitute the recommended evaluation toolkit. The most direct evidence comes from statements naming SummaC, QAFactEval, SBERTScore, and related evaluators in the context of factual consistency and evaluation, showing that the suite includes both inference-based and embedding-based approaches alongside QA-focused checks.",
      "confidence": "high"
    },
    {
      "field": "insightful_content_annotation_guidelines.development_principles",
      "citations": [
        {
          "title": "Description of a Twitter Corpus and Guidelines (PMCID: PMC7066507)",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7066507/",
          "excerpts": [
            "We executed a total of 4 such iterations over the same dataset, refining the guidelines at each iteration and expanding them to make distinctions between the different categories more explicit.",
            "\nThe full annotation guidelines used by the annotators, with details and examples of each subcategory within the 4 classes, are made available with this publication",
            "The annotators were instructed to code each tweet into only one category and were asked to create brief notes stating their thought process for instances in which coding was difficult or where they felt that the reason for their decision was not obvious."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "by ML McHugh · 2012 · Cited by 21858 — As a general heuristic, sample sizes should not consist of less than 30 comparisons. Sample sizes of 1,000 or more are mathematically most likely to produce ...",
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            " However, this interpretation allows for very little agreement among raters to be described as “substantial”.",
            "For percent agreement, 61% agreement can immediately be seen as problematic.",
            "Almost 40% of the data in the dataset represent faulty data.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        },
        {
          "title": "What is the minimum sample size for using statistical test ...",
          "url": "https://www.researchgate.net/post/What_is_the_minimum_sample_size_for_using_statistical_test_Krippendorffs_alpha",
          "excerpts": [
            "Dec 12, 2021 — Hello everyone,. What is the minimum sample size for using statistical test Krippendorff's alpha? Thanks."
          ]
        },
        {
          "title": "What is Kappa and How Does It Measure Inter-rater ...",
          "url": "https://www.theanalysisfactor.com/kappa-measures-inter-rater-reliability/",
          "excerpts": [
            "The Kappa statistic is a measure of inter-rater reliability. There is no absolute value for good agreement and depends on the nature of the study."
          ]
        },
        {
          "title": "Krippendorff's alpha",
          "url": "https://en.wikipedia.org/wiki/Krippendorff%27s_alpha",
          "excerpts": [
            "Krippendorff's alpha is applicable to any number of coders, each assigning one value to one unit of analysis, to incomplete (missing) data, to any number of values available for coding a variable, to binary, nominal, ordinal, interval, ratio, polar, and circular metrics (note that this is not a metric in the mathematical sense, but often the square of a mathematical metric, see levels of measurement), and it adjusts itself to small sample sizes of the reliability data.",
            "The minimum acceptable alpha coefficient should be chosen according to the importance of the conclusions to be drawn from imperfect data.",
            "In the absence of knowledge of the risks of drawing false conclusions from unreliable data, social scientists commonly rely on data with reliabilities α ≥ 0.800, consider data with 0.800 > α ≥ 0.667 only to draw tentative conclusions, and discard data whose agreement measures α < 0.667.",
            "Cohen's kappa, by contrast, defines expected agreement in terms of contingencies, as the agreement that would be expected if coders were statistically independent of each other. Cohen's conception of chance fails to include disagreements between coders’ individual predilections for particular categories, punishes coders who agree on their use of categories, and rewards those who do not agree with higher kappa-values."
          ]
        },
        {
          "title": "Cohen's Kappa Sample Size",
          "url": "https://real-statistics.com/reliability/interrater-reliability/cohens-kappa/cohens-kappa-sample-size/",
          "excerpts": [
            "Describes how to calculate the power and minimum sample size required for Cohen's kappa in the case where there are two categories."
          ]
        },
        {
          "title": "Natural Language Annotation for Machine Learning",
          "url": "https://www.oreilly.com/library/view/natural-language-annotation/9781449332693/ch06.html",
          "excerpts": [
            "A critical part of this stage is adjudication—where you take your annotators' work and use it to create the gold standard corpus that you will use for machine ..."
          ]
        },
        {
          "title": "Data annotation guidelines and best practices - Snorkel AI",
          "url": "https://snorkel.ai/blog/data-annotation/",
          "excerpts": [
            "Annotation guidelines are the guideposts that annotators, domain experts, and data scientists follow when labeling data."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpt describes performing multiple iterations of guideline refinement, explicitly stating that the team conducted iterations and refined categories with each pass. This maps to Iterative Refinement of guidelines in the field value. Another excerpt explicitly notes that the full annotation guidelines (with details and examples) are made available, aligning with Public Availability. Excerpts addressing Inter-Annotator Agreement discuss the rationale, methods, and interpretation of agreement among annotators, which supports the Measurement of IAA. Several excerpts on inter-rater reliability (e.g., Cohen's kappa, Krippendorff's alpha) provide concrete methodology and context for assessing agreement, reinforcing the IAA aspect. While some entries discuss broader annotation guidelines and best practices, they still contribute to the Comprehensive Annotator Training/Guidelines ecosystem by illustrating formalized processes, training, and dissemination. Taken together, the strongest support comes from the explicit iterative refinement description, the explicit public release of guidelines, and the standard IAA measurement discourse, with additional reinforcement from the broader guideline and training-related materials.",
      "confidence": "high"
    },
    {
      "field": "insightful_content_annotation_guidelines.adjudication_process",
      "citations": [
        {
          "title": "The adjudication process in collaborative annotation",
          "url": "https://medium.com/@jorgecp/the-adjudication-process-in-collaborative-annotation-61623c46b700",
          "excerpts": [
            "We define adjudication as the process to resolve inconsistencies among these versions before a version is promoted to the gold standard."
          ]
        },
        {
          "title": "Natural Language Annotation for Machine Learning",
          "url": "https://www.oreilly.com/library/view/natural-language-annotation/9781449332693/ch06.html",
          "excerpts": [
            "A critical part of this stage is adjudication—where you take your annotators' work and use it to create the gold standard corpus that you will use for machine ..."
          ]
        },
        {
          "title": "Automatic adjudication based on the inter-annotator agreement",
          "url": "https://tagtog.medium.com/automatic-adjudication-based-on-the-inter-annotator-agreement-a62f49be7bcf",
          "excerpts": [
            "When annotators label the same data, adjudication is the process to resolve inconsistencies among the different versions and to promote a final ..."
          ]
        },
        {
          "title": "Leveraging Annotator Disagreement for Text Classification",
          "url": "https://arxiv.org/html/2409.17577v1",
          "excerpts": [
            "Sep 26, 2024 — In this article, we propose three different strategies to leverage annotator disagreement during the training of text classification models: a ..."
          ]
        },
        {
          "title": "Interrater reliability: the kappa statistic - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052/",
          "excerpts": [
            "by ML McHugh · 2012 · Cited by 21858 — As a general heuristic, sample sizes should not consist of less than 30 comparisons. Sample sizes of 1,000 or more are mathematically most likely to produce ...",
            "\n\nCohen’s kappa, symbolized by the lower case Greek letter, κ ( [7]() ) is a robust statistic useful for either interrater or intrarater reliability testing.",
            " However, this interpretation allows for very little agreement among raters to be described as “substantial”.",
            "For percent agreement, 61% agreement can immediately be seen as problematic.",
            "Almost 40% of the data in the dataset represent faulty data.",
            "In healthcare research, this could lead to recommendations for changing practice based on faulty evidence."
          ]
        }
      ],
      "reasoning": "The target fine-grained field describes a formal adjudication workflow: when two annotators disagree on a label, the item is escalated to a senior adjudicator who makes a final binding decision, and disagreements are collected to refine guidelines. Excerpts that directly discuss the adjudication process in collaborative annotation provide the strongest support for this mechanism, including explicit references to the adjudication process and to automated adjudication workflows. Additional excerpts discuss adjudication in the context of data annotation and inter-annotator agreement, which reinforces that a structured adjudication step exists to resolve disagreements and improve reliability. Together, these excerpts support the existence and function of an adjudication step and its role in guiding guideline refinement, even if they do not reproduce the exact decision-tree wording. The combination of explicit adjudication mentions and related discussions about disagreement resolution and guideline improvement aligns with the described process and its purpose in refining annotation guidelines.",
      "confidence": "high"
    },
    {
      "field": "insightful_content_annotation_guidelines.exclusion_criteria",
      "citations": [
        {
          "title": "Description of a Twitter Corpus and Guidelines (PMCID: PMC7066507)",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7066507/",
          "excerpts": [
            "The annotators were instructed to code each tweet into only one category and were asked to create brief notes stating their thought process for instances in which coding was difficult or where they felt that the reason for their decision was not obvious.",
            "\nThe full annotation guidelines used by the annotators, with details and examples of each subcategory within the 4 classes, are made available with this publication",
            "The key objective behind creating detailed annotation guidelines and making them publicly available is to ensure the reproducibility of the annotation experiments.",
            "The annotation task was started as an iterative process both for training purposes and to test the efficacy and clarity of the guidelines over a small initial dataset.",
            "We executed a total of 4 such iterations over the same dataset, refining the guidelines at each iteration and expanding them to make distinctions between the different categories more explicit.",
            "The guidelines were improved over a series of iterations of annotation and reviewed until we reached an agreeable level of consistency in our annotations."
          ]
        }
      ],
      "reasoning": "The field value specifies a set of exclusions used to filter noise in annotation of insightful content. Excerpts describing annotation guidelines and the practice of documenting decision rationales align with this goal. Phrases that indicate researchers should code tweets into categories and that annotators must \"create brief notes stating their thought process for instances in which coding was difficult or where they felt that the reason for their decision was not obvious\" show how a formal exclusion framework could be implemented and justified. Other excerpts reference the existence of a full annotation guideline and iterative refinement of categories, which further corroborates that exclusion criteria are typically codified and publicly documented to support reproducibility. Although the exact exclusion categories in the field value are not spelled out in these excerpts, the material demonstrates the methodology and documentation culture around exclusion criteria that would support implementing the specified guidelines.",
      "confidence": "medium"
    },
    {
      "field": "insightful_content_annotation_guidelines.quality_metric",
      "citations": [
        {
          "title": "Krippendorff's alpha",
          "url": "https://en.wikipedia.org/wiki/Krippendorff%27s_alpha",
          "excerpts": [
            "In the absence of knowledge of the risks of drawing false conclusions from unreliable data, social scientists commonly rely on data with reliabilities α ≥ 0.800, consider data with 0.800 > α ≥ 0.667 only to draw tentative conclusions, and discard data whose agreement measures α < 0.667.",
            "Krippendorff's alpha is applicable to any number of coders, each assigning one value to one unit of analysis, to incomplete (missing) data, to any number of values available for coding a variable, to binary, nominal, ordinal, interval, ratio, polar, and circular metrics (note that this is not a metric in the mathematical sense, but often the square of a mathematical metric, see levels of measurement), and it adjusts itself to small sample sizes of the reliability data."
          ]
        },
        {
          "title": "A user-friendly tool for computing Krippendorff's Alpha inter-rater ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2215016123005411",
          "excerpts": [
            "Alpha ≥ 0.80 : This value is generally considered a satisfactory level of agreement, indicating a reliable rating. In many research contexts, a Krippendorff's Alpha equal to or above 0.80 is acceptable for drawing triangulated conclusions based on the rated data."
          ]
        },
        {
          "title": "Validity and Inter-Rater Reliability Testing of Quality Assessment Instruments",
          "url": "https://www.ncbi.nlm.nih.gov/books/NBK92287/table/executivesummary.t2/",
          "excerpts": [
            "| 0.61-0.80 | Substantial agreement |",
            "| 0.41-0.60 | Moderate agreement |",
            "Table BInterpretation of Fleiss' kappa (κ) (from Landis and Koch 1977) ; 0.0-0.20, Slight agreement ; 0.21-0.40, Fair agreement ; 0.41-0.60, Moderate agreement."
          ]
        },
        {
          "title": "Inter-annotator Agreement - DTIC",
          "url": "https://apps.dtic.mil/sti/trecms/pdf/AD1158943.pdf",
          "excerpts": [
            "by R Artstein · 2017 · Cited by 344 — This chapter touches upon several issues in the calculation and assessment of inter- annotator agreement. It gives an introduction to the theory behind ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field specifies Krippendorff's Alpha as the recommended inter-annotator reliability metric for a multi-label classification task, with a target threshold of Alpha ≥ 0.800 to indicate reliable agreement and clear guidelines for training/annotation workflows. Excerpts that explicitly state the applicability of Krippendorff's Alpha to multi-annotator settings and to multi-label contexts support choosing Alpha as the appropriate metric in this scenario. They also establish that a high Alpha (0.80 or above) is considered acceptable and reliable for drawing conclusions from annotations, which aligns exactly with the target value in the field. Additional excerpts reinforce practical aspects: there are tools available to compute Krippendorff's Alpha, which underpins the implementation detail of using Alpha as the reliability standard, and other sources discuss inter-annotator agreement and thresholds in practice, further supporting the overall framework of using Alpha as the primary quality metric and the 0.800 threshold as a decision boundary. Collectively, these excerpts anchor the field value in both methodological justification (Krippendorff's Alpha is appropriate for multi-annotator, multi-label contexts) and normative guidance (Alpha ≥ 0.800 signifies reliable agreement).",
      "confidence": "high"
    }
  ],
  "outputSchema": {
    "type": "object",
    "properties": {
      "project_summary": {
        "type": "string",
        "description": "A high-level overview of the feasibility of creating a database of 'underrated viral tweets', outlining the primary challenges such as high API costs, the subjective nature of 'insightful content', and the recommended hybrid approach to data collection and analysis."
      },
      "recommended_strategy_overview": {
        "type": "string",
        "description": "A concise summary of the optimal strategy for the project. This includes using a hybrid data acquisition model, implementing a multi-layered filtering process for source and content, leveraging NLP for enrichment, and adhering to a strict quality assurance and compliance framework."
      },
      "data_acquisition_plan": {
        "type": "object",
        "properties": {
          "recommended_approach": {
            "type": "string",
            "description": "The overall recommended strategy for acquiring data."
          },
          "primary_challenge": {
            "type": "string",
            "description": "The main technical hurdle in acquiring data via the API."
          },
          "primary_api_option": {
            "type": "string",
            "description": "The main recommended API for data acquisition."
          },
          "alternative_providers": {
            "type": "string",
            "description": "Alternative third-party data providers considered."
          },
          "cost_implication_summary": {
            "type": "string",
            "description": "A summary of the cost implications due to API limitations."
          }
        },
        "required": [
          "recommended_approach",
          "primary_challenge",
          "primary_api_option",
          "alternative_providers",
          "cost_implication_summary"
        ],
        "additionalProperties": false
      },
      "underrated_source_definition": {
        "type": "object",
        "properties": {
          "primary_filtering_method": {
            "type": "string",
            "description": "The main method used to filter for underrated sources."
          },
          "celebrity_exclusion_rules": {
            "type": "string",
            "description": "Rules for excluding major global celebrities, such as follower count thresholds and curated lists."
          },
          "politician_exclusion_rules": {
            "type": "string",
            "description": "Rules for excluding top-tier politicians, using official datasets and repositories."
          },
          "media_exclusion_rules": {
            "type": "string",
            "description": "Rules for excluding large media corporations, using news agency lists and X's Verified Organization status."
          },
          "final_heuristic": {
            "type": "string",
            "description": "The final decision-making process, such as a reproducible scoring rubric."
          }
        },
        "required": [
          "primary_filtering_method",
          "celebrity_exclusion_rules",
          "politician_exclusion_rules",
          "media_exclusion_rules",
          "final_heuristic"
        ],
        "additionalProperties": false
      },
      "insightful_content_annotation_guidelines": {
        "type": "object",
        "properties": {
          "development_principles": {
            "type": "string",
            "description": "Core principles for developing the annotation guidelines."
          },
          "definition_of_insightful": {
            "type": "string",
            "description": "The core definition of what constitutes 'Insightful Content'."
          },
          "exclusion_criteria": {
            "type": "string",
            "description": "A list of content types to be explicitly excluded."
          },
          "adjudication_process": {
            "type": "string",
            "description": "The process for resolving disagreements between annotators."
          },
          "quality_metric": {
            "type": "string",
            "description": "The metric used to measure consistency among annotators, such as Inter-Annotator Agreement (IAA)."
          }
        },
        "required": [
          "development_principles",
          "definition_of_insightful",
          "exclusion_criteria",
          "adjudication_process",
          "quality_metric"
        ],
        "additionalProperties": false
      },
      "nlp_classifier_plan_for_insightful_content": {
        "type": "object",
        "properties": {
          "training_methodology": {
            "type": "string",
            "description": "The overall approach for training the classifier, combining weak supervision and a gold set."
          },
          "labeling_strategy": {
            "type": "string",
            "description": "The method for efficiently selecting data for manual annotation."
          },
          "recommended_models": {
            "type": "string",
            "description": "The baseline machine learning models to be used for classification."
          },
          "optimization_target": {
            "type": "string",
            "description": "The primary performance metric the model will be optimized for."
          },
          "monitoring_plan": {
            "type": "string",
            "description": "The strategy for monitoring the model's performance post-deployment to detect drift."
          }
        },
        "required": [
          "training_methodology",
          "labeling_strategy",
          "recommended_models",
          "optimization_target",
          "monitoring_plan"
        ],
        "additionalProperties": false
      },
      "core_insight_summarization_methodology": {
        "type": "object",
        "properties": {
          "summarization_type": {
            "type": "string",
            "description": "The type of summarization technique to be used."
          },
          "hallucination_prevention_techniques": {
            "type": "string",
            "description": "Methods to ensure the summary is factually consistent with the source tweet."
          },
          "automated_quality_metrics": {
            "type": "string",
            "description": "Automated metrics used to evaluate the factual consistency of summaries."
          },
          "human_quality_control_framework": {
            "type": "string",
            "description": "The framework for manual review of summaries, based on academic benchmarks."
          },
          "prompt_engineering_strategy": {
            "type": "string",
            "description": "The approach for designing prompts for LLMs to generate high-quality summaries."
          }
        },
        "required": [
          "summarization_type",
          "hallucination_prevention_techniques",
          "automated_quality_metrics",
          "human_quality_control_framework",
          "prompt_engineering_strategy"
        ],
        "additionalProperties": false
      },
      "author_description_extraction_process": {
        "type": "object",
        "properties": {
          "pipeline_overview": {
            "type": "string",
            "description": "A summary of the multi-stage process for extraction and standardization."
          },
          "pre_processing_steps": {
            "type": "string",
            "description": "Initial steps to clean the raw bio text, such as removing URLs and emojis."
          },
          "extraction_tools": {
            "type": "string",
            "description": "The recommended tools for identifying job titles, including NER models and commercial APIs."
          },
          "normalization_taxonomies": {
            "type": "string",
            "description": "The authoritative dictionaries used to map extracted titles to a standard vocabulary."
          },
          "ambiguity_resolution_rule": {
            "type": "string",
            "description": "The heuristic used to handle bios with multiple or ambiguous roles."
          }
        },
        "required": [
          "pipeline_overview",
          "pre_processing_steps",
          "extraction_tools",
          "normalization_taxonomies",
          "ambiguity_resolution_rule"
        ],
        "additionalProperties": false
      },
      "originality_verification_protocol": {
        "type": "object",
        "properties": {
          "detection_methods": {
            "type": "string",
            "description": "The combination of techniques used to detect non-original content."
          },
          "reference_corpora": {
            "type": "string",
            "description": "The large text corpora used to check for known quotes and public-domain text."
          },
          "semantic_similarity_threshold": {
            "type": "string",
            "description": "The suggested threshold for semantic similarity scores to flag potential paraphrasing."
          },
          "manual_review_guideline": {
            "type": "string",
            "description": "The basis for manual review procedures for borderline cases."
          },
          "stylometry_application": {
            "type": "string",
            "description": "The use of writing style analysis for authorship verification."
          }
        },
        "required": [
          "detection_methods",
          "reference_corpora",
          "semantic_similarity_threshold",
          "manual_review_guideline",
          "stylometry_application"
        ],
        "additionalProperties": false
      },
      "content_category_taxonomy": {
        "type": "object",
        "properties": {
          "classification_approach": {
            "type": "string",
            "description": "The recommended approach for assigning categories to tweets."
          },
          "primary_categories": {
            "type": "string",
            "description": "The list of core categories for classification."
          },
          "consistency_metric": {
            "type": "string",
            "description": "The metric used to ensure high agreement among annotators."
          },
          "conflict_resolution_rules": {
            "type": "string",
            "description": "Rules and processes for handling cases where a tweet fits multiple categories."
          },
          "imbalance_diagnostics": {
            "type": "string",
            "description": "Procedures for monitoring the distribution of labels to identify and correct for skew."
          }
        },
        "required": [
          "classification_approach",
          "primary_categories",
          "consistency_metric",
          "conflict_resolution_rules",
          "imbalance_diagnostics"
        ],
        "additionalProperties": false
      },
      "data_pipeline_architecture": {
        "type": "object",
        "properties": {
          "architecture_style": {
            "type": "string",
            "description": "The overall design pattern for the data pipeline."
          },
          "pipeline_modules": {
            "type": "string",
            "description": "The distinct logical stages of the pipeline."
          },
          "api_ingestion_strategy": {
            "type": "string",
            "description": "The strategy for efficiently fetching all required data from the X API."
          },
          "rate_limit_handling": {
            "type": "string",
            "description": "The mechanism for managing API rate limits to prevent service interruptions."
          },
          "deduplication_method": {
            "type": "string",
            "description": "The method for ensuring that the final dataset contains only unique records."
          }
        },
        "required": [
          "architecture_style",
          "pipeline_modules",
          "api_ingestion_strategy",
          "rate_limit_handling",
          "deduplication_method"
        ],
        "additionalProperties": false
      },
      "quality_assurance_framework": {
        "type": "object",
        "properties": {
          "target_quality_level": {
            "type": "string",
            "description": "The minimum percentage of rows that must meet all constraints."
          },
          "statistical_sampling_plan": {
            "type": "string",
            "description": "The statistical method used to audit data quality on batches of data."
          },
          "human_review_process": {
            "type": "string",
            "description": "The process for manually reviewing subjective criteria, including IAA targets and adjudication."
          },
          "automated_validation_tools": {
            "type": "string",
            "description": "Tools used to automatically check for objective criteria and cross-field consistency."
          },
          "improvement_process": {
            "type": "string",
            "description": "The process for analyzing errors and continuously improving the QA framework."
          }
        },
        "required": [
          "target_quality_level",
          "statistical_sampling_plan",
          "human_review_process",
          "automated_validation_tools",
          "improvement_process"
        ],
        "additionalProperties": false
      },
      "legal_and_platform_compliance_summary": {
        "type": "object",
        "properties": {
          "data_collection_policy": {
            "type": "string",
            "description": "The primary rule regarding how data can be collected from the platform."
          },
          "content_redistribution_policy": {
            "type": "string",
            "description": "The rules governing the sharing of tweet content and metadata with third parties."
          },
          "rehydration_requirement": {
            "type": "string",
            "description": "The mandatory process for recipients of tweet IDs to obtain the full, current content."
          },
          "ai_model_training_restriction": {
            "type": "string",
            "description": "The specific prohibition on using X content for training large-scale AI models."
          },
          "recommended_dataset_release_format": {
            "type": "string",
            "description": "The compliant format for releasing the final dataset to the public."
          }
        },
        "required": [
          "data_collection_policy",
          "content_redistribution_policy",
          "rehydration_requirement",
          "ai_model_training_restriction",
          "recommended_dataset_release_format"
        ],
        "additionalProperties": false
      },
      "diversity_and_bias_mitigation_plan": {
        "type": "object",
        "properties": {
          "language_and_region_strategy": {
            "type": "string",
            "description": "The plan to ensure linguistic and geographic diversity, including targeted sampling."
          },
          "author_diversity_strategy": {
            "type": "string",
            "description": "The plan to ensure a diverse set of authors, primarily through follower-count stratification."
          },
          "topic_diversity_strategy": {
            "type": "string",
            "description": "The plan to ensure a balanced representation of disciplines, using topic modeling."
          },
          "niche_overrepresentation_prevention": {
            "type": "string",
            "description": "Procedures to avoid the dataset being dominated by a few authors or topics."
          },
          "documentation_requirement": {
            "type": "string",
            "description": "The requirement to create a datasheet or data statement detailing known biases."
          }
        },
        "required": [
          "language_and_region_strategy",
          "author_diversity_strategy",
          "topic_diversity_strategy",
          "niche_overrepresentation_prevention",
          "documentation_requirement"
        ],
        "additionalProperties": false
      },
      "project_timeline_and_budget": {
        "type": "object",
        "properties": {
          "estimated_timeline": {
            "type": "string",
            "description": "The total estimated duration of the project."
          },
          "project_phases": {
            "type": "string",
            "description": "The breakdown of the project into distinct phases with milestones."
          },
          "primary_cost_driver": {
            "type": "string",
            "description": "The single largest anticipated expense for the project."
          },
          "estimated_annotation_labor_cost": {
            "type": "string",
            "description": "The estimated cost for manual annotation labor."
          },
          "key_risks": {
            "type": "string",
            "description": "The most significant risks to the project's timeline and budget."
          }
        },
        "required": [
          "estimated_timeline",
          "project_phases",
          "primary_cost_driver",
          "estimated_annotation_labor_cost",
          "key_risks"
        ],
        "additionalProperties": false
      },
      "key_challenges_and_risks": {
        "type": "string",
        "description": "A consolidated overview of the primary risks and challenges facing the project. This includes the high and potentially volatile cost of the X API, the inherent subjectivity and difficulty in defining and finding 'insightful content', and the risk of data scarcity where few tweets meet all criteria."
      },
      "discovered_data_sources": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "source_name": {
              "type": "string",
              "description": "The name of the identified dataset or source."
            },
            "platform": {
              "type": "string",
              "description": "The platform where the dataset is hosted (e.g., Kaggle, GitHub)."
            },
            "relevance_to_project": {
              "type": "string",
              "description": "A description of why the source is highly relevant to the project's objectives."
            },
            "available_metadata": {
              "type": "string",
              "description": "The data columns available within the source dataset."
            },
            "license": {
              "type": "string",
              "description": "The licensing terms for the dataset."
            }
          },
          "required": [
            "source_name",
            "platform",
            "relevance_to_project",
            "available_metadata",
            "license"
          ],
          "additionalProperties": false
        },
        "description": "A list of promising public datasets and sources identified during the research phase that can serve as a starting point for data collection. This includes a detailed mention of the 'The Tweets of Wisdom' dataset on Kaggle."
      }
    },
    "required": [
      "project_summary",
      "recommended_strategy_overview",
      "data_acquisition_plan",
      "underrated_source_definition",
      "insightful_content_annotation_guidelines",
      "nlp_classifier_plan_for_insightful_content",
      "core_insight_summarization_methodology",
      "author_description_extraction_process",
      "originality_verification_protocol",
      "content_category_taxonomy",
      "data_pipeline_architecture",
      "quality_assurance_framework",
      "legal_and_platform_compliance_summary",
      "diversity_and_bias_mitigation_plan",
      "project_timeline_and_budget",
      "key_challenges_and_risks",
      "discovered_data_sources"
    ],
    "additionalProperties": false
  }
}