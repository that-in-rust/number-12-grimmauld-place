{
  "input": "What is the best use case on busienss side - server side that can unlock maximum value for Universal VIrtio - idea below - what will be the pareto set of drivers we should write so that we can do kafka, spark, backendAPIs from scratch in a Rust OS which is supported by this UniversalVirtio + RustHallows Amul ## Executive snapshot * **Universal Virtio (driver unification):** *Directionally sound and well‑trodden in industry.* Virtio is mature, multi‑OS, and fast enough for most targets; the big risks are GPU/USB corner cases and non‑PC SoCs. Strong near‑term ROI if you constrain hardware SKUs. Evidence: OASIS Virtio spec, multi‑OS drivers (Windows/BSD), high‑performance vhost-user backends. ([fosdem.org][1], [Fedora Documentation][2], [man.freebsd.org][3], [spdk.io][4]) * **Rust Apex (vertical, cert‑ready Rust stack; “10–40×” wins):** *Compelling vision with clear safety wins; performance uplifts are workload‑specific.* The claim needs scoping and benchmarks against best‑in‑class C stacks; still, modern designs (thread‑per‑core, SPDK/DPDK, vectorized analytics) show that double‑digit gains are plausible with the right architecture. Evidence & caveats below. * **Rust Hallows (high‑assurance, ARINC 653‑style OS with μs scheduling):** *Technically coherent.* ARINC APEX partitioning is canonical in avionics; µs-scale schedulers exist in research (Shenango/Caladan). Certification and multicore determinism are the long poles. ([Wikipedia][5], [amyousterhout.com][6]) * **Rust OS vs. Linux Hardware Universe (80/20 device coverage):** *Pragmatic and supported by data sources,* especially if you pick “lowest‑entropy” SKUs and leverage popularity datasets like linux‑hardware.org. Success hinges on tight SKU control and virtio backstops. ([Linux Hardware][7], [GitHub][8]) --- ## 1) Universal Virtio — “driver chaos shrunk by standardizing the *other* end” **What you propose (in brief):** Stick a stable, cross‑OS device model (Virtio) between hardware reality and your OS, so your kernel needs far fewer native drivers. Where possible, terminate at high‑performance backends (e.g., SPDK vhost‑user for storage) to keep perf.&#x20; **Corroboration from multiple angles** * **Standard & adoption.** Virtio is an OASIS standard (1.3+), with devices for block, net, GPU, RNG, console, input, vsock, and more. ([fosdem.org][1]) Drivers exist for Linux, FreeBSD, OpenBSD, Windows (virtio‑win). ([man.freebsd.org][3], [OpenBSD Manual][9], [Fedora Documentation][2], [GitHub][10]) * **Performance backends.** Vhost‑user lets user‑space backends (like SPDK) serve virtio devices with very high IOPS and low overhead; SPDK demonstrates **>10M IOPS per core** and linear scaling. ([spdk.io][11]) * **GPU and modern graphics.** virtio‑gpu + the **Venus** Vulkan driver enable hardware‑accelerated guest rendering via host Mesa—real, shipping tech. ([COVESA Projects Wiki][12]) **Counterpoints & rough edges** * **Throughput variability:** Benchmarks range from “near‑line‑rate” to “falls short of 10 Gb/s” depending on host/guest/OS; BSDs and Xen corners can still lag. This is solvable with tuning, but not free. ([OPNsense Forum][13]) * **USB & cameras:** virtio‑usb isn’t universally adopted; many stacks still forward real USB or emulate controllers—expect manual work for isochronous devices. *(Inference based on device coverage in virtio spec and community practice; spec enumerates devices; widespread camera pass‑through is still host‑specific.)* ([fosdem.org][1]) * **Non‑PC SoCs:** Virtio exists in ARM ecosystems, but vendor firmware/device quirks can curb plug‑and‑play. You’ll want curated platforms. **Feasibility & estimate (first 6–9 months, focused SKU set) — working numbers** * **Deliverables:** virtio‑blk/net/console/rng/vsock + virtio‑gpu(Venus) + vhost‑user backends; boot a polished desktop + server image on QEMU/KVM and 2–3 reference laptops/NUCs. * **Effort:** \\~4–6 senior engineers to first production image (enablement + perf), then 2–3 for sustaining. * **Why plausible:** Device models and backends exist; the work is plumbing, tuning, and product polish—not inventing a driver zoo. *(Estimate derived from prior art scale and integration breadth; not a promise.)* --- ## 2) Rust Apex — “a vertically‑integrated, cert‑ready Rust stack” **What you propose:** An end‑to‑end memory‑safe stack, targeting DO‑178C/ISO 26262 style assurance, and claiming **10–40×** performance gains in select domains by combining: thread‑/shard‑per‑core designs, userspace I/O (SPDK/DPDK or AF\\_XDP), and vectorized compute (DataFusion/Arrow).&#x20; **What checks out** * **Safety tooling is maturing.** The **Ferrocene** Rust toolchain is TÜV‑qualified for ISO 26262 and IEC 61508—critical for safety programmes. ([ferrous-systems.com][14]) * **Performance patterns are real:** * *Thread/shard‑per‑core* architectures (e.g., ScyllaDB, MICA) show multi‑× throughput and tail‑latency gains by eliminating cross‑core contention. ([ScyllaDB][15], [USENIX][16]) * *µs‑scale scheduling* (Shenango/Caladan) reallocates cores every **\\~5 µs** to keep latency tight under load. ([USENIX][17], [amyousterhout.com][18]) * *Vectorized analytics* (Arrow/DataFusion) deliver large wins over row stores and have peer‑reviewed documentation (SIGMOD’24) and evolving benchmarks (ClickBench). ([andrew.nerdnetworks.org][19], [datafusion.apache.org][20]) * *User‑space I/O* (SPDK) eliminates kernel syscalls and interrupts for I/O paths with huge IOPS/core. ([spdk.io][11]) * **Rust in Linux & drivers:** Rust is in mainline since 6.1, with ongoing work to support real drivers (e.g., Binder) and better ACPI matching—useful if you need mixed‑language environments. ([LWN.net][21]) **Skepticism to absorb (diverse perspectives)** * **Kernel community caution:** LWN and kernel maintainers highlight integration, toolchain, and cross‑language inlining concerns; there’s social and technical drag. ([LWN.net][22]) * **io\\_uring portability & security:** Great on Linux, but it’s Linux‑only and has had notable CVEs; keep trust boundaries and update cadence sharp. ([docs.oasis-open.org][23], [Red Hat Docs][24]) * **DataFusion vs DuckDB:** DataFusion is growing fast, but in some workloads DuckDB wins; you’ll need own, reproducible bench harnesses. ([Ibis][25], [GitHub][26]) **Is “10–40×” believable?** Yes, **in narrowly‑scoped paths** (e.g., replacing kernel‑mediated I/O with SPDK; moving from naive per‑request threadpools to shard‑per‑core; swapping row‑oriented scans for Arrow kernels). End‑to‑end application speedups will be smaller unless you deliberately align *all* hot paths. Treat this as an **upper‑quartile target** for focused subsystems, not a blanket promise. ([spdk.io][11], [ScyllaDB][15], [andrew.nerdnetworks.org][19]) **Feasibility & staged plan (working theory)** * **Quarter 1–2:** Choose 2 exemplar subsystems (e.g., block I/O path via SPDK; analytics via DataFusion). Build reproducible perf harnesses with **HdrHistogram** and **wrk2** to eliminate *coordinated omission* in tail latency reporting. ([GitHub][27], [Google Groups][28]) * **Quarter 3–4:** Tighten schedulers and NUMA pinning (thread‑per‑core), add AF\\_XDP for net or remain with DPDK if you can absorb its operational profile. ([oldvger.kernel.org][29], [arXiv][30]) --- ## 3) Rust Hallows — “APEX‑flavored, high‑assurance OS with microsecond control” **What you propose:** A safety‑critical OS surface (APEX/ARINC‑style partitions) married to µs‑level scheduling and modern async I/O, to make predictability the default.&#x20; **Corroboration & context** * **APEX (ARINC 653) is the right abstraction** for time/space partitioning and certification boundaries; it’s the lingua franca of Integrated Modular Avionics. ([Wikipedia][5], [NASA][31]) * **Multicore certification guidance exists** (EASA **AMC 20‑193**, FAA adoption path from CAST‑32A): interference channels and determinism are first‑class concerns; your µs scheduler can help if you prove bounded interference. ([EASA][32]) * **µs‑granularity schedulers are real** in research (Shenango/Caladan) and demonstrate tail‑latency control without low utilization. ([amyousterhout.com][6]) **Risks to plan for** * **Tool qualification / DO‑178C:** If you aim for DAL A/B, you’ll need qualified toolchains (Ferrocene helps) and process for verification (DO‑330). Budget for evidence production, not just code. ([ferrous-systems.com][14], [Federal Aviation Administration][33]) * **I/O determinism:** Moving fast I/O to user space (SPDK, AF\\_XDP) is great for performance; for certification, you’ll need rigorous partitions and bounded worst‑case timing, plus interference analysis across cores. *(Inference aligning CAST‑32A concerns with user‑space polling.)* ([EASA][32]) ftware branding is high‑risk. Consider renaming public artifacts. ([USPTO Report][36]) --- ## Quick “traffic‑light” table (today’s risk view) | Idea | Tech Risk | Ecosystem Risk | Time‑to‑Demonstration | Notes | | ------------------------- | -------------- | -------------- | ----------------------- | --------------------------------------------------------------------------------------------------------------------------------- | | **Universal Virtio** | 🟢 Low‑Med | 🟢 Low | **3–6 months** | Fastest path to a working OS experience on curated SKUs; GPU/USB are the hairy bits. ([fosdem.org][1], [spdk.io][4]) | | **Rust Apex** | 🟡 Med | 🟡 Med | **2–3 quarters** | Safety‑aligned stack; perf gains real but localized; needs rigorous benchmarking. ([spdk.io][11], [andrew.nerdnetworks.org][19]) | | **Rust Hallows** | 🟡–🟠 Med‑High | 🟡 Med | **12–18 months (demo)** | Certification & multicore determinism dominate schedule; technically coherent. ([Wikipedia][5]) | | **80/20 device coverage** | 🟢 Low‑Med | 🟡 Med | **4–6 months** | Succeeds with aggressive SKU control + virtio fallback; avoid the hardware long tail at first. ([Linux Hardware][7]) | --- ## Specific pushbacks (diverse perspectives you should entertain) * **“Virtio isn’t native; perf will suffer.”** In practice, **vhost‑user + SPDK** backends erase most overhead for storage; network often hits 10 Gb/s+ with tuning. Not a universal truth, but the *default expectation* for mainstream workloads. ([spdk.io][11]) * **“Rust in the kernel is premature.”** For *mainline Linux* maintainers, sometimes yes; for *your* kernel or user‑space components, Rust is already production‑grade (Cloudflare’s **Pingora** proxy handled *nearly a quadrillion* requests before open‑sourcing). ([The Cloudflare Blog][37]) * **“io\\_uring everywhere?”** Wonderful where Linux‑only and latency matters, but it ties you to Linux and needs vigilance on security fixes. Keep a clean abstraction over I/O so you can swap paths. ([docs.oasis-open.org][23], [Red Hat Docs][24]) * **“Exactly‑once streaming is easy.”** It isn’t; be precise about semantics and fault domains. Your memos’ emphasis on real workload generators is right; measure with tools that don’t lie to you. ([GitHub][27], [Google Groups][28]) --- ## Where your docs are strongest (and I’m citing you) * The *Virtio‑first* thesis and “avoid the driver zoo by standardizing the device model” is the cleanest path to shipping something valuable soon.&#x20; * The *80/20 device coverage* plan is data‑driven and realistic if you publish coverage transparently and lock SKUs. * The *Apex stack* sets the right bar: treat 10–40× as subsystem‑level targets under specific designs (shard‑per‑core + userspace I/O + vectorized compute), not as a blanket claim.&#x20; * The *Hallows* outline maps well to ARINC 653 and modern µs‑schedulers; just budget the verification grind.&#x20; --- ## Suggested next experiments (fastest learning per week) 1. **Virtio backplane bake‑off.** On two “golden” x86 SKUs, stand up virtio‑blk/net/gpu(+Venus) with **SPDK vhost‑user** storage; publish latency/throughput with **HdrHistogram** traces and **wrk2**. Target: parity with a native Linux guest within 10–20% for net/storage on 10 Gb. ([spdk.io][4], [GitHub][27]) 2. **Shard‑per‑core reference service.** Implement a simple KV or HTTP service in Rust, pinned one thread per core, compare against epoll‑threadpool baseline; measure tail latency with and without Shenango‑style core reallocation (static vs. µs‑scale). ([USENIX][16], [amyousterhout.com][6]) 3. **Arrow/DataFusion pathfinder.** Port one analytics workload; compare to DuckDB with ClickBench‑like queries; document when each wins. ([andrew.nerdnetworks.org][19], [datafusion.apache.org][20], [Ibis][25]) 4. **Certification scaffolding.** Pick a small APEX‑like service set; build a DO‑330‑aware test harness and lock toolchain (Ferrocene) to understand process cost early. ([ferrous-systems.com][14], [Federal Aviation Administration][33]) ---",
  "output": {
    "executive_summary": "The optimal business use cases for a high-performance Rust OS built on the proposed concepts are in server-side domains where extreme low latency, deterministic performance, and high assurance are non-negotiable. The top use cases identified are High-Frequency Trading (HFT), 5G User Plane Functions (UPF), and Ad-Tech Real-Time Bidding (RTB). For these workloads, the architecture's superiority over a tuned Linux stack stems from its ability to deliver microsecond-level scheduling and strict time/space partitioning ('Rust Hallows'), which provides a deterministic performance envelope and eliminates cross-tenant interference—a significant advantage over the jitter-prone nature of general-purpose kernels. The minimal (Pareto) set of drivers required to support Kafka, Spark, and backend APIs consists of `virtio-net` and `virtio-blk`/`virtio-scsi` as the primary performance drivers, supplemented by `virtio-console`, `virtio-rng`, and `virtio-vsock` for a functional system. Performance is unlocked by backing these standardized Virtio interfaces with high-performance user-space backends like DPDK/AF_XDP for networking and SPDK vhost-user for storage. The overall vision is technically coherent and feasible, with a first production image achievable in 6–9 months by focusing on a constrained set of hardware SKUs. The long-term strategic differentiation lies not in simply outperforming competitors like Redpanda on raw throughput, but in offering a unique, vertically-integrated stack ('Rust Apex') that combines end-to-end memory safety, high performance, and, crucially, a clear pathway to safety certification (DO-178C, ISO 26262). This positions the OS to capture high-value niches in regulated industries such as avionics, automotive, and telecommunications where competitors cannot meet the stringent assurance requirements.",
    "technology_concepts_clarification": "The terminology used in the query refers to a set of conceptual project names that map to established industry technologies and architectural patterns. \n\n*   **'Universal Virtio'** is a conceptual name for a strategy that leverages the **OASIS Virtio (Virtual I/O Device) specification** to create a standardized, minimal driver surface for a new operating system. Instead of writing numerous native drivers for specific hardware, this approach implements a small set of mature, cross-platform Virtio drivers (e.g., `virtio-net`, `virtio-blk`). Performance is maintained by connecting these standardized front-end drivers to high-performance, user-space backends on the host, such as **SPDK vhost-user** for storage and **DPDK/AF_XDP** for networking. This approach aims to reduce development complexity and enhance portability across different hypervisors and hardware platforms.\n\n*   **'Rust Apex'** is a conceptual name for a **vertically-integrated, memory-safe, and certifiable software stack built entirely in Rust**. It aims to achieve significant (10-40x) performance gains in specific subsystems by combining several modern architectural patterns. Key components of this concept include: 1) The use of a **safety-qualified Rust toolchain like Ferrocene** to target certifications such as ISO 26262 and DO-178C. 2) The adoption of **thread-per-core or shard-per-core architectures**, inspired by frameworks like Seastar (used in ScyllaDB and Redpanda), to eliminate cross-core contention. 3) The extensive use of **user-space I/O libraries like SPDK and DPDK** to bypass kernel overheads. 4) The implementation of **vectorized data processing** using technologies like Apache Arrow and the DataFusion query engine.\n\n*   **'Rust Hallows'** is a conceptual name for a **high-assurance operating system layer designed for predictability and safety**. It draws its principles from two main sources: 1) The **ARINC 653 specification**, which is the standard for time and space partitioning in safety-critical avionics, providing a robust model for isolating applications from each other. 2) Research on **microsecond-scale schedulers like Shenango and Caladan**, which demonstrate the ability to reallocate CPU cores in microseconds to maintain extremely tight tail latency under load. The goal of 'Rust Hallows' is to combine these concepts to create an OS surface that is both highly predictable and suitable for certification under standards like EASA AMC 20-193 for multicore systems.",
    "top_business_use_cases": [
      {
        "use_case_name": "High-Frequency Trading (HFT)",
        "workload_description": "HFT involves the automated execution of a massive number of financial orders at extremely high speeds. Algorithms analyze market data feeds in real-time to identify and capitalize on small, fleeting price discrepancies. The entire workflow, from ingesting market data packets to making a decision and sending an order, is a hyper-optimized, latency-critical path.",
        "performance_slos": "Latency is the paramount metric, with firms targeting sub-microsecond and even nanosecond-level decisioning times. Typical SLOs include P99 latencies of 75-157 microseconds for cloud-native exchanges and 90th-percentile round-trip latencies around 200 microseconds. The goal is to minimize jitter and achieve a predictable, low tail latency.",
        "market_analysis": "The HFT market is valued at approximately $10.36 billion in 2024, projected to grow to $16.03 billion by 2030. The willingness-to-pay for a latency advantage is exceptionally high, as it can generate an estimated $5 billion annually for winning firms at the expense of slower competitors. Firms invest heavily in co-location at exchanges and premium low-latency data feeds.",
        "competitive_advantage_summary": "A tuned Linux stack with DPDK can still suffer from OS jitter and high tail latency due to context switches and background tasks. The proposed Rust OS with a Shenango-style microsecond scheduler ('Rust Hallows') can reallocate cores every ~5µs, maintaining P99.9 latency below 100µs even under heavy load. Its APEX-style partitioning provides verifiable isolation, eliminating interference between the trading algorithm and other system processes, offering a level of determinism and safety (via Rust's memory safety) that Linux cannot match."
      },
      {
        "use_case_name": "5G User Plane Function (UPF)",
        "workload_description": "The UPF is a core component of a 5G network responsible for packet routing, forwarding, policy enforcement, and usage reporting. It is critical for enabling advanced 5G services, especially Ultra-Reliable Low Latency Communications (URLLC), which underpins applications like industrial automation, remote surgery, and connected vehicles.",
        "performance_slos": "URLLC services demand end-to-end latencies of 1ms or less. A high-performance UPF must maintain consistent low latency for priority packets, with demonstrated round-trip times of 70-90 microseconds and jitter as low as 12-14 microseconds, even at high CPU utilization (e.g., 87%). Throughput is also massive, with some solutions achieving over 500 Gbps.",
        "market_analysis": "The 5G infrastructure market is vast, with value driven by the enablement of new, high-margin URLLC services. Telcos are willing to invest in solutions that reduce TCO and guarantee Quality of Service (QoS), as this allows them to offer premium, differentiated services.",
        "competitive_advantage_summary": "A standard Linux scheduler struggles to prevent high-bandwidth traffic from impacting the latency of high-priority URLLC packets. The proposed Rust OS, with its microsecond-level scheduling and strict partitioning, can guarantee that URLLC traffic meets its sub-100µs latency budget regardless of other network load. This ability to maintain low latency and jitter at high CPU utilization provides a significant TCO advantage by allowing telcos to run UPFs on fewer cores. The memory safety of Rust also provides a more reliable and secure foundation for this critical network function."
      },
      {
        "use_case_name": "Ad-Tech Real-Time Bidding (RTB)",
        "workload_description": "RTB is a programmatic auction where ad inventory is bought and sold on a per-impression basis. When a user visits a webpage, a bid request is sent to numerous demand-side platforms (DSPs), which must analyze the request and return a bid within a very strict time limit (e.g., 100-200ms).",
        "performance_slos": "While the end-to-end timeout is longer than in HFT, low P99 latency is critical. Lower latency allows more bidders to participate in an auction, which directly increases revenue (CPMs) for publishers. The workload is characterized by massive, spiky network I/O, handling millions of bid requests per second.",
        "market_analysis": "The RTB market is enormous, estimated at $678.37 billion in 2023 and projected to grow. The willingness-to-pay for lower latency is indirect but powerful: lower latency leads to more successful bids and higher revenue. One study showed a 58% increase in average CPMs by increasing bidder participation.",
        "competitive_advantage_summary": "Ad exchanges experience highly variable traffic. The proposed OS's Shenango-style scheduler can react to sudden load spikes (e.g., from 100k to 5M requests/sec) with almost no additional tail latency, a significant advantage over Linux. This prevents dropped requests and maximizes auction participation. The combination of Universal Virtio with high-performance user-space backends (SPDK/DPDK) provides an efficient I/O path for the network-intensive workload, while the CPU efficiency allows for co-location of analytics tasks without performance interference, reducing TCO."
      }
    ],
    "pareto_set_of_virtio_drivers": {
      "device_name": "Core I/O Pair: virtio-net and virtio-blk/scsi",
      "purpose": "To provide the fundamental high-performance networking and storage I/O required by all target workloads, forming the 80/20 of the driver development effort. `virtio-net` handles all network traffic, while `virtio-blk` or the more feature-rich `virtio-scsi` handles all persistent storage operations.",
      "required_for_workloads": "This pair is essential for all target workloads. For Kafka, it supports high-throughput message replication and log persistence. For Spark, it is critical for network-intensive shuffle operations and spilling intermediate data to disk. For backend APIs, it provides the low-latency network communication for requests and the storage backend for databases and logging.",
      "recommended_backend": "For `virtio-net`, the recommended backends are user-space networking frameworks like DPDK or AF_XDP to achieve near line-rate performance. For `virtio-blk`/`virtio-scsi`, the recommended backend is the SPDK vhost-user target, which can deliver over 10 million IOPS per core and significantly lower latency than kernel-based alternatives."
    },
    "performance_analysis_for_target_workloads": [
      {
        "workload_name": "Kafka-like Streaming System",
        "hot_path_analysis": "The performance-critical hot paths in Kafka are the produce path (receiving message batches and appending them to an on-disk log), the consume path (reading batches from the OS page cache and sending them to the network), and the replication path (followers fetching data from the leader). Kafka's performance relies heavily on sequential disk I/O and zero-copy network transfers via the `sendfile()` syscall. A key source of tail latency in standard deployments is the JVM's garbage collection (GC) pauses and kernel-level I/O stalls.",
        "architectural_impact": "The proposed OS architecture addresses these hot paths by: 1) Replacing the kernel's storage stack with `virtio-blk` backed by SPDK vhost-user, providing a direct, zero-copy, low-latency path to NVMe storage that can achieve over 1M IOPS. 2) Replacing the kernel's networking stack with `virtio-net` backed by DPDK/AF_XDP, which offers 5-10x lower tail latency than the kernel stack. 3) Implementing the system in Rust on a Rust OS, which completely eliminates GC pauses. 4) Using a shard-per-core design (like Redpanda) to minimize cross-core contention and context switching.",
        "expected_performance_gains": "A 1.5x to 2.5x increase in message throughput per core is expected due to the efficiency of user-space I/O. A substantial reduction in tail latency, potentially 5x to 20x lower than a standard Kafka deployment, is anticipated. The combination of a GC-free runtime, low-latency I/O, and a shard-per-core architecture could push p99.9 latencies from tens or hundreds of milliseconds into the low-millisecond or high-microsecond range."
      },
      {
        "workload_name": "Spark-like Analytics",
        "hot_path_analysis": "The primary performance bottleneck in Spark-like analytics is the shuffle operation, which involves extensive network I/O to exchange intermediate data between executors and disk I/O when this data spills from memory. Other hot paths include reading data from storage and writing final results. Stragglers, or slow-running tasks often caused by data skew or resource contention, are a major source of high tail latency in job completion times.",
        "architectural_impact": "The proposed architecture addresses these bottlenecks by: 1) Using a Rust-native engine like DataFusion/Ballista, which leverages the efficient columnar Apache Arrow format for in-memory processing and network exchange. 2) Accelerating shuffle read/write and spill operations by using SPDK-backed storage, which offers up to 10x lower software overhead. 3) Using accelerated network transport (UCX/RDMA or DPDK/AF_XDP) to reduce CPU utilization for shuffle by 25-40%. 4) Employing a Shenango-style microsecond scheduler that can reallocate CPU cores every 5µs to mitigate stragglers and prevent them from delaying an entire job stage.",
        "expected_performance_gains": "A significant reduction in both average job completion time and high-percentile tail latency is expected. The combination of user-space I/O, a microsecond scheduler, and a vectorized engine directly attacks the primary bottlenecks. I/O tail latency can be reduced by up to 6.7x (p99.99) with SPDK optimizations. Scheduling latency is reduced to the microsecond scale, effectively eliminating the OS scheduler as a source of jitter and delay."
      },
      {
        "workload_name": "Latency-Critical Backend APIs (HTTP/gRPC)",
        "hot_path_analysis": "The hot path for a backend API involves receiving a request, TLS termination, deserializing the payload (JSON/Protobuf), executing business logic (often involving database or cache lookups), serializing a response, and sending it over the network. The key challenge is maintaining extremely low and predictable tail latency (p99.9) under bursty load conditions. Kernel overhead from interrupts, context switches, and data copies is a major source of latency.",
        "architectural_impact": "The proposed architecture addresses this by: 1) Using kernel-bypass networking (DPDK or AF_XDP) to eliminate kernel overhead, as demonstrated by Seastar/DPDK achieving 1.19M req/s with 403µs p99.99 latency vs. 358k req/s and 1.1ms with the kernel. 2) Employing a microsecond-scale scheduler like Shenango or Caladan. These schedulers can maintain p99.9 latency at 52-93µs even under extreme load shifts, an order of magnitude better than standard schedulers. 3) Utilizing a thread-per-core design to eliminate locking and context switching within the application.",
        "expected_performance_gains": "An order-of-magnitude or greater reduction in p99.9 tail latency is expected, moving from the millisecond range common with tuned Linux stacks to the low-microsecond range (e.g., <100µs). The system will be able to handle extreme, bursty loads with minimal impact on tail latency, while also achieving high CPU efficiency by allowing spare cycles to be used by batch tasks without performance interference."
      }
    ],
    "networking_stack_recommendation": {
      "technology_name": "AF_XDP (vs. DPDK)",
      "performance_profile": "AF_XDP can achieve very high throughput, sometimes outperforming DPDK in workloads with multiple buffers per packet, and can process approximately 2.5 times more packets than previous methods in some use cases. However, its latency profile is more variable than DPDK's, with higher average latency and significantly more jitter at the tail (p99.9). DPDK offers the lowest and most consistent latency, with one study showing its p99.9 latency to be 4.1x lower than AF_XDP Zero-Copy and 5.6x lower than the Linux kernel. DPDK consistently achieves line-rate throughput on high-speed NICs (e.g., 148.81 Mpps for 64B packets on 100GbE).",
      "operational_tradeoffs": "AF_XDP's primary operational advantage is its excellent integration with the Linux ecosystem. The NIC remains a standard kernel network device, allowing the use of familiar tools like `ifconfig` and `tcpdump`. This simplifies management and observability. Its main trade-off is a dependency on recent kernel versions (e.g., v4.18+ for basic sockets, v5.4+ for zero-copy). In contrast, DPDK completely bypasses the kernel, which increases operational complexity as standard tools cannot be used on the interface. DPDK requires dedicated CPU cores for polling, which can lead to high power consumption, but offers a more mature and feature-rich ecosystem.",
      "suitability_for_project": "AF_XDP is strategically better aligned with the 'Universal Virtio' project goals. Its ability to integrate with the kernel's networking stack means it can accelerate a `virtio-net` device without detaching it from the OS, which is a significant advantage for a standardized device model. The ongoing development of zero-copy support for `virtio-net` within the AF_XDP framework is a game-changer for achieving high performance in virtualized and cloud environments, making it the more forward-looking and flexible choice."
    },
    "storage_stack_recommendation": {
      "technology_name": "SPDK vhost-user",
      "performance_profile": "SPDK vhost-user dramatically outperforms kernel-based vhost. Benchmarks show it can achieve over 1,000,000 IOPS for 4K random writes compared to ~200,000 for traditional Virtio. For single-request latency, SPDK measures 20 microseconds versus 100 microseconds for Virtio. Compared to the kernel's vhost implementation, SPDK vhost-scsi can deliver up to 3.2x better IOPS for 4K random read workloads and offers up to 3x better efficiency and latency. This performance is achieved by avoiding costly VM-exits and kernel context switches, leading to lower CPU utilization within the guest for I/O-heavy tasks.",
      "durability_and_operability": "Durability is ensured through the `VIRTIO_BLK_F_FLUSH` feature. When a guest OS issues a flush command (e.g., via `fsync`), the SPDK vhost target maps this directly to the underlying NVMe FLUSH command, guaranteeing data is committed to persistent media. From an operability standpoint, achieving maximum performance requires careful configuration. This includes enabling the multi-queue block layer (`blk-mq`) in the guest, correctly setting the number of queues in the hypervisor, and properly configuring hugepages for the shared memory regions between the guest and the SPDK vhost process. Misconfiguration can lead to severe performance degradation.",
      "recommendation_for_workloads": "SPDK vhost-user is highly recommended for both Kafka logs and analytics spill files. For Kafka, which performs large, sequential writes, the high throughput and low latency of SPDK accelerate the underlying block device operations. For Spark, which can generate mixed I/O patterns when spilling data to disk, SPDK's high IOPS capability minimizes the performance penalty of these spills. For both workloads, it is recommended to use the SPDK backend in conjunction with a well-tuned filesystem like XFS, as applications like Kafka are designed to leverage the OS page cache, and SPDK accelerates the underlying persistence of that cached data."
    },
    "cloud_deployment_strategy": {
      "provider_name": "Amazon Web Services (AWS)",
      "hypervisor_and_architecture": "The AWS Nitro System, which uses a custom, lightweight Nitro Hypervisor. This hypervisor only manages CPU and memory allocation. All other I/O, including networking, storage, and security, is offloaded to dedicated hardware cards (Nitro Cards), delivering performance that is nearly indistinguishable from bare metal.",
      "high_performance_interfaces": "Networking: The proprietary Elastic Network Adapter (ENA), which leverages Single Root I/O Virtualization (SR-IOV) for a direct, low-latency path to the hardware, supporting speeds up to 100 Gbps and beyond with ENA Express. Storage: Amazon Elastic Block Store (EBS) volumes are exposed to instances as NVMe block devices, also using SR-IOV for high throughput and low latency. A custom OS must have specific drivers for ENA and AWS NVMe to achieve high performance.",
      "virtio_compatibility_conclusion": "A 'Universal Virtio' strategy is incompatible with achieving high performance on modern AWS instances. The high-performance paths for networking and storage rely on proprietary ENA and NVMe interfaces, not Virtio-net or Virtio-blk. While Virtio-vsock is used for niche cases like Nitro Enclaves, relying on generic Virtio for primary I/O would result in suboptimal performance or incompatibility with modern instance types. To be competitive on AWS, a custom OS must include drivers for AWS's specific hardware interfaces."
    },
    "security_and_isolation_architecture": {
      "threat_type": "DMA/PCIe Attack",
      "threat_description": "In a multi-tenant environment, peripherals with Direct Memory Access (DMA) capabilities pose a significant threat by bypassing traditional CPU memory protections. Malicious or compromised peripherals can be exploited to read sensitive system memory, such as data from other tenants or cryptographic keys, or even to hijack kernel control flow. This threat is exemplified by attacks like Thunderclap and vulnerabilities such as CVE-2022-21819, which allow for privilege escalation and arbitrary code execution via the PCIe bus, completely breaking tenant isolation.",
      "mitigation_strategy": "The primary hardware defense is the Input/Output Memory Management Unit (IOMMU), which must be enabled to virtualize I/O addresses and restrict a device's memory access to only its assigned buffers. However, given the IOMMU's own potential vulnerabilities (spatial and temporal attacks), it is part of a larger strategy. The architecture must use the VFIO (Virtual Function I/O) kernel driver to safely expose DMA capabilities to user-space applications like the Storage Performance Development Kit (SPDK). This combination contains the device within a sandboxed IOMMU domain, preventing unauthorized memory access and mitigating the risk of DMA-based tenant escape or system compromise."
    },
    "certification_pathway_for_high_assurance": {
      "pathway_component": "Multicore Processor (MCP) Interference Analysis",
      "governing_standard": "EASA AMC 20-193 / FAA AC 20-193 (superseding CAST-32A)",
      "implementation_details": "To comply with the standard for high-integrity systems (DAL A/B), the OS development process must systematically identify all potential interference channels on the target multicore hardware. This includes shared resources like memory controllers, last-level caches, and interconnects. For each identified channel, a mitigation strategy (e.g., cache partitioning, memory bandwidth regulation) must be implemented and verified. The verification process requires extensive testing on the final target hardware under maximum contention and stress conditions to determine and validate the Worst-Case Execution Time (WCET) of software components, ensuring that timing behavior remains bounded and predictable.",
      "key_challenges": "The primary challenge is the immense complexity and cost associated with a comprehensive multicore interference analysis. Identifying, mitigating, and providing verifiable evidence for *all* potential interference channels on a modern, complex System-on-a-Chip (SoC) is a massive and expensive undertaking. Proving that user-space I/O frameworks like SPDK and AF_XDP, which are optimized for best-case performance, have bounded worst-case timing under all conditions is a significant engineering and research challenge that is central to achieving certification."
    },
    "recommended_hardware_platform": [
      {
        "hardware_type": "NIC",
        "vendor_and_model": "Intel Ethernet 800 Series (e.g., E810)",
        "key_features_and_performance": "Supports speeds up to 200 Gbps. Features a mature `ice` PMD in DPDK, which is actively developed and requires tight coupling of driver, firmware, and DDP package versions. Supports AF_XDP, RDMA (iWARP/RoCEv2), and extensive virtualization with up to 256 VFs per device and 2K queue pairs. Offers advanced offloads like Inline Flow Director and Application Device Queues (ADQ).",
        "justification_for_selection": "Premier choice due to its advanced feature set, robust and actively maintained DPDK and AF_XDP support, and superior virtualization capabilities (SR-IOV/SIOV). It is a well-tested and documented platform within the high-performance networking community, making it ideal for a reference implementation."
      },
      {
        "hardware_type": "NIC",
        "vendor_and_model": "NVIDIA Mellanox ConnectX-5 / ConnectX-6 / ConnectX-7",
        "key_features_and_performance": "High-throughput cards supporting up to 400 Gbps (ConnectX-7). The `mlx5` DPDK PMD is highly mature and performant, with extensive testing in DPDK performance reports. Has long-standing, mature support for AF_XDP Zero Copy. Excellent SR-IOV support (up to 127 VFs per port) and advanced offloads for live migration and network service headers.",
        "justification_for_selection": "A strong competitor known for leading performance in HPC and cloud environments. Its mature `mlx5` driver for both DPDK and AF_XDP ensures high compatibility and performance, making it a reliable choice for demanding network workloads."
      },
      {
        "hardware_type": "NVMe SSD",
        "vendor_and_model": "Samsung PM1743",
        "key_features_and_performance": "A top-tier PCIe 5.0 SSD offering leading performance with sequential reads/writes of 13/6.6 GB/s and random read/write IOPS of 2.7M/1.3M. Supports dual-port operation for high availability and offers capacities up to 15.36TB. As a datacenter-grade drive, it has robust thermal management.",
        "justification_for_selection": "As a standards-compliant NVMe drive from a leading vendor, it is fully compatible with SPDK's user-space driver. Its PCIe 5.0 performance is ideal for saturating the capabilities of the SPDK storage stack and serving as a high-performance benchmark target for Kafka logs and analytics spill files."
      },
      {
        "hardware_type": "NVMe SSD",
        "vendor_and_model": "Kioxia CM7 Series",
        "key_features_and_performance": "A competitive PCIe 5.0 and NVMe 2.0 compliant drive. The CM7-V model delivers up to 2.7M random read IOPS. Offers high capacities up to 30.72TB and provides endurance options of 1 and 3 Drive Writes Per Day (DWPD), making it suitable for various write-intensive workloads.",
        "justification_for_selection": "Kioxia is a major enterprise SSD vendor, and its standards-compliant drives are well-supported by SPDK's generic NVMe driver. Its range of endurance options and form factors provides flexibility for different server builds and workload requirements."
      },
      {
        "hardware_type": "NVMe SSD",
        "vendor_and_model": "Micron 9400 NVMe SSD",
        "key_features_and_performance": "A high-performance PCIe 4.0 drive designed for critical workloads like caching, OLTP, and HFT. Its key advantage is offering very large capacities, with models over 30TB. Built for sustained performance and thermal stability in datacenter environments.",
        "justification_for_selection": "While based on PCIe 4.0, its high performance and massive capacity make it an excellent choice for capacity-sensitive workloads. Micron's adherence to NVMe standards ensures good compatibility and support within the SPDK ecosystem."
      }
    ],
    "developer_experience_and_adoption_strategy": {
      "strategy_area": "API and Compatibility",
      "component": "Syscall Surface and Compatibility Layer",
      "proposed_approach": "The OS should expose a minimal POSIX subset and a Linux-compatible syscall surface via a shim library, rather than attempting a full reimplementation. This compatibility layer, potentially built using a port of a standard C library like `musl`, would map the necessary Linux syscall numbers to the OS's internal handlers. The focus should be on implementing the critical syscalls required by target workloads like the JVM. This includes essential file I/O (`open`, `read`, `write`, `mmap`, `O_DIRECT`), zero-copy I/O (`sendfile`), atomic metadata operations (an atomic `rename` is critical for Spark), networking (`socket`, `epoll`), and thread synchronization (`futex`, including `FUTEX_WAIT_BITSET`). This allows applications to be recompiled and linked against the library, enabling them to run on the new OS without a full port.",
      "justification": "This approach is justified by its focus on accelerating adoption and minimizing developer effort. Requiring a complete application rewrite to a new, proprietary OS API would create an insurmountable barrier for complex, mature codebases like Kafka and Spark. By providing a familiar, compatible syscall surface for the most critical operations, developers can leverage their existing knowledge of Linux and POSIX. This strategy allows them to get their applications running quickly, see the performance benefits of the new OS, and then incrementally optimize specific hot paths using the OS's more advanced, non-POSIX features (like microsecond scheduling APIs or direct user-space I/O access). It dramatically lowers the learning curve and the initial investment required, making adoption a far more attractive proposition."
    },
    "observability_for_low_latency_systems": {
      "observability_layer": "Telemetry Primitives (Latency Measurement)",
      "recommended_technology": "HdrHistogram",
      "performance_characteristics": "HdrHistogram offers constant-time recording, typically taking only 3-6 nanoseconds on modern CPUs. It maintains a fixed, predictable memory footprint (e.g., an instance tracking values from 1 microsecond to 1 hour with 3 significant digits of precision requires only ~185 KB). The recording path involves zero memory allocations unless the histogram needs to be resized. It also supports highly compressed logging, capable of reducing a 31KB log file to just 1KB.",
      "role_in_system": "HdrHistogram plays a critical role by enabling the accurate, lossless recording of latency distributions without introducing locking or significant overhead. In a microsecond-scheduled system where tail latency is paramount, HdrHistogram allows developers to capture the full picture of performance, including extreme percentiles (p99.9, p99.999), without degrading that performance. Its lock-free concurrency models (per-thread instances or SingleWriterRecorders) align perfectly with the shard-per-core architecture of Rust Apex, ensuring that telemetry collection does not become a source of contention or jitter. This allows for the precise validation of SLOs and provides the high-fidelity data needed for post-mortem failure analysis."
    },
    "benchmark_methodology_for_validation": {
      "principle_or_tool": "Coordinated Omission (CO)",
      "description": "Coordinated Omission is a critical measurement pitfall where the load generator inadvertently synchronizes with the System Under Test (SUT), leading to a significant under-reporting of tail latencies. This typically occurs in closed-loop tests where the generator waits for a response before sending the next request. If the SUT stalls or experiences a high-latency event, the load generator also pauses, failing to send requests at the intended rate. As a result, these high-latency periods are omitted from the measurements, creating a deceptively optimistic view of system performance and masking the true worst-case behavior experienced by users.",
      "implementation_guidance": "To construct a rigorous benchmark that avoids Coordinated Omission, the following steps must be taken:\n1. **Use a Fixed-Rate Load Generator:** Employ an open-loop or fixed-rate load generator like `wrk2` or `k6`. These tools must be configured to send requests at a constant, predefined rate, regardless of how quickly the SUT responds. For `wrk2`, this is achieved using the `--rate` or `-R` parameter. For `k6`, the `constant-arrival-rate` executor must be used.\n2. **Utilize HdrHistogram for Measurement and Correction:** The HdrHistogram library should be used for all latency measurements due to its low overhead and high precision. It includes a built-in mechanism to correct for CO after the test run. By using the `recordValues(value, expectedIntervalBetweenValueSamples)` function, the histogram can infer and account for measurements that were missed due to system stalls, providing a more accurate picture of the true latency distribution.\n3. **Compare Corrected and Uncorrected Results:** To demonstrate the impact of CO and the validity of the methodology, both the raw (uncorrected) and corrected latency histograms should be reported. The difference between them highlights the measurements that would have been missed by a naive benchmark setup. For example, `wrk2`'s `--u_latency` flag can be used to show the uncorrected results for direct comparison."
    },
    "competitive_landscape_and_differentiation": {
      "competitor_name": "Redpanda",
      "architecture_summary": "Redpanda is a streaming data platform engineered to be a high-performance, API-compatible replacement for Apache Kafka. Its core architecture is built in C++ on the Seastar framework, which enables a shared-nothing, thread-per-core design. This approach dedicates a CPU core and its associated memory and I/O channels to a specific shard, minimizing cross-core contention and eliminating the need for locks. The entire platform is delivered as a single, self-contained binary, which notably eliminates dependencies on the Java Virtual Machine (JVM) and Apache ZooKeeper. Instead of ZooKeeper, Redpanda uses a native Raft implementation for consensus and metadata management. Its storage engine is designed from the ground up to maximize the performance of modern hardware, particularly NVMe SSDs, and it includes an intelligent memory auto-tuner to manage resources efficiently. Recent architectural enhancements include flexible write caching and native integrations for Apache Iceberg and Postgres Change Data Capture (CDC).",
      "performance_claims_and_benchmarks": "Redpanda makes aggressive performance claims, positioning itself as being up to 10 times faster and 6 times more cost-effective than Apache Kafka, with tail latencies up to 70 times lower under certain workloads. A 2024 report commissioned by Redpanda claimed a 60% cost saving over Confluent. Third-party benchmarks have shown P99 latencies in the 8-15ms range, and Snowflake reported achieving P99 latencies of 5 seconds or less in a December 2024 use case. However, these claims are contested. A critical analysis by Jack Vanlightly of Confluent in May 2023 suggested that Redpanda's performance can be highly unstable and workload-dependent, degrading significantly over long durations, when data retention limits are hit, or with changes in workload patterns (e.g., increasing producer count). In these specific tests, Kafka reportedly surpassed Redpanda's throughput and ability to drain backlogs on identical hardware. This highlights that while Redpanda can be extremely fast under ideal conditions, its performance envelope may be narrower than that of a mature Kafka deployment.",
      "market_positioning": "Redpanda is positioned as a modern, high-performance, and operationally simpler alternative to Apache Kafka for demanding streaming data workloads. Its target market consists of organizations that are either building new real-time applications or are struggling with the performance limitations, operational complexity, and high TCO of existing Kafka deployments. By offering Kafka API compatibility, Redpanda provides a direct migration path for existing applications. Its value proposition centers on superior performance (especially tail latency), lower infrastructure costs due to higher efficiency, and simplified operations by eliminating the need to manage a separate ZooKeeper cluster and the JVM. It is expanding its ecosystem through acquisitions (benthos.dev) and integrations with other data systems like Spark, Flink, and ClickHouse to position itself as a central hub in the modern data stack."
    },
    "go_to_market_and_packaging_strategy": {
      "strategy_element": "Packaging and Distribution Channels",
      "details": "A flexible, multi-channel packaging strategy is crucial. For cloud-native customers, the primary channel should be **Cloud Marketplaces**. This involves providing pre-configured, production-ready images for AWS, Azure, and GCP. These images must meet specific technical requirements, such as providing Amazon Machine Images (AMIs) for AWS that are HVM-virtualized and EBS-backed, Virtual Hard Disks (VHDs) for Azure, and Compute Engine images for GCP. For on-premise customers, two main packages are needed: **Virtual Appliance Images** in OVA and ISO formats for easy deployment on hypervisors like VMware ESXi and Nutanix, and a **Bare-Metal Installer** for customers in Telco and Gaming who require maximum performance and control. This installer should support automated provisioning via PXE boot and declarative configuration. Finally, to build trust and simplify procurement for large enterprises, a **Reference Hardware & OEM Program** should be established. This involves creating a Hardware Compatibility List (HCL) of certified servers and forming OEM partnerships with vendors like Dell, HPE, and Supermicro to offer pre-installed or certified hardware.",
      "example_from_industry": "Nutanix provides an excellent example of a successful multi-faceted packaging strategy. They offer their software for various on-premise deployments and have strong OEM partnerships with hardware vendors like Dell (XC series) and Lenovo (HX series), providing customers with integrated, pre-validated hyperconverged infrastructure appliances. This simplifies deployment and support. Similarly, Red Hat maintains an extensive certified hardware catalog, giving enterprise customers confidence that the OS will run reliably on their chosen servers, a strategy that has been fundamental to its widespread adoption in data centers."
    },
    "project_risk_register": {
      "risk_description": "The failure to achieve, or significant delays in achieving, high-assurance safety certifications (e.g., DO-178C DAL B/A for avionics, ISO 26262 ASIL D for automotive) for the 'Rust Hallows' OS variant. This process is exceptionally complex, costly, and time-consuming, requiring immense documentation, rigorous verification processes, traceability for every requirement, and satisfying dozens of formal objectives. A vehicle manufacturer might spend over 5,000 hours on process setup alone. Any failure in this area would prevent the product from entering its primary target markets, rendering a core part of its value proposition invalid.",
      "risk_category": "Business",
      "likelihood": "Medium",
      "impact": "Critical",
      "mitigation_plan": "1. **Early and Continuous Planning:** Begin the certification process on day one. Develop a comprehensive Plan for Software Aspects of Certification (PSAC) and engage with certification authorities (e.g., FAA, EASA) early and frequently to align on expectations. 2. **Leverage Qualified Toolchains:** Utilize pre-qualified and cert-ready tools like the Ferrocene Rust toolchain to significantly reduce the burden and cost of tool qualification under standards like DO-330. 3. **Acquire Specialized Expertise:** Hire or engage consultants with deep, proven experience in successfully navigating DO-178C and ISO 26262 certification processes. 4. **Automate Verification and Documentation:** Invest heavily in tools for automated testing, requirements traceability, static analysis, and documentation generation to reduce manual effort, minimize human error, and control costs. 5. **Adopt a Phased Certification Strategy:** Mitigate risk by certifying a minimal, core version of the OS first. Then, incrementally extend the certification to cover more complex components and features in subsequent phases, creating a reusable set of artifacts."
    },
    "product_roadmap_and_staffing_plan": {
      "phase_name": "Phase 1: First Production Image",
      "duration_months": 9,
      "objective": "Deliver a production-ready OS image featuring a standardized device model based on the 'Universal Virtio' concept. This includes developing and integrating core device drivers for virtio-blk, virtio-net, virtio-console, virtio-rng, and virtio-vsock, and ensuring they are backed by high-performance user-space frameworks like SPDK for storage and DPDK/AF_XDP for networking.",
      "key_deliverables": "The key deliverables for this phase include: refined and stabilized versions of all integrated Virtio drivers and their high-performance backends; finalized, comprehensive documentation including developer guides, user deployment instructions, and performance tuning guides; and the final release artifacts, including the bootable OS image and associated deployment scripts for supported hardware SKUs and virtualization platforms.",
      "staffing_requirements": "A team of 8-12 engineers is required. The team composition must include: 4-6 OS Kernel Developers with expertise in Rust and low-level systems programming; 2-3 Networking Specialists with deep knowledge of DPDK and AF_XDP; 1-2 Storage Specialists with expertise in SPDK and NVMe; 1-2 Performance Engineers for profiling and benchmarking; and 1-2 QA/Test Automation Engineers to build out the CI/CD and regression testing infrastructure."
    },
    "legal_and_licensing_considerations": {
      "component_or_topic": "Virtio Specification",
      "license_or_policy": "OASIS IPR Policy (Non-Assertion Mode)",
      "key_obligations_and_risks": "The primary risk is patent encumbrance. Red Hat, Inc. has disclosed numerous U.S. patents and patent applications that it believes may be essential to implementing the Virtio standard. The key obligation for any implementer is to abide by the OASIS IPR Policy. The risk is substantially mitigated because participants in the Virtio Technical Committee, including Red Hat, have agreed to the Non-Assertion covenant, meaning they will not assert their essential patent claims against any other implementer of the standard.",
      "compliance_recommendation": "The recommended action is to build the OS with confidence in the legal protections provided by the OASIS IPR Policy's Non-Assertion Mode. The project should maintain awareness of the policy and the specific patent disclosures made by Red Hat. Additionally, a formal trademark clearance should be conducted for any project names (e.g., 'Universal Virtio') to avoid conflicts with existing trademarks, including the registered 'OASIS' trademark."
    }
  },
  "outputBasis": [
    {
      "field": "legal_and_licensing_considerations",
      "citations": [
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device.",
            ". For\ninformation on whether any patents have been disclosed that may be essential to\nimplementing this specification, and any offers of patent licensing terms, please refer\nto the Intellectual Property Rights page in the TC’s GitHub repository\n( <https://github.com/oasis-tcs/virtio-admin/blob/master/IPR.md> ).",
            "This specification is provided under the Non-Assertion Mode of the OASIS IPR Policy, the mode chosen when the Technical Committee was established.",
            "The name \"OASIS\" is a trademark of [OASIS](https://www.oasis-open.org/) , the owner and developer of this\nspecification, and should be used only to refer to the organization and its official\noutputs. OASIS welcomes reference to, and implementation and use of, specifications,\nwhile reserving the right to enforce its marks against misleading uses"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Block Device Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/vhost-user-slave.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "oasis-tcs/virtio-spec: OASIS Virtual I/O Device TC",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "OASIS Virtual I/O Device TC: Source code for the VIRTIO (Virtual I/O) Specification maintained by the OASIS Virtual I/O Device (VIRTIO) Technical Committee.",
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms.",
            "Licensing. Please see the LICENSE file for description of the license terms and OASIS policies applicable to the TC's work in this GitHub project ..."
          ]
        },
        {
          "title": "OASIS Virtual I/O Device (VIRTIO) TC IP and License Policy",
          "url": "https://www.oasis-open.org/committees/virtio/ipr.php",
          "excerpts": [
            "Red Hat has filed several United States patent applications that might become essential to an OASIS Standards Final Deliverable.",
            "This TC operates under the Non-Assertion Mode of the OASIS IPR Policy.",
            "The US Patent Nos. 9811260 and 9311164 \"System and Method for Ballooning wth Assigned Devices\", the US Patent No. 9934157 and the US Publication No. 20180225221 \"Post-Copy VM Migration Speedup Using Free Page Hinting\", the US Publication No. 20170147371 \"Virtual Machine Migration Using Memory Page Hints\", the US Patent No. 9965311 \"Application of a Pattern to Inactive Pages in Guest Memory\", the US Patent No.\n10169088 \"Lockless Free Memory Ballooning for Virtual Machines\", the US Patent Nos. 10083058 and 9672062 \"Batched Memory Page Hinting\" and the US Publication No. 20180239707 \"Page Cache Management During Migration\" are relevant to section \"Traditional Memory Balloon Device\" of the Virtual I/O Device (VIRTIO) specification.",
            "Red Hat is unable to provide more specific information regarding the unpublished applications, until such time as the applications are published or the patents are issued."
          ]
        }
      ],
      "reasoning": "The most direct support for the requested fine-grained field comes from excerpts describing the governance and IP framework of Virtio. The Virtio specifications are published as OASIS documents, and later excerpts explicitly reference the OASIS IPR policy and the Non-Assertion mode, which is central to your field value. These excerpts establish that Virtio is an open standard with IP policy considerations managed through OASIS, including statements about IPR policy terms and the Non-Assertion covenant. Additional excerpts cite the existence of patent disclosures and the governance around essential patents within the Virtio community, which align with the field value's focus on patent encumbrance and obligations. The policy references are complemented by primary Virtio specification references, which ground the IP discussion in the standard itself. Together, these sources directly substantiate the core components of the field value: the existence and role of the OASIS IPR policy (Non-Assertion Mode), and the involvement of entities like Red Hat in patent disclosures related to Virtio. Supporting material from the same or closely related documents further reinforces the policy framework (IPR policy pages) and the notion that trademark considerations may be prudent (as hinted by discussions of policy and governance around Virtio branding). In parallel, other excerpts provide the technical background for Virtio (the device model and standardization) which contextualizes why IP policy matters for implementers of Virtio in OSes, including potential risk and governance steps. The combination of policy-focused excerpts and primary Virtio spec references yields a cohesive mapping to the fine-grained field value's components: the policy (Non-Assertion), the risk/patent disclosures, and the governance around compliance and branding, which informs the recommended compliance actions.",
      "confidence": "medium"
    },
    {
      "field": "technology_concepts_clarification",
      "citations": [
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device.",
            "A driver would perform a typical initialization routine like so:\n\n1\\. Identify and initialize the receive and transmission virtqueues, up to\n\n     N of each kind. If VIRTIO\\_NET\\_F\\_MQ feature bit is negotiated,\n\n     N= max\\_virtqueue\\_pairs , otherwise identify N=1. 2\\.\nIf the VIRTIO\\_NET\\_F\\_CTRL\\_VQ feature bit is negotiated, identify the\n\n     control",
            "The network device has the following device configuration layout. All of the device\nconfiguration fields are read-only for the driver. struct virtio\\_net\\_config {  \nu8 mac [6];  \nle16 status ;  \nle16 max\\_virtqueue\\_pairs ;  \nle16 mtu ;  \nle32 speed ;  \nu8 duplex ;  \nu8 rss\\_max\\_key\\_size ;  \nle16 rss\\_max\\_indirection\\_table\\_length ;  \nle32 supported\\_hash\\_types ;  \nle32 supported\\_tunne",
            "A driver SHOULD NOT negotiate VIRTIO\\_NET\\_F\\_HASH\\_REPORT if it does not\nnegotiate VIRTIO\\_NET\\_F\\_C",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            ". For\ninformation on whether any patents have been disclosed that may be essential to\nimplementing this specification, and any offers of patent licensing terms, please refer\nto the Intellectual Property Rights page in the TC’s GitHub repository\n( <https://github.com/oasis-tcs/virtio-admin/blob/master/IPR.md> )."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Block Device Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/vhost-user-slave.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues.",
            "SPDK optimizations",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and ...",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ..."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "it reallocates cores every 5 microseconds,",
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "Arachne improves upon Linux, maintaining\n99.9th percentile latency below 200 µs with batch work.",
            "Under sudden changes in load, low tail latency is\nonly possible with a short core allocation interval.",
            "by A Ousterhout · Cited by 428 — When handling latency-sensitive applications, such as memcached, we found that Shenango achieves tail latency and throughput comparable to ZygOS ..."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Caladan: Mitigating Interference at Microsecond Timescales",
          "url": "https://amyousterhout.com/papers/caladan_osdi20.pdf",
          "excerpts": [
            "Cal- adan consists of a centralized scheduler core that actively manages resource contention in the memory hierarchy and between hyperthreads, ...",
            "Caladan is a new CPU scheduler that can achieve signifi- cantly better quality of service (tail latency, throughput, etc.) through a collection ...",
            "by J Fried · 2020 · Cited by 251 — Caladan is a CPU scheduler that uses fast core allocation to manage resource contention and mitigate interference by restricting cores from ...",
            "\nmaintains low scheduling work even with many tasks by of-\n\nfloading expensive operations such as sending signals to re-\n\nmote cores.",
            "Caladan outperforms Parties, a state-of-the-art resource parti-\n\ntioning system, by 11,000 _×_ , reducing tail latency from 580\n\nms to 52 _µ_ s during shifts in resource usage while maintaining\n\nhigh CPU "
          ]
        },
        {
          "title": "Officially Qualified - Ferrocene - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/officially-qualified-ferrocene/",
          "excerpts": [
            "Ferrocene 23.06.0, based on Rust 1.68, is now fully usable in safety critical\nenvironments.",
            "This is\nthe first qualification of a Rust compiler. With these qualifications, Rust can now be used to develop software for\nelectronic systems in series production road vehicles.",
            " You can even find the certificate in TÜV SÜDs certificate database.",
            "Ferrocene for use in systems up to ASIL D – the highest classification of\ninitial hazard as defined by this standard. This standard provides automakers\nwith guidelines that make the software being used to develop, produce, operate\nand service the automotive sectors safe to use.",
            "Like the automotive\ncertification, we’ve also gone for the highest level of risk reduction and\nqualified it at SIL4."
          ]
        },
        {
          "title": "ARINC 653",
          "url": "https://en.wikipedia.org/wiki/ARINC_653",
          "excerpts": [
            "The position paper CAST-32A defines a set of requirements and guidance that should be met to certify and use multi-core processors in civil aviation by FAA and is expected to be replaced by an Advisory Circular, AC 20-193.",
            "The European Union aviation authority, EASA, published AMC 20-193 in January 2022.",
            "ARINC 653 P1-5 was updated to address multicore processor architectures.",
            "ARINC 653 is a software specification for space and time partitioning in safety-critical avionics real-time operating systems (RTOS).",
            "ARINC 653 (Avionics Application Software Standard Interface) is a software specification for space and time partitioning in safety-critical avionics real-time operating systems (RTOS). It allows the hosting of multiple applications of different software levels on the same hardware in the context of an Integrated Modular Avionics architecture.",
            "In order to decouple the real-time operating system platform from the application software, ARINC 653 defines an API called APplication EXecutive (APEX).",
            "The ARINC 653 APEX services are API calls belonging in six categories:"
          ]
        }
      ],
      "reasoning": "To map the fine-grained field to the excerpts, I identify excerpts that directly discuss the named technologies or architectures. Direct Virtio references (the Virtio specification family, vhost/vhost-user, and the Virtio transport concepts) are the strongest evidence for Universal Virtio, since this field is anchored to a cross-OS Virtio device model and its standardization. Excerpts that describe Virtio's device model, the role of virtqueues, and the use of vhost-user or vfio-user in SPDK contexts support the Universal Virtio concept by showing how a standardized device model enables driver-unification and high-performance backends. Explicit mentions of the Virtio specification, its transport layers, and the core VIRTIO_NET/VIRTIO_BLK features ground this claim.\n\nThe Rust Apex subfield is supported by excerpts that discuss a vertically integrated, safety-certified Rust stack, including Ferrocene as a DO-178C/ISO 26262-qualified toolchain, and assertions about safety tooling maturing and performance improvements through architectures like shard-per-core, SPDK/DPDK I/O, and vectorized analytics. The references to Ferrocene and to safety-certification frameworks (ISO 26262, DO-178C) provide concrete anchoring for the Rust Apex claim, as does the mention of safety-critical tool qualification and high-assurance design practices.\n\nThe Rust Hallows subfield maps to a high-assurance OS surface with ARINC 653-style partitioning, μs-scale schedulers, and certification concerns. Excerpts that align ARINC 653 concepts (APEX interfaces, partitioning, time/space isolation) with a safety-critical OS surface provide direct support. The Caladan/Shenango lines also reinforce microsecond-scale, interference-aware scheduling patterns that underpin a Hallows architecture aimed at predictability and certification readiness.\n\nIn addition, SPDK vhost and AF_XDP excerpts substantiate the needed high-performance I/O backends that would be part of a unified Virtio and Rust Apex/Hallows stack, showing practical pathways to achieve low-latency, kernel-bypass or user-space I/O paths that the target architectures rely on. The ARINC 653 references consolidate the safety-certification framing needed for Hallows. Taken together, these excerpts collectively anchor the stated field values to concrete technologies and architectural patterns.\n",
      "confidence": "high"
    },
    {
      "field": "certification_pathway_for_high_assurance",
      "citations": [
        {
          "title": "[PDF] Caladan: Mitigating Interference at Microsecond Timescales",
          "url": "https://amyousterhout.com/papers/caladan_osdi20.pdf",
          "excerpts": [
            "Cal- adan consists of a centralized scheduler core that actively manages resource contention in the memory hierarchy and between hyperthreads, ..."
          ]
        },
        {
          "title": "Robust partitioning for multicore systems doesnt mean freedom from interference",
          "url": "https://www.rapitasystems.com/blog/robust-partitioning-multicore-systems-doesnt-mean-freedom-interference",
          "excerpts": [
            "\nMost of the technology for partitioning of single core systems also applies to multicore systems. Multicore systems are more complex, however – for these systems, interference can exist between software hosted on different cores but running at the same time.",
            "The most rigorous separation is termed _robust partitioning_ . This is defined for single core systems in the ARINC 653 standard, where it refers to complete isolation of separated components, i.e. the execution of one component can have no effect on the oth"
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "**AMC 20-193**",
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ...",
            "MCP_Resource_Usage_3:  \n\nThe applicant has identified the interference channels that could permit interference to affect the \nsoftware applications hosted on the MCP cores, and has verified the applicant’s chosen means of \nmitigation of the interfer"
          ]
        },
        {
          "title": "Time Protection: The Missing OS Abstraction",
          "url": "https://tomchothia.gitlab.io/Papers/EuroSys19.pdf",
          "excerpts": [
            "by Q Ge · 2019 · Cited by 113 — Time sharing cannot produce interference on these, while concurrent access can be observed as a reduction of available bandwidth. Cache channels work by the ..."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "ARINC 653",
          "url": "https://en.wikipedia.org/wiki/ARINC_653",
          "excerpts": [
            "The European Union aviation authority, EASA, published AMC 20-193 in January 2022."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "AMC 20-193 was released in January 2022, and AC 20-193 was released some two years later in January 2024. The two guidance documents are fully aligned from a technical and certification perspective."
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant material describes interference-aware partitioning and mitigation approaches for multicore MCPs in safety-critical contexts. The Caladan and Shenango family papers explicitly address microsecond-scale interference management and core allocation policies, including the concept of fast interference detection and centralized control that mitigates cross-core interference, which is central to DO-178C CAST-32A AMC 20-193 MCP guidance. Discussions of robust space-time partitioning, and how interference channels can be identified and mitigated, map directly to the need to identify and bound interference in high-assurance MCP configurations. Doctrinal AMC/CAST materials provide the regulatory framing that identifies acceptance criteria for interference analysis and tool qualification, which aligns with the need for WCET verification and evidence generation in DAL A/B contexts. ARINC 653 context broadens the avionics partitioning discipline and reinforces the two-level scheduling paradigm (time partitioning and APC-like execution domains), supporting the notion that MCP interference analysis must consider partitioned execution environments. The DO-330/DO-178C tool qualification materials give concrete steps for evidencing tool readiness and the nature of artifacts required to certify the verification process. Additional material on DO-178C/CAST-32A alignment with multi-core processing and interference risk management informs how a practical MCP interference program should be structured, including evidence collection, test harness design, and worst-case timing analyses. Collectively, these excerpts validate the core field value: a structured, certified interference analysis framework for MCPs that includes identification of interference channels, partitioning strategies, mitigation, WCET verification, and governance under AMC 20-193/CAST-32A and related avionics standards.",
      "confidence": "high"
    },
    {
      "field": "competitive_landscape_and_differentiation",
      "citations": [
        {
          "title": "Redpanda vs Kafka",
          "url": "https://www.redpanda.com/compare/redpanda-vs-kafka",
          "excerpts": [
            "Redpanda provides 3-6x greater cost efficiency compared to traditional Kafka infrastructure, lowering both infrastructure and operational costs for your team. ..."
          ]
        },
        {
          "title": "A tour of Redpanda Streamfest 2024",
          "url": "https://www.redpanda.com/blog/redpanda-streamfest-2024",
          "excerpts": [
            "... P99 latencies of 5 seconds or less. Simplicity is a core principle at Snowflake, so naturally, they found a kindred soul in Redpanda ..."
          ]
        },
        {
          "title": "How Redpanda Works | Redpanda Self-Managed",
          "url": "https://docs.redpanda.com/current/get-started/architecture/",
          "excerpts": [
            "Redpanda implements a thread-per-core programming model through its use of the Seastar library. This allows Redpanda to pin each of its application threads to a ..."
          ]
        },
        {
          "title": "Apache Pulsar vs Kafka vs RedPanda: Modern Streaming | sanj.dev",
          "url": "https://sanj.dev/post/pulsar-kafka-redpanda-streaming-comparison",
          "excerpts": [
            "Pulsar: P99 latency 20-35ms (includes BookKeeper overhead); RedPanda: P99 latency 8-15ms (C++ implementation advantage). ³ Independent ..."
          ]
        },
        {
          "title": "Redpanda vs Kafka - A Detailed Comparison",
          "url": "https://www.ksolves.com/blog/big-data/redpanda-vs-kafka-a-detailed-comparison",
          "excerpts": [
            "Oct 23, 2024 — He found that Kafka surpassed Redpanda in terms of performance to a large extent, offering high throughput and achieving low latencies."
          ]
        },
        {
          "title": "How Redpanda built a Distributed Data Streaming Architecture for ...",
          "url": "https://resources.scylladb.com/video-conference-talk/how-vectorized-built-a-distributed-data-streaming-architecture-for-modern-hardware-with-seastar",
          "excerpts": [
            "Watch this talk from Alexander Gallego to learn how Redpanda (previously named Vectorized) leverages Seastar to build a new streaming engine that is Kafka API ..."
          ]
        },
        {
          "title": "Ecosystem",
          "url": "https://www.redpanda.com/categories/ecosystem",
          "excerpts": [
            "Explore integrations and tooling across the Redpanda ecosystem to expand your data streaming capabilities. Discover new possibilities now."
          ]
        }
      ],
      "reasoning": "The most weighty evidence comes from directly addressing Redpanda vs Kafka and its architectural choices. Excerpts describing Redpanda as a \"modern, high-performance, and operationally simpler alternative to Apache Kafka\" and noting that it is \"built in C++ on the Seastar framework\" provide direct support for the field's architectural claims and its positioning in the competitive landscape. Additional statements that Redpanda operates as a single binary with no JVM, and that it uses a native Raft for consensus, align with the architecture and deployment model described in the field value. Claims about tail latency improvements (for example, \"tail latencies up to 70 times lower under certain workloads\") bolster the performance positioning against Kafka, and are directly relevant to the performance portion of the field value. The references that discuss Redpanda's ecosystem expansion (integrations with Spark, Flink, and ClickHouse) support market positioning and ecosystem breadth as described in the field value. Benchmark discussions that compare throughput and latency between Redpanda and Kafka, including third‑party analyses and industry commentary, corroborate the notion of competitive dynamics and contested benchmarks. The cited material about Redpanda's architecture and performance, and its ecosystem trajectory, most directly maps to the field value's core claims about architecture, performance, and market positioning. Additional excerpts that discuss Kafka's traditional durability and fsync considerations provide context for why Redpanda's competitive advantages may be perceived in practice, though they are secondary to the primary Redpanda-focused evidence. Overall, the strongest alignment comes from explicit architecture and performance comparisons, followed by ecosystem and benchmarking discussions, with some caveats noted in third‑party analyses.",
      "confidence": "medium"
    },
    {
      "field": "networking_stack_recommendation",
      "citations": [
        {
          "title": "AF_XDP - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "To discover if a driver supports multi-buffer AF\\_XDP in zero-copy\nmode, use XDP\\_FEATURES and first check the NETDEV\\_XDP\\_ACT\\_XSK\\_ZEROCOPY",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing.",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "AF_XDP Documentation (Kernel.org) – Networking/af_xdp.html",
          "url": "https://www.kernel.org/doc/html/v6.9/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing."
          ]
        },
        {
          "title": "A story about AF_XDP, network namespaces and a cookie",
          "url": "https://blog.cloudflare.com/a-story-about-af-xdp-network-namespaces-and-a-cookie/",
          "excerpts": [
            "Jul 18, 2022 — It uses the Linux kernel AF_XDP feature to transfer packets from a network device in kernel space to a memory buffer in user space without going through the ..."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "When you bind to a socket, the kernel will first try to use zero-copy copy. ... Running the rxdrop benchmark in XDP_DRV mode can then be done using:.",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. If zero-copy is not supported, it will fall back on using copy mode, i.e. copying ...",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "Performance comparison of DPDK vs AF_XDP and Linux networking (thesis findings)",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "DPDK and AF_XDP are competitive. DPDK\n\nis more consistent and resulted in more or less the same maximum throughput\nbetween measurements, while AF_XDP achieved higher throughput at the\nmulti-buffer-based workloads the cost of more variation between tests",
            "DPDK mandates a DPDK-specific driver for the interface,\npotentially leading to vendor lock-in. If only a few manufacturers support\nDPDK-specific drivers, the choice of NIC will be limited by the software\napplication.",
            "e multi-buffer workloads, AF_XDP outperforms DPDK on\nall workloads, even if some of them were close. This shows that multi-buffers\nin AF_XDP are already performant even though it is a new feature. ",
            "The results indicate that DPDK delivers optimal performance when using a single buffer per packet, whereas AF_XDP outperforms in workloads employing multiple ..."
          ]
        },
        {
          "title": "DPDK Performance Report Release 20.02",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_20_02_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The test case is to measure DPDK vhost PMD's capability for supporting the maximum TCP bandwidth with virtio-net device.",
            "DPDK Performance Report"
          ]
        }
      ],
      "reasoning": "The most directly relevant evidence shows that AF_XDP is an optimized path for high-performance packet processing that can operate as a kernel-bypass style option while remaining integrated with the Linux networking stack. The kernel documentation describes AF_XDP as an optimized address family for high performance packet processing, with explicit notes about zero-copy and multi-buffer support, and how it can deliver zero-copy data paths and configurable modes. This directly supports the claim that AF_XDP integrates with the kernel and enables high-performance, low-latency networking while still allowing forward visibility into kernel tooling and observability. Additional excerpts explain specific AF_XDP capabilities such as zero-copy operation, multi-buffer handling, and how XDP-based paths can bypass kernel copies, which aligns with the asserted advantage of AF_XDP's integration and performance potential in a Virtio-based virtualization stack, particularly when considering a unified Virtio driver model and related vhost/vhost-user configurations. Separate excerpts provide concrete performance measurements contrasting DPDK and AF_XDP: they report throughput benchmarks (e.g., tens to hundreds of Mbps or Gbps-scale throughput depending on test setup) and tail latency observations that AF_XDP can achieve high throughput but may exhibit higher tail latency or greater variance, especially at the p99.9 level, compared to DPDK, which has historically shown very tight tail latency and line-rate throughput in similar settings. This direct performance comparison is critical for assessing the stated performance profile where DPDK yields lower tail latency (lower p99.9) and AF_XDP offers strong kernel-integrated performance with modern zero-copy capabilities. The cited thesis-style excerpts further reinforce that DPDK tends to deliver the most consistent latency and lower tail latency in many workloads, while AF_XDP can perform very well on multi-buffer workloads but with more latency variability and tail latency concerns in head-to-tail measurements. Additional supportive excerpts describe empirical AF_XDP expectations in practice (e.g., integration with virtio-net, potential zero-copy paths for vhost, and multi-queue configurations) which directly relate to the Operational Tradeoffs section of the field value. Finally, the excerpts that document DPDK performance benchmarks (throughput and latency) help triangulate the comparison, illustrating the default expectation that DPDK can achieve high throughput with favorable latency characteristics, albeit with kernel bypass complexity and higher CPU core dedication, while AF_XDP remains kernel-integrated and easier to observe and manage within existing Linux tooling, but with some tail latency risk under certain configurations.",
      "confidence": "high"
    },
    {
      "field": "legal_and_licensing_considerations.license_or_policy",
      "citations": [
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This specification is provided under the Non-Assertion Mode of the OASIS IPR Policy, the mode chosen when the Technical Committee was established."
          ]
        },
        {
          "title": "OASIS Virtual I/O Device (VIRTIO) TC IP and License Policy",
          "url": "https://www.oasis-open.org/committees/virtio/ipr.php",
          "excerpts": [
            "This TC operates under the Non-Assertion Mode of the OASIS IPR Policy.",
            "Red Hat has filed several United States patent applications that might become essential to an OASIS Standards Final Deliverable.",
            "The US Patent Nos. 9811260 and 9311164 \"System and Method for Ballooning wth Assigned Devices\", the US Patent No. 9934157 and the US Publication No. 20180225221 \"Post-Copy VM Migration Speedup Using Free Page Hinting\", the US Publication No. 20170147371 \"Virtual Machine Migration Using Memory Page Hints\", the US Patent No. 9965311 \"Application of a Pattern to Inactive Pages in Guest Memory\", the US Patent No.\n10169088 \"Lockless Free Memory Ballooning for Virtual Machines\", the US Patent Nos. 10083058 and 9672062 \"Batched Memory Page Hinting\" and the US Publication No. 20180239707 \"Page Cache Management During Migration\" are relevant to section \"Traditional Memory Balloon Device\" of the Virtual I/O Device (VIRTIO) specification.",
            "Red Hat is unable to provide more specific information regarding the unpublished applications, until such time as the applications are published or the patents are issued."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpts explicitly state that the policy or the specification is governed by the Non-Assertion Mode of the OASIS IPR Policy. One excerpt states, 'This specification is provided under the Non-Assertion Mode of the OASIS IPR Policy,' which precisely matches the requested field value and confirms the mode. Another excerpt repeats that 'This TC operates under the Non-Assertion Mode of the OASIS IPR Policy,' which again aligns exactly with the field value. Additional excerpts reference the OASIS Virtual I/O Device TC IPR Policy more generally, indicating licensing and IPR policy discussions in the same policy framework, which supports the connection to the stated field value, though not as verbatim about the mode. Collectively, these excerpts corroborate the specific Non-Assertion Mode policy stance within the OASIS IPR framework and provide contextual backing from related policy documents and patent policy notes.",
      "confidence": "high"
    },
    {
      "field": "benchmark_methodology_for_validation",
      "citations": [
        {
          "title": "Correcting for \"coordinated omission\"",
          "url": "https://github.com/tdunning/t-digest/issues/128",
          "excerpts": [
            "The technique used by the HdrHistogram is far from perfect for monitoring use cases. I suspect that this is a reason that even though the HdrHistogram get wide use, people don't exploit its coordinated omission correction features as often as one might expect.",
            "Correcting for \"coordinated omission\"\nFortunately the technique used in the HDR Histogram to correct for\ncoordinated omission can be directly applied to the t-digest. The basic\nidea is that a user picks some value that they \"expect\" an operation to\ntake and then the code will take all of the data points which exceed that\nexpected quantity and adds new values to the histogram decreasing from the\nbucket value at by the expected value until from the value is lower than\nthe expected value.\nThis same approach can be applied either while\nrecording data into the histogram (\naddWhileCorrectingForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html.HdrHistogram.AbstractHistogram-long->>)\nor after the fact (copyCorrectedForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html>>\n)",
            "\"Coordinated Omission\"\n----------------------\n\nGil Tene [in this talk](https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf) introduced the concept of \"coordinated omission\" and a common error made when trying to understand tail latency of systems. makes the valuable observation that in the context of application latency monitoring, there is often a relatively fixed concurrency. This means that in response to application stalls, the throughput during the period of the stall will be dramatically lowered. Say for example you have one worker calling some function and you want to understand the performance characteristics of that function.",
            "Correcting for \"coordinated omission\". Fortunately the technique used in the HDR Histogram to correct for coordinated omission can be directly ... I have an ongoing goal to push the world of application monitoring towards a data-structure that provides high precision quantile estimation over extremely large ranges of values without manual configuration. In order to do this, there are a number of details which need to be worked out to supplant the existing dominant technique in this space which is the HDR Histogram. Practically speaking, with good configuration, the HDR Histogram is a very effective data structure. My qualms with it are:\n\n1. the configuration requirement\n2. the size-precision trade-off that must be decided at configuration time"
          ]
        },
        {
          "title": "Mechanical Sympathy Google Group Discussion on Coordinated Omission",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/icNZJejUHfE/m/BfDekfBEs_sJ",
          "excerpts": [
            "As one of the people who've had a spirited hour+ long debate with Gil on this topic, I would like to defend the position briefly: coordinated omission detection is far from perfect (it will by no means detect all sources), and requires various assumptions such as a steady rate of message arrival, a symmetrical distribution of message size/workload, pauses being system-wide... Coordinated omission correction is tricky business, for sure. Detection is a bit easier. E.g. if you detect a process-wide-affecting glitch (e.g.",
            "I started talking about Coordinated Omission in test systems mostly, but I kept finding it everywhere, including in many production monitoring systems. Coordinated Omission is nothing more than a measurement methodology problem. It exists in both test and production environments, and occurs whenever data is omitted ahead of analysis in a way that is coordinated with the [usually bad] behavior of the system. Coordinated omission basically guarantees a huge garbage-in-garbage-out relationship between the data feeding into [downstream] analysis and most operational conclusions that people make based on that data.",
            "A common measurement technique problem I call \"Coordinated Omission\" for a while, which can often render percentile data useless. There is good news here, too. [HdrHistogram](http://giltene.github.io/HdrHistogram/) has a recording mode that automatically corrects for coordinated omission if you know what your expected interval between measurements is (and here it is ~msec). If you add an HdrHistogram to such a test and record your call latencies using the [recordValues(value, expectedIntervalBetweenValueSamples)](http://giltene.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html(long,%20long)) form, the Histogram will automatically re-created the missing measurements. It is often useful to compare the output of two HdrHistogram's one of which uses the correcting form and one of which that just records the raw values, as the differences in output at the higher percentiles is often \"impressive\".",
            "Mike, I think that you are correct in that the simple correction mode offered by HdrHistogram will not work for your scenarios below. Correcting for coordinated omission can be challenging. When your test scenario is simple enough, it can be easily corrected, and HdrHistogram's can optionally help with some of those scenarios. e.g. Whenever a you have a fixed expected interval between requests or a fixed delay (\"think time\") between requests, and your load tester uses synchronous client threads (each thread waits for a response before adding a new request), an expectedIntervalBetweenValueSamples can be computed and passed as an extra parameter to HdrHistogram's recordValue call. This sort of correction can be applied either at test time or as a post-processing step to tester log files.",
            "The explanation is simple: it is mostly so I can highlight exactly how ridiculous they are as metrics for describing latency behavior...",
            "The output from HdrHistogram's histogram.getHistogramData().outputPercentileDistribution() was specifically designed so that a few tens of percentile report lines still provide enough dynamic range and percentile resolution that it can be taken straight into a plot like the above, and keep the plot nice, smooth and accurate. This is exactly where reports from linear or logarithmic histogram buckets fail, as quantization kills the graphs, and that (I think) is why most people report histograms in counts-per-bucket format instead of like the above...",
            "In contrast, HdrHistogram starts with the assumption that required accuracy and range are dictated by the user (as opposed to \"try to be as accurate as possible), and it is designed to (also) work well in  the very real world where results can be \"clumped\" in both value range and arrival times (e.g. bad results tend to happen close to each other) - which is the situation that seems to describe the degenerate case (from a size or computation complexity perspective) for t-digets. HdrHistogram is also designed to be FAST, and fixed in both space and time. Still, I'd be happy to see a performant implementation of t-diget compared head-to-head against HdrHistogram...",
            "Solving \"Coordinated Omission\" problem inside benchmarking/load testing tools is however very reasonable approach, but in my opinion compensating for CO should remain within the realm of testing tools and must not be spilled into production codebase. Thanks"
          ]
        },
        {
          "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch ...",
          "url": "https://www.vldb.org/pvldb/vol12/p2195-masson.pdf",
          "excerpts": [
            "by C Masson · Cited by 99 — Merging two DDSketches is very fast—it takes around 10 microseconds or less to merge two sketches containing up to fifty million values each—depending on the ..."
          ]
        },
        {
          "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch ...",
          "url": "https://arxiv.org/pdf/1908.10693",
          "excerpts": [
            "by C Masson · 2019 · Cited by 99 — Merging two DDSketches is very fast—it takes around 10 microseconds or less to merge two sketches containing up to fifty million values each— ..."
          ]
        },
        {
          "title": "otelcol.processor.probabilistic_sampler",
          "url": "https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.processor.probabilistic_sampler/",
          "excerpts": [
            "The probabilistic sampler processor supports several modes of sampling for spans and log records. Sampling is performed on a per-request basis, considering ..."
          ]
        },
        {
          "title": "Constant arrival rate | Grafana k6 documentation",
          "url": "https://grafana.com/docs/k6/latest/using-k6/scenarios/executors/constant-arrival-rate/",
          "excerpts": [
            "The constant-arrival-rate executor starts a fixed number of iterations over a period, independent of system response, to maintain a constant rate.",
            "With the constant-arrival-rate executor, k6 starts a fixed number of iterations over a specified period of time. It is an open-model executor, meaning ..."
          ]
        },
        {
          "title": "Correcting YCSB's Coordinated Omission problem",
          "url": "http://psy-lob-saw.blogspot.com/2015/03/fixing-ycsb-coordinated-omission.html",
          "excerpts": [
            "Mar 11, 2015 — So as a first step toward correcting YCSB I have gone in and added an HdrHistogram measurement container. This is pretty straight forward as all ..."
          ]
        },
        {
          "title": "Tag: hdrhistogram",
          "url": "https://bravenewgeek.com/tag/hdrhistogram/",
          "excerpts": [
            "In this article, we look at Apache Kafka and NATS Streaming, two messaging systems based on the idea of a commit log. We'll compare some of the features of ..."
          ]
        },
        {
          "title": "Spdk bdev - General",
          "url": "https://forums.linbit.com/t/spdk-bdev/650",
          "excerpts": [
            "Mar 3, 2025 — The SPDK fio example contains an ioengine (ioengine=/opt/spdk/build/fio/spdk_bdev) for testing, this made me thinking, that it might be possible ...",
            "What I am looking for is using the “local” SPDK driver to acces the NVMe devices. (similar to fio with SPDK_BDEV driver)."
          ]
        },
        {
          "title": "using-SPDK-vhostuser-and-kata.md - GitHub",
          "url": "https://github.com/kata-containers/documentation/blob/master/use-cases/using-SPDK-vhostuser-and-kata.md",
          "excerpts": [
            "Launch a Kata container with SPDK vhost-user block device. To use vhost-user-blk device, use Docker to pass a host vhost-user-blk device to the ..."
          ]
        },
        {
          "title": "databricks/spark-sql-perf - GitHub",
          "url": "https://github.com/databricks/spark-sql-perf",
          "excerpts": [
            "TPC-DS. Setup a benchmark. Before running any query, a dataset needs to be setup by creating a Benchmark object. Generating the TPCDS data requires dsdgen ..."
          ]
        },
        {
          "title": "Use the TPC-DS benchmark to test Spark SQL performance - GitHub",
          "url": "https://github.com/IBM/spark-tpc-ds-performance-test",
          "excerpts": [
            "This code pattern is aimed at helping Spark developers quickly setup and run the TPC-DS benchmark in their own development setup."
          ]
        },
        {
          "title": "Managed Service for Kafka benchmarking and scaling ...",
          "url": "https://cloud.google.com/blog/products/data-analytics/managed-service-for-kafka-benchmarking-and-scaling-guidance",
          "excerpts": [
            "Jun 13, 2025 — Learn about Google Managed Service for Kafka performance benchmarking and how to optimize your deployment for throughput and latency."
          ]
        },
        {
          "title": "Optimising Kafka for stream processing in latency sensitive ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S1877050918315473",
          "excerpts": [
            "by R Wiatr · 2018 · Cited by 17 — We propose a set of performance tests that can be used to measure Kafka impact on the system and performance test results of KafkaProducer Java API."
          ]
        },
        {
          "title": "Does K6 deal with the coordinated omission problem? Gil Tene ...",
          "url": "https://news.ycombinator.com/item?id=25240114",
          "excerpts": [
            "He argues that percentiles simply aren't useful, because, statistically, most requests will experience >= 99.99-percentile response times. All ..."
          ]
        },
        {
          "title": "Apache Kafka: How to Test Performance for Clients ...",
          "url": "https://medium.com/metrosystemsro/apache-kafka-how-to-test-performance-for-clients-configured-with-ssl-encryption-3356d3a0d52b",
          "excerpts": [
            "Test the Producer Performance. First let's create a topic for the test data. The following example will result in a topic named ssl-perf-test ..."
          ]
        },
        {
          "title": "Performance testing",
          "url": "https://cwiki.apache.org/confluence/display/KAFKA/Performance+testing",
          "excerpts": [
            "kafka-producer-perf-test.sh - We will add a csv option ... How have you configured multiple number of producer threads using kafka-producer-perf-tool.sh?"
          ]
        },
        {
          "title": "Load testing producer, consumer and end-to-end latencies ...",
          "url": "https://medium.com/google-cloud/load-testing-producer-consumer-and-end-to-end-latencies-for-kafka-on-google-kubernetes-engine-8191c09f57e7",
          "excerpts": [
            "This blog post will walk you through benchmarking a Kafka on GKE with Kraft mode enabled. We'll explore producer and consumer performance, end-to-end latencies ..."
          ]
        },
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "So let’s try a benchmark of ScyllaDB and see what we get.",
            "How Not to YCSB",
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency.",
            "Apr 22, 2021 — Coordinated omission is a term coined by Gil Tene to describe the phenomenon when the measuring system inadvertently coordinates with the system being measured."
          ]
        },
        {
          "title": "GitHub - giltene/wrk2: A constant throughput, correct ...",
          "url": "https://github.com/giltene/wrk2",
          "excerpts": [
            "HdrHistogram includes\na simple way to compensate for Coordinated Omission when a known\nexpected interval between measurements exists.",
            "One of wrk2's main modification to wrk's current (Nov. 2014) measurement\nmodel has to do with how request latency is computed and recorded. wrk's model, which is similar to the model found in many current load\ngenerators, computes the latency for a given request as the time from\nthe sending of the first byte of the request to the time the complete\nresponse was received. While this model correctly measures the actual completion time of\nindividual requests, it exhibits a strong Coordinated Omission effect,",
            "Constant throughput load generators\nMake this trivial to model."
          ]
        },
        {
          "title": "Use HDR histograms for calculating percentiles in ...",
          "url": "https://github.com/loadimpact/k6/issues/763",
          "excerpts": [
            "Sep 12, 2018 — HDR Histogram is designed for recoding histograms of value measurements in latency and performance sensitive applications. [ ... ] The HDR ...",
            "After all, the HdrHistogram is a good fit for the normal Trend metrics that k6 emits, since they are positive numbers that are usually close to ...",
            "Sep 12, 2018 — I've been using HDR Histograms to provide a consistent interface over different load-testing tools output stats/metrics to be able to chart them together ..."
          ]
        },
        {
          "title": "Can someone explain in words what the coordinated ...",
          "url": "https://news.ycombinator.com/item?id=10486215",
          "excerpts": [
            "Nov 1, 2015 — Coordinated omission occurs when a closed-system workload generator is used against an open or partly-open system, where the system's ..."
          ]
        },
        {
          "title": "HdrHistogram by giltene",
          "url": "https://hdrhistogram.github.io/HdrHistogram/",
          "excerpts": [
            "Measurements show value recording times as low as 3-6 nanoseconds on modern (circa 2014) Intel CPUs. The HDR Histogram maintains a fixed cost in both space and ...",
            "Missing: accuracy figures"
          ]
        },
        {
          "title": "hdrhistogram - Rust - Docs.rs",
          "url": "https://docs.rs/hdrhistogram",
          "excerpts": [
            "Measurements show value recording times as low as 3-6 nanoseconds on modern (circa 2014) Intel CPUs. The HDR Histogram maintains a fixed cost in both space ...",
            "HDR Histogram is designed for recording histograms of value measurements in latency and performance sensitive applications. Measurements show value recording ...",
            "Value precision is expressed as the number of significant digits in the value recording, and provides control over value quantization behavior across the value ..."
          ]
        },
        {
          "title": "Computing accurate percentiles with DDSketch - Datadog",
          "url": "https://www.datadoghq.com/blog/engineering/computing-accurate-percentiles-with-ddsketch/",
          "excerpts": [
            "This paper discusses DDSketch, a new sketch that we designed and implemented in order to tackle the challenges of working with monitoring data."
          ]
        },
        {
          "title": "Optimizing Event Loads with CI/MON's eBPF Sensor - Cycode",
          "url": "https://cycode.com/blog/how-we-optimized-ci-mon-ebpf-sensor-to-handle-thousands-of-events-per-second/",
          "excerpts": [
            "In this article, we detail our journey in developing and optimizing the Cimon eBPF agent, a tool designed to process thousands of events per second."
          ]
        },
        {
          "title": "Clarify whether logging to standard output is blocking #668 - GitHub",
          "url": "https://github.com/tokio-rs/tracing/issues/668",
          "excerpts": [
            "Non-blocking logging is theoretically superior, but the difference is probably only meaningful in particularly performance-sensitive code."
          ]
        },
        {
          "title": "journald.conf(5) — systemd — Debian testing",
          "url": "https://manpages.debian.org/testing/systemd/journald.conf.5.en.html",
          "excerpts": [
            "Note: Forwarding is performed synchronously within journald, and may significantly affect its performance. This is particularly relevant ..."
          ]
        },
        {
          "title": "Sockperf – UDP/TCP Latency and Throughput Benchmarking Tool",
          "url": "https://docs.nvidia.com/networking/display/VMAv9840/Appendix+Sockperf+%E2%80%93+UDP/TCP+Latency+and+Throughput+Benchmarking+Tool",
          "excerpts": [
            "Sockperf can provide a full log of all packets' tx and rx times by dumping all the data that it uses for calculating percentiles and building ..."
          ]
        },
        {
          "title": "Cloudflare eBPF benchmark README",
          "url": "https://github.com/cloudflare/ebpf_exporter/blob/master/benchmark/README.md",
          "excerpts": [
            "This is a simple eBPF benchmark showing overhead of eBPF probes. Setup ... Notice how tracepoints are faster than fentry and fentry is faster than kprobe.",
            "The results below are from MacBook Air (M1, 2020) running Linux 6.5-rc1\nin QEMU.",
            "```\nBenchmarkGetpidWithoutAnyProbes/getpid             9954225       117.3 ns/op\nBenchmarkGetpidTracepointWithNoMap/getpid          9098228       132.2 ns/op\nBenchmarkGetpidTracepointWithSimpleMap/getpid      7995439       152.2 ns/op\nBenchmarkGetpidTracepointWithComplexMap/getpid     5655841       212.8 ns/op\nBenchmarkGetpidFentryWithNoMap/getpid              8481037       141.0 ns/op\nBenchmarkGetpidFentryWithSimpleMap/getpid          7582813       159.1 ns/op\nBenchmarkGetpidFentryWithComplexMap/getpid         4579310       220.7 ns/op\nBenchmarkGetpidKprobeWithNoMap/getpid              4725835       253.8 ns/op\nBenchmarkGetpidKprobeWithSimpleMap/getpid          4306387       277.1 ns/op\nBenchmarkGetpidKprobeWithComplexMap/getpid         3460576       346.3 ns/op\n```\n",
            "Empty probe attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n| tracepoint empty | 132 | 15 | 13% |\n| fentry empty | 141 | 24 | 21% |\n| kprobe empty | 254 | 137 | 117% |\n",
            "Probe with a simple map increment attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n| tracepoint simple | 152 | 35 | 30% |\n| fentry simple | 159 | 42 | 36% |\n| kprobe simple | 277 | 160 | 136% |\n",
            "Probe with a complex map increment attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n|\ntracepoint complex | 213 | 96 | 82% |\n| fentry complex | 220 | 103 | 88% |\n| kprobe complex | 346 | 229 | 196% |\n",
            "Big slowdown in terms of % for complex case may sounds like terrible,\nbut you have to remember that we're using a fast `getpid()` syscall. The main number to look at above is overhead in nanoseconds, because that's\nwhat you're going to pay no matter how fast or frequent function you're\nprobing is. 200-300ns overhead for the complex case of `getpid` is a lot, but\nfor tracing operations like disk access it's nothing compared to baseline. Notice how tracepoints are faster than fentry and fentry is faster than kprobe. Keep in mind that these numbers are for a single logical CPU core. Your mileage may vary depending on your hardware, make sure to test\nif you are hyper sensitive to any slowdowns."
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch.",
            "Code here:  <https://github.com/DataDog/sketches-go>  <https://github.com/DataDog/sketches-py>  <https://github.com/DataDog/sketches-java>"
          ]
        },
        {
          "title": "Systemd journald performance discussions (GH issue 2815)",
          "url": "https://github.com/systemd/systemd/issues/2815",
          "excerpts": [
            "Performance issues occur when a high throughput workload emits a large number of logs."
          ]
        }
      ],
      "reasoning": "Directly supporting the target field requires identifying excerpts that (a) define Coordinated Omission, (b) describe how it distorts latency measurements, and (c) present recommended mitigation techniques (e.g., fixed-rate load generation, HDRHistogram correction, and comparing corrected vs uncorrected results). The most relevant excerpts explicitly name Coordinated Omission and discuss its impact and remedies, including HDR histogram corrections and the practice of using fixed-rate load generators like wrk2 or k6 to avoid the bias from the load generator. The excerpts that discuss SHORTEST-CASE workflows of fixed-rate testing, and those that compare corrected and uncorrected results, directly tie to the field value and its implementation guidance. Those mentioning HDRHistogram correction functions or methods for compensating for missed samples also directly map to the described field. These excerpts form a tight chain: they identify the CO problem, describe how traditional benchmarks can miss tail latency during stalls, and propose concrete instrumentation and methodology to produce accurate tail latency measurements (fixed-rate load generators, HDR histogram corrections, and comparing corrected vs raw results). The excerpts about HDR histograms (and their correction APIs) provide the concrete mechanisms to implement the described mitigation. Additional excerpts discuss related scheduling and latency themes (Shenango, Caladan, etc.), which provide broader context but are less directly tied to the CO field value than the core CO-focused texts. Overall, the strongest direct matches are the excerpts that explicitly discuss Coordinated Omission and corrective techniques, while others contribute useful context but are secondary for this finegrained field.",
      "confidence": "high"
    },
    {
      "field": "product_roadmap_and_staffing_plan",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "Virtqueue generally consists of an array of descriptors and each I/O needs to be converted into a chain of such descriptors.",
            "Virtio devices use virtqueues to transport data efficiently.",
            "Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O."
          ]
        },
        {
          "title": "Virtio driver - SPDK",
          "url": "https://spdk.io/doc/virtio.html",
          "excerpts": [
            "SPDK Virtio driver is a C library that allows communicating with Virtio devices. It allows any SPDK application to become an initiator for (SPDK) vhost targets."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        },
        {
          "title": "Azure Accelerated Networking overview",
          "url": "https://learn.microsoft.com/en-us/azure/virtual-network/accelerated-networking-overview",
          "excerpts": [
            "The following diagram illustrates how two VMs communicate with and without Accelerated Networking. ! [Screenshot that shows communication between Azure VMs with and without Accelerated Networking. ](../reusable-content/ce-skilling/azure/media/virtual-network/accelerated-networking.png)"
          ]
        }
      ],
      "reasoning": "Direct references describe the core building blocks of the Phase 1 plan: a standardized Virtio device model that unifies drivers across OSes, and high-performance backends to back Virtio devices. The SPDK vhost Target and related SPDK Virtio driver material illustrate how Virtio-backed storage can achieve near-native performance by moving IO paths into user space and bypassing kernel overhead. Descriptions of vhost-processing and SPDK-backed virtio setups demonstrate the intended production-ready storage backend for Virtio devices, which aligns with the field value's tooling and performance goals. Additional excerpts outline Virtio's device model for net/block and its orchestration via vhost-user, SPDK, and AF_XDP, reinforcing the networking facet of the Phase 1 plan. The Virtio spec entries provide authoritative background on the standard's breadth and how devices like net, blk, rng, console, and vsock are modeled, which supports the roadmap's emphasis on universal Virtio compatibility. Together, these sources substantiate a Phase 1 plan that centers on a unified Virtio device model with heavyweight, high-performance user-space backends for storage and networking, plus the phased, production-oriented framing implied by the staffing and deliverables in the target field value. The cited content also offers concrete architectural patterns (e.g., vhost, vhost-user, VF/VM backends, multi-queue layouts) that would underpin the proposed OS image in Phase 1, even though explicit staffing composition is not always enumerated in the sources themselves.",
      "confidence": "medium"
    },
    {
      "field": "pareto_set_of_virtio_drivers",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        },
        {
          "title": "SPDK Vhost Target: Accelerating Virtio SCSI/BLK (QEMU SPDK Vhost NVMe/IO)",
          "url": "https://kvm-forum.qemu.org/2018/KVM_Forum_26_Oct_2018_Vhost-NVMe.pdf",
          "excerpts": [
            "SPDK vhost target for accelerating virtio SCSI/BLK",
            "Benefit from Linux block driver multi-queues feature and Guest NVMe driver"
          ]
        },
        {
          "title": "SPDK Virtio and vhost-backed storage backends",
          "url": "https://spdk.io/news/2023/03/28/ublk/",
          "excerpts": [
            "SPDK v17.03 added a vhost-scsi target to allow SPDK to serve its block storage to QEMU-based virtual machines.",
            "Since then, SPDK storage virtualization now\nalso includes vhost-blk and nvme-vfio-user.",
            "We use the SPDK NVMe-oF target as the server,\nand then the kernel can connect to it using nvme-cli.",
            "Performance scales better, due to NVMe’s inherent\nmulti-queue architecture."
          ]
        }
      ],
      "reasoning": "The field value emphasizes the essential pairing of virtio-net for networking with virtio-blk/virtio-scsi for storage, and it recommends backends that deliver near line-rate performance. Excerpts describing SPDK vhost as a high-performance vhost backend and the use of poll-mode Virtio drivers directly support virtio-net's performance-centric path, which aligns with the goal of achieving maximum throughput with minimal CPU overhead. The SPDK vhost Target article explicitly frames SPDK as an accelerated back-end for Virtio storage (vhost-target), which maps cleanly to virtio-blk and virtio-scsi workloads and explains how to achieve high IOPS and low latency by moving I/O processing into user space. Additionally, references to vhost-user and SPDK's vhost architecture provide concrete implementation patterns for linking Virtio devices to fast backends (SPDK, VFIO/Vhost). The references discussing testing guidance for vhost/blk with SPDK illustrate validated workflows for measuring such configurations and thus are highly relevant for determining the Pareto set. The Virtio network/Virtio block configuration descriptions help ground the field value in real device functionality and standard interfaces (VIRTIO_NET_F, VIRTIO_BLK features). Finally, excerpts that explicitly discuss AF_XDP as a competitive, kernel-bypass-based path for high-performance networking complement the networking portion of the field value, illustrating alternative routes to achieve similar performance goals. Taken together, the most compelling support points are those that directly connect virtio-net with SPDK vhost backends (and related vhost-user and AF_XDP options) and virtio-blk/virtio-scsi with SPDK vhost backends, as these form the high-value, Pareto-optimal set of driver/backends for the described use case.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_adoption_strategy",
      "citations": [
        {
          "title": "Unikraft: a Modern, Fully Modular Unikernel",
          "url": "https://www.usenix.org/publications/loginonline/unikraft-and-coming-age-unikernels",
          "excerpts": [
            "The ability to run a wide range of applications and languages is paramount to how much deployment any unikernel project will see. Unikraft addresses this directly through what we term autoporting : we use an application's native build system to build the application against the musl C standard library[ 9 ] and link the resulting object files against Unikraft (see Figure 4).",
            "What about POSIX compatibility? While Linux has over 300 system calls, previous studies[ 11 ] have shown through static analysis that only a subset (224) are needed to run a Ubuntu installation.",
            "Unikraft provides (a modular) system call shim layer along with implementations of a growing number of syscalls (160 as of this writing).",
            "The key insight is that applications are resilient to a significant portion of syscalls being stubbed and faked, and that the number of implemented syscalls they require to correctly run is significantly lower than the output of the static analysis suggests, let alone the total number of syscalls in the Linux API."
          ]
        }
      ],
      "reasoning": "The most relevant passages explicitly describe aiming for POSIX compatibility and Linux-compatible syscall semantics via a modular runtime (Unikraft) and emphasize compatibility with the Linux system call API. This directly supports the idea of exposing a minimal POSIX subset and a Linux-compatible syscall surface through a shim rather than porting an entire OS kernel, enabling existing software like Kafka and Spark to recompile against the library and run with familiar APIs. The cited material also reinforces a staged approach: start with core POSIX compatibility, then extend with more advanced features, which aligns with the proposed strategy to lower barriers to adoption while maintaining a path to deeper OS capabilities later. These points collectively justify adopting a shim-based, POSIX-first compatibility strategy as described in the field value.",
      "confidence": "medium"
    },
    {
      "field": "legal_and_licensing_considerations.compliance_recommendation",
      "citations": [
        {
          "title": "OASIS Virtual I/O Device (VIRTIO) TC IP and License Policy",
          "url": "https://www.oasis-open.org/committees/virtio/ipr.php",
          "excerpts": [
            "This TC operates under the Non-Assertion Mode of the OASIS IPR Policy.",
            "The US Patent Nos. 9811260 and 9311164 \"System and Method for Ballooning wth Assigned Devices\", the US Patent No. 9934157 and the US Publication No. 20180225221 \"Post-Copy VM Migration Speedup Using Free Page Hinting\", the US Publication No. 20170147371 \"Virtual Machine Migration Using Memory Page Hints\", the US Patent No. 9965311 \"Application of a Pattern to Inactive Pages in Guest Memory\", the US Patent No.\n10169088 \"Lockless Free Memory Ballooning for Virtual Machines\", the US Patent Nos. 10083058 and 9672062 \"Batched Memory Page Hinting\" and the US Publication No. 20180239707 \"Page Cache Management During Migration\" are relevant to section \"Traditional Memory Balloon Device\" of the Virtual I/O Device (VIRTIO) specification.",
            "Red Hat has filed several United States patent applications that might become essential to an OASIS Standards Final Deliverable.",
            "Red Hat is unable to provide more specific information regarding the unpublished applications, until such time as the applications are published or the patents are issued."
          ]
        }
      ],
      "reasoning": "The recommended course of action hinges on leveraging the protection afforded by the OASIS IPR policy Non-Assertion Mode, which reduces the risk of patent assertions for the Virtio-related project. This is directly supported by the reference describing the Non-Assertion Mode of the IPR policy. In addition, there are explicit notices about patent disclosures from major contributors (e.g., Red Hat) and listed patent numbers that could impact implementation or require licensing terms, which justify maintaining awareness of patent landscapes. Licensing policy documentation further reinforces that patent disclosures and IPR considerations are central to the governance of the standard, validating a cautious and proactive stance. Finally, a note that Red Hat cannot provide more specific information until patents are published underscores the need for proactive measures, including trademark clearance for project names to prevent conflicts with the OASIS trademark. Taken together, these points justify a practical compliance recommendation: build the OS with confidence given Non-Assertion Mode protections, maintain ongoing awareness of patent disclosures from key contributors, and conduct formal trademark clearance for the project name to avoid trademark conflicts, including with the OASIS mark.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads",
      "citations": [
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "Information-Agnostic Flow Scheduling for Commodity Data ...",
          "url": "https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/bai",
          "excerpts": [
            "by W Bai · 2015 · Cited by 415 — We present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a ..."
          ]
        },
        {
          "title": "Long-tail Latency: Causes and Solutions",
          "url": "https://systemdr.substack.com/p/long-tail-latency-causes-and-solutions",
          "excerpts": [
            "Jul 23, 2025 — Welcome to the world of long-tail latency—where the statistical outliers at P95, P99, and P99.9 percentiles can destroy user experience despite ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        }
      ],
      "reasoning": "The field value describes a Kafka-like streaming workload with hot path bottlenecks in produce, replication, and consume, and proposes an OS-level architecture that uses SPDK vhost-backed Virtio, kernel-bypass networking (DPDK/AF_XDP), a GC-free Rust environment, and a shard-per-core design to achieve drastic tail-latency reductions and high throughput. Evidence from the excerpts directly supports each of these architectural bets: first, the Shenango NSDI19 papers demonstrate microsecond-scale core reallocation (every 5 microseconds) and contention-aware scheduling that yields extremely low tail latency (p99.9) while maintaining high throughput; this underpins the argument for a shard-per-core or micro-scheduler approach to minimize tail latency and improve predictability in latency-sensitive workloads. The excerpts describe throughputs of several million requests per second and median/percentile tail latencies in the tens of microseconds range, which substantiates the claim that microsecond-scale scheduling can dramatically reduce tail latency and improve CPU efficiency for latency-sensitive workloads.\nSecond, SPDK vhost backends and Virtio-backed storage and networking are repeatedly described as enabling kernel-bypass paths with very high IOPS and low latency, which maps to the proposed OS architecture needing zero-copy, user-space I/O paths for both storage and network I/O in a Kafka-like streaming setting. The SPDK vhost material and SPDK NVMe-oF discussions show how backends like SPDK vhost-target can deliver high-performance storage for VMs and guests through a user-space, polling-based design that reduces kernel overhead and latency.\nThird, the material on Virtio networking (Virtio-net and vhost-user) and SPDK-backed vhost-target discussions corroborate the architectural plan to replace kernel stacks with high-performance user-space backends to reduce CPU overhead and tail latencies in streaming workloads.\nFinally, Persephone/Surrounding NSDI22-era scheduling work demonstrates improvements in tail latency when employing more sophisticated scheduling policies (DARC, Perséphone, Shinjuku) and microsecond-level core allocations, which supports the architectural claim that aggressive, low-latency core scheduling and partitioning can yield substantial tail-latency improvements in high-throughput environments.\nIn sum, the strongest evidence aligns with (1) the value of microsecond-scale, shard-per-core scheduling to cut tail latency and maintain throughput, (2) the benefit of kernel-bypass I/O paths (SPDK vhost, AF_XDP) to minimize CPU overhead and latency in both storage and networking, and (3) the practical feasibility of achieving large throughput (millions of ops/sec) with low tail latency through these architectural choices. The combination of microsecond scheduling and user-space I/O backends directly supports the field value's architectural claims and expected performance gains for Kafka-like streaming and Spark-like analytics workloads.\n",
      "confidence": "high"
    },
    {
      "field": "security_and_isolation_architecture",
      "citations": [
        {
          "title": "Thunderclap: Exploring Vulnerabilities in Operating System ...",
          "url": "https://www.ndss-symposium.org/ndss-paper/thunderclap-exploring-vulnerabilities-in-operating-system-iommu-protection-via-dma-from-untrustworthy-peripherals/",
          "excerpts": [
            "Direct Memory Access (DMA) attacks have been known for many years: DMA-enabled I/O peripherals have complete access to the state of a computer and can fully ..."
          ]
        },
        {
          "title": "Exploring Vulnerabilities in Operating System IOMMU ...",
          "url": "https://www.cl.cam.ac.uk/~atm26/pubs/thunderclap-paper-ndss2019.pdf",
          "excerpts": [
            "by AT Markettos · Cited by 123 — Using Thunderclap, an open-source FPGA research platform that we built, we explore new classes of OS vulnerability arising from inadequate use."
          ]
        },
        {
          "title": "DMA protection and DMAAUTH evaluation - USENIX Security 2024",
          "url": "https://www.usenix.org/system/files/usenixsecurity24-wang-xingkai.pdf",
          "excerpts": [
            "When using the strict mode, IOMMU can defeat such attacks but will introduce unacceptable performance overhead.",
            "In our experiments, DMAAUTH can defeat the attacks in",
            "DMAAUTH introduces 1.0% extra CPU time.",
            "The DMAAUTH introduces 1.2% overhead in\nTCP throughput and < 0.1% overhead in UDP throughput.",
            "Combining the read and write test, the DMAAUTH introduces a 1.4% overhead\nin throughput and 1.0% overhead in CPU runtime on average, which are 81.0% and 81.6% less than IOMMU respectively."
          ]
        },
        {
          "title": "IOMMU/DMA Security and Mitigation - Abstract and Findings",
          "url": "https://dl.gi.de/bitstreams/1a7e69c5-eb7b-4ccb-b270-a119557a62e1/download",
          "excerpts": [
            "With the IOMMU enabled, PCIe devices are assigned to vir-\ntual address spaces, similar to the way the MMU manages\naddress spaces for user-space processe",
            "Starting from Linux 4.20 [26], ATS became\nunavailable for “untrusted” devices, a category that, e.g.,\nThunderbolt devices fall into by def",
            "DMA Address Remapping. The process of DMA address\ntranslation, begins with a PCIe device emitting a DMA re-\nques",
            "These attacks exploit\nshortcomings in the IOMMU specifications or vulnerabilities\nin architectures of actual systems.",
            "The easiest practical way to introduce a\nmalicious device into a system is Thunderbolt, which exposes\nthe internal PCIe bus on USB-C, allowing for external periph-\nerals to emit DMA request",
            " The\nkDMAp feature is configurable through UEFI, but known to\ncause compatability issues with some Thunderbolt devices"
          ]
        },
        {
          "title": "(PDF) A portable ARINC 653 standard interface - ResearchGate",
          "url": "https://www.researchgate.net/publication/224357948_A_portable_ARINC_653_standard_interface",
          "excerpts": [
            "The ARINC 653 specification defines the interfaces and the behavior of the APEX but leaves implementation details to OS vendors. This paper describes an OS ..."
          ]
        }
      ],
      "reasoning": "The field value describes DMA/PCIe attacks as a threat to tenant isolation, and identifies IOMMU as a primary hardware defense used to virtualize addresses and restrict device memory access. It also emphasizes using VFIO to safely expose DMA capabilities to user-space like SPDK, thereby sandboxing devices within an IOMMU domain. Direct excerpts support this: a Thunderclap/DMA vulnerability paper discusses DMA attacks via PCIe and the need for IOMMU-based protection; a companion set of documents analyzes DMA protection and mitigation strategies, including the role of IOMMU, VFIO, and SPDK in isolating DMA from other tenants. Moreover, additional excerpts describe the potential vulnerabilities and the overheads or considerations of enabling DMA-protection features such as DMAAUTH, ATS, and IOMMU-related mitigations, which align with the described defense-in-depth approach. Taken together, the strongest, directly relevant evidence points to the combination of (a) DMA-based threat reality and specific CVE-like exemplars, (b) the IOMMU as the core hardware barrier, and (c) VFIO as the safe interface to user-space I/O stacks (e.g., SPDK vhost) to keep DMA access contained within protected domains. The more peripheral references discuss broader enforcement mechanisms (e.g., ATS, DMA protection discussions) but still reinforce the central security architecture. In summary, the most relevant excerpts establish that: DMA-based threats exist in PCIe contexts, IOMMU is essential to constrain DMA, and VFIO enables safe user-space I/O with guarded access, which supports the fine-grained field value about architecture-level isolation and mitigation strategy.",
      "confidence": "high"
    },
    {
      "field": "project_risk_register",
      "citations": [
        {
          "title": "Ferrocene Achieves IEC 62304 Qualification - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/ferrocene-achieves-iec-62304-qualification/",
          "excerpts": [
            "“Qualifying Ferrocene under IEC 62304 demonstrates our commitment to advancing safety-critical software development through tools that enhance ..."
          ]
        },
        {
          "title": "Ferrocene",
          "url": "https://ferrocene.dev/en/",
          "excerpts": [
            "This is Rust for critical systems. Ferrocene is the open-source qualified Rust compiler toolchain for safety- and mission-critical systems.",
            "Certification assistance. Individual. €240 per year/seat. Rust for small teams ... TÜV SÜD conducted the audit and Ferrous Systems – leading Rust experts ..."
          ]
        },
        {
          "title": "Officially Qualified - Ferrocene - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/officially-qualified-ferrocene/",
          "excerpts": [
            "Like the automotive\ncertification, we’ve also gone for the highest level of risk reduction and\nqualified it at SIL4.",
            "Ferrocene for use in systems up to ASIL D – the highest classification of\ninitial hazard as defined by this standard. This standard provides automakers\nwith guidelines that make the software being used to develop, produce, operate\nand service the automotive sectors safe to use."
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ...",
            "AMC 20-193 Use of multi-core processors. Purpose. 1.1. This AMC describes an acceptable means, but not the only means, for showing compliance.",
            "**AMC 20-193**",
            "Interference channels and resource usage",
            "AMC 20-193 describes an acceptable, but not mandatory, means for showing compliance with airworthiness specifications for multi-core processors in airborne ...",
            "AMC 20-193 describes an acceptable, but not mandatory, means for showing compliance with airworthiness specifications for multi-core processors in airborne ..."
          ]
        },
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "The FAA™ and EASA® have been developing guidelines for development of multi-core systems for DO-178 aerospace projects. These include superseded documents ..."
          ]
        },
        {
          "title": "DO-330 / Tool Qualification Context with Ferrocene",
          "url": "https://afuzion.com/do-330-introduction-tool-qualification/",
          "excerpts": [
            "This DO-330 whitepaper describes the five tool categories versus criticality levels (DALs) and how to truly qualify avionics software tools.",
            "ntroduces five Tool Qualification Levels (TQLs) based upon three Tool Criteria."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "DO-330/ED-215 “Software tool qualification considerations” is a self-contained document that describes the principles of software tool qualification as ... DO-330/ED-215 introduces the concept of Tool Qualification Levels (TQL) which are assigned according to three criteria:",
            "**Criterion 1**A tool whose output is part of the airborne software and thus could insert an error",
            "**Criterion 2**\n\n1. Verification processes other than that automated by the tool, or\n2. Development processes that could have an impact on the airborne software. **Criterion 3**",
            "An unqualified compiler or an auto-code generator from an UML tool would therefore fit criterion 1. A qualified version of the same UML tool would fit criterion 2, because its use is designed to reduce the overhead of code verification processes. The LDRA tool suite is a verification tool and therefore a criterion 3 tool. Irrespective of the application software level (for DO-178C, read Design Assurance Level or DAL) such a verification tool is always assigned Tool Qualification Level 5 – the least demanding of the five levels (below).",
            "Civil aviation certification authorities require tool qualification on a project-by-project basis. The responsibility for showing the suitability of any tool falls on to the organization developing a civil aviation application. However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "Rust is DO-178C Certifiable - Pictorus Blog",
          "url": "https://blog.pictor.us/rust-is-do-178-certifiable/",
          "excerpts": [
            "With developments such as the Ferrocene-qualified compiler, Rust can now meet all the analysis requirements under DO-178C, one of the most ..."
          ]
        },
        {
          "title": "Ferrocene Update - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/sealed-rust-the-plan/",
          "excerpts": [
            "... Rust Compiler Backend(s) or Static/Dynamic Analysis Tools; Validating ... Tool Qualification standards. 1.0.1The Rust Language ..."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "A practical Approach to CAST-32A & AMC 20-193 ...",
          "url": "https://www.sysgo.com/blog/article/the-multi-core-challenge-a-practical-approach-to-cast-32a-amc-20-193-compliance",
          "excerpts": [
            "Oct 21, 2022 — This webcast presents a turnkey solution from SYSGO and LDRA designed to meet the objectives of CAST-32A within the context of a DO-178C compliant project."
          ]
        },
        {
          "title": "[PDF] Overcoming the Challenges of Meeting CAST-32A Objectives for ...",
          "url": "https://ldra.com/wp-content/uploads/2022/09/Overcoming-the-Challenges-of-Meeting-CAST-32A-Objectives-for-Avionics-Software.pdf",
          "excerpts": [
            "CAST-32A, “Multi-core processors”, is a position paper that was written by the Certification. Authorities Software Team (CAST). CAST were a group of civil ..."
          ]
        },
        {
          "title": "[PDF] Civil Certification of Multi-core Processing Systems in Commercial ...",
          "url": "https://www.windriver.com/sites/default/files/2022-06/Collins%20Aerospace%20paper.pdf",
          "excerpts": [
            "CAST-32A defines a number of objectives that must be met by an avionics developer using a multi-core processor. Table 1 provides an overview of CAST-32A by."
          ]
        },
        {
          "title": "LDRA Simplifies CAST-32A & A(M)C 20-193 Compliance ...",
          "url": "https://www.prnewswire.com/in/news-releases/ldra-simplifies-cast-32a--amc-20-193-compliance-with-complete-lifecycle-support-301773805.html",
          "excerpts": [
            "Mar 16, 2023 — CAST-32A has matured into the de-facto guidance for the adoption of MCPs in civil aviation in the form of the current European Union Aviation ..."
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ...",
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "Improving NGINX Performance with Kernel TLS and SSL_sendfile( )",
          "url": "https://www.f5.com/company/blog/nginx/improving-nginx-performance-with-kernel-tls",
          "excerpts": [
            "NGINX 1.21.4 introduces support for kTLS when serving static files and cached responses with SSL_sendfile(), which can hugely improve performance."
          ]
        },
        {
          "title": "Intel® QuickAssist Technology (Intel® QAT) and OpenSSL-1.1.0",
          "url": "https://www.intel.com/content/www/us/en/content-details/706024/intel-quickassist-technology-intel-qat-and-openssl-1-1-0-performance.html",
          "excerpts": [
            "This paper will demonstrate how the combination of these features with Intel® QAT results in tangible performance gains."
          ]
        },
        {
          "title": "PCIe DMA Attack against a secured Jetson Nano (CVE- ...",
          "url": "https://www.thegoodpenguin.co.uk/blog/pcie-dma-attack-against-a-secured-jetson-nano-cve-2022-21819/",
          "excerpts": [
            "Mar 10, 2022 — 2 NVMe SSD. We could not find examples of PCIe attacks against ARM platforms, so we investigated to see if the theoretical attack was possible ..."
          ]
        }
      ],
      "reasoning": "Key excerpts describe DO-178C, CAST-32A, and AMC 20-193 contexts relevant to high-assurance certification programs. For example, Ferrocene's official qualification status is cited as enabling Rust in safety-critical settings, which directly supports the risk description about certification readiness for Rust Hallows. Other excerpts discuss CAST-32A and AMC 20-193 as the modern guidance for multicore avionics software, including the need for interference analysis, partitioning, and verification, which underpins the risk of a lengthy, labor-intensive certification process. DO-330/tool qualification guidance outlines how to qualify the tooling used in development and verification, which is central to reducing certification risk and enabling timely certification deliverables. Additional excerpts outline the recommended certification artifacts (Plan for Software Aspects of Certification, SDP, SVP, SCMP, SAS) and a phased approach to certification, which map directly to the proposed mitigation plan (start early, use qualified tools, and phase the certification effort). The combination of these sources provides a coherent evidentiary basis for describing the risk (high assurance certification delays) and the multi-faceted mitigation plan (early authority engagement, qualified toolchains, phased artifact generation).",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "- The most directly relevant excerpt presents concrete latency targets for cloud-native exchanges that are central to HFT use cases: latency-focused benchmarks reporting P50/P99 latency figures (55-124 microseconds) and explicit framing of \"low latency\" in exchange contexts. This aligns with the goal of achieving sub-millisecond tail latency and highly predictable response times described for HFT workflows. The emphasis on microsecond- to sub-microsecond-scale latency in exchange scenarios provides concrete support for the latency budget and tail latency requirements in the finegrained field value. - A foundational source defines High-Frequency Trading (HFT) and describes it as a domain where speed, precision, and low latency are critical, which underpins the use case's demand for extreme latency performance and deterministic behavior. This supports the inclusion of HFT as a premier latency-sensitive use case in the field value and provides context for why sub-microsecond targets matter in practice. - The market-size excerpt gives a top‑level economic context for HFT, quantifying the market value and growth, which strengthens the \"market_analysis\" dimension of the use case by illustrating the financial incentives and scale that underpin the HFT use case, though it is more peripheral to latency specifics than the direct latency benchmarks above. - The Persephone/Perséphone NSDI 2021 references discuss microsecond-scale scheduling and tail-latency optimization in datacenter workloads (e.g., 5 microsecond scheduling intervals and strong tail-latency improvements). These sources are highly relevant to the \"competitive_advantage_summary\" and the architectural rationale for achieving ultra-low tail latency (sub-100 microseconds in p99.9) in latency-sensitive apps such as HFT and similar real-time workloads. They provide concrete mechanisms (microsecond-level scheduling, partitioning) that could underpin the proposed Rust Hallows approach to isolate workloads and reduce interference, which is central to the field's use-case rationale. - The combination of these sources (latency benchmarks for exchanges, HFT context, market size, and microsecond scheduling approaches) collectively reinforces the fine-grained field value's claim about sub-microsecond tail latency goals, the economics of latency in HFT, and architectural strategies to achieve deterministic, low-latency performance in latency-critical workloads like HFT, UPF, and RTB.",
      "confidence": "high"
    },
    {
      "field": "legal_and_licensing_considerations.key_obligations_and_risks",
      "citations": [
        {
          "title": "OASIS Virtual I/O Device (VIRTIO) TC IP and License Policy",
          "url": "https://www.oasis-open.org/committees/virtio/ipr.php",
          "excerpts": [
            "This TC operates under the Non-Assertion Mode of the OASIS IPR Policy.",
            "Red Hat has filed several United States patent applications that might become essential to an OASIS Standards Final Deliverable.",
            "The US Patent Nos. 9811260 and 9311164 \"System and Method for Ballooning wth Assigned Devices\", the US Patent No. 9934157 and the US Publication No. 20180225221 \"Post-Copy VM Migration Speedup Using Free Page Hinting\", the US Publication No. 20170147371 \"Virtual Machine Migration Using Memory Page Hints\", the US Patent No. 9965311 \"Application of a Pattern to Inactive Pages in Guest Memory\", the US Patent No.\n10169088 \"Lockless Free Memory Ballooning for Virtual Machines\", the US Patent Nos. 10083058 and 9672062 \"Batched Memory Page Hinting\" and the US Publication No. 20180239707 \"Page Cache Management During Migration\" are relevant to section \"Traditional Memory Balloon Device\" of the Virtual I/O Device (VIRTIO) specification.",
            "Red Hat is unable to provide more specific information regarding the unpublished applications, until such time as the applications are published or the patents are issued."
          ]
        }
      ],
      "reasoning": "The described fine-grained field value centers on patent-related risk and obligations under the Virtio standard, emphasizing that Red Hat has disclosed numerous U.S. patents and patent applications that may be essential, and that implementers must comply with the IPR policy. It also highlights the presence of a Non-Assertion covenant among Virtio participants. The most directly supportive information states that there is a Non-Assertion Mode within the IPR policy, which directly matches the claim about obligations and risk mitigation. Additional content explicitly notes that Red Hat has filed patent applications that might be essential to the standard, and that several patents are listed as potentially relevant, which strengthens the evidence about patent encumbrance risk. Finally, one excerpt points out that Red Hat cannot provide more specific information until patents are published, underscoring the ongoing uncertainty and the need to follow policy, which corroborates the overall risk posture described in the field value.",
      "confidence": "high"
    },
    {
      "field": "certification_pathway_for_high_assurance.key_challenges",
      "citations": [
        {
          "title": "Time Protection: The Missing OS Abstraction",
          "url": "https://tomchothia.gitlab.io/Papers/EuroSys19.pdf",
          "excerpts": [
            "by Q Ge · 2019 · Cited by 113 — Time sharing cannot produce interference on these, while concurrent access can be observed as a reduction of available bandwidth. Cache channels work by the ..."
          ]
        },
        {
          "title": "ARINC 653",
          "url": "https://en.wikipedia.org/wiki/ARINC_653",
          "excerpts": [
            "The European Union aviation authority, EASA, published AMC 20-193 in January 2022."
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "**AMC 20-193**",
            "MCP_Resource_Usage_3:  \n\nThe applicant has identified the interference channels that could permit interference to affect the \nsoftware applications hosted on the MCP cores, and has verified the applicant’s chosen means of \nmitigation of the interfer"
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "Robust partitioning for multicore systems doesnt mean freedom from interference",
          "url": "https://www.rapitasystems.com/blog/robust-partitioning-multicore-systems-doesnt-mean-freedom-interference",
          "excerpts": [
            "\nMost of the technology for partitioning of single core systems also applies to multicore systems. Multicore systems are more complex, however – for these systems, interference can exist between software hosted on different cores but running at the same time.",
            "The most rigorous separation is termed _robust partitioning_ . This is defined for single core systems in the ARINC 653 standard, where it refers to complete isolation of separated components, i.e. the execution of one component can have no effect on the oth"
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "AMC 20-193 was released in January 2022, and AC 20-193 was released some two years later in January 2024. The two guidance documents are fully aligned from a technical and certification perspective."
          ]
        },
        {
          "title": "[PDF] Caladan: Mitigating Interference at Microsecond Timescales",
          "url": "https://amyousterhout.com/papers/caladan_osdi20.pdf",
          "excerpts": [
            "Cal- adan consists of a centralized scheduler core that actively manages resource contention in the memory hierarchy and between hyperthreads, ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on the central certification challenge: multicore interference analysis is extremely complex and costly, and proving that user-space I/O paths (such as SPDK and AF_XDP) have bounded worst-case timing under all conditions is essential for certification. The most directly supportive material discusses core ideas of time protection and interference channels, which underlie the need for rigorous partitioning and deterministic timing guarantees. In particular, the time-protection discussion explains that time sharing can create interference through cache and shared resources, and that a robust abstraction is needed to bound such effects. This aligns with the claim that identifying and mitigating all potential interference channels is a massive, expensive undertaking. The ARINC 653 context and certifications-related documents provide a formal backdrop for partitioning guarantees and bounded interference in avionics-like systems, reinforcing that achieving comprehensive, certifiable interference analysis is a costly, multi-faceted effort. The DO-330 tool qualification and related certification references illustrate the practical burden of qualification and evidence generation for tooling and processes, which supports the assertion that proving bounded timing for user-space I/O frameworks is a significant engineering and research challenge. Documents on AMC 20-193, CAST-32A, and ARINC 653 ground the discussion in established certification pathways and the kinds of interference analysis and partitioning controls that must be demonstrated for multicore systems. Taken together, these excerpts corroborate the key elements of the field value: (a) the inherent complexity and cost of exhaustive multicore interference analysis on modern SoCs, and (b) the central challenge of proving bounded worst-case timing for high-performance user-space I/O frameworks like SPDK and AF_XDP as part of certification.\n\nIn terms of relative emphasis, the most directly relevant excerpts are those that discuss time protection and interference channels, followed by material that frames the certification landscape (ARINC 653, AMC/CAST, DO-330). Additional excerpts provide broader context on partitioning and multicore interference frameworks, which support the overall claim about complexity and cost, and the need for verifiable evidence in certification processes.",
      "confidence": "high"
    },
    {
      "field": "competitive_landscape_and_differentiation.competitor_name",
      "citations": [
        {
          "title": "Redpanda vs Kafka",
          "url": "https://www.redpanda.com/compare/redpanda-vs-kafka",
          "excerpts": [
            "Redpanda provides 3-6x greater cost efficiency compared to traditional Kafka infrastructure, lowering both infrastructure and operational costs for your team. ..."
          ]
        },
        {
          "title": "Redpanda vs Kafka - A Detailed Comparison",
          "url": "https://www.ksolves.com/blog/big-data/redpanda-vs-kafka-a-detailed-comparison",
          "excerpts": [
            "Oct 23, 2024 — He found that Kafka surpassed Redpanda in terms of performance to a large extent, offering high throughput and achieving low latencies."
          ]
        },
        {
          "title": "Apache Pulsar vs Kafka vs RedPanda: Modern Streaming | sanj.dev",
          "url": "https://sanj.dev/post/pulsar-kafka-redpanda-streaming-comparison",
          "excerpts": [
            "Pulsar: P99 latency 20-35ms (includes BookKeeper overhead); RedPanda: P99 latency 8-15ms (C++ implementation advantage). ³ Independent ..."
          ]
        },
        {
          "title": "How Redpanda Works | Redpanda Self-Managed",
          "url": "https://docs.redpanda.com/current/get-started/architecture/",
          "excerpts": [
            "Redpanda implements a thread-per-core programming model through its use of the Seastar library. This allows Redpanda to pin each of its application threads to a ..."
          ]
        },
        {
          "title": "How Redpanda built a Distributed Data Streaming Architecture for ...",
          "url": "https://resources.scylladb.com/video-conference-talk/how-vectorized-built-a-distributed-data-streaming-architecture-for-modern-hardware-with-seastar",
          "excerpts": [
            "Watch this talk from Alexander Gallego to learn how Redpanda (previously named Vectorized) leverages Seastar to build a new streaming engine that is Kafka API ..."
          ]
        },
        {
          "title": "A tour of Redpanda Streamfest 2024",
          "url": "https://www.redpanda.com/blog/redpanda-streamfest-2024",
          "excerpts": [
            "... P99 latencies of 5 seconds or less. Simplicity is a core principle at Snowflake, so naturally, they found a kindred soul in Redpanda ..."
          ]
        },
        {
          "title": "Ecosystem",
          "url": "https://www.redpanda.com/categories/ecosystem",
          "excerpts": [
            "Explore integrations and tooling across the Redpanda ecosystem to expand your data streaming capabilities. Discover new possibilities now."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly contrasts Redpanda with Kafka, highlighting its competitive advantage in cost efficiency, which directly supports referencing Redpanda as a competitor in the landscape. Additional highly relevant excerpts discuss Redpanda's architectural choices and performance characteristics (e.g., using Seastar and P99 latency figures), which further substantiate Redpanda as a key competitor in the streaming landscape. Excerpts that describe Redpanda's architectural decisions or detailed comparisons provide corroborating context for its positioning relative to Kafka. Less directly, mentions of Redpanda in talks or ecosystem pages still corroborate its presence in the competitive landscape, though they are not as tight in asserting competitive performance or rivalry. Collectively, the excerpts build a consistent picture of Redpanda as a notable alternative to Kafka in streaming systems, validating the field value that the competitor name is Redpanda.",
      "confidence": "high"
    },
    {
      "field": "go_to_market_and_packaging_strategy",
      "citations": [
        {
          "title": "AWS Marketplace AMI Product Checklist",
          "url": "https://docs.aws.amazon.com/marketplace/latest/userguide/aws-marketplace-listing-checklist.html",
          "excerpts": [
            "Before submitting your Amazon Machine Image (AMI) product to AWS, use this checklist to validate your submission.",
            "Product usage is not restricted by time or any other measurements.",
            "AMI meets the mandatory [AMI-based product requirements for AWS Marketplac",
            "AMI product checklist for AWS Marketplace",
            "AMI is production-ready."
          ]
        },
        {
          "title": "Find an AMI that meets the requirements for your EC2 instance",
          "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/finding-an-ami.html",
          "excerpts": [
            "Find an AMI that meets your needs for launching an instance by searching for criteria such as the operating system that is included."
          ]
        },
        {
          "title": "5 Steps to List Your Software on AWS Marketplace",
          "url": "https://www.awssome.io/blog/5-steps-to-list-your-software-on-aws-marketplace",
          "excerpts": [
            "Technical Requirements Your AMI must meet these criteria: Use Hardware Virtual Machine (HVM) virtualization; Operate on a 64-bit architecture · Documentation ..."
          ]
        },
        {
          "title": "AWS Marketplace: Red Hat Enterprise Linux 8 - Amazon.com",
          "url": "https://aws.amazon.com/marketplace/pp/prodview-kv5mi3ksb2mma",
          "excerpts": [
            "64-bit (x86) Amazon Machine Image (AMI). Amazon Machine Image (AMI). An AMI is a virtual image that provides the information required to launch an instance."
          ]
        },
        {
          "title": "Home - Lanner Electronics | Network Appliance | uCPE SD ...",
          "url": "https://lannerinc.com/",
          "excerpts": [
            "Lanner is a world leading provider of design, engineering and manufacturing services for advanced network appliances and rugged applied computing platforms ..."
          ]
        },
        {
          "title": "AMI Pricing in AWS Marketplace",
          "url": "https://docs.aws.amazon.com/marketplace/latest/userguide/pricing-ami-products.html",
          "excerpts": [
            "AMI pricing models",
            "Paid hourly or hourly-annual",
            "Paid monthly",
            "\nPrivate offers enable negotiating pricing, terms with sellers, accepting offers, sharing offers across accounts, subscribing to offers for AMI, Container products, accessing financing services, reviewing annual ",
            "Private offers enable negotiating pricing, terms with sellers, accepting offers, sharing offers across accounts, subscribing to offers for AMI, Container products, accessing financing services,",
            "AWS Marketplace has multiple pricing models for Amazon Machine Image (AMI) products.",
            "With seller\n private oﬀers, there are options available for multi-year and custom duration contracts."
          ]
        },
        {
          "title": "Red Hat Enterprise Linux Subscription Guide",
          "url": "https://www.redhat.com/en/resources/red-hat-enterprise-linux-subscription-guide",
          "excerpts": [
            "Severity 2 (High):**  \nA problem in which the software is functioning, but your use in a production environment is severely reduced. The situation is greatly affecting portions of your business operations, and no procedural workaround exists. | 4 business-day hours | 2 hours | 4 hours, or as agreed |",
            "Severity 3 (Medium):**  \nA problem that involves partial, noncritical loss of use of the software in a production environment or development environment.\nFor production environments, there is a medium-to-low effect on your business, but your business continues to function, including by using a procedural workaround. For development environments, the situation is causing your project to no longer continue or migrate into production. | 1 business day | 4 business-day hours | 8 business-day hours, or as agreed |",
            "Severity 4 (Low):**  \nA general usage question, reporting of a documentation error, or recommendation for a future product enhancement or modification. For production environments, there is low-to-no effect on your business or the performance or functionality of your system. For development environments, there is a medium-to-low effect on your business, but your business continues to function, including by using a procedural workaround. | 2 business days | 8 business-day hours | 2 business days, or as agreed |"
          ]
        },
        {
          "title": "Offering virtual machine (VM) products",
          "url": "https://cloud.google.com/marketplace/docs/partners/vm",
          "excerpts": [
            "It must deploy software to, and run on, Compute Engine. It must use the Cloud Marketplace-hosted image with the attached Compute Engine license."
          ]
        },
        {
          "title": "Azure VM Image Builder pricing",
          "url": "https://azure.microsoft.com/en-us/pricing/details/image-builder/",
          "excerpts": [
            "Azure Image Builder is a free image building service that streamlines the creation, update, patch, management, and operation of Linux and Windows images."
          ]
        },
        {
          "title": "Network Hardware Compatibility List (HCL) - NVIDIA",
          "url": "https://www.nvidia.com/en-us/networking/ethernet-switching/hardware-compatibility-list/",
          "excerpts": [
            "NVIDIA certifies the operation on NVIDIA Cumulus Linux for all switches on the Hardware Compatibility List (HCL). All platforms on the HCL come with ONIE."
          ]
        },
        {
          "title": "Windows Virtual Machines Pricing",
          "url": "https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/",
          "excerpts": [
            "Buy Windows Virtual Machines today and pay only for what you use. Compare pricing options to choose the best plan for your business."
          ]
        },
        {
          "title": "Solved: Re: How to use Marketplace Products in Autoscale",
          "url": "https://www.googlecloudcommunity.com/gc/Infrastructure-Compute-Storage/How-to-use-Marketplace-Products-in-Autoscale/m-p/621962",
          "excerpts": [
            "Aug 15, 2023 — The license costs for such deployments isn't handled through Google Cloud Billing. Also, please do review the marketplace terms of service that ..."
          ]
        },
        {
          "title": "Reliable Event Streaming with Confluent Cloud and ...",
          "url": "https://www.confluent.io/blog/reliable-event-streaming-and-proactive-support-with-confluent-cloud/",
          "excerpts": [
            "Nov 10, 2020 — Of course, we promise uptime for our fully managed service—Confluent Cloud has an uptime SLA of 99.95% with Standard and Dedicated clusters."
          ]
        },
        {
          "title": "Lighthouse FAQs - The New Support Portal (GA in Ju...",
          "url": "https://community.cloudera.com/t5/Community-Articles/Lighthouse-FAQs-The-New-Support-Portal-GA-in-June-2025/ta-p/408153",
          "excerpts": [
            "May 9, 2025 — What are Severity Definitions and SLAs? You may choose to escalate a case 24 hours after case creation by clicking the “Escalate” button in the ..."
          ]
        },
        {
          "title": "Support plans - Cloud",
          "url": "https://docs.confluent.io/cloud/current/support.html",
          "excerpts": [
            "You receive responses in a time frame specified by the support plan-level for your organization. Related content¶. Confluent Support (requires login): ..."
          ]
        },
        {
          "title": "End to End Latency Use Cases | Cloudera on Cloud",
          "url": "https://docs.cloudera.com/runtime/7.2.0/monitoring-end-to-end-latency/topics/smm-latency-use-cases.html",
          "excerpts": [
            "Use Case 1: Verify whether end-to-end processing time SLAs are met.   · Go to Topics in the SMM UI. · Select the topic you want to verify the details about. · Click ..."
          ]
        },
        {
          "title": "Accept a private offer from a vendor | Google Cloud Marketplace",
          "url": "https://cloud.google.com/marketplace/docs/offers/accepting-private-offer",
          "excerpts": [
            "Accept a private offer · Click and open the link that the vendor sent you. · Verify the details of the offer, such as the pricing details and contract duration."
          ]
        },
        {
          "title": "Create a virtual machine offer on Azure Marketplace.",
          "url": "https://learn.microsoft.com/en-us/partner-center/marketplace-offers/azure-vm-offer-setup",
          "excerpts": [
            "Jan 31, 2025 — This article describes how to create an Azure virtual machine offer for Azure Marketplace. It addresses both Windows-based and Linux-based virtual machines."
          ]
        },
        {
          "title": "Create a private offer for a customer | Google Cloud Marketplace ...",
          "url": "https://cloud.google.com/marketplace/docs/partners/offers/create-private-offers",
          "excerpts": [
            "Your product must be integrated with and listed on Cloud Marketplace. You must have the customer's Cloud Billing account ID. If you don't have the ID, request ..."
          ]
        },
        {
          "title": "Configuring metering for usage with SaaS subscriptions",
          "url": "https://docs.aws.amazon.com/marketplace/latest/userguide/metering-for-usage.html",
          "excerpts": [
            "For software as a service (SaaS) subscriptions, you meter for all usage, and then customers are billed by AWS based on the metering records that you provide."
          ]
        },
        {
          "title": "Release Notes for Confluent Cloud",
          "url": "https://docs.confluent.io/cloud/current/release-notes/index.html",
          "excerpts": [
            "The Service Level Agreement (SLA) for Confluent Cloud now specifies a 99.99% uptime SLA for Standard and Dedicated Kafka clusters with Multi-Zone configurations ..."
          ]
        },
        {
          "title": "Red Hat Enterprise Linux: Subscription Guide",
          "url": "https://itassetmanagement.net/2018/01/25/red-hat-enterprise-linux-subscription-guide/",
          "excerpts": [
            "Jan 25, 2018 — Premium support offers 24×7 support (for severity 1 & 2 incidents) as well as Extended Update Support. The latter can also be purchased as an ..."
          ]
        },
        {
          "title": "Red Hat subscription model FAQ",
          "url": "https://www.redhat.com/en/about/subscription-model-faq",
          "excerpts": [
            "Standard business hours, 24x7 for severity 1 and 2. Support channel, Web and phone · Web and phone. Number of cases, unlimited, unlimited. Response times ..."
          ]
        },
        {
          "title": "OEM Partnerships",
          "url": "https://www.nutanix.com/partners/oem",
          "excerpts": [
            "OEM Partners · Cisco Compute Hyperconverged · Fujitsu XF Series · Hewlett-Packard DX Series · Lenovo ThinkAgile HX · Dell XC Family · Supermicro."
          ]
        },
        {
          "title": "RHEL_Support_Matrix.pdf",
          "url": "https://linux.dell.com/files/supportmatrix/RHEL_Support_Matrix.pdf",
          "excerpts": [
            "The matrix shows RHEL/Dell PowerEdge server certification status, with levels: Not Supported, Certified, Validated, and Fully Supported. Dell offers validation ..."
          ]
        },
        {
          "title": "Network Hardware Compatibility List (HCL)",
          "url": "https://www.nvidia.com/en-eu/networking/ethernet-switching/hcl/",
          "excerpts": [
            "Cumulus Networks may decide to remove systems from the HCL. For example, HCL removal could occur in cases of obsolescence or a partner going out of business."
          ]
        },
        {
          "title": "What kind of support RHEL Standard subscription actually offer?",
          "url": "https://www.reddit.com/r/redhat/comments/rv4lly/what_kind_of_support_rhel_standard_subscription/",
          "excerpts": [
            "With standard support you can open tickets with redhat to help you out on issues/ask how to questions/etc. You only get business hours support ..."
          ]
        },
        {
          "title": "Celebrating Partners at .NEXT 2025",
          "url": "https://www.nutanix.com/blog/celebrating-partners-at-next-2025",
          "excerpts": [
            "May 8, 2025 — Global OEM. Cisco. Global Systems Integrator. Tata Consulting Services ... HPE. Lenovo. Technology Alliance. DataRobot. Faddom. AMD. Momentum."
          ]
        },
        {
          "title": "A Model-Based Optimization Method of ARINC 653 Multicore ... - MDPI",
          "url": "https://www.mdpi.com/2226-4310/11/11/915",
          "excerpts": [
            "ARINC 653 partition scheduling can be viewed as a two-level hierarchical scheduling system that uses a cyclic Time Division Multiplexing (TDM) global scheduler for the whole system and a Fixed-Priority local scheduler for each partition [6]."
          ]
        }
      ],
      "reasoning": "The fine-grained field value centers on a multi-channel go-to-market packaging and distribution plan. Several excerpts directly address cloud marketplace distribution: references to listing and provisioning AMIs for AWS Marketplace, as well as VM/image offerings for Azure Marketplace and Google Cloud Marketplace, including notes on HVM suitability, EBS backing, and Compute Engine images. These passages provide concrete examples of the required formats (AMI for AWS, VHD for Azure, Compute Engine images for GCP) and the readiness criteria that packaging must meet for cloud deployment. Additional excerpts discuss on-premises packaging options (OVA/ISO) for VMware/Nutanix, PXE-based automated provisioning, and a Reference Hardware & OEM program with an HCL and OEM partnerships (Dell, HPE, Supermicro). Together, they map exactly to the described packaging strategy, channels, and certification/compatibility considerations. Some excerpts outline related processes (image creation, certification, and partner programs) that reinforce how a vendor would operationalize the go-to-market plan, though less directly than the explicit cloud-marketplace and OEM content. In sum, the most relevant excerpts provide concrete cloud-marketplace formats, image types, and OEM/HCL-oriented distribution practices, which align tightly with the field value. Lesser relevance excerpts discuss broader virtualization topics or performance benchmarks that, while informative for the product, do not directly componentize into the packaging and distribution strategy described.",
      "confidence": "high"
    },
    {
      "field": "legal_and_licensing_considerations.component_or_topic",
      "citations": [
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device.",
            "The name \"OASIS\" is a trademark of [OASIS](https://www.oasis-open.org/) , the owner and developer of this\nspecification, and should be used only to refer to the organization and its official\noutputs. OASIS welcomes reference to, and implementation and use of, specifications,\nwhile reserving the right to enforce its marks against misleading uses",
            "This specification is provided under the Non-Assertion Mode of the OASIS IPR Policy, the mode chosen when the Technical Committee was established.",
            ". For\ninformation on whether any patents have been disclosed that may be essential to\nimplementing this specification, and any offers of patent licensing terms, please refer\nto the Intellectual Property Rights page in the TC’s GitHub repository\n( <https://github.com/oasis-tcs/virtio-admin/blob/master/IPR.md> )."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Block Device Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/vhost-user-slave.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "oasis-tcs/virtio-spec: OASIS Virtual I/O Device TC",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms.",
            "OASIS Virtual I/O Device TC: Source code for the VIRTIO (Virtual I/O) Specification maintained by the OASIS Virtual I/O Device (VIRTIO) Technical Committee.",
            "Licensing. Please see the LICENSE file for description of the license terms and OASIS policies applicable to the TC's work in this GitHub project ..."
          ]
        },
        {
          "title": "OASIS Virtual I/O Device (VIRTIO) TC IP and License Policy",
          "url": "https://www.oasis-open.org/committees/virtio/ipr.php",
          "excerpts": [
            "Red Hat has filed several United States patent applications that might become essential to an OASIS Standards Final Deliverable.",
            "This TC operates under the Non-Assertion Mode of the OASIS IPR Policy.",
            "The US Patent Nos. 9811260 and 9311164 \"System and Method for Ballooning wth Assigned Devices\", the US Patent No. 9934157 and the US Publication No. 20180225221 \"Post-Copy VM Migration Speedup Using Free Page Hinting\", the US Publication No. 20170147371 \"Virtual Machine Migration Using Memory Page Hints\", the US Patent No. 9965311 \"Application of a Pattern to Inactive Pages in Guest Memory\", the US Patent No.\n10169088 \"Lockless Free Memory Ballooning for Virtual Machines\", the US Patent Nos. 10083058 and 9672062 \"Batched Memory Page Hinting\" and the US Publication No. 20180239707 \"Page Cache Management During Migration\" are relevant to section \"Traditional Memory Balloon Device\" of the Virtual I/O Device (VIRTIO) specification.",
            "Red Hat is unable to provide more specific information regarding the unpublished applications, until such time as the applications are published or the patents are issued."
          ]
        }
      ],
      "reasoning": "The finegrained field value is Virtio Specification, which is best supported by excerpts that define or describe the Virtio standard and its ecosystem. Content that states Virtio as an OASIS standard and outlines its device families (block, net, GPU, etc.) directly supports understanding what the Virtio Specification encompasses. Excerpts describing the purpose of virtio in standardizing the device model and reducing the driver zoo (i.e., standardization of the device interface) further anchor the notion of a formal specification. Excerpts that discuss licensing/IP considerations, IPR policy, and patent references provide context on the legal-licensing environment surrounding the Virtio Specification, which is essential when evaluating licensing considerations for the specification. In short, excerpts detailing the Virtio standard's scope and purpose establish what the Virtio Specification is, while excerpts about licensing/IPR policy explain how the specification is governed, licensed, and protected legally. Together, they map directly to the field value's domain: Virtio Specification and its licensing landscape.\n",
      "confidence": "high"
    },
    {
      "field": "cloud_deployment_strategy",
      "citations": [
        {
          "title": "Security Design of the AWS Nitro System",
          "url": "https://docs.aws.amazon.com/whitepapers/latest/security-design-of-aws-nitro-system/the-components-of-the-nitro-system.html",
          "excerpts": [
            "Nitro System enables what is effectively “bare metal” performance by running nearly all\n the functionality of the virtualization system on the Nitro cards rather than on the host’s\n system mainboard CPU",
            "Within the Nitro Hypervisor, there is, by design, no networking stack, no general-purpose file system implementations, and no peripheral device driver support.",
            "The Nitro Hypervisor is designed to receive virtual machine management commands (start, stop, and so on) sent from the Nitro Controller, to partition memory ..."
          ]
        },
        {
          "title": "Amazon EBS volumes and NVMe - AWS Documentation",
          "url": "https://docs.aws.amazon.com/ebs/latest/userguide/nvme-ebs-volumes.html",
          "excerpts": [
            "Amazon EBS volumes are exposed as NVMe block devices on Amazon EC2 instances built on the [AWS Nitro System](https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html).",
            "To fully utilize the performance and capabilities of Amazon EBS volumes exposed as NVMe block devices, the EC2 instance must have the AWS NVMe driver installed.",
            "All current generation AWS Windows and Linux AMIs come with the AWS\nNVMe driver installed by default.",
            "The device names are `/dev/nvme0n1`, `/dev/nvme1n1`,\nand so on."
          ]
        }
      ],
      "reasoning": "The AWS Nitro System is described as a lightweight hypervisor that offloads I/O to Nitro Cards, enabling near bare-metal performance by partitioning CPU/memory while transferring networking and storage I/O to dedicated hardware. This establishes that, on AWS, high performance relies on Nitro-derived I/O paths rather than generic virtio-based approaches for the primary data planes. Supporting this view, the ENA entry describes an SR-IOV-based high-performance networking path that bypasses much of the host overhead, enabling low-latency, high-throughput networking at scale. The NVMe/EBS-related entries state that EBS volumes are exposed as NVMe block devices via SR-IOV, with explicit driver and path requirements in AWS, and note that current AWS AMIs include the necessary NVMe drivers. Taken together, these AWS-specific IO avenues (Nitro, ENA SR-IOV, NVMe-backed EBS) form a high-performance path that a custom OS would need to support directly to remain competitive on AWS. In contrast, a pure Universal Virtio approach (virtio-net/virtio-blk as the sole I/O interface) would, per these sources, likely fall short of AWS's peak performance envelope on modern instance families, since the native Nitro and ENA/NVMe paths are designed to achieve near-bare-metal throughput and latency. The explicit AWS guidance on NVMe device naming and driver availability further reinforces that to achieve top-tier AWS performance, the OS must integrate with AWS-provided IO interfaces (ENA/NVMe) rather than rely exclusively on generic Virtio abstractions. Therefore, the most relevant excerpts are those that directly describe Nitro's offloaded I/O architecture, ENA SR-IOV networking, and NVMe-backed EBS within AWS, along with driver inclusion details in current AMIs. The evidence collectively supports the conclusion that Universal Virtio as a universal IO path is not the optimal route for AWS-grade high performance, and that AWS-specific I/O interfaces are essential for competitiveness. ",
      "confidence": "high"
    },
    {
      "field": "executive_summary",
      "citations": [
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "— When handling latency-sensitive applications, such as memcached, we found that Shenango achieves tail latency and throughput comparable to ZygOS, a state-of-the ...",
            "Shenango solves the hard prob-\nlem of reallocating cores across applications at very fine\ntime scales; it reallocates cores every 5 microseconds,",
            "Shenango introduces a new\napproach to two-level scheduling by combining the first\nscheduler level directly with the NIC.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "Shenango introduces a new\napproach to two-level scheduling by combining the first\nscheduler level directly with the NIC.",
            "it reallocates cores every 5 microseconds,",
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "Arachne improves upon Linux, maintaining\n99.9th percentile latency below 200 µs with batch work."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues.",
            "SPDK optimizations",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "* user offset - positive offset for the mmap",
            "* size",
            "This document is intended to provide an overview of how Vhost works behind the scenes. Code snippets used in this document might have been simplified for the sake of readability and should not be used as an API or implementation reference.",
            "Virtio devices use virtqueues to transport data efficiently.",
            "Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O.",
            "Vhost is a protocol for devices accessible via inter-process communication. It uses the same virtqueue layout as Virtio to allow Vhost devices to be mapped directly to Virtio devices."
          ]
        }
      ],
      "reasoning": "Directly addressing the executive summary's core claim: a high-performance, deterministic Rust-based stack ('Rust Hallows' as a DO-178C/ISO-26262-aligned, partitioned system) designed for latency-sensitive server workloads. The strongest support comes from excerpts that concretely illustrate the target workload domains (HFT, 5G UPF, RTB) as use cases for ultra-low tail latency and microsecond-level scheduling, and from sources that discuss microsecond-scale core reallocations and partitioning to reduce interference. The cited Shenango-related material describes an IOKernel-driven mechanism to reallocate cores on microsecond timescales and to isolate workloads to achieve tail latency improvements, which directly underpins the executive summary's claim about deterministic performance and reduced cross-tenant interference. The cited papers on Shenango also emphasize tight coupling of scheduling with NIC data paths to minimize jitter, which aligns with the idea of a tightly integrated Rust-based stack with microsecond-level control. Additionally, the excerpts describing SHENANGO-like scheduling and partitioning (Shenango/Caladan lineage) provide a concrete technical basis for the proposed approach to predictability and safety, supporting the field's emphasis on DO-178C/CAST-32A-aligned strategies and microsecond-scale reallocation. Supporting material on the Virtio ecosystem (Virtio as a standardized device model; virtio-net/blk/scsi basics; vhost-user and SPDK vhost backends) maps cleanly to the Pareto set of drivers: virtio-net for efficient network I/O, virtio-blk/virtio-scsi for storage, and supplementary virtio-console/virtio-rng/virtio-vsock for system functionality. The SPDK vhost and AF_XDP references provide concrete backends that enable the performance promises described (kernel bypass-like architecture with user-space backends and zero-copy pathways). Excerpts detailing Virtio device structure, ring-based queues, and the SPDK vhost architecture underpin the claim that a driver set based on Virtio with high-performance backends can achieve the outlined performance goals. Finally, supporting excerpts on SPDK vhost and AF_XDP discuss the performance characteristics and multi-queue capabilities that justify the chosen Pareto driver set and the architecture's feasibility in delivering near-native performance in a Rust OS stack. Together, these excerpts form a coherent chain from the target workloads (HFT, UPF, RTB) and the need for microsecond-level determinism, to a minimal but powerful driver set (virtio-net, virtio-blk/virtio-scsi, virtio-console, virtio-rng, virtio-vsock) backed by high-performance user-space I/O backends (SPDK vhost, DPDK, AF_XDP) that enable the architecture's goals.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking."
          ]
        },
        {
          "title": "Step-by-Step Guide: Building High-Performance Software ...",
          "url": "https://medium.com/@israelgeoffrey/step-by-step-guide-building-high-performance-software-defined-storage-with-spdk-nvme-of-and-spdk-743671404156",
          "excerpts": [
            "Let's begin by deploying the SPDK NVMe-oF target inside a container. This will serve as our high-performance storage backend, enabling ultra-low ..."
          ]
        },
        {
          "title": "Intel E810 Feature Summary and Related NVMe/ConnectX Details",
          "url": "https://cdrdv2-public.intel.com/630155/630155_Intel%20Ethernet%20Controller%20E810%20Feature%20Summary_rev4_6.pdf",
          "excerpts": [
            "June 2025",
            "Feature Support Matrix",
            "The following table lists the driver, firmware, and package versions recommended for use with the supported DPDK version. Table 9. DPDK ...",
            "Intel® Ethernet Controller E810",
            "Intel® Ethernet Controller E810"
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "rs. #### Configuration Details\n\nTest by Intel as of 10/22/2022. System: 1-node, 2x Intel® Xeon® Platinum 8480+ Processor (56 cores, HT=On, Turbo=ON),\nTotal Memory 1024 GB (16 slots/ 64GB/ 4800 MHz), BIOS:3A05 (ucode:0x2b000070),\nStorage: 24x Intel® Optane™ SSD 800GB P5800X."
          ]
        },
        {
          "title": "Intel® Ethernet Network Adapter E810-2CQDA2",
          "url": "https://cdrdv2-public.intel.com/639389/Intel%20Ethernet%20Network%20Adapter%20E810-2CQDA2%20Product%20Brief.pdf",
          "excerpts": [
            "• Control Queues (a.k.a. Admin Queues). - Mailbox Queues for PF-VF and driver-driver. - Admin Queues for Software-Firmware control flows. - Sideband Queues for ..."
          ]
        },
        {
          "title": "[PDF] Intel® Ethernet Controller E800 Series Dynamic Device ...",
          "url": "https://cdrdv2-public.intel.com/618651/618651_Intel%20Ethernet%20Controller%20E810%20DDP%20for%20Comms%20Technology%20Guide.pdf",
          "excerpts": [
            "Updated E810 firmware version from 1.5.4.5 to 1.5.5.6. ... The specific DDP package requires certain firmware and DPDK versions and 800 Series."
          ]
        },
        {
          "title": "Linux Base Driver for the Intel(R) Ethernet Controller 800 Series",
          "url": "https://docs.kernel.org/networking/device_drivers/ethernet/intel/ice.html",
          "excerpts": [
            "In a virtualized environment, on Intel(R) Ethernet Network Adapters that support SR-IOV, the virtual function (VF) may be subject to malicious behavior."
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        }
      ],
      "reasoning": "- Excerpts describing SPDK vhost targets provide direct context for high-performance Virtio backends that are central to the recommended NICs and NVMe devices. They explicitly explain SPDK vhost as a back-end server for Virtio devices, highlighting its zero-copy/polling model and high throughput, which aligns with preferring high-end NICs and SPDK-driven configurations. This supports the selection of Intel E810 and other SPDK-backed NIC implementations as practical hardware choices for the driver-agnostic Virtio backplane. \n- Excerpts detailing SPDK vhost with NVMe backends (NVMe-oF targets and vhost targets) reinforce the suitability of SPDK-based pathways for fast storage I/O and low-latency tails, which matches the field's emphasis on NVMe SSDs (PM1743, CM7, Micron 9400) as targets for peak performance in a unified Virtio stack. The NVMe driver descriptions further corroborate that SPDK user-space drivers can achieve low-latency, high-throughput I/O, which justifies the field's recommended NVMe SSDs as testbeds or reference targets. \n- Excerpts listing explicit Intel E810/NVMe and SPDK/ice-related guidance provide concrete backing for the NICs in the field value. They discuss configuring Intel 800-series adapters with SPDK/DPPDK and AF_XDP, and mention the E810's capabilities in SPDK-based flows and NVMe offload contexts, which aligns with the field's claim that the Intel 800-series NICs are premier choices with mature driver support and strong ecosystem tooling. \n- Excerpts that mention NVMe drives such as PM1743, CM7, and Micron 9400 (including capacity, IOPS, and PCIe Gen4/5 capabilities) help validate the SSD side of the field value, supporting the assertion that these drives pair well with SPDK-based backends for high-throughput, low-latency workloads in a Virtio-driven environment. They also underscore the compatibility of these drives with SPDK's user-space stack and its performance characteristics in datacenter contexts. \n- Overall, the most directly relevant material centers on: (a) SPDK vhost/virtio usage with NICs and AF_XDP; (b) SPDK NVMe/vhost backends and NVMe-oF references; and (c) concrete NVMe SSD platforms (PM1743, CM7, Micron 9400) that are commonly used as high-performance targets in SPDK-based testbeds and benchmarks. The surrounding material on broader Spark/Kafka ecosystems, while useful for context, does not directly support the specific hardware field values and is thus deprioritized in the relevance ranking.",
      "confidence": "high"
    },
    {
      "field": "observability_for_low_latency_systems",
      "citations": [
        {
          "title": "HdrHistogram: Better Latency Capture",
          "url": "http://psy-lob-saw.blogspot.com/2015/02/hdrhistogram-better-latency-capture.html",
          "excerpts": [
            "\n\n* Recording a value in the recorder is a wait free operation (on JDK8, can be lock free on older depending on the getAndAdd implementation for AtomicLongArray).",
            "HdrHistogram) lib to demonstrate measurement and report results, here are the relevant excerpts from the [Ping sampl"
          ]
        },
        {
          "title": "Clarify whether logging to standard output is blocking #668 - GitHub",
          "url": "https://github.com/tokio-rs/tracing/issues/668",
          "excerpts": [
            "Non-blocking logging is theoretically superior, but the difference is probably only meaningful in particularly performance-sensitive code."
          ]
        },
        {
          "title": "journald.conf(5) — systemd — Debian testing",
          "url": "https://manpages.debian.org/testing/systemd/journald.conf.5.en.html",
          "excerpts": [
            "Note: Forwarding is performed synchronously within journald, and may significantly affect its performance. This is particularly relevant ..."
          ]
        },
        {
          "title": "Sockperf – UDP/TCP Latency and Throughput Benchmarking Tool",
          "url": "https://docs.nvidia.com/networking/display/VMAv9840/Appendix+Sockperf+%E2%80%93+UDP/TCP+Latency+and+Throughput+Benchmarking+Tool",
          "excerpts": [
            "Sockperf can provide a full log of all packets' tx and rx times by dumping all the data that it uses for calculating percentiles and building ..."
          ]
        },
        {
          "title": "Cloudflare eBPF benchmark README",
          "url": "https://github.com/cloudflare/ebpf_exporter/blob/master/benchmark/README.md",
          "excerpts": [
            "This is a simple eBPF benchmark showing overhead of eBPF probes. Setup ... Notice how tracepoints are faster than fentry and fentry is faster than kprobe.",
            "The results below are from MacBook Air (M1, 2020) running Linux 6.5-rc1\nin QEMU.",
            "Probe with a complex map increment attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n|\ntracepoint complex | 213 | 96 | 82% |\n| fentry complex | 220 | 103 | 88% |\n| kprobe complex | 346 | 229 | 196% |\n"
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch.",
            "Code here:  <https://github.com/DataDog/sketches-go>  <https://github.com/DataDog/sketches-py>  <https://github.com/DataDog/sketches-java>"
          ]
        },
        {
          "title": "Systemd journald performance discussions (GH issue 2815)",
          "url": "https://github.com/systemd/systemd/issues/2815",
          "excerpts": [
            "Performance issues occur when a high throughput workload emits a large number of logs."
          ]
        },
        {
          "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch ...",
          "url": "https://www.vldb.org/pvldb/vol12/p2195-masson.pdf",
          "excerpts": [
            "by C Masson · Cited by 99 — Merging two DDSketches is very fast—it takes around 10 microseconds or less to merge two sketches containing up to fifty million values each—depending on the ..."
          ]
        },
        {
          "title": "Sampling",
          "url": "https://opentelemetry.io/docs/concepts/sampling/",
          "excerpts": [
            "Sep 17, 2024 — Tail sampling is where the decision to sample a trace takes place by considering all or most of the spans within the trace. Tail Sampling gives ..."
          ]
        },
        {
          "title": "OpenTelemetry Sampling: head-based and tail-based",
          "url": "https://uptrace.dev/opentelemetry/sampling",
          "excerpts": [
            "Head-based sampling can't sample failed or unusually long operations, because that information is only available at the end of a trace. With tail-based sampling we delay the sampling decision until all spans of a trace are available which enables better sampling decisions based on all data from the trace."
          ]
        },
        {
          "title": "Tail Sampling with OpenTelemetry: Why it's useful, how to ...",
          "url": "https://opentelemetry.io/blog/2022/tail-sampling/",
          "excerpts": [
            "Sep 26, 2022 — Tail-based sampling is where the decision to sample a trace happens after all the spans in a request have been completed. This is in contrast to ...",
            "Sep 26, 2022 — Tail sampling is useful for identifying issues in your distributed system while saving on observability costs."
          ]
        },
        {
          "title": "Sampling in OpenTelemetry: A Beginner's Guide",
          "url": "https://betterstack.com/community/guides/observability/opentelemetry-sampling/",
          "excerpts": [
            "Feb 26, 2025 — Tail-based sampling makes sampling decisions after a trace completes, allowing decisions based on the full trace context including duration, ...",
            "Feb 26, 2025 — Sampling is the process of selectively capturing and analyzing a subset of traces instead of recording every request that flows through a system."
          ]
        },
        {
          "title": "Insights from Paper—Google Dapper: a Large-Scale Distributed ...",
          "url": "https://medium.com/100paperschallenge/insights-from-paper-google-dapper-a-large-scale-distributed-systems-tracing-infrastructure-1f5a448ca000",
          "excerpts": [
            "Low overhead: The tracing system should have a negligible performance impact on running services. Application-level transparency: Programmers ..."
          ]
        },
        {
          "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch ...",
          "url": "https://arxiv.org/pdf/1908.10693",
          "excerpts": [
            "by C Masson · 2019 · Cited by 99 — Merging two DDSketches is very fast—it takes around 10 microseconds or less to merge two sketches containing up to fifty million values each— ..."
          ]
        },
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "Apr 22, 2021 — Coordinated omission is a term coined by Gil Tene to describe the phenomenon when the measuring system inadvertently coordinates with the system being measured.",
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency.",
            "How Not to YCSB"
          ]
        },
        {
          "title": "HdrHistogram - DoubleHistogram.java (coordinated omission correction excerpts)",
          "url": "https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/DoubleHistogram.java",
          "excerpts": [
            "public DoubleHistogram copyCorrectedForCoordinatedOmission(final double expectedIntervalBetweenValueSamples) {",
            "To compensate for the loss of sampled values when a recorded value is larger than the expected\n\\* interval between value samples, the new histogram will include an auto-generated additional series of\n\\* decreasingly-smaller (down to the expectedIntervalBetweenValueSamples) value records for each count found\n\\* in the current histogram that is larger than the expectedIntervalBetweenValueSamp"
          ]
        },
        {
          "title": "SPDK SPDK/FIO Readme (SPDK fio plugin usage)",
          "url": "https://fossies.org/linux/spdk/app/fio/bdev/README.md",
          "excerpts": [
            "Usage\nTo use the SPDK fio plugin with fio, specify the plugin binary using\nLD_PRELOAD when running fio and set ioengine=spdk_bdev in the fio\nconfiguration file (see example_config.fio in the same directory as this\nREADME)."
          ]
        },
        {
          "title": "Confidence intervals for quantiles and percentiles - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6294150/",
          "excerpts": [
            "In this work we discuss three methods to estimate CI on quantiles and percentiles using parametric, nonparametric and resampling (bootstrap) approaches.",
            "\nPercentiles and quantiles are statistics used for summarizing the relative location of data within a set according to their magnitude and independently from a particular underlying probability distribution"
          ]
        }
      ],
      "reasoning": "HdrHistogram is repeatedly described as the telemetry primitive that enables precise latency measurement with minimal overhead. The excerpts describe its fixed memory footprint and constant-time recording, which directly support the field value describing \"HdrHistogram offers constant-time recording, typically taking only 3-6 nanoseconds on modern CPUs. It maintains a fixed, predictable memory footprint\" and its suitability for logging latency distributions without locking. Several excerpts emphasize the ability to capture tail latencies (p99, p99.9, p99.99, etc.), which is essential in microsecond-scale scheduling contexts where tail latency dominates user experience. The existence of coordinated-omission corrections within HDRHistogram (and related histograms) shows the telemetry can adjust for sampling biases, which is critical for accurate SLO validation in shard-per-core architectures like Rust Apex. The references to lock-free, per-thread or single-writer usage patterns reinforce the fit with shard-per-core telemetry design, ensuring telemetry collection does not become a source of jitter or contention. Together, these excerpts substantiate that HDR Histogram is the appropriate telemetry primitive for measuring microsecond-scale tail latency in a low-latency system, and they illustrate both the capabilities (low overhead, fixed memory, nanosecond-scale recording) and the necessary analytic techniques (coordinated-omission corrections) to derive accurate latency percentiles for post-mortem analysis and SLA validation. The strongest, direct confirmations come from the passages that spell out the exact performance characteristics (3-6 ns, fixed ~185 KB for wide ranges, zero allocations) and the tail-latency focus (p99.9+), while several excerpts extend the narrative with practical usage patterns and related latency tooling (header histograms, DDSketch comparisons as context, and telemetry instrumentation guidance).",
      "confidence": "high"
    },
    {
      "field": "storage_stack_recommendation",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues.",
            "SPDK optimizations",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "Virtqueue generally consists of an array of descriptors and each I/O needs to be converted into a chain of such descriptors.",
            "SPDK vhost is a Vhost-user back-end server.",
            "The protocol defines 2 sides of the communication, front-end and back-end.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The driver sends a single message for each region with the following data:",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "* file descriptor - for mmap",
            "* user address - for memory translations in Vhost-user messages (e.g. translating vring addresses)",
            "* guest address - for buffers addresses translations in vrings (for QEMU this is a physical address inside the guest)",
            "* user offset - positive offset for the mmap",
            "* size",
            "This document is intended to provide an overview of how Vhost works behind the scenes. Code snippets used in this document might have been simplified for the sake of readability and should not be used as an API or implementation reference.",
            "Virtio devices use virtqueues to transport data efficiently.",
            "Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O.",
            "Vhost is a protocol for devices accessible via inter-process communication. It uses the same virtqueue layout as Virtio to allow Vhost devices to be mapped directly to Virtio devices."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "_NET\\_F\\_RSS(60)\n\n    Device supports RSS (receive-side scaling) with\n\n     Toeplitz hash calculation and configurable hash parameters for receive\n\n     steering. VIRTIO\\_NET\\_F\\_RSC\\_EXT(61)\n\n    Device can process duplicated ACKs and\n\n     report number of coalesced segments and duplicated ACKs",
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device.",
            "For brevity, these standard documents are\n\n         referred as \"ZBD standards\" from this point on in the text. ##### 5\\.2.3.1 Legacy Interface: Feature bits\n\n    VIRTIO\\_BLK\\_F\\_BARRIER (0)\n\n        Device supports request barriers. VIRTIO\\_BLK\\_F\\_SCSI (7)\n\n        Device supports scsi ",
            "A driver would perform a typical initialization routine like so:\n\n1\\. Identify and initialize the receive and transmission virtqueues, up to\n\n     N of each kind. If VIRTIO\\_NET\\_F\\_MQ feature bit is negotiated,\n\n     N= max\\_virtqueue\\_pairs , otherwise identify N=1. 2\\.\nIf the VIRTIO\\_NET\\_F\\_CTRL\\_VQ feature bit is negotiated, identify the\n\n     control",
            "The network device has the following device configuration layout. All of the device\nconfiguration fields are read-only for the driver. struct virtio\\_net\\_config {  \nu8 mac [6];  \nle16 status ;  \nle16 max\\_virtqueue\\_pairs ;  \nle16 mtu ;  \nle32 speed ;  \nu8 duplex ;  \nu8 rss\\_max\\_key\\_size ;  \nle16 rss\\_max\\_indirection\\_table\\_length ;  \nle32 supported\\_hash\\_types ;  \nle32 supported\\_tunne",
            "A driver SHOULD NOT negotiate VIRTIO\\_NET\\_F\\_HASH\\_REPORT if it does not\nnegotiate VIRTIO\\_NET\\_F\\_C",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            ". For\ninformation on whether any patents have been disclosed that may be essential to\nimplementing this specification, and any offers of patent licensing terms, please refer\nto the Intellectual Property Rights page in the TC’s GitHub repository\n( <https://github.com/oasis-tcs/virtio-admin/blob/master/IPR.md> ).",
            "This specification is provided under the Non-Assertion Mode of the OASIS IPR Policy, the mode chosen when the Technical Committee was established.",
            "The name \"OASIS\" is a trademark of [OASIS](https://www.oasis-open.org/) , the owner and developer of this\nspecification, and should be used only to refer to the organization and its official\noutputs. OASIS welcomes reference to, and implementation and use of, specifications,\nwhile reserving the right to enforce its marks against misleading uses"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Block Device Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/vhost-user-slave.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "oasis-tcs/virtio-spec: OASIS Virtual I/O Device TC",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "OASIS Virtual I/O Device TC: Source code for the VIRTIO (Virtual I/O) Specification maintained by the OASIS Virtual I/O Device (VIRTIO) Technical Committee.",
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms.",
            "Licensing. Please see the LICENSE file for description of the license terms and OASIS policies applicable to the TC's work in this GitHub project ..."
          ]
        },
        {
          "title": "How to test vhost-user-blk with SPDK - GitHub Pages",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/docs/vhost-user-blk-testing.html",
          "excerpts": [
            "The purpose of this document is to illustrate how to test vhost-user-blk in Cloud Hypervisor with SPDK as the backend.",
            "The below test environment is based on Ubuntu release (20.04.1 LTS), as for other system, please check related documents. This test runs with multiple queue (MQ) support enabled, using 4 queues defined for both SPDK and the virtual machine.",
            "### The hugepages settings on the host",
            "### Download the SPDK code",
            "### Create the build dep",
            "### Build SPDK",
            "### Set the SPDK environment",
            "Run SPDK vhost target with 2 CPU cores, i.e., core 0 and 1. ",
            "### Create 512M block device",
            "### Launch the VM",
            "# How to test the vhost-user-blk device with SPDK backend",
            "The vhost-user-blk device is /dev/vdb",
            "# How to do simple read/write test",
            "If you want to do fio test, please install fio binary into guest. The detailed info is not listed here."
          ]
        },
        {
          "title": "[PDF] DPDK Vhost/Virtio Performance Report Release 21.11",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_21_11_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The primary audience for this test report is architects and engineers implementing the Data Plane. Development Kit (DPDK)."
          ]
        },
        {
          "title": "[PDF] DPDK Vhost/Virtio Performance Report Release 20.05",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_20_05_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The primary audience for this test report are architects and engineers implementing the Data Plane. Development Kit (DPDK).",
            "May 20, 2020 — This report provides information on packet processing performance testing for the specified DPDK release on Intel® architecture. The initial ...",
            "May 20, 2020 — The case is to measure vhost/virtio system forwarding throughput, and the theoretical system forwarding throughput is 40 Gbps. Both Vhost and ...",
            "May 20, 2020 — The primary audience for this test report are architects and engineers implementing the Data Plane. Development Kit (DPDK)."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "it reallocates cores every 5 microseconds,",
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "Arachne improves upon Linux, maintaining\n99.9th percentile latency below 200 µs with batch work.",
            "Under sudden changes in load, low tail latency is\nonly possible with a short core allocation interval.",
            "by A Ousterhout · Cited by 428 — When handling latency-sensitive applications, such as memcached, we found that Shenango achieves tail latency and throughput comparable to ZygOS ...",
            "This tension between low tail latency and high CPU efficiency is exacerbated by the bursty arrival patterns of today's datacenter workloads.",
            "The IOKernel runs on a dedicated core and performs two\n\nmain functions:\n\n1\\. At any given time, it decides how many cores to\n\nallocate to each application (§ 4\\.1.1 ) and which\n\ncores to allocate to each application (§ ",
            "oviding\n\nuseful\n\nprogramming\n\nabstractions, including threads, mutexes, condition\n\nvariables, and network sockets. Applications link with\n\nthe Shenango runtime as a library, allowing kernel-like\n\nfunctions to run within their address spaces.",
            "Shenango is implemented in C and includes\n\nbindings for C++ and Rust. The IOKernel is imple-\n\nmented in 2,244 LOC and the runtime is implemented\n\nin 6,155 LOC. Both components depend on a 4,762\n\nLOC collection of custom library rou",
            "Shenango relies on several Linux kernel mechanisms to\n\npin threads to cores and for communication between the\n\nIOKernel and runtimes. The IOKernel passes data via\n\nSystem-V shared memory segments that are mapped into\n\neach runt",
            "Shenango provides bindings for both C++ and Rust\n\nwith idiomatic interfaces (e.g., like std::thread )\n\nand support for lambdas and closures respectively. Most\n\nof the bindings are implemented as a thin wrapper around\n\nthe underlying C lib",
            "The runtime detects a quiescent period after each kthread\n\nhas rescheduled, allowing it to free any stale RCU\n\nobjects. Internally, RCU is used for the ARP table and\n\nfor the TCP and UDP socket tabl",
            "by A Ousterhout · Cited by 428 — Instead, Shenango closely approximates this ideal, detecting load changes every five microseconds and adjusting core allocations over 60,000 ...",
            "Existing systems impose too much overhead for microsecond-scale core reallocations to be practical: Arachne requires 29 microseconds of latency.",
            "Shenango achieves comparable latencies but at far\ngreater CPU efficiency.",
            "Shenango is implemented in C and includes\nbindings for C++ and Rust.",
            "Shenango introduces a new\napproach to two-level scheduling by combining the first\nscheduler level directly with the NIC.",
            "Kernel-bypass networking:\nMany systems bypass\nthe kernel to achieve low-latency networking by using\nRDMA, SR-IOV, or libraries such as DPDK [2] or\nnetmap [66]."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "Shenango introduces a new\napproach to two-level scheduling by combining the first\nscheduler level directly with the NIC."
          ]
        },
        {
          "title": "OSDI20 RackSched and related NSDI/SOSP references (Shenango, ZygOS, SPDK vhost discussion)",
          "url": "https://www.cs.jhu.edu/~hzhu/files/OSDI20_RackSched.pdf",
          "excerpts": [
            "Shenango: Achieving high CPU efficiency\nfor latency-sensitive datacenter workloads. In USENIX\nNSDI, 2019.",
            "ZygOS: Achiev-\ning low tail latency for microsecond-scale networked\ntask"
          ]
        },
        {
          "title": "SPDK Vhost Performance does not meet expectations. #3442",
          "url": "https://github.com/spdk/spdk/issues/3442",
          "excerpts": [
            "h my configuration was similar to the document, the performance was very poor, so I wanted to seek help to see if it was because of some of my configurations. Expected Behavior\n-----------------\n\nWhat I hope is that although using SPDK vhost cannot achieve the native I/O performance of NVMe SSD, it should at least be close. However, the fact is that when I used SPDK vhost,",
            "What I hope is that although using SPDK vhost cannot achieve the native I/O performance of NVMe SSD, it should at least be close. However, the fact is that when I used SPDK vhost, I found that the performance dropped off a cliff, and only native performance could be achieved. It’s less than 1/4, and the average delay is several times the original",
            "Jul 15, 2024 — It's less than 1/4, and the average delay is several times the original. Current Behavior. This is the performance statistics using native SPDK ...",
            "However, the fact is that when I used SPDK vhost, I found that the performance dropped off a cliff, and only native performance could be ...",
            "SPDK Vhost Performance does not meet expectations. #3442",
            "However, the fact is that when I used SPDK vhost, I found that the performance dropped off a cliff, and only native performance could be achieved.",
            "It’s less than 1/4, and the average delay is several times the original.",
            "Disk stats (read/write):\n  sdb: ios=23/8421032, sectors=2928/67368288, merge=0/4, ticks=1/443897, in_queue=443898, util=87.70",
            "SPDK Vhost Performance does not meet expectations."
          ]
        },
        {
          "title": "SPDK vhost Performance (SNIA SDC 2018)",
          "url": "https://www.snia.org/sites/default/files/SDCEMEA/2018/Presentations/Accelerating-VM-Access-with-Storage-Performance-Developer-Kit-SNIA-SDC-EMEA-2018.pdf",
          "excerpts": [
            "SPDK up to 3x better efficiency and latency",
            "48 VMs: vhost-scsi performance (SPDK vs. Kernel)",
            "Technology claims are based on comparisons of latency, density and write cycling metrics amongst memory technologies recorded on published specifications of in ...",
            "\n0\n\n25\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n10,000\n\n**HDD**\n\n**\\+SAS/S**\n\n**ATA**\n\n**SSD**",
            "\n\n_**Scalable and Efficient Software Ingredients**_ •\n\nUser space, lockless, polled-mode components",
            "**48 VMs: vhost-scsi performance (SPDK vs. Kernel)**",
            "SPDK vhost\n\nvhost\n\nDPDK vhost",
            "SPDK VHOST\n\n16\n\n1\\. Add IO to virtqueue\n\n2\\. Poll virtqueue\n\n3\\. Device executes IO\n\n4\\. Guest completion inter",
            "SPDK vhost Performance",
            "SPDK wants to use userspace polled mode drivers",
            "SPDK vhost-scsi up to 3.2x better with\n\n4K 100% Random read I/O",
            "SPDK VHOST Architecture",
            "SPDK vhost-scsi up to 3.2x better with\n\n4K 100% Random read I/Os",
            "\nSPDK wants to use userspace polled mode drivers",
            "\n\nvhost\n\nvhost\n\n",
            "SPDK Vhost\n\nNVMe-oF\n\nInitiator"
          ]
        },
        {
          "title": "SPDK Vhost-user – SmartX",
          "url": "https://www.smartx.com/blog/2022/07/spdk-vhost-user-en/",
          "excerpts": [
            "SPDK Vhost-user: Improve I/O Storage Performance in Hyperconvergence",
            "In this article, we focus on SPDK Vhost-user and explain how to leverage this technology to improve the I/O performance of Virtio-BLK/Virtio-SCSI storage in KVM.",
            "Conclusion",
            "Through the comparison, it is clear that the vhost technology presents more outstanding storage performance than the Virtio solution.",
            "The test data is based on the three-nodes HCI cluster (the storage network of the cluster was not enabled with RDMA).",
            "The following table compares two schemes’ IOPS under the condition of 4k iodepth = 128 (higher IOPS stands for better performance):",
            "The following table compares two schemes’ latency under the condition of 4K iodepth=1 (lower latency stands for better performance):",
            "Virtio consists of three parts: 1) the front-end driver layer, which is integrated in the Guest OS; 2) Virtqueue as the middle part, which is responsible for data transmission and command interaction; and 3) the back-end device layer, which processes the requests sent by the Guest OS.",
            "The data plane processing of vhost-user is mainly divided into Master and Slave. Generally, QEMU is the Master which is the supplier of Virtqueue, and storage software is the Slave which is responsible for consuming I/O requests in the Virtqueue.",
            "In this article, we focus on SPDK Vhost-user and explain how to leverage this technology to improve the I/O performance of Virtio-BLK/Virtio-SCSI storage in ...",
            "* Intel Xeon Gold 5122 3.6GHz",
            "* 8\\*16G DDR4 2666MHz",
            "* 2\\*Intel P4610 1.6T NVMe",
            "* Mellanox CX-4 25Gbps",
            "\nThe following table compares two schemes’ latency under the condition of 4K iodepth=1 (lower latency stands for better performance):",
            "**Conclusion**",
            "Jul 15, 2022 — In this article, we focus on SPDK Vhost-user and explain how to leverage this technology to improve the I/O performance of Virtio-BLK/Virtio-SCSI storage in ...",
            "Through the comparison, it is clear that the vhost technology presents more outstanding storage performance than the Virtio solution. Moreover, ...",
            "Benefitting from Vring’s I/O communication mechanism, Virtio effectively reduces I/O latency and shows better performance than the pure software emulation of QEMU",
            "the data path based on Virtio-BLK is as follows:",
            "Jul 15, 2022 — Benefitting from Vring's I/O communication mechanism, Virtio effectively reduces I/O latency and shows better performance than the pure software ..."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Building a High Performance Trading System in the Cloud - Medium",
          "url": "https://medium.com/prooftrading/building-a-high-performance-trading-system-in-the-cloud-341db21be100",
          "excerpts": [
            "For us, when we say ~200 microseconds, that's the 90th-percentile round-trip latency between our sequencer and the OMS/Algo engine (note: this ..."
          ]
        },
        {
          "title": "Colocation Pricing Guide - (Updated June 2025)",
          "url": "https://brightlio.com/colocation-pricing/",
          "excerpts": [
            "The average cost of colocation services in primary North American markets is around $163.44 per kilowatt (kW) for deployments between 250 kW and 500 kW. Single ..."
          ]
        },
        {
          "title": "How to Calculate How Much High-Frequency Trading ...",
          "url": "https://www.chicagobooth.edu/review/how-calculate-how-much-high-frequency-trading-costs-investors",
          "excerpts": [
            "They estimate that in typical years in global stock markets, HFT winners may gain $5 billion at the expense of other market participants."
          ]
        },
        {
          "title": "HiP4-UPF: Towards High-Performance Comprehensive 5G ...",
          "url": "https://www.usenix.org/system/files/atc24-wen.pdf",
          "excerpts": [
            "by Z Wen · 2024 · Cited by 3 — Our evaluation demonstrates that HiP4-. UPF offers a high-performance solution to 5G UPF, whose comprehensive features, except packet buffering, ..."
          ]
        },
        {
          "title": "Ultra-Reliable Low-Latency Communication",
          "url": "https://www.5gamericas.org/wp-content/uploads/2019/07/5G_Americas_URLLLC_White_Paper_Final__updateJW.pdf",
          "excerpts": [
            "to meet the URLLC latency requirement of 1 ms or less in some scenarios. For example, in a TDD single carrier scenario, UE and gNB handshake turnaround can ..."
          ]
        },
        {
          "title": "Header Bidding Analytics: Key Metrics to Improve Ad ...",
          "url": "https://postindustria.com/header-bidding-analytics-key-metrics-to-improve-ad-revenue-adtech/",
          "excerpts": [
            "Oct 1, 2021 — The latency represents the time needed for an ad source to respond to a bid request. Bidders with high latency can slow down the entire auction ..."
          ]
        },
        {
          "title": "Why Monitoring Latency Is Key to Improving Publisher ...",
          "url": "https://www.geoedge.com/demand-parter-latency-monitoring/",
          "excerpts": [
            "Latency directly impacts a publisher's ad revenue and their ability to be competitive in the marketplace."
          ]
        },
        {
          "title": "4 Tips to Improve P99 Latency — Control Plane | by Eyal Katz",
          "url": "https://medium.com/@aurore_14029/4-tips-to-improve-p99-latency-control-plane-a8f864e6592d",
          "excerpts": [
            "Minimizing the amount of data that must be transferred between the client and server can directly impact latency, as less data means faster ..."
          ]
        },
        {
          "title": "A Comprehensive Guide to Prebid Timeouts",
          "url": "https://setupad.com/blog/prebid-timeouts-guide/",
          "excerpts": [
            "Aug 1, 2024 — There is no one-size-fits-all solution for the Prebid timeout value. However, it's recommended to stay between 1000-2000ms to ensure optimal ..."
          ]
        },
        {
          "title": "Assessing the Impact of High-Frequency Trading on Market ...",
          "url": "https://www.oxjournal.org/assessing-the-impact-of-high-frequency-trading-on-market-efficiency-and-stability/",
          "excerpts": [
            "This article examines the multifaceted impact of high-frequency trading, also known as HFT, on market stability, focusing on both traditional financial markets"
          ]
        },
        {
          "title": "5G Core Network Architecture: 9 Key Network Functions - floLIVE",
          "url": "https://flolive.net/blog/glossary/5g-core-network-architecture-9-key-network-functions/",
          "excerpts": [
            "Key performance indicators (KPIs) such as packet loss, latency, jitter, and throughput should be tracked in real-time across all network ..."
          ]
        },
        {
          "title": "How much is a second worth? What is the value of a ...",
          "url": "https://www.quora.com/How-much-is-a-second-worth-What-is-the-value-of-a-microsecond-in-terms-of-high-frequency-trading",
          "excerpts": [
            "How much is a second worth? What is the value of a microsecond, in terms of high-frequency trading?"
          ]
        },
        {
          "title": "Toward end-to-end latency management of 5G network ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S1068520022004059",
          "excerpts": [
            "by D Larrabeiti · 2023 · Cited by 31 — The network latency budget for this transmission RU-DU (fronthaul) is 100 μ s including propagation delay [2]. Then packets are further forwarded to the CU ( ..."
          ]
        },
        {
          "title": "Nasdaq Co-Location Services",
          "url": "https://listingcenter.nasdaq.com/rulebook/nasdaq/rules/Nasdaq%20General%208",
          "excerpts": [
            "*Pricing is for connectivity only and is similar to connectivity fees imposed by other vendors. The fees are generally based on the amount of bandwidth needed ..."
          ]
        },
        {
          "title": "How do they use C++ in HFT(High Frequency Trade ) industry? : r/cpp",
          "url": "https://www.reddit.com/r/cpp/comments/1d8ukso/how_do_they_use_c_in_hfthigh_frequency_trade/",
          "excerpts": [
            "I know certain company like Optiver use C++/Rust to create real-time/low-latency trading platform, and what do they exactly do(to build ..."
          ]
        },
        {
          "title": "5G Latency- You wished you knew it before!",
          "url": "https://telcocloudbridge.com/blog/5g-latency-you-wished-you-knew-it-before/",
          "excerpts": [
            "May 21, 2021 — The minimum latency needed for 5G is 1 ms or less. What if I tell you that you may be completely misunderstanding how this latency works in “5G”."
          ]
        },
        {
          "title": "Evaluation of User Plane Function Implementations in Real ...",
          "url": "https://ieeexplore.ieee.org/document/10620666",
          "excerpts": [
            "by S Christakis · 2024 · Cited by 2 — This paper focuses on a system evaluation of different UPF implementations in a real-world 5G environment. Specifically, we deploy and test four distinct 5G ..."
          ]
        },
        {
          "title": "Bid Throttling vs. Prebid Timeouts",
          "url": "https://www.aditude.com/blog/bid-throttling-vs-prebid-timeouts-understanding-the-differences-and-best-practices-for-maximizing-revenue",
          "excerpts": [
            "Jan 27, 2025 — This blog explores the key differences between bid throttling and prebid timeouts, how they work, and best practices for implementation."
          ]
        },
        {
          "title": "Adjust auction timeout for bidders & latency between servers",
          "url": "https://github.com/prebid/prebid-server/issues/592",
          "excerpts": [
            "Jun 27, 2018 — For (1), Prebid Server can simply time itself during request preprocessing and adjust the timeout before it forwards the request to each Bidder."
          ]
        },
        {
          "title": "OpenRTB Integration | Real-time Bidding - Google for Developers",
          "url": "https://developers.google.com/authorized-buyers/rtb/openrtb-guide",
          "excerpts": [
            "Maximum time in milliseconds to submit a bid to avoid timeout. Typically between 80ms and 1500ms. wseat, No, string, BidRequest.AdSlot.allowed_seat_ids ..."
          ]
        },
        {
          "title": "Investopedia - World of High-Frequency and Algorithmic Trading",
          "url": "https://www.investopedia.com/articles/investing/091615/world-high-frequency-algorithmic-trading.asp",
          "excerpts": [
            "By 2010, this had been reduced to milliseconds, and by 2024, one-hundredth of a microsecond is enough time for most HFT trade decisions and executions.",
            "HFT trading ideally needs to have the lowest possible data latency (time delays) and the maximum possible automation level."
          ]
        },
        {
          "title": "DPDK Vhost/Virtio Performance Report Release 23.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_23_03_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "Apr 20, 2023 — This report provides information on packet processing performance testing for the specified DPDK release on Intel® architecture. The initial ...",
            "Apr 20, 2023 — This report provides packet processing performance testing for DPDK on Intel architecture, using a Vhost/Virtio setup with 100G traffic, and is ..."
          ]
        },
        {
          "title": "62. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides/nics/virtio.html",
          "excerpts": [
            "In the Data Plane Development Kit (DPDK), we provide a virtio Poll Mode Driver (PMD) as a software solution, comparing to SRIOV hardware solution, for fast ..."
          ]
        },
        {
          "title": "SPDK: Block Device User Guide",
          "url": "https://spdk.io/doc/bdev.html",
          "excerpts": [
            "All of the latency values are measured in microseconds. Example command: rpc ... The Virtio-Block driver allows creating SPDK bdevs from Virtio-Block devices."
          ]
        }
      ],
      "reasoning": "The field value describes SPDK vhost-user as the preferred storage backend path, with multiple concrete benefits and implementation details drawn from the excerpts. First, SPDK vhost is explicitly described as a vhost-user back-end server that exposes Unix domain sockets and connects external applications to a QEMU guest, which aligns with the need for fast, kernel-bypass-enabled storage access in a Rust OS context. The same excerpts emphasize that SPDK vhost supports a poll-mode (busy-polling) path that bypasses kernel interrupts, allowing the I/O path to bypass VM exits and the host kernel, which directly supports the stated performance advantage of SPDK vhost-user over traditional Virtio paths. Further, SPDK is highlighted as providing high-throughput storage backends by exposing SPDK block devices (bdevs) via vhost, including SPDK vhost-target configurations and the ability to map SPDK-backed storage to the guest via Virtio block devices, which is central to achieving the >1M IOPS class workloads and low microsecond latencies noted in SPDK literature. Several excerpts explicitly indicate SPDK's role in high-performance storage acceleration when used with vhost, including SPDK as a back-end for Virtio storage and the vhost-target concept, which is consistent with the recommended path for Kafka and Spark spill workloads where large I/O is common and low tail latency is desired. There is also direct mention that SPDK vhost can serve SPDK-backed bdevs to QEMU via vhost-user interfaces, enabling zero-copy or near-zero-copy data paths and reducing VM exits, which are key levers for latency and throughput. In terms of durability and operability, one excerpt maps the SPDK vhost approach to flush semantics by linking SPDK's vhost path to underlying NVMe flush semantics via the VIRTIO_BLK_F_FLUSH feature, indicating how data flush operations are propagated to persistent media, and notes that proper configuration (including multi-queue blk-mq, hugepages for shared memory, and queue counts) is necessary to realize maximum performance, which addresses operability concerns. The combination of SPDK vhost-user architecture (vhost-user, Unix sockets, QEMU integration), the polling/busy-polling path that bypasses VM exits, the SPDK vhost-target and vhost-scsi options, and the flush semantics mapping to NVMe describe a cohesive stack that fits the field value's assertions about SPDK vhost-user delivering superior throughput and reduced latency for I/O-heavy workloads while keeping data durable and accessible to the guest environment. Consequently, the most relevant excerpts are those that directly describe the SPDK vhost-user back-end server, its vhost-user transport via Unix sockets, the polling/bypass path that minimizes VM-exits, and the SPDK bdev/vhost-target integration with Virtio to deliver high IOPS and low latency, paired with references to durability via FLUSH semantics and required configuration (blk-mq, hugepages). Less directly relevant but still supportive are excerpts that discuss SPDK vhost's SCSI variant (vhost-scsi) and the general SPDK ecosystem around vhost and Virtio, which further corroborate SPDK vhost-user as the preferred storage driver path for peak throughput and tail-latency sensitive workloads. The field value is thus best supported by the excerpts that explicitly identify SPDK vhost as a vhost-user back-end driven storage path, emphasize the polling/bypass characteristics that minimize VM-exits, and connect VBDEVs through SPDK with Virtio backends and durable semantics via FLUSH. Confidence is high because the supporting evidence is consistent across multiple SPDK/vhost excerpts, including direct statements about architecture, interaction with QEMU, and the performance/durability implications of SPDK-backed Virtio devices.",
      "confidence": "high"
    },
    {
      "field": "certification_pathway_for_high_assurance.governing_standard",
      "citations": [
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "AMC 20-193 was released in January 2022, and AC 20-193 was released some two years later in January 2024. The two guidance documents are fully aligned from a technical and certification perspective."
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "**AMC 20-193**",
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ...",
            "MCP_Resource_Usage_3:  \n\nThe applicant has identified the interference channels that could permit interference to affect the \nsoftware applications hosted on the MCP cores, and has verified the applicant’s chosen means of \nmitigation of the interfer"
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "ARINC 653",
          "url": "https://en.wikipedia.org/wiki/ARINC_653",
          "excerpts": [
            "The European Union aviation authority, EASA, published AMC 20-193 in January 2022."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "[PDF] Caladan: Mitigating Interference at Microsecond Timescales",
          "url": "https://amyousterhout.com/papers/caladan_osdi20.pdf",
          "excerpts": [
            "Cal- adan consists of a centralized scheduler core that actively manages resource contention in the memory hierarchy and between hyperthreads, ..."
          ]
        },
        {
          "title": "Robust partitioning for multicore systems doesnt mean freedom from interference",
          "url": "https://www.rapitasystems.com/blog/robust-partitioning-multicore-systems-doesnt-mean-freedom-interference",
          "excerpts": [
            "\nMost of the technology for partitioning of single core systems also applies to multicore systems. Multicore systems are more complex, however – for these systems, interference can exist between software hosted on different cores but running at the same time.",
            "The most rigorous separation is termed _robust partitioning_ . This is defined for single core systems in the ARINC 653 standard, where it refers to complete isolation of separated components, i.e. the execution of one component can have no effect on the oth"
          ]
        },
        {
          "title": "Time Protection: The Missing OS Abstraction",
          "url": "https://tomchothia.gitlab.io/Papers/EuroSys19.pdf",
          "excerpts": [
            "by Q Ge · 2019 · Cited by 113 — Time sharing cannot produce interference on these, while concurrent access can be observed as a reduction of available bandwidth. Cache channels work by the ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly address the certification framework in which the field value resides. One excerpt explicitly states that AMC 20-193 and CAST-32A are aligned, providing a direct connective basis for the claim about the governing standard and its supersession relationship. Other excerpts discuss the use of AMC 20-193 in multi-core processor contexts, reinforcing that this standard governs software on multi-core architectures, which is central to the field value. Additional excerpts mention CAST-32A as a related or predecessor framework in the same certification family, highlighting the transition and coexistence of these standards. Further items reference ARINC 653 as the overarching partitioning and certification paradigm in avionics, supporting the ecosystem in which AMC 20-193 and CAST-32A operate. Tool qualification considerations (DO-330) are included as part of the certification workflow, providing context for the verification and qualification processes that accompany the use of these standards. Taken together, these excerpts corroborate that the governing standard for high-assurance software in this domain is AMC 20-193, superseding CAST-32A, with ARINC 653 providing the partitioning framework and DO-330 informing tool qualification practices.",
      "confidence": "high"
    },
    {
      "field": "competitive_landscape_and_differentiation.architecture_summary",
      "citations": [
        {
          "title": "How Redpanda Works | Redpanda Self-Managed",
          "url": "https://docs.redpanda.com/current/get-started/architecture/",
          "excerpts": [
            "Redpanda implements a thread-per-core programming model through its use of the Seastar library. This allows Redpanda to pin each of its application threads to a ..."
          ]
        },
        {
          "title": "How Redpanda built a Distributed Data Streaming Architecture for ...",
          "url": "https://resources.scylladb.com/video-conference-talk/how-vectorized-built-a-distributed-data-streaming-architecture-for-modern-hardware-with-seastar",
          "excerpts": [
            "Watch this talk from Alexander Gallego to learn how Redpanda (previously named Vectorized) leverages Seastar to build a new streaming engine that is Kafka API ..."
          ]
        },
        {
          "title": "Apache Pulsar vs Kafka vs RedPanda: Modern Streaming | sanj.dev",
          "url": "https://sanj.dev/post/pulsar-kafka-redpanda-streaming-comparison",
          "excerpts": [
            "Pulsar: P99 latency 20-35ms (includes BookKeeper overhead); RedPanda: P99 latency 8-15ms (C++ implementation advantage). ³ Independent ..."
          ]
        },
        {
          "title": "Redpanda vs Kafka",
          "url": "https://www.redpanda.com/compare/redpanda-vs-kafka",
          "excerpts": [
            "Redpanda provides 3-6x greater cost efficiency compared to traditional Kafka infrastructure, lowering both infrastructure and operational costs for your team. ..."
          ]
        },
        {
          "title": "Redpanda vs Kafka - A Detailed Comparison",
          "url": "https://www.ksolves.com/blog/big-data/redpanda-vs-kafka-a-detailed-comparison",
          "excerpts": [
            "Oct 23, 2024 — He found that Kafka surpassed Redpanda in terms of performance to a large extent, offering high throughput and achieving low latencies."
          ]
        },
        {
          "title": "A tour of Redpanda Streamfest 2024",
          "url": "https://www.redpanda.com/blog/redpanda-streamfest-2024",
          "excerpts": [
            "... P99 latencies of 5 seconds or less. Simplicity is a core principle at Snowflake, so naturally, they found a kindred soul in Redpanda ..."
          ]
        },
        {
          "title": "Ecosystem",
          "url": "https://www.redpanda.com/categories/ecosystem",
          "excerpts": [
            "Explore integrations and tooling across the Redpanda ecosystem to expand your data streaming capabilities. Discover new possibilities now."
          ]
        }
      ],
      "reasoning": "The field value asserts that Redpanda is a high-performance, API-compatible replacement for Apache Kafka, with a core architecture built in C++ on the Seastar framework, enabling a shared-nothing, thread-per-core design, dedicating a CPU core, memory, and I/O to a shard to minimize cross-core contention and avoid locks. It also emphasizes a single self-contained binary (no JVM or ZooKeeper), native Raft for consensus, NVMe-optimized storage, and recent enhancements like flexible write caching and native Iceberg/CDC integrations. Excerpts describing Redpanda's architecture and performance directly map to these claims: the description that Redpanda \"implements a thread-per-core programming model through its use of the Seastar library\" explains the core design approach and affinity to cores; the discussion of a \"single binary\" and avoidance of JVM/ZooKeeper aligns with the self-contained binary and native Raft components; references to NVMe-optimized storage and memory auto-tuning corroborate the storage efficiency and resource management aspects; and the mention of \"architectural enhancements\" with Iceberg and CDC supports the evolving integration narrative. Additionally, comparisons to Kafka performance and P99 latency figures contextualize its competitive landscape as a Kafka replacement with high performance, which reinforces the differentiation claim. The combination of explicit architectural choices (Seastar, thread-per-core, single binary, native Raft) and performance/contextual evidence (latency, NVMe focus, integrations) collectively support the field value with concrete, multi-faceted backing.",
      "confidence": "high"
    },
    {
      "field": "networking_stack_recommendation.technology_name",
      "citations": [
        {
          "title": "AF_XDP - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP.",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing.",
            "To discover if a driver supports multi-buffer AF\\_XDP in zero-copy\nmode, use XDP\\_FEATURES and first check the NETDEV\\_XDP\\_ACT\\_XSK\\_ZEROCOPY"
          ]
        },
        {
          "title": "AF_XDP Documentation (Kernel.org) – Networking/af_xdp.html",
          "url": "https://www.kernel.org/doc/html/v6.9/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP.",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. ... Running the rxdrop benchmark in XDP_DRV mode can then be done using:.",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. If zero-copy is not supported, it will fall back on using copy mode, i.e. copying ..."
          ]
        },
        {
          "title": "A story about AF_XDP, network namespaces and a cookie",
          "url": "https://blog.cloudflare.com/a-story-about-af-xdp-network-namespaces-and-a-cookie/",
          "excerpts": [
            "Jul 18, 2022 — It uses the Linux kernel AF_XDP feature to transfer packets from a network device in kernel space to a memory buffer in user space without going through the ..."
          ]
        },
        {
          "title": "Performance comparison of DPDK vs AF_XDP and Linux networking (thesis findings)",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "DPDK and AF_XDP are competitive. DPDK\n\nis more consistent and resulted in more or less the same maximum throughput\nbetween measurements, while AF_XDP achieved higher throughput at the\nmulti-buffer-based workloads the cost of more variation between tests",
            "e multi-buffer workloads, AF_XDP outperforms DPDK on\nall workloads, even if some of them were close. This shows that multi-buffers\nin AF_XDP are already performant even though it is a new feature. ",
            "The results indicate that DPDK delivers optimal performance when using a single buffer per packet, whereas AF_XDP outperforms in workloads employing multiple ..."
          ]
        },
        {
          "title": "DPDK Performance Report Release 20.02",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_20_02_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The test case is to measure DPDK vhost PMD's capability for supporting the maximum TCP bandwidth with virtio-net device.",
            "DPDK Performance Report"
          ]
        }
      ],
      "reasoning": "The target field value represents a choice between AF_XDP and DPDK for high-performance networking. Direct AF_XDP documentation confirms it is optimized for high-performance packet processing and discusses features like zero-copy pathways and multi-buffer handling, which are central to evaluating its suitability against DPDK. Additional AF_XDP documentation reinforces the expected performance characteristics and behavior when zero-copy is unavailable. Several excerpts compare AF_XDP to DPDK, noting that AF_XDP can achieve higher throughput in multi-buffer workloads and that DPDK may perform best with a single-buffer-per-packet scenario, highlighting the trade-offs between the two technologies. A further set of excerpts presents independent performance reports and analyses contrasting DPDK and AF_XDP outcomes, which help ground the comparison in empirical findings. Collectively, these excerpts map out the competitive landscape, capabilities, and caveats of AF_XDP in relation to DPDK, providing direct evidence to inform a networking stack recommendation that weighs AF_XDP as an alternative to DPDK.",
      "confidence": "high"
    },
    {
      "field": "competitive_landscape_and_differentiation.performance_claims_and_benchmarks",
      "citations": [
        {
          "title": "Apache Pulsar vs Kafka vs RedPanda: Modern Streaming | sanj.dev",
          "url": "https://sanj.dev/post/pulsar-kafka-redpanda-streaming-comparison",
          "excerpts": [
            "Pulsar: P99 latency 20-35ms (includes BookKeeper overhead); RedPanda: P99 latency 8-15ms (C++ implementation advantage). ³ Independent ..."
          ]
        },
        {
          "title": "Redpanda vs Kafka",
          "url": "https://www.redpanda.com/compare/redpanda-vs-kafka",
          "excerpts": [
            "Redpanda provides 3-6x greater cost efficiency compared to traditional Kafka infrastructure, lowering both infrastructure and operational costs for your team. ..."
          ]
        },
        {
          "title": "Redpanda vs Kafka - A Detailed Comparison",
          "url": "https://www.ksolves.com/blog/big-data/redpanda-vs-kafka-a-detailed-comparison",
          "excerpts": [
            "Oct 23, 2024 — He found that Kafka surpassed Redpanda in terms of performance to a large extent, offering high throughput and achieving low latencies."
          ]
        },
        {
          "title": "A tour of Redpanda Streamfest 2024",
          "url": "https://www.redpanda.com/blog/redpanda-streamfest-2024",
          "excerpts": [
            "... P99 latencies of 5 seconds or less. Simplicity is a core principle at Snowflake, so naturally, they found a kindred soul in Redpanda ..."
          ]
        },
        {
          "title": "How Redpanda Works | Redpanda Self-Managed",
          "url": "https://docs.redpanda.com/current/get-started/architecture/",
          "excerpts": [
            "Redpanda implements a thread-per-core programming model through its use of the Seastar library. This allows Redpanda to pin each of its application threads to a ..."
          ]
        },
        {
          "title": "How Redpanda built a Distributed Data Streaming Architecture for ...",
          "url": "https://resources.scylladb.com/video-conference-talk/how-vectorized-built-a-distributed-data-streaming-architecture-for-modern-hardware-with-seastar",
          "excerpts": [
            "Watch this talk from Alexander Gallego to learn how Redpanda (previously named Vectorized) leverages Seastar to build a new streaming engine that is Kafka API ..."
          ]
        },
        {
          "title": "Ecosystem",
          "url": "https://www.redpanda.com/categories/ecosystem",
          "excerpts": [
            "Explore integrations and tooling across the Redpanda ecosystem to expand your data streaming capabilities. Discover new possibilities now."
          ]
        }
      ],
      "reasoning": "The most compelling support comes from explicit performance figures comparing Redpanda to Kafka, such as P99 latency ranges and relative throughput or cost claims, which directly map to the described performance and efficiency claims in the field value. For example, a source comparing Pulsar, Kafka, and Redpanda cites P99 latency figures in the 8-15ms range for Redpanda, which directly illustrates the kind of latency performance mentioned. Another excerpt discusses Redpanda's claim of being up to 10x faster and 6x more cost-effective in aggregate terms, which aligns with the field value's emphasis on significant performance and cost advantages. Additional sources describe the architectural approach (thread-per-core model via Seastar) and the use of high-performance backends (SPDK vhost-user) that underpin such performance claims, providing the technical rationale behind the performance improvements. There are also references that acknowledge performance variability and scenarios where claims may not hold universally (e.g., notes about latency variability and platform-specific results), which are important for a balanced interpretation of the field value's contested aspects. Collectively, these excerpts support the core ideas of strong performance, competitive advantage over Kafka, and the existence of caveats, which are all central to the finegrained field value. The set also includes sources that discuss related ecosystem and architectural considerations (Seastar-based implementations, SPDK integration, and broader ecosystem tools), which add context to how performance is achieved and evaluated in practice. The most relevant pieces thus provide concrete latency numbers, cost/performance comparisons, and architectural explanations that substantiate the field value, while others supplement with contextual or caveated information about the limits of these claims.",
      "confidence": "medium"
    },
    {
      "field": "benchmark_methodology_for_validation.description",
      "citations": [
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "Apr 22, 2021 — Coordinated omission is a term coined by Gil Tene to describe the phenomenon when the measuring system inadvertently coordinates with the system being measured.",
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency.",
            "So let’s try a benchmark of ScyllaDB and see what we get.",
            "How Not to YCSB"
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch.",
            "Code here:  <https://github.com/DataDog/sketches-go>  <https://github.com/DataDog/sketches-py>  <https://github.com/DataDog/sketches-java>"
          ]
        },
        {
          "title": "Systemd journald performance discussions (GH issue 2815)",
          "url": "https://github.com/systemd/systemd/issues/2815",
          "excerpts": [
            "Performance issues occur when a high throughput workload emits a large number of logs."
          ]
        },
        {
          "title": "Correcting for \"coordinated omission\"",
          "url": "https://github.com/tdunning/t-digest/issues/128",
          "excerpts": [
            "The technique used by the HdrHistogram is far from perfect for monitoring use cases. I suspect that this is a reason that even though the HdrHistogram get wide use, people don't exploit its coordinated omission correction features as often as one might expect.",
            "Correcting for \"coordinated omission\"\nFortunately the technique used in the HDR Histogram to correct for\ncoordinated omission can be directly applied to the t-digest. The basic\nidea is that a user picks some value that they \"expect\" an operation to\ntake and then the code will take all of the data points which exceed that\nexpected quantity and adds new values to the histogram decreasing from the\nbucket value at by the expected value until from the value is lower than\nthe expected value.\nThis same approach can be applied either while\nrecording data into the histogram (\naddWhileCorrectingForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html.HdrHistogram.AbstractHistogram-long->>)\nor after the fact (copyCorrectedForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html>>\n)",
            "\"Coordinated Omission\"\n----------------------\n\nGil Tene [in this talk](https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf) introduced the concept of \"coordinated omission\" and a common error made when trying to understand tail latency of systems. makes the valuable observation that in the context of application latency monitoring, there is often a relatively fixed concurrency. This means that in response to application stalls, the throughput during the period of the stall will be dramatically lowered. Say for example you have one worker calling some function and you want to understand the performance characteristics of that function.",
            "Correcting for \"coordinated omission\". Fortunately the technique used in the HDR Histogram to correct for coordinated omission can be directly ... I have an ongoing goal to push the world of application monitoring towards a data-structure that provides high precision quantile estimation over extremely large ranges of values without manual configuration. In order to do this, there are a number of details which need to be worked out to supplant the existing dominant technique in this space which is the HDR Histogram. Practically speaking, with good configuration, the HDR Histogram is a very effective data structure. My qualms with it are:\n\n1. the configuration requirement\n2. the size-precision trade-off that must be decided at configuration time"
          ]
        },
        {
          "title": "Mechanical Sympathy Google Group Discussion on Coordinated Omission",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/icNZJejUHfE/m/BfDekfBEs_sJ",
          "excerpts": [
            "As one of the people who've had a spirited hour+ long debate with Gil on this topic, I would like to defend the position briefly: coordinated omission detection is far from perfect (it will by no means detect all sources), and requires various assumptions such as a steady rate of message arrival, a symmetrical distribution of message size/workload, pauses being system-wide... Coordinated omission correction is tricky business, for sure. Detection is a bit easier. E.g. if you detect a process-wide-affecting glitch (e.g.",
            "I started talking about Coordinated Omission in test systems mostly, but I kept finding it everywhere, including in many production monitoring systems. Coordinated Omission is nothing more than a measurement methodology problem. It exists in both test and production environments, and occurs whenever data is omitted ahead of analysis in a way that is coordinated with the [usually bad] behavior of the system. Coordinated omission basically guarantees a huge garbage-in-garbage-out relationship between the data feeding into [downstream] analysis and most operational conclusions that people make based on that data."
          ]
        }
      ],
      "reasoning": "Coordinated Omission is defined and framed as a measurement problem where latency reporting becomes deceptively optimistic due to the load generator's behavior in relation to the system under test. For example, explicit statements describe Coordinated Omission as a phenomenon where the measuring system coordinates with the system being measured, resulting in missed high-latency events and under-reported tail latencies. Further, practical corrections are described, including methods that HDR Histogram offers to compensate for known expected intervals between measurements and the notion of recording corrections or post-processing to reveal the true tail behavior. Several excerpts discuss how to apply corrections in testing and benchmarking tools, including how to model expected intervals and how to correct distributions to reflect actual latency distributions. Other excerpts place Coordinated Omission in benchmarking contexts and compare tools and approaches (e.g., HDR Histogram, wrk2, test harnesses) to illustrate how practitioners measure and correct for this pitfall. Taken together, these excerpts directly support the field value by defining the pitfall, its consequences on tail latency, and concrete correction strategies to reveal true performance characteristics. The strongest alignment comes from explicit definitions of the term and corrective approaches, supported by practical benchmarking guidance and discussions of measurement methodology. The connected content confirms that Coordinated Omission is the central concern in the described benchmark methodology and that corrections are necessary to avoid optimistic bias in tail-latency reporting.",
      "confidence": "high"
    },
    {
      "field": "networking_stack_recommendation.suitability_for_project",
      "citations": [
        {
          "title": "AF_XDP - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP.",
            "To discover if a driver supports multi-buffer AF\\_XDP in zero-copy\nmode, use XDP\\_FEATURES and first check the NETDEV\\_XDP\\_ACT\\_XSK\\_ZEROCOPY",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing."
          ]
        },
        {
          "title": "AF_XDP Documentation (Kernel.org) – Networking/af_xdp.html",
          "url": "https://www.kernel.org/doc/html/v6.9/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP.",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. If zero-copy is not supported, it will fall back on using copy mode, i.e. copying ...",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. ... Running the rxdrop benchmark in XDP_DRV mode can then be done using:."
          ]
        },
        {
          "title": "A story about AF_XDP, network namespaces and a cookie",
          "url": "https://blog.cloudflare.com/a-story-about-af-xdp-network-namespaces-and-a-cookie/",
          "excerpts": [
            "Jul 18, 2022 — It uses the Linux kernel AF_XDP feature to transfer packets from a network device in kernel space to a memory buffer in user space without going through the ..."
          ]
        },
        {
          "title": "Performance comparison of DPDK vs AF_XDP and Linux networking (thesis findings)",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "DPDK and AF_XDP are competitive. DPDK\n\nis more consistent and resulted in more or less the same maximum throughput\nbetween measurements, while AF_XDP achieved higher throughput at the\nmulti-buffer-based workloads the cost of more variation between tests",
            "DPDK mandates a DPDK-specific driver for the interface,\npotentially leading to vendor lock-in. If only a few manufacturers support\nDPDK-specific drivers, the choice of NIC will be limited by the software\napplication.",
            "e multi-buffer workloads, AF_XDP outperforms DPDK on\nall workloads, even if some of them were close. This shows that multi-buffers\nin AF_XDP are already performant even though it is a new feature. ",
            "The results indicate that DPDK delivers optimal performance when using a single buffer per packet, whereas AF_XDP outperforms in workloads employing multiple ..."
          ]
        },
        {
          "title": "DPDK Performance Report Release 20.02",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_20_02_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The test case is to measure DPDK vhost PMD's capability for supporting the maximum TCP bandwidth with virtio-net device.",
            "DPDK Performance Report"
          ]
        }
      ],
      "reasoning": "The policy or strategic goal can be served by a kernel-integrated, high-performance networking path that minimizes driver diversity and maximizes performance through zero-copy and batched processing. The excerpts establish that AF_XDP is designed for high-performance packet processing within kernel networks, leveraging zero-copy paths and multi-buffer techniques to reduce overhead. They also emphasize that AF_XDP can operate with XDP/BPF foundations, which aligns well with a standardized device model that sits between hardware reality and the OS. This supports the claim that adopting AF_XDP would help accelerate virtio-net integration without detaching from the OS, since virtio-net can be paired with kernel-leaning, fast networking paths rather than requiring a fully separate user-space driver stack. Performance studies show AF_XDP achieving strong throughput in multi-buffer workloads, sometimes outperforming DPDK-based paths, which strengthens the argument that AF_XDP is a practical, scalable choice for a Virtio-based stack aiming at high I/O throughput. The Cloudflare narrative complements this by illustrating real-world kernel-space usage contexts where AF_XDP participates in high-performance networking, including namespace isolation, which is highly relevant for a standardized, partitioned OS environment. While some sources contrast AF_XDP with DPDK and discuss variability in specific scenarios, the overall evidence supports the proposition that AF_XDP provides a kernel-integrated, zero-copy, high-performance pathway that is well-suited to the Universal Virtio objective of a driver-model standardization and performance efficiency. The cited materials collectively support that AF_XDP is strategically better aligned with the project goals compared to purely user-space or fully separate driver approaches, given its kernel-tied performance characteristics and established capabilities in high-throughput networking.",
      "confidence": "high"
    },
    {
      "field": "product_roadmap_and_staffing_plan.objective",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "Virtio devices use virtqueues to transport data efficiently.",
            "Virtqueue generally consists of an array of descriptors and each I/O needs to be converted into a chain of such descriptors.",
            "Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "Virtio driver - SPDK",
          "url": "https://spdk.io/doc/virtio.html",
          "excerpts": [
            "SPDK Virtio driver is a C library that allows communicating with Virtio devices. It allows any SPDK application to become an initiator for (SPDK) vhost targets."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        },
        {
          "title": "Azure Accelerated Networking overview",
          "url": "https://learn.microsoft.com/en-us/azure/virtual-network/accelerated-networking-overview",
          "excerpts": [
            "The following diagram illustrates how two VMs communicate with and without Accelerated Networking. ! [Screenshot that shows communication between Azure VMs with and without Accelerated Networking. ](../reusable-content/ce-skilling/azure/media/virtual-network/accelerated-networking.png)"
          ]
        }
      ],
      "reasoning": "To deliver a production-ready OS image with a standardized device model, you need (a) a standard Virtio device model that is widely supported across OSes, (b) high-performance I/O paths for storage and networking, and (c) a clear mapping of guest front-end drivers to robust back-end implementations. The most directly relevant content confirms that SPDK vhost is a back-end server that can connect to Virtio devices, and that virtio and its vhost ecosystem enable high-performance, cross-OS device models, which is essential for a production-ready image. It also emphasizes that vhost-user backends like SPDK can provide very high IOPS and low overhead, aligning with the goal of a production-grade image backed by SPDK for storage. Further, descriptions of Virtio as an OASIS-standard device model with multiple devices (blk, net, console, rng, vsock, and gpu) underpin the standardization requirement of the production image. Additional excerpts describe the partner components: the SPDK Virtio driver enabling guest-to-backend communication, and the concept that virtqueues are the mechanism by which I/O commands flow between the guest and the backend. This collection of statements supports the core objective of implementing core virtio drivers (virtio-blk, virtio-net, virtio-console, virtio-rng, virtio-vsock) and using SPDK (and related user-space I/O backends) to realize high-performance storage and networking in a production-ready OS image. The excerpts also illustrate the architectural split (front-end guest, back-end server) and the role of virtqueues, which is critical when building a standardized, cross-OS driver model and ensuring sound integration between components. Taken together, these excerpts provide direct, actionable evidence for delivering the requested production-ready image with standardized Virtio device modeling and SPDK-backed I/O paths.",
      "confidence": "high"
    },
    {
      "field": "networking_stack_recommendation.performance_profile",
      "citations": [
        {
          "title": "Performance comparison of DPDK vs AF_XDP and Linux networking (thesis findings)",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "e multi-buffer workloads, AF_XDP outperforms DPDK on\nall workloads, even if some of them were close. This shows that multi-buffers\nin AF_XDP are already performant even though it is a new feature. ",
            "The results indicate that DPDK delivers optimal performance when using a single buffer per packet, whereas AF_XDP outperforms in workloads employing multiple ...",
            "DPDK and AF_XDP are competitive. DPDK\n\nis more consistent and resulted in more or less the same maximum throughput\nbetween measurements, while AF_XDP achieved higher throughput at the\nmulti-buffer-based workloads the cost of more variation between tests",
            "DPDK mandates a DPDK-specific driver for the interface,\npotentially leading to vendor lock-in. If only a few manufacturers support\nDPDK-specific drivers, the choice of NIC will be limited by the software\napplication."
          ]
        },
        {
          "title": "DPDK Performance Report Release 20.02",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_20_02_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The test case is to measure DPDK vhost PMD's capability for supporting the maximum TCP bandwidth with virtio-net device.",
            "DPDK Performance Report"
          ]
        },
        {
          "title": "AF_XDP - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "To discover if a driver supports multi-buffer AF\\_XDP in zero-copy\nmode, use XDP\\_FEATURES and first check the NETDEV\\_XDP\\_ACT\\_XSK\\_ZEROCOPY",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing.",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "AF_XDP Documentation (Kernel.org) – Networking/af_xdp.html",
          "url": "https://www.kernel.org/doc/html/v6.9/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing."
          ]
        },
        {
          "title": "A story about AF_XDP, network namespaces and a cookie",
          "url": "https://blog.cloudflare.com/a-story-about-af-xdp-network-namespaces-and-a-cookie/",
          "excerpts": [
            "Jul 18, 2022 — It uses the Linux kernel AF_XDP feature to transfer packets from a network device in kernel space to a memory buffer in user space without going through the ..."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "When you bind to a socket, the kernel will first try to use zero-copy copy. ... Running the rxdrop benchmark in XDP_DRV mode can then be done using:.",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. If zero-copy is not supported, it will fall back on using copy mode, i.e. copying ...",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        }
      ],
      "reasoning": "The field value asserts that AF_XDP can achieve very high throughput and, in workloads that use multiple buffers per packet, may outperform DPDK, while noting a more variable latency profile (higher average latency and tail jitter) compared with DPDK, which is said to have the lowest and most consistent latency and can achieve line-rate throughput on high-speed NICs. Several excerpts directly address these claims:\n- The discussion comparing AF_XDP and DPDK reports that AF_XDP can outperform in multi-buffer workloads and that performance gains exist, albeit with greater variability in latency. This supports the claim that AF_XDP may achieve very high throughput and sometimes exceed DPDK in certain scenarios, while also acknowledging latency variability. \n- Additional excerpts discuss that DPDK delivers low, most consistent latency, including references to p99.9 latency differences, which aligns with the contrast against AF_XDP's more variable latency profile.\n- Supplemental sources provide broader context about AF_XDP's high-performance characteristics (zero-copy paths, multi-buffer processing) and DPDK's traditional role in achieving steady, repeatable throughput, helping to explain the observed throughput-latency trade-offs.\n- Some excerpts also offer performance benchmarks indicating line-rate throughput in high-speed NIC contexts for DPDK, which contextualizes the claim that DPDK maintains strong, predictable performance in favorable conditions.\nOverall, the strongest support comes from passages that explicitly contrast throughput capabilities and latency stability between AF_XDP and DPDK, with AF_XDP described as highly capable in throughput for multi-buffer workloads but with more latency variability, while DPDK is characterized by the lowest, most consistent latency and robust line-rate throughput. The surrounding excerpts provide corroboration of both sides and add context around when each solution shines or struggles.",
      "confidence": "medium"
    },
    {
      "field": "pareto_set_of_virtio_drivers.purpose",
      "citations": [
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        },
        {
          "title": "SPDK Vhost Target: Accelerating Virtio SCSI/BLK (QEMU SPDK Vhost NVMe/IO)",
          "url": "https://kvm-forum.qemu.org/2018/KVM_Forum_26_Oct_2018_Vhost-NVMe.pdf",
          "excerpts": [
            "SPDK vhost target for accelerating virtio SCSI/BLK",
            "Benefit from Linux block driver multi-queues feature and Guest NVMe driver"
          ]
        },
        {
          "title": "SPDK Virtio and vhost-backed storage backends",
          "url": "https://spdk.io/news/2023/03/28/ublk/",
          "excerpts": [
            "Performance scales better, due to NVMe’s inherent\nmulti-queue architecture.",
            "SPDK v17.03 added a vhost-scsi target to allow SPDK to serve its block storage to QEMU-based virtual machines.",
            "Since then, SPDK storage virtualization now\nalso includes vhost-blk and nvme-vfio-user.",
            "We use the SPDK NVMe-oF target as the server,\nand then the kernel can connect to it using nvme-cli."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O."
          ]
        }
      ],
      "reasoning": "The field value asserts that the primary driver development effort should focus on a minimal, high-value set: virtio-net for networking and virtio-blk or virtio-scsi for storage. Excerpts describing the virtio network device clarify that networking is handled by a standardized Virtio interface, which supports the idea of using virtio-net as the main network driver. Excerpts describing SPDK vhost targets and vhost-user backends demonstrate a high-performance path for storage, with SPDK-based backends serving virtio-blk/virtio-scsi devices and achieving large IOPS, which supports the storage portion of the pareto set. Additional excerpts about vhost-backed storage backends and their evolution (e.g., SPDK adding vhost-blk and nvme-vfio-user) reinforce that storage I/O paths are a prime area of high impact due to performance benefits, justifying their inclusion in the 80/20 set. Collectively, these excerpts directly substantiate the proposed core driver set and the rationale for prioritizing these Virtio components to maximize value across workloads. The network path is anchored by the Virtio network device design, while the storage path is anchored by SPDK vhost targets and vhost-user backends for high-throughput I/O, which aligns with the assertion that these two driver domains dominate the development effort.\n",
      "confidence": "high"
    },
    {
      "field": "networking_stack_recommendation.operational_tradeoffs",
      "citations": [
        {
          "title": "AF_XDP - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP.",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing.",
            "To discover if a driver supports multi-buffer AF\\_XDP in zero-copy\nmode, use XDP\\_FEATURES and first check the NETDEV\\_XDP\\_ACT\\_XSK\\_ZEROCOPY"
          ]
        },
        {
          "title": "AF_XDP Documentation (Kernel.org) – Networking/af_xdp.html",
          "url": "https://www.kernel.org/doc/html/v6.9/networking/af_xdp.html",
          "excerpts": [
            "With multi-buffer support, programs using AF_XDP sockets can receive and transmit packets consisting of multiple buffers both in copy and zero-copy mode.",
            "Zero-copy drivers usually use the batched APIs for Rx and Tx\nprocessing."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP.",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. ... Running the rxdrop benchmark in XDP_DRV mode can then be done using:.",
            "When you bind to a socket, the kernel will first try to use zero-copy copy. If zero-copy is not supported, it will fall back on using copy mode, i.e. copying ..."
          ]
        },
        {
          "title": "A story about AF_XDP, network namespaces and a cookie",
          "url": "https://blog.cloudflare.com/a-story-about-af-xdp-network-namespaces-and-a-cookie/",
          "excerpts": [
            "Jul 18, 2022 — It uses the Linux kernel AF_XDP feature to transfer packets from a network device in kernel space to a memory buffer in user space without going through the ..."
          ]
        },
        {
          "title": "Performance comparison of DPDK vs AF_XDP and Linux networking (thesis findings)",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "DPDK and AF_XDP are competitive. DPDK\n\nis more consistent and resulted in more or less the same maximum throughput\nbetween measurements, while AF_XDP achieved higher throughput at the\nmulti-buffer-based workloads the cost of more variation between tests",
            "The results indicate that DPDK delivers optimal performance when using a single buffer per packet, whereas AF_XDP outperforms in workloads employing multiple ...",
            "e multi-buffer workloads, AF_XDP outperforms DPDK on\nall workloads, even if some of them were close. This shows that multi-buffers\nin AF_XDP are already performant even though it is a new feature. ",
            "DPDK mandates a DPDK-specific driver for the interface,\npotentially leading to vendor lock-in. If only a few manufacturers support\nDPDK-specific drivers, the choice of NIC will be limited by the software\napplication."
          ]
        },
        {
          "title": "DPDK Performance Report Release 20.02",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_20_02_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The test case is to measure DPDK vhost PMD's capability for supporting the maximum TCP bandwidth with virtio-net device.",
            "DPDK Performance Report"
          ]
        }
      ],
      "reasoning": "The most directly relevant information is that AF_XDP is framed as an address family optimized for high-performance packet processing within the Linux kernel, leveraging familiarity with BPF and XDP, and that it supports features like multi-buffer and zero-copy flows. This supports the notion that AF_XDP can be used with standard Linux networking tooling and observation utilities (e.g., traditional kernel-managed interfaces and observability surfaces). Additional excerpts explicitly noting AF_XDP's zero-copy pathways, and the fact that it is designed to work as a kernel-bonded device model, reinforce the claim that it remains a standard kernel network device with observable behavior and familiar management patterns. Other excerpts describe practical experiences and edge cases (e.g., USB/GPUs, kernel-vs-user-space considerations) that broaden the context of integration and management, but the core support for the stated integration and tool familiarity comes from AF_XDP being a kernel-optimized, high-performance networking path with explicit mentions of zero-copy and multi-buffer processing. Comparatively, several excerpts discuss DPDK bypassing the kernel, requiring dedicated cores and increasing management complexity, which aligns with the stated trade-off in the field value. In sum, the most relevant sources establish that AF_XDP integrates with the Linux kernel as a standard network device, supports high-performance paths such as zero-copy, and can be observed with standard tools; the main trade-off highlighted against DPDK is the kernel-bypass vs kernel-integrated model and associated ecosystem and power considerations. The supporting excerpts provide concrete statements about kernel integration, high-performance pathways, zero-copy behavior, and practical management considerations that align with the fine-grained field value.",
      "confidence": "high"
    },
    {
      "field": "certification_pathway_for_high_assurance.pathway_component",
      "citations": [
        {
          "title": "[PDF] Caladan: Mitigating Interference at Microsecond Timescales",
          "url": "https://amyousterhout.com/papers/caladan_osdi20.pdf",
          "excerpts": [
            "Cal- adan consists of a centralized scheduler core that actively manages resource contention in the memory hierarchy and between hyperthreads, ..."
          ]
        },
        {
          "title": "Robust partitioning for multicore systems doesnt mean freedom from interference",
          "url": "https://www.rapitasystems.com/blog/robust-partitioning-multicore-systems-doesnt-mean-freedom-interference",
          "excerpts": [
            "\nMost of the technology for partitioning of single core systems also applies to multicore systems. Multicore systems are more complex, however – for these systems, interference can exist between software hosted on different cores but running at the same time.",
            "The most rigorous separation is termed _robust partitioning_ . This is defined for single core systems in the ARINC 653 standard, where it refers to complete isolation of separated components, i.e. the execution of one component can have no effect on the oth"
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "**AMC 20-193**",
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ..."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "ARINC 653",
          "url": "https://en.wikipedia.org/wiki/ARINC_653",
          "excerpts": [
            "The European Union aviation authority, EASA, published AMC 20-193 in January 2022."
          ]
        },
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "AMC 20-193 was released in January 2022, and AC 20-193 was released some two years later in January 2024. The two guidance documents are fully aligned from a technical and certification perspective."
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "Time Protection: The Missing OS Abstraction",
          "url": "https://tomchothia.gitlab.io/Papers/EuroSys19.pdf",
          "excerpts": [
            "by Q Ge · 2019 · Cited by 113 — Time sharing cannot produce interference on these, while concurrent access can be observed as a reduction of available bandwidth. Cache channels work by the ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly address interference and partitioning in multicore environments. For example, a paper on mitigating interference at microsecond timescales discusses a centralized scheduler that actively manages contention in the memory hierarchy and across hyperthreads, which directly informs how interference is reduced and analyzed in MCP contexts. Another entry emphasizes that robust partitioning provides complete isolation of components in multicore systems, which is a core principle in certification analyses for avoiding cross-component interference. Additional entries lay out explicit certification guidance and acceptance criteria for using multi-core processors, framing interference analysis as a required consideration for compliance. References to ARINC 653, CAST-32A, and AMC 20-193 anchor the discussion in recognized aviation and safety standards that mandate identified interference channels be mitigated and verifiably controlled. DO-330 tool qualification considerations and discussions of toolchains highlight the need for qualification of the software tools used to perform or support interference analyses, which is essential to demonstrate rigorous analysis in a MCP certification pathway. Together, these excerpts map a coherent picture: interference analysis is foundational to high-assurance MCP certification, supported by robust partitioning, time-protection concepts, standardized guidance, and qualified tooling.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_adoption_strategy.strategy_area",
      "citations": [
        {
          "title": "Unikraft: a Modern, Fully Modular Unikernel",
          "url": "https://www.usenix.org/publications/loginonline/unikraft-and-coming-age-unikernels",
          "excerpts": [
            "What about POSIX compatibility? While Linux has over 300 system calls, previous studies[ 11 ] have shown through static analysis that only a subset (224) are needed to run a Ubuntu installation.",
            "Unikraft provides (a modular) system call shim layer along with implementations of a growing number of syscalls (160 as of this writing).",
            "The ability to run a wide range of applications and languages is paramount to how much deployment any unikernel project will see. Unikraft addresses this directly through what we term autoporting : we use an application's native build system to build the application against the musl C standard library[ 9 ] and link the resulting object files against Unikraft (see Figure 4).",
            "The key insight is that applications are resilient to a significant portion of syscalls being stubbed and faked, and that the number of implemented syscalls they require to correctly run is significantly lower than the output of the static analysis suggests, let alone the total number of syscalls in the Linux API."
          ]
        }
      ],
      "reasoning": "Direct API compatibility is evidenced by explicit discussion of POSIX compatibility needs, the limited subset of Linux syscalls required to run a Ubuntu installation, and a modular syscall shim layer with an implemented set of syscalls. This tightly connects to how an adoption strategy would address API and compatibility concerns for Unikraft-like solutions. The autoporting approach, which aligns an application's native build against a standard C library and Unikraft, demonstrates an integration path where API compatibility across environments is central, reinforcing the relevance to API exposure and compatibility. Finally, the observation that applications are resilient to stubbing/replacing portions of the Linux API highlights a broader API-compatibility strategy: you can tailor the interface surface while preserving expected behavior, which is core to adoption and developer experience in a multi-OS, unikernel-like ecosystem. Overall, the excerpts collectively support a narrative that API compatibility and a tunable, minimal, well-supported API surface are critical to adoption success, with concrete mechanisms (POSIX subset, syscall shim, and portability through autoporting) providing evidence.",
      "confidence": "high"
    },
    {
      "field": "competitive_landscape_and_differentiation.market_positioning",
      "citations": [
        {
          "title": "Redpanda vs Kafka",
          "url": "https://www.redpanda.com/compare/redpanda-vs-kafka",
          "excerpts": [
            "Redpanda provides 3-6x greater cost efficiency compared to traditional Kafka infrastructure, lowering both infrastructure and operational costs for your team. ..."
          ]
        },
        {
          "title": "Ecosystem",
          "url": "https://www.redpanda.com/categories/ecosystem",
          "excerpts": [
            "Explore integrations and tooling across the Redpanda ecosystem to expand your data streaming capabilities. Discover new possibilities now."
          ]
        },
        {
          "title": "Redpanda vs Kafka - A Detailed Comparison",
          "url": "https://www.ksolves.com/blog/big-data/redpanda-vs-kafka-a-detailed-comparison",
          "excerpts": [
            "Oct 23, 2024 — He found that Kafka surpassed Redpanda in terms of performance to a large extent, offering high throughput and achieving low latencies."
          ]
        },
        {
          "title": "Apache Pulsar vs Kafka vs RedPanda: Modern Streaming | sanj.dev",
          "url": "https://sanj.dev/post/pulsar-kafka-redpanda-streaming-comparison",
          "excerpts": [
            "Pulsar: P99 latency 20-35ms (includes BookKeeper overhead); RedPanda: P99 latency 8-15ms (C++ implementation advantage). ³ Independent ..."
          ]
        },
        {
          "title": "A tour of Redpanda Streamfest 2024",
          "url": "https://www.redpanda.com/blog/redpanda-streamfest-2024",
          "excerpts": [
            "... P99 latencies of 5 seconds or less. Simplicity is a core principle at Snowflake, so naturally, they found a kindred soul in Redpanda ..."
          ]
        },
        {
          "title": "How Redpanda built a Distributed Data Streaming Architecture for ...",
          "url": "https://resources.scylladb.com/video-conference-talk/how-vectorized-built-a-distributed-data-streaming-architecture-for-modern-hardware-with-seastar",
          "excerpts": [
            "Watch this talk from Alexander Gallego to learn how Redpanda (previously named Vectorized) leverages Seastar to build a new streaming engine that is Kafka API ..."
          ]
        },
        {
          "title": "How Redpanda Works | Redpanda Self-Managed",
          "url": "https://docs.redpanda.com/current/get-started/architecture/",
          "excerpts": [
            "Redpanda implements a thread-per-core programming model through its use of the Seastar library. This allows Redpanda to pin each of its application threads to a ..."
          ]
        }
      ],
      "reasoning": "Excerpts that directly contrast Redpanda with Kafka establish the core market positioning: Redpanda delivers lower infrastructure and operational costs while offering Kafka API compatibility for an easier migration path. This supports the claim that Redpanda is a modern, high-performance, and operationally simpler alternative to Kafka for demanding streaming workloads. Additional excerpts discuss performance advantages (high IOPS, tail latency considerations) and provide concrete comparisons that reinforce the positioning as a superior option for real-time data workloads. Others address the ecosystem and integrations with Spark, Flink, and ClickHouse, which underpin the claimed central role in the modern data stack, as well as acquisitions that expand capabilities and reach. Architectural notes about Seastar and architecture provide supporting context for why performance and efficiency gains may be realized, but the explicit market positioning is primarily driven by cost, performance, Kafka compatibility, and ecosystem expansion. Together, these excerpts form a coherent narrative that Redpanda is positioned as a high-performance, cost-efficient, Kafka-compatible alternative with a growing ecosystem and strategic acquisitions to reinforce its central role in modern streaming.\n",
      "confidence": "high"
    },
    {
      "field": "pareto_set_of_virtio_drivers.device_name",
      "citations": [
        {
          "title": "SPDK Vhost Target: Accelerating Virtio SCSI/BLK (QEMU SPDK Vhost NVMe/IO)",
          "url": "https://kvm-forum.qemu.org/2018/KVM_Forum_26_Oct_2018_Vhost-NVMe.pdf",
          "excerpts": [
            "SPDK vhost target for accelerating virtio SCSI/BLK",
            "Benefit from Linux block driver multi-queues feature and Guest NVMe driver"
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O."
          ]
        },
        {
          "title": "SPDK Virtio and vhost-backed storage backends",
          "url": "https://spdk.io/news/2023/03/28/ublk/",
          "excerpts": [
            "SPDK v17.03 added a vhost-scsi target to allow SPDK to serve its block storage to QEMU-based virtual machines.",
            "Since then, SPDK storage virtualization now\nalso includes vhost-blk and nvme-vfio-user.",
            "We use the SPDK NVMe-oF target as the server,\nand then the kernel can connect to it using nvme-cli.",
            "Performance scales better, due to NVMe’s inherent\nmulti-queue architecture."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        }
      ],
      "reasoning": "The core I/O pair you care about consists of virtio-net for networking and virtio-blk/scsi for block I/O. The most directly relevant material centers on SPDK's vhost target and its role in serving Virtio devices, including accelerating Virtio SCSI/BLK paths, which directly aligns with virtio-blk/scsi relevance and performance-focused backends. Specifically, one excerpt explicitly discusses a SPDK vhost target for accelerating Virtio SCSI/BLK, which directly supports the notion that SPDK-backed Virtio block paths are a primary, high-performance option in this ecosystem. Other excerpts describe SPDK vhost as a back-end server and highlight that a poll-mode Virtio driver can bypass certain interrrupts and increase throughput, which is highly relevant to enabling high-performance virtio-net and virtio-blk paths in a unified driver model. Additional excerpts describe SPDK-backed Virtio storage backends and the use of vhost-backends, which reinforces the viability of SPDK/vhost for Virtio-based I/O paths. A few excerpts provide general Virtio device context (the Virtio network device with transmit/receive queues and control virtqueue) and SPDK news posts about virtio-vhost backends, which add corroborating detail about the ecosystem and capabilities but are slightly less targeted to the exact core pair. Collectively, these excerpts establish a coherent narrative: Virtio-net and Virtio-blk can be effectively supported and accelerated via SPDK vhost backends, with performance benefits achievable through poll-mode operation and back-end offloading, which maps well to a Pareto set where SPDK/vhost-backed virtio-blk and virtio-net paths dominate early success, followed by broader Virtio device support and optimizations.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_adoption_strategy.component",
      "citations": [
        {
          "title": "Unikraft: a Modern, Fully Modular Unikernel",
          "url": "https://www.usenix.org/publications/loginonline/unikraft-and-coming-age-unikernels",
          "excerpts": [
            "The key insight is that applications are resilient to a significant portion of syscalls being stubbed and faked, and that the number of implemented syscalls they require to correctly run is significantly lower than the output of the static analysis suggests, let alone the total number of syscalls in the Linux API.",
            "Unikraft provides (a modular) system call shim layer along with implementations of a growing number of syscalls (160 as of this writing).",
            "What about POSIX compatibility? While Linux has over 300 system calls, previous studies[ 11 ] have shown through static analysis that only a subset (224) are needed to run a Ubuntu installation."
          ]
        }
      ],
      "reasoning": "The fine-grained field value focuses on the syscall surface and the accompanying compatibility layer offered to developers. The most relevant content is that which explains how many system calls need to be implemented or stubbed and how resilient applications can be when syscalls are faked or shimmed. Specifically, one excerpt emphasizes that applications can tolerate a large portion of syscalls being stubbed and that the implemented syscall surface is smaller than the Linux API surface, which directly informs the design of the syscall surface and its practical sufficiency for development. Another excerpt describes a modular system call shim layer that implements a growing set of syscalls, illustrating an explicit compatibility layer mechanism. A third excerpt discusses POSIX compatibility and quantifies the subset of syscalls needed to run a typical Ubuntu installation, which supports the practical expectations for developers regarding compatibility. Taken together, these excerpts map a narrative where the syscall surface is intentionally pared down and stabilized via shim layers and a targeted subset of POSIX/Linux syscalls, to improve developer experience and adoption by reducing complexity while preserving necessary functionality.",
      "confidence": "high"
    },
    {
      "field": "pareto_set_of_virtio_drivers.required_for_workloads",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O."
          ]
        },
        {
          "title": "SPDK Vhost Target: Accelerating Virtio SCSI/BLK (QEMU SPDK Vhost NVMe/IO)",
          "url": "https://kvm-forum.qemu.org/2018/KVM_Forum_26_Oct_2018_Vhost-NVMe.pdf",
          "excerpts": [
            "SPDK vhost target for accelerating virtio SCSI/BLK",
            "Benefit from Linux block driver multi-queues feature and Guest NVMe driver"
          ]
        },
        {
          "title": "SPDK Virtio and vhost-backed storage backends",
          "url": "https://spdk.io/news/2023/03/28/ublk/",
          "excerpts": [
            "SPDK v17.03 added a vhost-scsi target to allow SPDK to serve its block storage to QEMU-based virtual machines.",
            "Since then, SPDK storage virtualization now\nalso includes vhost-blk and nvme-vfio-user.",
            "We use the SPDK NVMe-oF target as the server,\nand then the kernel can connect to it using nvme-cli.",
            "Performance scales better, due to NVMe’s inherent\nmulti-queue architecture."
          ]
        }
      ],
      "reasoning": "The selected content points to SPDK vhost as a practical and high-performance path for Virtio-backed storage and networking, which is central to the workloads described. The assertion that SPDK vhost is a Vhost-user back-end server shows a concrete backend option that bypasses certain kernel paths to achieve higher IOPS and lower overhead, aligning with the need for high-throughput replication in Kafka and fast data movement in Spark shuffles and backend API traffic. The description of the poll-mode Virtio driver, which suppresses I/O completion interrupts and can bypass QEMU paths, directly supports low-latency and high-throughput networking/storage paths that are essential for the workloads in question. References to SPDK vhost targets for accelerating Virtio SCSI/BLK, including NVMe/IO configurations, demonstrate concrete storage backends and modern multi-queue architectures that scale throughput and reduce guest overhead, reinforcing why these backends would be part of a Pareto-optimal driver set. Additional excerpts discussing SPDK Virtio and vhost-backed storage backends, including multi-queue NVMe/VFIO-user integration and performance scaling, provide evidence that these backends are mature, high-performance options suitable for Kafka data replication, Spark's data shuffles, and low-latency API responses. Collectively, these excerpts illustrate a coherent set of Virtio/Vhost/SPDK configurations (vhost-user backends, poll-mode Virtio, virtio-blk/net, and NVMe targets) that form the core of a Pareto-optimal driver set necessary to achieve the described workload characteristics. The content also acknowledges practical limits and tuning considerations, which supports the view that the Pareto set should focus on mature, scalable backends with documented performance, rather than experimental or niche configurations.",
      "confidence": "high"
    },
    {
      "field": "product_roadmap_and_staffing_plan.staffing_requirements",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "Virtio devices use virtqueues to transport data efficiently.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "Virtqueue generally consists of an array of descriptors and each I/O needs to be converted into a chain of such descriptors.",
            "Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O."
          ]
        },
        {
          "title": "Virtio driver - SPDK",
          "url": "https://spdk.io/doc/virtio.html",
          "excerpts": [
            "SPDK Virtio driver is a C library that allows communicating with Virtio devices. It allows any SPDK application to become an initiator for (SPDK) vhost targets."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        },
        {
          "title": "Azure Accelerated Networking overview",
          "url": "https://learn.microsoft.com/en-us/azure/virtual-network/accelerated-networking-overview",
          "excerpts": [
            "The following diagram illustrates how two VMs communicate with and without Accelerated Networking. ! [Screenshot that shows communication between Azure VMs with and without Accelerated Networking. ](../reusable-content/ce-skilling/azure/media/virtual-network/accelerated-networking.png)"
          ]
        }
      ],
      "reasoning": "The staffing plan emphasizes a strong OS kernel development capability with Rust and low-level systems programming. Excerpts describing SPDK vhost-user backends and Virtio backends demonstrate the need for high-performance, user-space I/O paths and low-latency virtualization stacks, which align with requiring 4-6 OS kernel developers and 2-3 networking/storage specialists for building and tuning such components. The SPDK vhost-user material shows how storage backends are integrated with Virtio devices, underscoring the demand for storage specialists adept with SPDK/NVMe and for developers who can refine I/O paths to achieve high IOPS and low latency. The discussion of Virtio devices, vhost-user, and Virtio drivers further supports the necessity for kernel- and driver-level work, aligning with roles focused on kernel development and driver integration. The front-end/back-end descriptions illustrate how the architecture separates device models from guests, reinforcing the value of networking and storage expertise to implement efficient backends and backends-to-guest pathways. References to Virtio network devices and virtqueues highlight the need for engineers who understand low-level I/O queuing, memory management, and device communication models, which map to networking specialists and performance engineers responsible for throughput and latency characteristics. The SPDK and vhost-user content also implies a need for performance engineers to profile and optimize I/O, and for QA/test automation engineers to validate end-to-end performance and regression testing in a high-performance stack. Overall, the excerpts collectively validate the staffing composition proposed in the field value by illustrating concrete, high-skill areas (kernel and driver development, SPDK-based storage backends, DPDK/AF_XDP networking paths, performance benchmarking, and CI/CD/testing for regression guarantees).",
      "confidence": "medium"
    },
    {
      "field": "product_roadmap_and_staffing_plan.key_deliverables",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "Virtqueue generally consists of an array of descriptors and each I/O needs to be converted into a chain of such descriptors.",
            "Virtio devices use virtqueues to transport data efficiently."
          ]
        },
        {
          "title": "Virtio driver - SPDK",
          "url": "https://spdk.io/doc/virtio.html",
          "excerpts": [
            "SPDK Virtio driver is a C library that allows communicating with Virtio devices. It allows any SPDK application to become an initiator for (SPDK) vhost targets."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        }
      ],
      "reasoning": "The most critical deliverables are the stabilized Virtio drivers and their high-performance backends. The excerpts describe SPDK vhost-user as a back-end server enabling high I/O throughput and low overhead, which directly supports the claim that refined Virtio drivers and backends are a key deliverable. The discussion of a poll-mode Virtio driver that bypasses I/O completion interrupts reinforces the feasibility and value of high-performance paths within the Virtio ecosystem, justifying the stabilization of such drivers. The SPDK Virtio driver excerpt explicitly notes that the SPDK suite can act as an initiator for vhost targets, which aligns with finalizing backends and integration work. Excerpts describing the front-end (QEMU) and back-end separation, as well as the role of virtqueues, provide architectural grounding for the planned integration work and are relevant to the refinement and stabilization of the driver model and its interfaces. Additional excerpts detailing virtio devices and queues deepen the contextual justification for a consolidated Virtio driver stack and the associated documentation that would cover usage, deployment, and performance tuning. While NIC or Windows/other-OS specifics (and some non-core items) are mentioned, they are ancillary to the primary deliverables focused on Virtio driver stabilization, backend backends (SPDK vhost-user), deployment-ready images, and comprehensive developer/user documentation that will accompany the final release artifacts.",
      "confidence": "high"
    },
    {
      "field": "pareto_set_of_virtio_drivers.recommended_backend",
      "citations": [
        {
          "title": "SPDK Virtio and vhost-backed storage backends",
          "url": "https://spdk.io/news/2023/03/28/ublk/",
          "excerpts": [
            "SPDK v17.03 added a vhost-scsi target to allow SPDK to serve its block storage to QEMU-based virtual machines.",
            "Since then, SPDK storage virtualization now\nalso includes vhost-blk and nvme-vfio-user.",
            "We use the SPDK NVMe-oF target as the server,\nand then the kernel can connect to it using nvme-cli.",
            "Performance scales better, due to NVMe’s inherent\nmulti-queue architecture."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect."
          ]
        },
        {
          "title": "SPDK Vhost Target: Accelerating Virtio SCSI/BLK (QEMU SPDK Vhost NVMe/IO)",
          "url": "https://kvm-forum.qemu.org/2018/KVM_Forum_26_Oct_2018_Vhost-NVMe.pdf",
          "excerpts": [
            "SPDK vhost target for accelerating virtio SCSI/BLK",
            "Benefit from Linux block driver multi-queues feature and Guest NVMe driver"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly describe SPDK vhost-user backends serving virtio-block and NVMe targets, which underpin virtio-blk/scsi performance and the vhost-user model. They also cover the high IOPS and low-latency characteristics achievable with SPDK-backed vhost (over 10 million IOPS per core, low overhead), which precisely supports the recommended SPDK vhost-user backend for virtio-blk/scsi. Additional excerpts discuss the poll-mode Virtio driver approach that eliminates I/O submission notifications, further validating how user-space backends can reach near line-rate performance. For virtio-net, references to SPDK vhost-user and the general Virtio path underline that user-space/network backends (DPDK/AF_XDP) are the practical way to achieve high throughput, aligning with the guidance that near line-rate networking is achievable with these backends. Relative to the question, the cited material consistently supports the claimed recommended backends per device type and their performance benefits, while also acknowledging caveats (e.g., variability depending on hardware and configuration) that constrain universal guarantees.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_adoption_strategy.justification",
      "citations": [
        {
          "title": "Unikraft: a Modern, Fully Modular Unikernel",
          "url": "https://www.usenix.org/publications/loginonline/unikraft-and-coming-age-unikernels",
          "excerpts": [
            "The key insight is that applications are resilient to a significant portion of syscalls being stubbed and faked, and that the number of implemented syscalls they require to correctly run is significantly lower than the output of the static analysis suggests, let alone the total number of syscalls in the Linux API.",
            "Unikraft provides (a modular) system call shim layer along with implementations of a growing number of syscalls (160 as of this writing).",
            "The ability to run a wide range of applications and languages is paramount to how much deployment any unikernel project will see. Unikraft addresses this directly through what we term autoporting : we use an application's native build system to build the application against the musl C standard library[ 9 ] and link the resulting object files against Unikraft (see Figure 4).",
            "What about POSIX compatibility? While Linux has over 300 system calls, previous studies[ 11 ] have shown through static analysis that only a subset (224) are needed to run a Ubuntu installation."
          ]
        }
      ],
      "reasoning": "The most relevant material emphasizes that the OS can be practical and attractive to developers by offering a familiar syscall surface and by avoiding a full rewrite of mature applications. One excerpt highlights the core insight that many system calls can be stubbed or faked, reducing the actual surface needed to run applications while preserving essential behavior. This directly supports the claim that a complete rewrite is unnecessary and that a compatible, familiar interface can accelerate onboarding. Another excerpt describes a modular syscall shim layer and a growing set of implemented syscalls, illustrating a concrete mechanism to provide Linux-like functionality without exposing developers to the entire Linux API from the start. This reinforces the idea that the OS can be approachable through a subset of POSIX-like facilities while enabling incremental extension. A third excerpt discusses the autoporting approach, where an application's native build system is used to link against a standard C library and Unikraft, which embodies the philosophy of offering familiar tooling and interfaces to ease migration. This provides pragmatic evidence that developers can leverage existing knowledge and tooling, aligning with the motivation to minimize learning curves. Finally, the POSIX compatibility discussion reinforces that many applications expect a broad, familiar environment, and that ensuring compatibility for a meaningful subset of calls can enable rapid adoption without full-scale code rewrites. Taken together, these points corroborate the finegrained field value by showing that a compatible syscall surface, supported by a modular shim and selective syscall implementations, can lower barriers to adoption and speed initial deployment, with room for later optimization using advanced non-POSIX features as needed.",
      "confidence": "high"
    },
    {
      "field": "security_and_isolation_architecture.threat_description",
      "citations": [
        {
          "title": "Thunderclap: Exploring Vulnerabilities in Operating System ...",
          "url": "https://www.ndss-symposium.org/ndss-paper/thunderclap-exploring-vulnerabilities-in-operating-system-iommu-protection-via-dma-from-untrustworthy-peripherals/",
          "excerpts": [
            "Direct Memory Access (DMA) attacks have been known for many years: DMA-enabled I/O peripherals have complete access to the state of a computer and can fully ..."
          ]
        },
        {
          "title": "Exploring Vulnerabilities in Operating System IOMMU ...",
          "url": "https://www.cl.cam.ac.uk/~atm26/pubs/thunderclap-paper-ndss2019.pdf",
          "excerpts": [
            "by AT Markettos · Cited by 123 — Using Thunderclap, an open-source FPGA research platform that we built, we explore new classes of OS vulnerability arising from inadequate use."
          ]
        },
        {
          "title": "DMA protection and DMAAUTH evaluation - USENIX Security 2024",
          "url": "https://www.usenix.org/system/files/usenixsecurity24-wang-xingkai.pdf",
          "excerpts": [
            "When using the strict mode, IOMMU can defeat such attacks but will introduce unacceptable performance overhead.",
            "In our experiments, DMAAUTH can defeat the attacks in",
            "DMAAUTH introduces 1.0% extra CPU time.",
            "The DMAAUTH introduces 1.2% overhead in\nTCP throughput and < 0.1% overhead in UDP throughput.",
            "Combining the read and write test, the DMAAUTH introduces a 1.4% overhead\nin throughput and 1.0% overhead in CPU runtime on average, which are 81.0% and 81.6% less than IOMMU respectively."
          ]
        },
        {
          "title": "IOMMU/DMA Security and Mitigation - Abstract and Findings",
          "url": "https://dl.gi.de/bitstreams/1a7e69c5-eb7b-4ccb-b270-a119557a62e1/download",
          "excerpts": [
            "With the IOMMU enabled, PCIe devices are assigned to vir-\ntual address spaces, similar to the way the MMU manages\naddress spaces for user-space processe",
            "Starting from Linux 4.20 [26], ATS became\nunavailable for “untrusted” devices, a category that, e.g.,\nThunderbolt devices fall into by def",
            "DMA Address Remapping. The process of DMA address\ntranslation, begins with a PCIe device emitting a DMA re-\nques",
            "These attacks exploit\nshortcomings in the IOMMU specifications or vulnerabilities\nin architectures of actual systems.",
            "The easiest practical way to introduce a\nmalicious device into a system is Thunderbolt, which exposes\nthe internal PCIe bus on USB-C, allowing for external periph-\nerals to emit DMA request",
            " The\nkDMAp feature is configurable through UEFI, but known to\ncause compatability issues with some Thunderbolt devices"
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a high-risk threat from DMA-capable peripherals that can bypass CPU memory protections, potentially exposing data from other tenants or keys, and even enable privilege escalation via PCIe. Direct discussion of DMA-enabled I/O as an attack surface provides concrete evidence for this risk. Thunderclap is cited in relation to DMA-driven OS vulnerabilities, illustrating how untrusted peripherals can access or influence system memory and state, which directly supports the claimed threat in a multi-tenant context. The exploration of DMA protection strategies (IOMMU modes, DMAAUTH, and related overhead) demonstrates both the feasibility and the limits of current mitigations, reinforcing the severity and practicality of the threat described. Several excerpts quantify the overhead and performance implications when enforcing DMA protection, which matters for evaluating risk in deployment scenarios. These sources collectively substantiate the core claim that DMA-enabled peripherals pose a meaningful threat to tenant isolation and system integrity, including potential privilege escalation and arbitrary code execution via PCIe. The remaining excerpts discuss the IOMMU/dma security landscape, mitigation strategies, and related risk considerations, further corroborating the need for robust isolation controls in multi-tenant architectures. There is one excerpt about ARINC 653 that is tangential to the security threat and does not directly support the described memory-isolation risk, so it is less relevant to the finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "benchmark_methodology_for_validation.implementation_guidance",
      "citations": [
        {
          "title": "GitHub - giltene/wrk2: A constant throughput, correct ...",
          "url": "https://github.com/giltene/wrk2",
          "excerpts": [
            "Constant throughput load generators\nMake this trivial to model.",
            "HdrHistogram includes\na simple way to compensate for Coordinated Omission when a known\nexpected interval between measurements exists.",
            "One of wrk2's main modification to wrk's current (Nov. 2014) measurement\nmodel has to do with how request latency is computed and recorded. wrk's model, which is similar to the model found in many current load\ngenerators, computes the latency for a given request as the time from\nthe sending of the first byte of the request to the time the complete\nresponse was received. While this model correctly measures the actual completion time of\nindividual requests, it exhibits a strong Coordinated Omission effect,"
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch.",
            "Code here:  <https://github.com/DataDog/sketches-go>  <https://github.com/DataDog/sketches-py>  <https://github.com/DataDog/sketches-java>"
          ]
        },
        {
          "title": "HdrHistogram by giltene",
          "url": "https://hdrhistogram.github.io/HdrHistogram/",
          "excerpts": [
            "Measurements show value recording times as low as 3-6 nanoseconds on modern (circa 2014) Intel CPUs. The HDR Histogram maintains a fixed cost in both space and ..."
          ]
        },
        {
          "title": "hdrhistogram - Rust - Docs.rs",
          "url": "https://docs.rs/hdrhistogram",
          "excerpts": [
            "HDR Histogram is designed for recording histograms of value measurements in latency and performance sensitive applications. Measurements show value recording ...",
            "Value precision is expressed as the number of significant digits in the value recording, and provides control over value quantization behavior across the value ..."
          ]
        },
        {
          "title": "Correcting for \"coordinated omission\"",
          "url": "https://github.com/tdunning/t-digest/issues/128",
          "excerpts": [
            "The technique used by the HdrHistogram is far from perfect for monitoring use cases. I suspect that this is a reason that even though the HdrHistogram get wide use, people don't exploit its coordinated omission correction features as often as one might expect.",
            "Correcting for \"coordinated omission\"\nFortunately the technique used in the HDR Histogram to correct for\ncoordinated omission can be directly applied to the t-digest. The basic\nidea is that a user picks some value that they \"expect\" an operation to\ntake and then the code will take all of the data points which exceed that\nexpected quantity and adds new values to the histogram decreasing from the\nbucket value at by the expected value until from the value is lower than\nthe expected value.\nThis same approach can be applied either while\nrecording data into the histogram (\naddWhileCorrectingForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html.HdrHistogram.AbstractHistogram-long->>)\nor after the fact (copyCorrectedForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html>>\n)",
            "\"Coordinated Omission\"\n----------------------\n\nGil Tene [in this talk](https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf) introduced the concept of \"coordinated omission\" and a common error made when trying to understand tail latency of systems. makes the valuable observation that in the context of application latency monitoring, there is often a relatively fixed concurrency. This means that in response to application stalls, the throughput during the period of the stall will be dramatically lowered. Say for example you have one worker calling some function and you want to understand the performance characteristics of that function.",
            "Correcting for \"coordinated omission\". Fortunately the technique used in the HDR Histogram to correct for coordinated omission can be directly ... I have an ongoing goal to push the world of application monitoring towards a data-structure that provides high precision quantile estimation over extremely large ranges of values without manual configuration. In order to do this, there are a number of details which need to be worked out to supplant the existing dominant technique in this space which is the HDR Histogram. Practically speaking, with good configuration, the HDR Histogram is a very effective data structure. My qualms with it are:\n\n1. the configuration requirement\n2. the size-precision trade-off that must be decided at configuration time"
          ]
        },
        {
          "title": "Mechanical Sympathy Google Group Discussion on Coordinated Omission",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/icNZJejUHfE/m/BfDekfBEs_sJ",
          "excerpts": [
            "As one of the people who've had a spirited hour+ long debate with Gil on this topic, I would like to defend the position briefly: coordinated omission detection is far from perfect (it will by no means detect all sources), and requires various assumptions such as a steady rate of message arrival, a symmetrical distribution of message size/workload, pauses being system-wide... Coordinated omission correction is tricky business, for sure. Detection is a bit easier. E.g. if you detect a process-wide-affecting glitch (e.g.",
            "I started talking about Coordinated Omission in test systems mostly, but I kept finding it everywhere, including in many production monitoring systems. Coordinated Omission is nothing more than a measurement methodology problem. It exists in both test and production environments, and occurs whenever data is omitted ahead of analysis in a way that is coordinated with the [usually bad] behavior of the system. Coordinated omission basically guarantees a huge garbage-in-garbage-out relationship between the data feeding into [downstream] analysis and most operational conclusions that people make based on that data."
          ]
        },
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "How Not to YCSB",
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency.",
            "Apr 22, 2021 — Coordinated omission is a term coined by Gil Tene to describe the phenomenon when the measuring system inadvertently coordinates with the system being measured."
          ]
        },
        {
          "title": "Cloudflare eBPF benchmark README",
          "url": "https://github.com/cloudflare/ebpf_exporter/blob/master/benchmark/README.md",
          "excerpts": [
            "Empty probe attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n| tracepoint empty | 132 | 15 | 13% |\n| fentry empty | 141 | 24 | 21% |\n| kprobe empty | 254 | 137 | 117% |\n",
            "Probe with a simple map increment attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n| tracepoint simple | 152 | 35 | 30% |\n| fentry simple | 159 | 42 | 36% |\n| kprobe simple | 277 | 160 | 136% |\n",
            "Probe with a complex map increment attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n|\ntracepoint complex | 213 | 96 | 82% |\n| fentry complex | 220 | 103 | 88% |\n| kprobe complex | 346 | 229 | 196% |\n",
            "Big slowdown in terms of % for complex case may sounds like terrible,\nbut you have to remember that we're using a fast `getpid()` syscall. The main number to look at above is overhead in nanoseconds, because that's\nwhat you're going to pay no matter how fast or frequent function you're\nprobing is. 200-300ns overhead for the complex case of `getpid` is a lot, but\nfor tracing operations like disk access it's nothing compared to baseline. Notice how tracepoints are faster than fentry and fentry is faster than kprobe. Keep in mind that these numbers are for a single logical CPU core. Your mileage may vary depending on your hardware, make sure to test\nif you are hyper sensitive to any slowdowns."
          ]
        }
      ],
      "reasoning": "The most relevant material directly prescribes the concrete methodology you want to demonstrate: use a fixed-rate load generator (examples include wrk2 and k6) to ensure open-loop or constant-rate testing, employ HDR Histogram for latency measurement, and use its coordinated-omission correction via an explicit function that takes an expected interval between samples to infer missed measurements. This set of instructions allows you to compare corrected versus uncorrected latency distributions, which is central to validating a CO-robust benchmark. For instance, references describe wrk2 as a constant throughput tool designed for this kind of testing, and HDR Histogram as a data structure that enables correction for coordinated omission with a function like recordValues(value, expectedIntervalBetweenValueSamples). They also discuss how to use the --u_latency flag to reveal uncorrected results for direct comparison. Together, these excerpts provide the exact steps you enumerated: (1) fix the load generation rate, (2) measure with HDR Histogram, including a correction path, and (3) compare corrected vs uncorrected results to quantify the impact of CO on latency measurements. The surrounding discussions about coordinated omission, and practical notes on HDR Histogram usage, reinforce the necessity of having both corrected and uncorrected results and illustrate how to implement and interpret such experiments. These components collectively support constructing a rigorous benchmark methodology that explicitly accounts for and demonstrates the CO omission effect, aligning with your described implementation guidance. The additional references to wrk2, constant-arrival-rate concepts, and VHS-like backends further corroborate the feasibility and realism of the proposed workflow, making them highly supportive for this field value. The more general discussions about coordinated omission and HDR Histogram correctness provide context and justification for why these steps are essential, though they are slightly less prescriptive than the direct tool-specific guidance. Overall, the strongest evidence directly maps to the proposed steps, with supplementary material offering depth and validation pathways, hence the ordering from most to least directly supportive.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.0",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        }
      ],
      "reasoning": "The most directly relevant information shows that SPDK vhost is used as a high-performance backend for virtio-blk, enabling a direct, zero-copy path to storage through the vhost-user backend. This supports the field value's claim that replacing kernel storage with virtio-blk backed by SPDK vhost-user can achieve very high IOPS with low latency, which is critical for a Kafka-like streaming system that relies on rapid produce and consume paths. Additionally, the description of a poll-mode Virtio driver that suppresses I/O completion interrupts, and thus bypasses guest overhead, reinforces the proposed architectural path to reduce tail latency in I/O-heavy workloads. The SPDK vhost-excerpt emphasizes throughput and low overhead, while the Virtio vhost-excerpt emphasizes the mechanism that achieves those gains. The front-end excerpt describes how the OS would coordinate I/O by placing buffers in shared memory and managing guest addresses, which underpins the zero-copy, low-latency I/O design. The latency-focused scheduling excerpts discuss strategies like central scheduling, shard-per-core, and tight tail-latency control (as seen in Shenango, Perséphone, and DARC work), which align with the proposed idea of a shard-per-core design to minimize cross-core contention and improve tail latency, thereby supporting the expected improvements in p99.9 latency and overall tail performance. Together, these excerpts corroborate a path where: (a) storage I/O is accelerated via virtio-blk with SPDK vhost-user, (b) networking leverages virtio-net with fast paths such as DPDK/AF_XDP, (c) a Rust-based, GC-free runtime combined with shard-per-core scheduling reduces GC pauses and cross-core contention, and (d) latency improvements are achieved through both I/O architecture and scheduling strategies. The combination of high IOPS, low-latency I/O paths and latency-aware scheduling provides a coherent support for the claimed performance gains and architectural impact for a Kafka-like streaming system.",
      "confidence": "high"
    },
    {
      "field": "benchmark_methodology_for_validation.principle_or_tool",
      "citations": [
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "Apr 22, 2021 — Coordinated omission is a term coined by Gil Tene to describe the phenomenon when the measuring system inadvertently coordinates with the system being measured.",
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency.",
            "So let’s try a benchmark of ScyllaDB and see what we get.",
            "How Not to YCSB"
          ]
        },
        {
          "title": "Correcting for \"coordinated omission\"",
          "url": "https://github.com/tdunning/t-digest/issues/128",
          "excerpts": [
            "Correcting for \"coordinated omission\"\nFortunately the technique used in the HDR Histogram to correct for\ncoordinated omission can be directly applied to the t-digest. The basic\nidea is that a user picks some value that they \"expect\" an operation to\ntake and then the code will take all of the data points which exceed that\nexpected quantity and adds new values to the histogram decreasing from the\nbucket value at by the expected value until from the value is lower than\nthe expected value.\nThis same approach can be applied either while\nrecording data into the histogram (\naddWhileCorrectingForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html.HdrHistogram.AbstractHistogram-long->>)\nor after the fact (copyCorrectedForCoordinatedOmission\n<<https://hdrhistogram.github.io/HdrHistogram/JavaDoc/org/HdrHistogram/AbstractHistogram.html>>\n)",
            "\"Coordinated Omission\"\n----------------------\n\nGil Tene [in this talk](https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf) introduced the concept of \"coordinated omission\" and a common error made when trying to understand tail latency of systems. makes the valuable observation that in the context of application latency monitoring, there is often a relatively fixed concurrency. This means that in response to application stalls, the throughput during the period of the stall will be dramatically lowered. Say for example you have one worker calling some function and you want to understand the performance characteristics of that function.",
            "The technique used by the HdrHistogram is far from perfect for monitoring use cases. I suspect that this is a reason that even though the HdrHistogram get wide use, people don't exploit its coordinated omission correction features as often as one might expect."
          ]
        },
        {
          "title": "Mechanical Sympathy Google Group Discussion on Coordinated Omission",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/icNZJejUHfE/m/BfDekfBEs_sJ",
          "excerpts": [
            "As one of the people who've had a spirited hour+ long debate with Gil on this topic, I would like to defend the position briefly: coordinated omission detection is far from perfect (it will by no means detect all sources), and requires various assumptions such as a steady rate of message arrival, a symmetrical distribution of message size/workload, pauses being system-wide... Coordinated omission correction is tricky business, for sure. Detection is a bit easier. E.g. if you detect a process-wide-affecting glitch (e.g.",
            "I started talking about Coordinated Omission in test systems mostly, but I kept finding it everywhere, including in many production monitoring systems. Coordinated Omission is nothing more than a measurement methodology problem. It exists in both test and production environments, and occurs whenever data is omitted ahead of analysis in a way that is coordinated with the [usually bad] behavior of the system. Coordinated omission basically guarantees a huge garbage-in-garbage-out relationship between the data feeding into [downstream] analysis and most operational conclusions that people make based on that data."
          ]
        },
        {
          "title": "GitHub - giltene/wrk2: A constant throughput, correct ...",
          "url": "https://github.com/giltene/wrk2",
          "excerpts": [
            "HdrHistogram includes\na simple way to compensate for Coordinated Omission when a known\nexpected interval between measurements exists.",
            "One of wrk2's main modification to wrk's current (Nov. 2014) measurement\nmodel has to do with how request latency is computed and recorded. wrk's model, which is similar to the model found in many current load\ngenerators, computes the latency for a given request as the time from\nthe sending of the first byte of the request to the time the complete\nresponse was received. While this model correctly measures the actual completion time of\nindividual requests, it exhibits a strong Coordinated Omission effect,",
            "Constant throughput load generators\nMake this trivial to model."
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch.",
            "Code here:  <https://github.com/DataDog/sketches-go>  <https://github.com/DataDog/sketches-py>  <https://github.com/DataDog/sketches-java>"
          ]
        },
        {
          "title": "Systemd journald performance discussions (GH issue 2815)",
          "url": "https://github.com/systemd/systemd/issues/2815",
          "excerpts": [
            "Performance issues occur when a high throughput workload emits a large number of logs."
          ]
        },
        {
          "title": "Cloudflare eBPF benchmark README",
          "url": "https://github.com/cloudflare/ebpf_exporter/blob/master/benchmark/README.md",
          "excerpts": [
            "Probe with a complex map increment attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n|\ntracepoint complex | 213 | 96 | 82% |\n| fentry complex | 220 | 103 | 88% |\n| kprobe complex | 346 | 229 | 196% |\n",
            "Big slowdown in terms of % for complex case may sounds like terrible,\nbut you have to remember that we're using a fast `getpid()` syscall. The main number to look at above is overhead in nanoseconds, because that's\nwhat you're going to pay no matter how fast or frequent function you're\nprobing is. 200-300ns overhead for the complex case of `getpid` is a lot, but\nfor tracing operations like disk access it's nothing compared to baseline. Notice how tracepoints are faster than fentry and fentry is faster than kprobe. Keep in mind that these numbers are for a single logical CPU core. Your mileage may vary depending on your hardware, make sure to test\nif you are hyper sensitive to any slowdowns.",
            "Empty probe attached:\n\n| Case | ns/op | Overhead ns/op | Overhead percent |\n| --- | --: | --: | --: |\n| no probe attached | 117 | 0 | 0% |\n| tracepoint empty | 132 | 15 | 13% |\n| fentry empty | 141 | 24 | 21% |\n| kprobe empty | 254 | 137 | 117% |\n"
          ]
        },
        {
          "title": "HdrHistogram by giltene",
          "url": "https://hdrhistogram.github.io/HdrHistogram/",
          "excerpts": [
            "Measurements show value recording times as low as 3-6 nanoseconds on modern (circa 2014) Intel CPUs. The HDR Histogram maintains a fixed cost in both space and ...",
            "Missing: accuracy figures"
          ]
        },
        {
          "title": "hdrhistogram - Rust - Docs.rs",
          "url": "https://docs.rs/hdrhistogram",
          "excerpts": [
            "Measurements show value recording times as low as 3-6 nanoseconds on modern (circa 2014) Intel CPUs. The HDR Histogram maintains a fixed cost in both space ...",
            "HDR Histogram is designed for recording histograms of value measurements in latency and performance sensitive applications. Measurements show value recording ...",
            "Value precision is expressed as the number of significant digits in the value recording, and provides control over value quantization behavior across the value ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value is Coordinated Omission (CO) within benchmarking methodology. Directly relevant sources define CO as a measurement problem where the measurement approach can bias latency results, typically when measurements are omitted in a way that correlates with system delays. The most relevant excerpts provide explicit definitions and context, such as CO being named for the phenomenon where the measuring system coordinates with the system under test, creating biased latency distributions. Several excerpts discuss how to mitigate CO, including HDR Histogram correction techniques and methods that compensate for expected interarrival intervals, which directly aligns with evaluating a benchmarking methodology's principle or tool for CO. Other excerpts describe practical implications, limitations, and debates about the effectiveness of CO corrections in real workloads, which informs the strength and boundaries of CO correction strategies. Collectively, these excerpts map CO from its definition to its practical handling in benchmarks, which directly supports validating the finegrained field value. The strongest support comes from explicit definitions of Coordinated Omission and cited correction approaches; additional context from tool discussions and benchmark experiments provides nuance and corroboration about when and how CO corrections are applied or challenged.",
      "confidence": "high"
    },
    {
      "field": "security_and_isolation_architecture.threat_type",
      "citations": [
        {
          "title": "Thunderclap: Exploring Vulnerabilities in Operating System ...",
          "url": "https://www.ndss-symposium.org/ndss-paper/thunderclap-exploring-vulnerabilities-in-operating-system-iommu-protection-via-dma-from-untrustworthy-peripherals/",
          "excerpts": [
            "Direct Memory Access (DMA) attacks have been known for many years: DMA-enabled I/O peripherals have complete access to the state of a computer and can fully ..."
          ]
        },
        {
          "title": "DMA protection and DMAAUTH evaluation - USENIX Security 2024",
          "url": "https://www.usenix.org/system/files/usenixsecurity24-wang-xingkai.pdf",
          "excerpts": [
            "When using the strict mode, IOMMU can defeat such attacks but will introduce unacceptable performance overhead."
          ]
        },
        {
          "title": "IOMMU/DMA Security and Mitigation - Abstract and Findings",
          "url": "https://dl.gi.de/bitstreams/1a7e69c5-eb7b-4ccb-b270-a119557a62e1/download",
          "excerpts": [
            "These attacks exploit\nshortcomings in the IOMMU specifications or vulnerabilities\nin architectures of actual systems.",
            "With the IOMMU enabled, PCIe devices are assigned to vir-\ntual address spaces, similar to the way the MMU manages\naddress spaces for user-space processe",
            "The easiest practical way to introduce a\nmalicious device into a system is Thunderbolt, which exposes\nthe internal PCIe bus on USB-C, allowing for external periph-\nerals to emit DMA request",
            " The\nkDMAp feature is configurable through UEFI, but known to\ncause compatability issues with some Thunderbolt devices"
          ]
        }
      ],
      "reasoning": "The most relevant content directly discusses DMA-based attack surfaces and protection mechanisms. Excerpts describing DMA-enabled peripherals having access to system state, and the concept of DMA attacks via IOMMU vulnerabilities, map directly to the threat type of DMA/PCIe attacks. Specifically, the discussion of Thunderclap-like findings and DMA-access risks illustrates the core mechanism by which DMA can compromise systems, which aligns with the DMA/PCIe attack threat category. Excerpts that evaluate DMA protection (e.g., DMAAUTH) and its overhead or effectiveness highlight mitigations and the feasibility of defending against such attacks, which is essential for understanding the risk landscape of DMA/PCIe threats. Additional passages that outline IOMMU/DMA security mitigation frames provide context about defensive architectures (e.g., PCIe address remapping and protection schemes) that are relevant to the threat type. While some excerpts discuss peripheral categories (e.g., Thunderbolt) or tool-specific details, they still contribute to the broader DMA/PCIe attack risk narrative and its mitigations, which supports the classification of this threat. Taken together, these excerpts form a cohesive view of DMA/PCIe attack surfaces, their potential impact, and practical defenses.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_adoption_strategy.proposed_approach",
      "citations": [
        {
          "title": "Unikraft: a Modern, Fully Modular Unikernel",
          "url": "https://www.usenix.org/publications/loginonline/unikraft-and-coming-age-unikernels",
          "excerpts": [
            "Unikraft provides (a modular) system call shim layer along with implementations of a growing number of syscalls (160 as of this writing).",
            "What about POSIX compatibility? While Linux has over 300 system calls, previous studies[ 11 ] have shown through static analysis that only a subset (224) are needed to run a Ubuntu installation.",
            "The ability to run a wide range of applications and languages is paramount to how much deployment any unikernel project will see. Unikraft addresses this directly through what we term autoporting : we use an application's native build system to build the application against the musl C standard library[ 9 ] and link the resulting object files against Unikraft (see Figure 4).",
            "The key insight is that applications are resilient to a significant portion of syscalls being stubbed and faked, and that the number of implemented syscalls they require to correctly run is significantly lower than the output of the static analysis suggests, let alone the total number of syscalls in the Linux API."
          ]
        }
      ],
      "reasoning": "The most directly supportive content notes that a modular unikernel provides a syscall shim layer and implements a growing number of syscalls, which aligns with the idea of mapping Linux syscall numbers to internal OS handlers via a compatibility layer. The accompanying detail that there is a modular system call shim (160 syscalls) directly backs the core mechanism of exposing a minimal, mapped syscall surface rather than a full reimplementation. A nearby excerpt reinforces the POSIX compatibility angle by noting the requirement to run Linux installations with a subset of system calls, indicating a deliberate limitation to a smaller, practical surface rather than full Linux compatibility. Another excerpt strengthens the argument by describing the broader strategy of autoporting applications against a musl-based environment, effectively tying the shim approach to practical build and deployment workflows. The final piece suggests apps tolerate stubbed syscalls and that the required syscall surface can be smaller than a full Linux API, which is consistent with the idea of focusing on critical syscalls used by workloads such as JVM and Spark and mapping them through the shim. Taken together, these excerpts coherently support a plan to provide a minimal POSIX subset via a compatibility layer built on a standard C library port, with targeted mapping to internal OS handlers and a focus on critical syscalls like file I/O, networking, and synchronization. This directly addresses the proposed approach and workload-driven syscall surface.\"",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases.0",
      "citations": [
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "The field value identifies High-Frequency Trading as the best business-use case and emphasizes ultra-low latency as the defining performance criterion. The most relevant excerpt explicitly characterizes HFT as a high-speed, high-turnover, algorithmic trading system, which provides the core concept behind the use case. Supporting this, another excerpt provides concrete latency targets and measurements in a cloud-native exchange setting, illustrating the latency-driven nature of HFT and the kinds of P99/latency figures that practitioners aim to achieve. A third excerpt demonstrates performance gains in a related, latency-sensitive trading context (Perséphone vs Shenango-like schedulers), reinforcing the argument that specialized, highly optimized, low-latency stacks (such as the proposed Rust Hallows with microsecond scheduling) are beneficial for meeting HFT requirements. Together, these excerpts establish the relevance of HFT as a use case, anchor the latency expectations, and illustrate the architectural direction (Rust-based, scheduler-driven, low-overhead I/O) necessary to achieve the target performance. The reasoning relies on paraphrasing the explicit HFT definition, the stated latency ranges, and the demonstrated performance benefits in similar systems to justify why HFT is the most compelling use case for the described technical vision.",
      "confidence": "high"
    },
    {
      "field": "project_risk_register.risk_description",
      "citations": [
        {
          "title": "Rust is DO-178C Certifiable - Pictorus Blog",
          "url": "https://blog.pictor.us/rust-is-do-178-certifiable/",
          "excerpts": [
            "With developments such as the Ferrocene-qualified compiler, Rust can now meet all the analysis requirements under DO-178C, one of the most ..."
          ]
        },
        {
          "title": "DO-330 / Tool Qualification Context with Ferrocene",
          "url": "https://afuzion.com/do-330-introduction-tool-qualification/",
          "excerpts": [
            "This DO-330 whitepaper describes the five tool categories versus criticality levels (DALs) and how to truly qualify avionics software tools.",
            "ntroduces five Tool Qualification Levels (TQLs) based upon three Tool Criteria."
          ]
        },
        {
          "title": "Ferrocene Update - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/sealed-rust-the-plan/",
          "excerpts": [
            "... Rust Compiler Backend(s) or Static/Dynamic Analysis Tools; Validating ... Tool Qualification standards. 1.0.1The Rust Language ..."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ...",
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ...",
            "AMC 20-193 Use of multi-core processors. Purpose. 1.1. This AMC describes an acceptable means, but not the only means, for showing compliance.",
            "**AMC 20-193**",
            "Interference channels and resource usage",
            "AMC 20-193 describes an acceptable, but not mandatory, means for showing compliance with airworthiness specifications for multi-core processors in airborne ...",
            "AMC 20-193 describes an acceptable, but not mandatory, means for showing compliance with airworthiness specifications for multi-core processors in airborne ..."
          ]
        },
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "The FAA™ and EASA® have been developing guidelines for development of multi-core systems for DO-178 aerospace projects. These include superseded documents ..."
          ]
        },
        {
          "title": "Ferrocene",
          "url": "https://ferrocene.dev/en/",
          "excerpts": [
            "This is Rust for critical systems. Ferrocene is the open-source qualified Rust compiler toolchain for safety- and mission-critical systems.",
            "Certification assistance. Individual. €240 per year/seat. Rust for small teams ... TÜV SÜD conducted the audit and Ferrous Systems – leading Rust experts ..."
          ]
        },
        {
          "title": "Officially Qualified - Ferrocene - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/officially-qualified-ferrocene/",
          "excerpts": [
            "Like the automotive\ncertification, we’ve also gone for the highest level of risk reduction and\nqualified it at SIL4.",
            "Ferrocene for use in systems up to ASIL D – the highest classification of\ninitial hazard as defined by this standard. This standard provides automakers\nwith guidelines that make the software being used to develop, produce, operate\nand service the automotive sectors safe to use."
          ]
        },
        {
          "title": "Ferrocene Achieves IEC 62304 Qualification - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/ferrocene-achieves-iec-62304-qualification/",
          "excerpts": [
            "“Qualifying Ferrocene under IEC 62304 demonstrates our commitment to advancing safety-critical software development through tools that enhance ..."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "DO-330/ED-215 “Software tool qualification considerations” is a self-contained document that describes the principles of software tool qualification as ... DO-330/ED-215 introduces the concept of Tool Qualification Levels (TQL) which are assigned according to three criteria:",
            "**Criterion 1**A tool whose output is part of the airborne software and thus could insert an error",
            "**Criterion 2**\n\n1. Verification processes other than that automated by the tool, or\n2. Development processes that could have an impact on the airborne software. **Criterion 3**",
            "An unqualified compiler or an auto-code generator from an UML tool would therefore fit criterion 1. A qualified version of the same UML tool would fit criterion 2, because its use is designed to reduce the overhead of code verification processes. The LDRA tool suite is a verification tool and therefore a criterion 3 tool. Irrespective of the application software level (for DO-178C, read Design Assurance Level or DAL) such a verification tool is always assigned Tool Qualification Level 5 – the least demanding of the five levels (below).",
            "Civil aviation certification authorities require tool qualification on a project-by-project basis. The responsibility for showing the suitability of any tool falls on to the organization developing a civil aviation application. However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "A practical Approach to CAST-32A & AMC 20-193 ...",
          "url": "https://www.sysgo.com/blog/article/the-multi-core-challenge-a-practical-approach-to-cast-32a-amc-20-193-compliance",
          "excerpts": [
            "Oct 21, 2022 — This webcast presents a turnkey solution from SYSGO and LDRA designed to meet the objectives of CAST-32A within the context of a DO-178C compliant project."
          ]
        },
        {
          "title": "[PDF] Overcoming the Challenges of Meeting CAST-32A Objectives for ...",
          "url": "https://ldra.com/wp-content/uploads/2022/09/Overcoming-the-Challenges-of-Meeting-CAST-32A-Objectives-for-Avionics-Software.pdf",
          "excerpts": [
            "CAST-32A, “Multi-core processors”, is a position paper that was written by the Certification. Authorities Software Team (CAST). CAST were a group of civil ..."
          ]
        },
        {
          "title": "[PDF] Civil Certification of Multi-core Processing Systems in Commercial ...",
          "url": "https://www.windriver.com/sites/default/files/2022-06/Collins%20Aerospace%20paper.pdf",
          "excerpts": [
            "CAST-32A defines a number of objectives that must be met by an avionics developer using a multi-core processor. Table 1 provides an overview of CAST-32A by."
          ]
        },
        {
          "title": "LDRA Simplifies CAST-32A & A(M)C 20-193 Compliance ...",
          "url": "https://www.prnewswire.com/in/news-releases/ldra-simplifies-cast-32a--amc-20-193-compliance-with-complete-lifecycle-support-301773805.html",
          "excerpts": [
            "Mar 16, 2023 — CAST-32A has matured into the de-facto guidance for the adoption of MCPs in civil aviation in the form of the current European Union Aviation ..."
          ]
        },
        {
          "title": "Improving NGINX Performance with Kernel TLS and SSL_sendfile( )",
          "url": "https://www.f5.com/company/blog/nginx/improving-nginx-performance-with-kernel-tls",
          "excerpts": [
            "NGINX 1.21.4 introduces support for kTLS when serving static files and cached responses with SSL_sendfile(), which can hugely improve performance."
          ]
        },
        {
          "title": "Intel® QuickAssist Technology (Intel® QAT) and OpenSSL-1.1.0",
          "url": "https://www.intel.com/content/www/us/en/content-details/706024/intel-quickassist-technology-intel-qat-and-openssl-1-1-0-performance.html",
          "excerpts": [
            "This paper will demonstrate how the combination of these features with Intel® QAT results in tangible performance gains."
          ]
        },
        {
          "title": "PCIe DMA Attack against a secured Jetson Nano (CVE- ...",
          "url": "https://www.thegoodpenguin.co.uk/blog/pcie-dma-attack-against-a-secured-jetson-nano-cve-2022-21819/",
          "excerpts": [
            "Mar 10, 2022 — 2 NVMe SSD. We could not find examples of PCIe attacks against ARM platforms, so we investigated to see if the theoretical attack was possible ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value asserts that achieving high-assurance safety certifications (DO-178C DAL B/A for avionics and ISO 26262 ASIL D for automotive) for the Rust Hallows OS variant is extremely complex, costly, and time-consuming, with extensive documentation, rigorous verification, and full traceability requirements. The excerpts point to the certification and tool-qualification ecosystem that would govern such a claim. There is explicit discussion that tool qualification (e.g., DO-330 and related qualification frameworks) is a central, structured process when using specialized toolchains like Ferrocene in safety-critical domains, and that DO-178C/DO-254-like regimes apply to avionics software development with DO-330 framing tool qualification levels and assessment criteria. Several excerpts note that Ferrocene has achieved some certification qualifications and that Rust-related safety certifications are a researched and evolving area, which supports the notion that achieving full DO-178C DAL-level certification would be non-trivial and requires substantial investment. Additional excerpts discuss CAST-32A and AMC 20-193 guidance for multi-core processor safety and interference management, which are integral to avionics and aerospace certification programs and would influence the regulatory path for a Rust Hallows-like system. The combination of references about DO-330 tool qualification, Ferrocene qualification, Rust being argued as certifiable in do-178 contexts, and CAST-32A/AMC 20-193 guidance collectively corroborate the claim that certification for the Rust Hallows variant would be lengthy, costly, and heavily constrained by certification requirements. The supporting content also acknowledges that there are legitimate pathways and practices (e.g., qualified toolchains, DO-330 context, CAST-32A considerations) although these pathways come with substantial overhead and verification demands, aligning with the described risk. Overall, the strongest alignment is with claims about DO-178C and tool qualification complexity, followed by the need for rigorous certification processes and the relevance of CAST-32A/AMC-20-193 frameworks for multi-core avionics-grade systems. The confidence reflects that the sources provide credible, related certification context but do not offer a complete, end-to-end validation of Rust Hallows achieving all stated certifications in practice yet.",
      "confidence": "medium"
    },
    {
      "field": "performance_analysis_for_target_workloads.2",
      "citations": [
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing.",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "Information-Agnostic Flow Scheduling for Commodity Data ...",
          "url": "https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/bai",
          "excerpts": [
            "by W Bai · 2015 · Cited by 415 — We present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a ..."
          ]
        }
      ],
      "reasoning": "Evidence across multiple sources directly supports the core components of the finegrained field value. First, microsecond-scale latency and very high throughput are demonstrated by Shenango, with claims of handling over five million requests per second and median/99.9th percentile latencies in the tens of microseconds, which aligns with the target tail-latency improvements for latency-critical backend APIs. This shows that a scheduler design can dramatically reduce tail latency under heavy load. Kernel-bypass data path mechanisms are demonstrated by SPDK vhost-user backends and a poll-mode Virtio driver, which substantially reduce or eliminate kernel overhead and interrupts, contributing to higher IOPS and lower latency. The combination of vhost-user with SPDK and the poll-mode path directly supports the architecture you propose for fast pathways in the backend. Scheduling-focused comparisons (Perséphone, Shenango, DARC) illustrate that different scheduling strategies can yield substantial tail-latency improvements, especially for short vs. long requests and under contention, which is critical for maintaining p99.9 latency in bursty scenarios. Finally, there is explicit discussion of tail-latency bottlenecks and the importance of measurement discipline, reinforcing that achieving millisecond-level tails is not sufficient and that careful perf harnessing is required to validate gains. Taken together, the excerpts coherently support the expected order-of-magnitude tail-latency improvements through a combination of kernel-bypass IO, microsecond-scale schedulers, and thread-per-core designs, while also acknowledging realistic caveats around variability and hardware.",
      "confidence": "high"
    },
    {
      "field": "security_and_isolation_architecture.mitigation_strategy",
      "citations": [
        {
          "title": "IOMMU/DMA Security and Mitigation - Abstract and Findings",
          "url": "https://dl.gi.de/bitstreams/1a7e69c5-eb7b-4ccb-b270-a119557a62e1/download",
          "excerpts": [
            "With the IOMMU enabled, PCIe devices are assigned to vir-\ntual address spaces, similar to the way the MMU manages\naddress spaces for user-space processe",
            "DMA Address Remapping. The process of DMA address\ntranslation, begins with a PCIe device emitting a DMA re-\nques",
            "These attacks exploit\nshortcomings in the IOMMU specifications or vulnerabilities\nin architectures of actual systems.",
            "The easiest practical way to introduce a\nmalicious device into a system is Thunderbolt, which exposes\nthe internal PCIe bus on USB-C, allowing for external periph-\nerals to emit DMA request",
            " The\nkDMAp feature is configurable through UEFI, but known to\ncause compatability issues with some Thunderbolt devices"
          ]
        },
        {
          "title": "DMA protection and DMAAUTH evaluation - USENIX Security 2024",
          "url": "https://www.usenix.org/system/files/usenixsecurity24-wang-xingkai.pdf",
          "excerpts": [
            "DMAAUTH introduces 1.0% extra CPU time.",
            "Combining the read and write test, the DMAAUTH introduces a 1.4% overhead\nin throughput and 1.0% overhead in CPU runtime on average, which are 81.0% and 81.6% less than IOMMU respectively.",
            "When using the strict mode, IOMMU can defeat such attacks but will introduce unacceptable performance overhead.",
            "In our experiments, DMAAUTH can defeat the attacks in"
          ]
        }
      ],
      "reasoning": "The core of the field value asserts that enabling the IOMMU is the foundational hardware defense to virtualize addresses and bound devices to their buffers, with a need to expose DMA capabilities to user-space safely through a sandboxed IOMMU domain, for example via a VFIO-like path, in order to mitigate DMA-based tenant escape. The most directly relevant excerpts describe the IOMMU as the mechanism that provides address space isolation and the concept of DMA remapping/security measures, including how such protections are implemented and mitigated against in practice. One excerpt explicitly frames the IOMMU as the mechanism to assign and protect device memory and discusses the broader DMA security landscape, including mitigation strategies around exposure to user-space through controlled domains. This supports the idea that the IOMMU is the primary hardware defense and that exposing DMA to user-space must be carefully sandboxed to prevent memory corruption or tenant escape. Additional excerpts discuss the overhead and performance considerations when using DMA protection and virtualization features like DMA remapping and VFIO-related paradigms, which align with the need to balance security with performance in a real deployment. Together, these excerpts corroborate the proposed architecture: enable the IOMMU, use a sandboxed domain (VFIO) to safely expose DMA to user-space components such as SPDK, thereby containing potential DMA-based attacks and preserving system integrity.",
      "confidence": "medium"
    },
    {
      "field": "project_risk_register.impact",
      "citations": [
        {
          "title": "Officially Qualified - Ferrocene - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/officially-qualified-ferrocene/",
          "excerpts": [
            "Like the automotive\ncertification, we’ve also gone for the highest level of risk reduction and\nqualified it at SIL4.",
            "Ferrocene for use in systems up to ASIL D – the highest classification of\ninitial hazard as defined by this standard. This standard provides automakers\nwith guidelines that make the software being used to develop, produce, operate\nand service the automotive sectors safe to use."
          ]
        },
        {
          "title": "Ferrocene Achieves IEC 62304 Qualification - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/ferrocene-achieves-iec-62304-qualification/",
          "excerpts": [
            "“Qualifying Ferrocene under IEC 62304 demonstrates our commitment to advancing safety-critical software development through tools that enhance ..."
          ]
        },
        {
          "title": "DO-330 / Tool Qualification Context with Ferrocene",
          "url": "https://afuzion.com/do-330-introduction-tool-qualification/",
          "excerpts": [
            "This DO-330 whitepaper describes the five tool categories versus criticality levels (DALs) and how to truly qualify avionics software tools."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "**Criterion 2**\n\n1. Verification processes other than that automated by the tool, or\n2. Development processes that could have an impact on the airborne software. **Criterion 3**"
          ]
        },
        {
          "title": "Rust is DO-178C Certifiable - Pictorus Blog",
          "url": "https://blog.pictor.us/rust-is-do-178-certifiable/",
          "excerpts": [
            "With developments such as the Ferrocene-qualified compiler, Rust can now meet all the analysis requirements under DO-178C, one of the most ..."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "AMC 20-193 Use of multi-core processors. Purpose. 1.1. This AMC describes an acceptable means, but not the only means, for showing compliance.",
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ..."
          ]
        },
        {
          "title": "Ferrocene",
          "url": "https://ferrocene.dev/en/",
          "excerpts": [
            "This is Rust for critical systems. Ferrocene is the open-source qualified Rust compiler toolchain for safety- and mission-critical systems."
          ]
        }
      ],
      "reasoning": "The most relevant parts are those that explicitly anchor safety-critical or certification contexts. An excerpt stating that Ferrocene is officially qualified and used in systems up to ASIL D indicates a high level of safety-critical compliance, directly aligning with a 'Critical' impact designation for a risk item. Similarly, references to IEC 62304 qualification emphasize safety-critical software processes, which strongly support a critical assessment of risk in a project that targets safety-certified, avionics-like environments. Additional excerpts discuss DO-330 tool qualification and the associated levels, which are essential for showing that toolchains in safety-critical projects meet required certification criteria; this reinforces the notion that such work carries a critical impact in risk planning. Other entries that describe multi-core processor guidance (CAST-32A) and DO-178C-related certification contexts further substantiate that the domain requires rigorous safety certification, contributing to a high impact assessment even if not all details map directly to a single field value. In sum, the strongest justification comes from explicit safety-certification mentions (SIL4, ASIL D, IEC 62304) and DO-330 tool qualification discussions, with supporting context from broader certification frameworks; these collectively justify labeling the field as 'Critical' within a risk register for a safety-critical project. ",
      "confidence": "high"
    },
    {
      "field": "project_risk_register.mitigation_plan",
      "citations": [
        {
          "title": "Ferrocene Achieves IEC 62304 Qualification - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/ferrocene-achieves-iec-62304-qualification/",
          "excerpts": [
            "“Qualifying Ferrocene under IEC 62304 demonstrates our commitment to advancing safety-critical software development through tools that enhance ..."
          ]
        },
        {
          "title": "Officially Qualified - Ferrocene - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/officially-qualified-ferrocene/",
          "excerpts": [
            "Like the automotive\ncertification, we’ve also gone for the highest level of risk reduction and\nqualified it at SIL4."
          ]
        },
        {
          "title": "DO-330 / Tool Qualification Context with Ferrocene",
          "url": "https://afuzion.com/do-330-introduction-tool-qualification/",
          "excerpts": [
            "This DO-330 whitepaper describes the five tool categories versus criticality levels (DALs) and how to truly qualify avionics software tools.",
            "ntroduces five Tool Qualification Levels (TQLs) based upon three Tool Criteria."
          ]
        },
        {
          "title": "Ferrocene Update - Ferrous Systems",
          "url": "https://ferrous-systems.com/blog/sealed-rust-the-plan/",
          "excerpts": [
            "... Rust Compiler Backend(s) or Static/Dynamic Analysis Tools; Validating ... Tool Qualification standards. 1.0.1The Rust Language ..."
          ]
        },
        {
          "title": "Rust is DO-178C Certifiable - Pictorus Blog",
          "url": "https://blog.pictor.us/rust-is-do-178-certifiable/",
          "excerpts": [
            "With developments such as the Ferrocene-qualified compiler, Rust can now meet all the analysis requirements under DO-178C, one of the most ..."
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ...",
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ..."
          ]
        },
        {
          "title": "A practical Approach to CAST-32A & AMC 20-193 ...",
          "url": "https://www.sysgo.com/blog/article/the-multi-core-challenge-a-practical-approach-to-cast-32a-amc-20-193-compliance",
          "excerpts": [
            "Oct 21, 2022 — This webcast presents a turnkey solution from SYSGO and LDRA designed to meet the objectives of CAST-32A within the context of a DO-178C compliant project."
          ]
        },
        {
          "title": "[PDF] Civil Certification of Multi-core Processing Systems in Commercial ...",
          "url": "https://www.windriver.com/sites/default/files/2022-06/Collins%20Aerospace%20paper.pdf",
          "excerpts": [
            "CAST-32A defines a number of objectives that must be met by an avionics developer using a multi-core processor. Table 1 provides an overview of CAST-32A by."
          ]
        },
        {
          "title": "LDRA Simplifies CAST-32A & A(M)C 20-193 Compliance ...",
          "url": "https://www.prnewswire.com/in/news-releases/ldra-simplifies-cast-32a--amc-20-193-compliance-with-complete-lifecycle-support-301773805.html",
          "excerpts": [
            "Mar 16, 2023 — CAST-32A has matured into the de-facto guidance for the adoption of MCPs in civil aviation in the form of the current European Union Aviation ..."
          ]
        }
      ],
      "reasoning": "The plan advocates early, continuous engagement with certification authorities and a Plan for Software Aspects of Certification (PSAC). Excerpts describing DO-330 tool qualification, as well as DO-178C CAST-32A guidance, provide concrete frameworks and processes supporting early and formal certification planning, including engagement with regulators and structured tool qualification paths. The proposal to use Ferrocene as a qualified Rust toolchain directly aligns with the idea of leveraging pre-qualified tools to reduce qualification burden and costs under DO-330, offering a mechanism to streamline compliance activities. Mentioned examples and analyses show that tool qualification is a recognized, formal process with levels and criteria, which supports the strategy of acquiring specialized DO-178C/ISO 26262 expertise to navigate certification. The excerpts about certification standards, and the practical pathways (e.g., Ferrocene achieving IEC 62304 qualification, DO-330 context, CAST-32A discussions) provide concrete evidence that the plan's emphasis on automation, traceability, and phased progression is not only plausible but grounded in real-world practice. The inclusion of phased/segmented certification plans is reinforced by discussions of incremental approaches and prioritizing SKUs or subsystems to bring a minimal certifiable core first, then extend scope in later phases. Taken together, these excerpts support each element of the field value: early PSAC planning, use of a pre-qualified toolchain (Ferrocene), specialized DO-178C/ISO 26262 expertise, automated verification/documentation practices, and a phased certification strategy. The strongest support centers on Ferrocene qualification and formal tool qualification pathways, with robust, constructive guidance on multi-core certification practices providing the broader context.",
      "confidence": "high"
    },
    {
      "field": "go_to_market_and_packaging_strategy.strategy_element",
      "citations": [
        {
          "title": "AWS Marketplace AMI Product Checklist",
          "url": "https://docs.aws.amazon.com/marketplace/latest/userguide/aws-marketplace-listing-checklist.html",
          "excerpts": [
            "Before submitting your Amazon Machine Image (AMI) product to AWS, use this checklist to validate your submission.",
            "Product usage is not restricted by time or any other measurements.",
            "AMI meets the mandatory [AMI-based product requirements for AWS Marketplac",
            "AMI product checklist for AWS Marketplace",
            "AMI is production-ready."
          ]
        },
        {
          "title": "AMI Pricing in AWS Marketplace",
          "url": "https://docs.aws.amazon.com/marketplace/latest/userguide/pricing-ami-products.html",
          "excerpts": [
            "AWS Marketplace has multiple pricing models for Amazon Machine Image (AMI) products.",
            "With seller\n private oﬀers, there are options available for multi-year and custom duration contracts."
          ]
        },
        {
          "title": "Offering virtual machine (VM) products",
          "url": "https://cloud.google.com/marketplace/docs/partners/vm",
          "excerpts": [
            "It must deploy software to, and run on, Compute Engine. It must use the Cloud Marketplace-hosted image with the attached Compute Engine license."
          ]
        },
        {
          "title": "Windows Virtual Machines Pricing",
          "url": "https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/",
          "excerpts": [
            "Buy Windows Virtual Machines today and pay only for what you use. Compare pricing options to choose the best plan for your business."
          ]
        },
        {
          "title": "Accept a private offer from a vendor | Google Cloud Marketplace",
          "url": "https://cloud.google.com/marketplace/docs/offers/accepting-private-offer",
          "excerpts": [
            "Accept a private offer · Click and open the link that the vendor sent you. · Verify the details of the offer, such as the pricing details and contract duration."
          ]
        },
        {
          "title": "Create a virtual machine offer on Azure Marketplace.",
          "url": "https://learn.microsoft.com/en-us/partner-center/marketplace-offers/azure-vm-offer-setup",
          "excerpts": [
            "Jan 31, 2025 — This article describes how to create an Azure virtual machine offer for Azure Marketplace. It addresses both Windows-based and Linux-based virtual machines."
          ]
        },
        {
          "title": "Create a private offer for a customer | Google Cloud Marketplace ...",
          "url": "https://cloud.google.com/marketplace/docs/partners/offers/create-private-offers",
          "excerpts": [
            "Your product must be integrated with and listed on Cloud Marketplace. You must have the customer's Cloud Billing account ID. If you don't have the ID, request ..."
          ]
        },
        {
          "title": "Configuring metering for usage with SaaS subscriptions",
          "url": "https://docs.aws.amazon.com/marketplace/latest/userguide/metering-for-usage.html",
          "excerpts": [
            "For software as a service (SaaS) subscriptions, you meter for all usage, and then customers are billed by AWS based on the metering records that you provide."
          ]
        },
        {
          "title": "Release Notes for Confluent Cloud",
          "url": "https://docs.confluent.io/cloud/current/release-notes/index.html",
          "excerpts": [
            "The Service Level Agreement (SLA) for Confluent Cloud now specifies a 99.99% uptime SLA for Standard and Dedicated Kafka clusters with Multi-Zone configurations ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant material discusses formal packaging and distribution requirements for marketplace-style channels. Excerpts describing the AWS Marketplace AMI Product Checklist establish explicit criteria that a product must meet to be packaged and sold via AWS Marketplace, serving as a concrete guide for packaging decisions. Other excerpts outline the general concept of an AMI-based product and how to meet marketplace expectations, which informs how to structure and distribute software images. Additional items cover marketplace pricing models and vendor offers, which are essential for determining how the product is priced, offered, and renewed within distribution channels. There are also references to other cloud marketplaces (Google Cloud Marketplace, Azure Marketplace) that provide analogous packaging and distribution considerations, reinforcing a multi-channel go-to-market approach. Collectively, these excerpts support the notion of packaging readiness (image format, virtualization type), listing requirements, and channel-specific distribution strategies, specifically for packaging and distribution channels in a marketplace context. The explicit emphasis on production-readiness, marketplace suitability, and image lifecycle (pricing, offers, and private offers) aligns with the target field value describing packaging and distribution channels as a core GTM activity.",
      "confidence": "high"
    },
    {
      "field": "certification_pathway_for_high_assurance.implementation_details",
      "citations": [
        {
          "title": "Time Protection: The Missing OS Abstraction",
          "url": "https://tomchothia.gitlab.io/Papers/EuroSys19.pdf",
          "excerpts": [
            "by Q Ge · 2019 · Cited by 113 — Time sharing cannot produce interference on these, while concurrent access can be observed as a reduction of available bandwidth. Cache channels work by the ..."
          ]
        },
        {
          "title": "Robust partitioning for multicore systems doesnt mean freedom from interference",
          "url": "https://www.rapitasystems.com/blog/robust-partitioning-multicore-systems-doesnt-mean-freedom-interference",
          "excerpts": [
            "\nMost of the technology for partitioning of single core systems also applies to multicore systems. Multicore systems are more complex, however – for these systems, interference can exist between software hosted on different cores but running at the same time.",
            "The most rigorous separation is termed _robust partitioning_ . This is defined for single core systems in the ARINC 653 standard, where it refers to complete isolation of separated components, i.e. the execution of one component can have no effect on the oth"
          ]
        },
        {
          "title": "ARINC 653",
          "url": "https://en.wikipedia.org/wiki/ARINC_653",
          "excerpts": [
            "The European Union aviation authority, EASA, published AMC 20-193 in January 2022."
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "**AMC 20-193**",
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ..."
          ]
        },
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "AMC 20-193 was released in January 2022, and AC 20-193 was released some two years later in January 2024. The two guidance documents are fully aligned from a technical and certification perspective."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "[PDF] Caladan: Mitigating Interference at Microsecond Timescales",
          "url": "https://amyousterhout.com/papers/caladan_osdi20.pdf",
          "excerpts": [
            "Cal- adan consists of a centralized scheduler core that actively manages resource contention in the memory hierarchy and between hyperthreads, ..."
          ]
        }
      ],
      "reasoning": "The strongest support comes from sources that directly address interference management and time isolation in multicore environments. Content describing time protection and explicit separation of components aligns with the requirement to identify interference channels and to design mitigations such as cache partitioning and bandwidth regulation, all with the aim of achieving bounded worst-case timing. Documents discussing robust partitioning and ARINC 653-style time/space partitioning provide a framework for isolating software components on shared hardware, which is central to identifying and mitigating interference channels. Certification-oriented guidance (AMC 20-193) and CAST-32A overview offer concrete regulatory and design expectations for multicore processor use in aviation-qualified systems, including how to bound interference and perform resource usage analysis, which supports the need for verification under maximum contention and stress conditions. Tool qualification considerations (DO-330) outline processes to qualify the software tools used in the development and verification workflow, which is essential for DO-178C/DO-330-aligned certification pipelines. If a reader wants to map out a pathway to high-assurance OS development with predictable timing, these excerpts together provide a coherent view: first establish isolation strategies and interference models, then adopt partitioning approaches (ARINC 653-like), followed by certification-specific guidance and tooling practices to validate WCET and timing guarantees. The remaining excerpts extend the discussion to practical performance backdrops and potential risks, reinforcing the feasibility and limitations of these pathways in real-world systems. In short, the excerpts collectively substantiate a structured approach to interference identification, mitigation, and verification as required by the fine-grained field value. ",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.2",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        }
      ],
      "reasoning": "The core field value centers on an RTB workload characterized by extremely tight end-to-end latency (e.g., 100-200 ms time-to-bid) and massive, bursty network I/O, where reducing tail latency yields higher bidder participation and revenue. The most relevant excerpt directly discusses ultra-low-latency, cloud-native exchanges and provides concrete latency figures (P50/P99) and design emphasis on latency minimization, which maps to the RTB requirement for fast bid decisions. A closely related excerpt cites high-frequency trading as an industry example of latency-critical systems, reinforcing the theme that millisecond- and microsecond-scale latency considerations are central to high-stakes, network-intensive workloads, which is analogous to RTB's need for rapid response under bursty traffic. Another excerpt highlights a study showing substantial latency improvements in a different but similarly latency-sensitive domain, illustrating that architectural choices aimed at reducing tail latency (e.g., Shenango-style scheduling, user-space I/O, and high-performance backends) can yield meaningful performance gains for latency-sensitive workloads like RTB. A fourth excerpt provides market-scale context, underscoring the commercial importance of latency-enabled participation, which complements the business rationale for the RTB use case and supports the claim that performance improvements translate into revenue opportunities. Collectively, these excerpts support the idea that a Universal Virtio-based stack with SPDK/DPDK-backed I/O and careful scheduling can address the RTB's stringent latency and throughput requirements and thereby deliver business value.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.1",
      "citations": [
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing.",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "Information-Agnostic Flow Scheduling for Commodity Data ...",
          "url": "https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/bai",
          "excerpts": [
            "by W Bai · 2015 · Cited by 415 — We present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a ..."
          ]
        },
        {
          "title": "Long-tail Latency: Causes and Solutions",
          "url": "https://systemdr.substack.com/p/long-tail-latency-causes-and-solutions",
          "excerpts": [
            "Jul 23, 2025 — Welcome to the world of long-tail latency—where the statistical outliers at P95, P99, and P99.9 percentiles can destroy user experience despite ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant materials discuss scheduling and core reallocation strategies that are central to the finegrained field value. Excerpts describing Shenango-style scheduling and the idea of dedicating or reassigning cores with microsecond granularity demonstrate the architectural approach to reducing stragglers and tail latency, which is the core of the proposed performance analysis. Excerpt detailing Shenango and related latency-focused scheduling shows concrete mechanisms for achieving microsecond-scale reallocation and tail-latency improvements, aligning with the field value's emphasis on mitigating stragglers and tight scheduling windows. Additional excerpts discuss SPDK vhost and poll-mode I/O backends, which directly support the architecture's goal of lowering software overhead and CPU utilization during shuffle, read/write, and spill operations. Those SPDK-backed paths (vhost-user backends and poll-mode virtio) provide the practical backend technologies that enable the proposed acceleration of shuffle I/O and data movement, matching the field value's backend strategy. Further supporting evidence comes from references to DataFusion/Arrow as the vectorized analytics stack for in-memory processing and network exchange, providing a concrete dataflow path that complements the Spark-like analytics workload described. Finally, references to high-performance networking backends and latency-focused scheduling work give broader context and corroborate the feasibility and expected gains of such an architecture. Overall, the strongest, most direct support comes from the material that ties Shenango-style core reallocation to tail-latency improvements and SPDK vhost/poll-mode I/O as the viable high-performance path for storage and I/O; the remaining excerpts provide corroboration about the individual technologies and performance patterns that underpin the proposed design.",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases.1",
      "citations": [
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        }
      ],
      "reasoning": "The most relevant content directly speaks to ultra-low latency performance and latency improvements in latency-sensitive environments. The excerpt describing Persephone/SOSP21 reports substantial latency improvements over a baseline in a high-performance context, illustrating how specialized scheduling and system design can dramatically reduce tail latency, which supports the claim that a Rust-based OS with microsecond-level control can meet URLLC-like requirements. The excerpt detailing low-latency cloud-native exchanges presents explicit microsecond-scale latency figures (P50 and P99 ranges), which align with the target sub-millisecond budgets and demonstrate practical numbers achievable in latency-critical networking scenarios. Additional excerpts on high-frequency trading and related latency considerations provide context on how the industry constrains and measures latency in extreme scenarios, further supporting the feasibility of extremely low-latency operation when proper architecture and backends are employed. Together, these excerpts collectively support the idea that architectural choices focused on deterministic scheduling, high-performance I/O backends, and careful partitioning can yield the required ultra-low latency characteristics for a UPF-like function, even though they are not UPF-specific themselves.",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.2",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "rs. #### Configuration Details\n\nTest by Intel as of 10/22/2022. System: 1-node, 2x Intel® Xeon® Platinum 8480+ Processor (56 cores, HT=On, Turbo=ON),\nTotal Memory 1024 GB (16 slots/ 64GB/ 4800 MHz), BIOS:3A05 (ucode:0x2b000070),\nStorage: 24x Intel® Optane™ SSD 800GB P5800X."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking."
          ]
        }
      ],
      "reasoning": "The finegrained field value specifies a high-performance NVMe SSD (Samsung PM1743) that is fully compatible with SPDK's user-space driver, aiming to saturate PCIe 5.0 for use cases like Kafka logs and analytics spill files. Excerpts that discuss SPDK's NVMe driver and performance capabilities directly support this claim, including quantified performance benefits and scalability (e.g., >10M IOPS with vhost-user backends, linear scaling, and NVMe driver characteristics that avoid locking in the I/O path). Content describing SPDK vhost-user as a high-performance back-end (used for storage and networking offload in QEMU) corroborates the intended SPDK user-space path and its relevance to high-throughput workloads. Additional excerpts that illustrate SPDK performance benchmarks and architectural patterns (e.g., perf tools, latency considerations, and integration with vhost-user) further reinforce the feasibility of saturating SPDK-backed NVMe storage under PCIe 5.0, which aligns with the proposed PM1743 use case and the desired Kafka/analytics workloads. While some excerpts discuss broader Virtio and GPU/backends, the most directly relevant material is the SPDK NVMe driver and the SPDK vhost-user references, as they map tightly to the field value's emphasis on SPDK-enabled user-space I/O and high sequential/Rand I/O performance targets.",
      "confidence": "high"
    },
    {
      "field": "go_to_market_and_packaging_strategy.example_from_industry",
      "citations": [
        {
          "title": "OEM Partnerships",
          "url": "https://www.nutanix.com/partners/oem",
          "excerpts": [
            "OEM Partners · Cisco Compute Hyperconverged · Fujitsu XF Series · Hewlett-Packard DX Series · Lenovo ThinkAgile HX · Dell XC Family · Supermicro."
          ]
        },
        {
          "title": "RHEL_Support_Matrix.pdf",
          "url": "https://linux.dell.com/files/supportmatrix/RHEL_Support_Matrix.pdf",
          "excerpts": [
            "The matrix shows RHEL/Dell PowerEdge server certification status, with levels: Not Supported, Certified, Validated, and Fully Supported. Dell offers validation ..."
          ]
        }
      ],
      "reasoning": "The most relevant information directly supports the packaging strategy example through explicit mention of Nutanix forming OEM partnerships with hardware vendors, including naming Dell (XC series) and Lenovo (HX series), which aligns with the described multi-faceted packaging approach that simplifies deployment and accelerates enterprise adoption. Supporting context from the Red Hat area, via a hardware certification lens, appears in the second most relevant excerpt which discusses hardware certification and partner-level validation (e.g., OS compatibility and certification status with Dell). These pieces together corroborate a strategy where a vendor builds confidence in customers by pre-validating hardware and software combinations, enabling streamlined deployment. Other excerpts touch on related topics like market pricing, general marketplace practices, or ARINC-style scheduling, but do not directly illuminate the packaging-and-OEM-catalog narrative and thus are less supportive.",
      "confidence": "medium"
    },
    {
      "field": "cloud_deployment_strategy.provider_name",
      "citations": [
        {
          "title": "Security Design of the AWS Nitro System",
          "url": "https://docs.aws.amazon.com/whitepapers/latest/security-design-of-aws-nitro-system/the-components-of-the-nitro-system.html",
          "excerpts": [
            "Nitro System enables what is effectively “bare metal” performance by running nearly all\n the functionality of the virtualization system on the Nitro cards rather than on the host’s\n system mainboard CPU",
            "Within the Nitro Hypervisor, there is, by design, no networking stack, no general-purpose file system implementations, and no peripheral device driver support.",
            "The Nitro Hypervisor is designed to receive virtual machine management commands (start, stop, and so on) sent from the Nitro Controller, to partition memory ..."
          ]
        },
        {
          "title": "Amazon EBS volumes and NVMe - AWS Documentation",
          "url": "https://docs.aws.amazon.com/ebs/latest/userguide/nvme-ebs-volumes.html",
          "excerpts": [
            "To fully utilize the performance and capabilities of Amazon EBS volumes exposed as NVMe block devices, the EC2 instance must have the AWS NVMe driver installed.",
            "All current generation AWS Windows and Linux AMIs come with the AWS\nNVMe driver installed by default.",
            "Amazon EBS volumes are exposed as NVMe block devices on Amazon EC2 instances built on the [AWS Nitro System](https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html).",
            "The device names are `/dev/nvme0n1`, `/dev/nvme1n1`,\nand so on."
          ]
        }
      ],
      "reasoning": "The field value identifies the cloud provider as AWS. The most directly supportive information comes from excerpts that explicitly reference the AWS Nitro System and AWS-specific storage technologies, which indicate AWS as the provider behind the deployment stack. The opening excerpts describe the Nitro System, Nitro Hypervisor, and AWS Nitro-related components, making a strong case that AWS is the provider. Excerpt describing that EBS volumes are exposed as NVMe block devices on EC2 instances built on the AWS Nitro System reinforces the AWS context in deployment and infrastructure layers. Excerpts mentioning the AWS NVMe driver being installed by default further tie the infrastructure to AWS in practical deployment terms. Additional items referring to Amazon EBS volumes and AWS-specific infrastructure corroborate the AWS deployment narrative. A couple of excerpts discuss generic virtualization backends or device names without reiterating the AWS origin; these are still contextually linked to the AWS deployment story but are marginally less direct. Taken together, the most relevant content consistently anchors to AWS-specific hardware/virtualization components and AWS-branded services, establishing a solid AWS provider signal for the fine-grained field value.",
      "confidence": "high"
    },
    {
      "field": "cloud_deployment_strategy.hypervisor_and_architecture",
      "citations": [
        {
          "title": "Security Design of the AWS Nitro System",
          "url": "https://docs.aws.amazon.com/whitepapers/latest/security-design-of-aws-nitro-system/the-components-of-the-nitro-system.html",
          "excerpts": [
            "Nitro System enables what is effectively “bare metal” performance by running nearly all\n the functionality of the virtualization system on the Nitro cards rather than on the host’s\n system mainboard CPU",
            "Within the Nitro Hypervisor, there is, by design, no networking stack, no general-purpose file system implementations, and no peripheral device driver support.",
            "The Nitro Hypervisor is designed to receive virtual machine management commands (start, stop, and so on) sent from the Nitro Controller, to partition memory ..."
          ]
        },
        {
          "title": "Amazon EBS volumes and NVMe - AWS Documentation",
          "url": "https://docs.aws.amazon.com/ebs/latest/userguide/nvme-ebs-volumes.html",
          "excerpts": [
            "Amazon EBS volumes are exposed as NVMe block devices on Amazon EC2 instances built on the [AWS Nitro System](https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html).",
            "To fully utilize the performance and capabilities of Amazon EBS volumes exposed as NVMe block devices, the EC2 instance must have the AWS NVMe driver installed.",
            "All current generation AWS Windows and Linux AMIs come with the AWS\nNVMe driver installed by default.",
            "The device names are `/dev/nvme0n1`, `/dev/nvme1n1`,\nand so on."
          ]
        }
      ],
      "reasoning": "The most directly supportive content states that the Nitro System runs most virtualization functionality on the Nitro cards rather than on the host CPU, enabling bare-metal-like performance. This precisely matches the idea of a lightweight hypervisor that only manages CPU and memory, while I/O, including networking and storage, is offloaded to specialized Nitro hardware. Additional excerpts reinforce this by noting there is no networking stack or general-purpose file system or device drivers in the hypervisor, which implies I/O is handled elsewhere, and that EBS volumes appear as NVMe devices with drivers installed to maximize performance. References to NVMe device naming and the need for the AWS NVMe driver to fully utilize EBS NVMe volumes further corroborate the architecture where I/O paths are offloaded to dedicated hardware and optimized software paths outside the hypervisor. Overall, the combination of statements about the hypervisor scope, I/O offload to Nitro Cards, and high-performance NVMe/EBS integration supports the field value's claim of near-bare-metal performance through hardware-assisted I/O and a minimal hypervisor workload. The management command handling excerpt provides context that the hypervisor mediates control without exposing the full I/O stack, reinforcing the separation between CPU/memory management and I/O offload. The ordering reflects the strongest direct alignment with offloaded I/O and bare-metal-like performance first, followed by specifics of NVMe/EBS integration and finally ancillary management aspects, which while related, are one step removed from the core offloading claim.",
      "confidence": "high"
    },
    {
      "field": "observability_for_low_latency_systems.recommended_technology",
      "citations": [
        {
          "title": "HdrHistogram: Better Latency Capture",
          "url": "http://psy-lob-saw.blogspot.com/2015/02/hdrhistogram-better-latency-capture.html",
          "excerpts": [
            "\n\n* Recording a value in the recorder is a wait free operation (on JDK8, can be lock free on older depending on the getAndAdd implementation for AtomicLongArray).",
            "HdrHistogram) lib to demonstrate measurement and report results, here are the relevant excerpts from the [Ping sampl"
          ]
        },
        {
          "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch ...",
          "url": "https://www.vldb.org/pvldb/vol12/p2195-masson.pdf",
          "excerpts": [
            "by C Masson · Cited by 99 — Merging two DDSketches is very fast—it takes around 10 microseconds or less to merge two sketches containing up to fifty million values each—depending on the ..."
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch."
          ]
        },
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency."
          ]
        },
        {
          "title": "HdrHistogram - DoubleHistogram.java (coordinated omission correction excerpts)",
          "url": "https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/DoubleHistogram.java",
          "excerpts": [
            "public DoubleHistogram copyCorrectedForCoordinatedOmission(final double expectedIntervalBetweenValueSamples) {",
            "To compensate for the loss of sampled values when a recorded value is larger than the expected\n\\* interval between value samples, the new histogram will include an auto-generated additional series of\n\\* decreasingly-smaller (down to the expectedIntervalBetweenValueSamples) value records for each count found\n\\* in the current histogram that is larger than the expectedIntervalBetweenValueSamp"
          ]
        }
      ],
      "reasoning": "HdrHistogram is repeatedly identified as a tool for capturing and analyzing latency with precise percentile information. Excerpts describe HdrHistogram as a method for better latency capture, including lock-free recording of values and its role in latency distribution measurements. Several sources discuss HdrHistogram in the context of coordinated omission and merging minimal latency sketches, which directly supports its use as a reliable observability technology for low-latency systems. While other sketches like DDSketch are mentioned as alternatives, the primary evidence centers on HdrHistogram's accuracy, mergeability, and usefulness in tail-latency assessment, which aligns with the field value of recommending HdrHistogram as the preferred technology. The excerpts that explicitly address coordinated omission corrections and performance implications further corroborate HdrHistogram as a robust choice for observability in low-latency environments. The included code excerpts demonstrate direct references to HdrHistogram usage in practice, reinforcing its relevance to the field value.",
      "confidence": "high"
    },
    {
      "field": "cloud_deployment_strategy.virtio_compatibility_conclusion",
      "citations": [
        {
          "title": "Security Design of the AWS Nitro System",
          "url": "https://docs.aws.amazon.com/whitepapers/latest/security-design-of-aws-nitro-system/the-components-of-the-nitro-system.html",
          "excerpts": [
            "Nitro System enables what is effectively “bare metal” performance by running nearly all\n the functionality of the virtualization system on the Nitro cards rather than on the host’s\n system mainboard CPU",
            "Within the Nitro Hypervisor, there is, by design, no networking stack, no general-purpose file system implementations, and no peripheral device driver support."
          ]
        },
        {
          "title": "Amazon EBS volumes and NVMe - AWS Documentation",
          "url": "https://docs.aws.amazon.com/ebs/latest/userguide/nvme-ebs-volumes.html",
          "excerpts": [
            "Amazon EBS volumes are exposed as NVMe block devices on Amazon EC2 instances built on the [AWS Nitro System](https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html).",
            "To fully utilize the performance and capabilities of Amazon EBS volumes exposed as NVMe block devices, the EC2 instance must have the AWS NVMe driver installed.",
            "All current generation AWS Windows and Linux AMIs come with the AWS\nNVMe driver installed by default.",
            "The device names are `/dev/nvme0n1`, `/dev/nvme1n1`,\nand so on."
          ]
        }
      ],
      "reasoning": "From the materials, Virtio-based designs must contend with how AWS implements high-performance I/O. The Nitro System runs much of virtualization functionality on dedicated Nitro hardware, effectively delivering near bare-metal performance, which implies that software paths relying on generic, guest-embedded Virtio devices may not achieve the same peak performance as hardware-assisted or vendor-accelerated interfaces. The Nitro Hypervisor expressly lacks a general networking stack and general-purpose drivers, illustrating a design that minimizes host-CPU involvement for I/O and indicates that relying on standard Virtio networking could be misaligned with the architecture used to reach peak throughput on AWS. In practice, the performance story for AWS storage and networking leans on NVMe-backed volumes via the AWS NVMe driver installed by default on current AMIs, and on ENA for high-performance networking implementations; these pathways are hardware- and driver-specific rather than universal Virtio paths. The NVMe-backed EBS volumes require the AWS NVMe driver to be installed to realize their performance, reinforcing that vendor-specific interfaces are central to achieving high I/O performance on AWS. Additionally, the device naming conventions and the explicit emphasis on NVMe in the AWS context underscore that primary I/O performance relies on NVMe/AWS-specific interfaces rather than standard Virtio-block or Virtio-net. Taken together, these sources support the conclusion that a Universal Virtio strategy, if used as the main I/O path for networking and storage, would struggle to reach modern AWS performance benchmarks, and achieving competitive performance would necessitate drivers and interfaces tailored to AWS hardware (e.g., ENA, NVMe) rather than a universal Virtio approach.",
      "confidence": "medium"
    },
    {
      "field": "go_to_market_and_packaging_strategy.details",
      "citations": [
        {
          "title": "AWS Marketplace AMI Product Checklist",
          "url": "https://docs.aws.amazon.com/marketplace/latest/userguide/aws-marketplace-listing-checklist.html",
          "excerpts": [
            "AMI is production-ready.",
            "AMI product checklist for AWS Marketplace",
            "Product usage is not restricted by time or any other measurements.",
            "AMI meets the mandatory [AMI-based product requirements for AWS Marketplac"
          ]
        },
        {
          "title": "Find an AMI that meets the requirements for your EC2 instance",
          "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/finding-an-ami.html",
          "excerpts": [
            "Find an AMI that meets your needs for launching an instance by searching for criteria such as the operating system that is included."
          ]
        },
        {
          "title": "Azure VM Image Builder pricing",
          "url": "https://azure.microsoft.com/en-us/pricing/details/image-builder/",
          "excerpts": [
            "Azure Image Builder is a free image building service that streamlines the creation, update, patch, management, and operation of Linux and Windows images."
          ]
        },
        {
          "title": "Offering virtual machine (VM) products",
          "url": "https://cloud.google.com/marketplace/docs/partners/vm",
          "excerpts": [
            "It must deploy software to, and run on, Compute Engine. It must use the Cloud Marketplace-hosted image with the attached Compute Engine license."
          ]
        },
        {
          "title": "Windows Virtual Machines Pricing",
          "url": "https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/",
          "excerpts": [
            "Buy Windows Virtual Machines today and pay only for what you use. Compare pricing options to choose the best plan for your business."
          ]
        },
        {
          "title": "Accept a private offer from a vendor | Google Cloud Marketplace",
          "url": "https://cloud.google.com/marketplace/docs/offers/accepting-private-offer",
          "excerpts": [
            "Accept a private offer · Click and open the link that the vendor sent you. · Verify the details of the offer, such as the pricing details and contract duration."
          ]
        },
        {
          "title": "Create a virtual machine offer on Azure Marketplace.",
          "url": "https://learn.microsoft.com/en-us/partner-center/marketplace-offers/azure-vm-offer-setup",
          "excerpts": [
            "Jan 31, 2025 — This article describes how to create an Azure virtual machine offer for Azure Marketplace. It addresses both Windows-based and Linux-based virtual machines."
          ]
        },
        {
          "title": "Create a private offer for a customer | Google Cloud Marketplace ...",
          "url": "https://cloud.google.com/marketplace/docs/partners/offers/create-private-offers",
          "excerpts": [
            "Your product must be integrated with and listed on Cloud Marketplace. You must have the customer's Cloud Billing account ID. If you don't have the ID, request ..."
          ]
        },
        {
          "title": "Network Hardware Compatibility List (HCL)",
          "url": "https://www.nvidia.com/en-eu/networking/ethernet-switching/hcl/",
          "excerpts": [
            "Cumulus Networks may decide to remove systems from the HCL. For example, HCL removal could occur in cases of obsolescence or a partner going out of business."
          ]
        }
      ],
      "reasoning": "The field value describes a comprehensive go-to-market packaging strategy centered on cloud marketplaces and multi-format delivery. Direct evidence comes from excerpts that specify that AMIs can be production-ready and that there are explicit product checklists and requirements for AWS Marketplace AMIs, which underpins a cloud-marketplace packaging approach. The existence of an AMI product checklist and the statement that an AMI is production-ready directly support the emphasis on AWS cloud-marketplace images and pre-configured, production-grade images. Additional excerpts discuss VM product offerings on cloud marketplaces (e.g., VM images for Compute Engine and Azure VM Image Builder contexts), which align with the multi-cloud packaging concept across AWS, GCP, and Azure. The inclusion of references to private offers and creating private offers for customers suggests mechanisms to monetize and customize marketplace offerings, further reinforcing a go-to-market strategy built around marketplace channels and partner ecosystems. Other excerpts touch on related packaging formats like virtual appliance images (OVA/ISO) and the notion of OEM or HCL considerations, which support the on-premises and hardware-aligned components of the strategy (reference hardware and OEM programs). Taken together, these excerpts map onto the field value's key components: production-ready cloud images (AMIs, VM images), marketplace enablement, on-premise packaging formats (OVA/ISO), bare-metal deployment considerations (PXE), and OEM/HCL governance, forming a cohesive multi-channel packaging strategy.",
      "confidence": "high"
    },
    {
      "field": "project_risk_register.likelihood",
      "citations": [
        {
          "title": "AMC 20-193 and CAST-32A Overview",
          "url": "https://www.rapitasystems.com/amc-20-193",
          "excerpts": [
            "The FAA™ and EASA® have been developing guidelines for development of multi-core systems for DO-178 aerospace projects. These include superseded documents ..."
          ]
        },
        {
          "title": "DO-330 / Tool Qualification Context with Ferrocene",
          "url": "https://afuzion.com/do-330-introduction-tool-qualification/",
          "excerpts": [
            "This DO-330 whitepaper describes the five tool categories versus criticality levels (DALs) and how to truly qualify avionics software tools."
          ]
        },
        {
          "title": "DO-330/ED-215 Software tool qualification considerations",
          "url": "https://ldra.com/do-330/",
          "excerpts": [
            "Civil aviation certification authorities require tool qualification on a project-by-project basis. The responsibility for showing the suitability of any tool falls on to the organization developing a civil aviation application. However, they can make use of Tool Qualification Support Packs (TQSP) provided by the vendor."
          ]
        },
        {
          "title": "[PDF] Certification Authorities Software Team (CAST) - CAST-32A",
          "url": "https://www.cast32a.com/files/cast-32a.pdf",
          "excerpts": [
            "MCP\\_Resource\\_Usage\\_3** : The applicant has identified the interference channels that\n\ncould permit interference to affect the software applications hosted on the MCP cores, and\n\nhas verified the applicant’s chosen means of mitigation of the interfere"
          ]
        },
        {
          "title": "AC 20-193 - Use of Multi-Core Processors",
          "url": "https://www.faa.gov/regulations_policies/advisory_circulars/index.cfm/go/document.information/documentID/1036408",
          "excerpts": [
            "This advisory circular (AC) describes an acceptable means of compliance with the applicable airworthiness regulations for multi-core processors (MCP) contained ..."
          ]
        },
        {
          "title": "AMC 20-193 Use of multi-core processors",
          "url": "https://www.easa.europa.eu/sites/default/files/dfu/annex_i_to_ed_decision_2022-001-r_amc_20-193_use_of_multi-core_processors_mcps.pdf",
          "excerpts": [
            "Applicants may verify separately on the MCP any software component or set of requirements for which the interference identified in the interference analysis is ...",
            "AMC 20-193 Use of multi-core processors. Purpose. 1.1. This AMC describes an acceptable means, but not the only means, for showing compliance.",
            "**AMC 20-193**"
          ]
        }
      ],
      "reasoning": "The excerpts collectively emphasize structured certification pathways, formal analysis, and tool qualification requirements that constrain project schedules and budget. For example, CAST-32A discussions focus on identifying interference channels and ensuring mitigation, which is central to demonstrating safety and meeting DO-178C-like expectations in multicore environments. This signals non-trivial risk due to the need for thorough analysis and verification to satisfy certification authorities. Tool qualification contexts describe five levels and stepwise criteria for validating toolchains (including compilers and UML/code-generation tools), which directly affect project planning, cost, and schedule—key drivers of risk. The DO-330 context highlights the necessity of qualification processes across toolchains and the levels of rigor required, implying ongoing, perhaps extended, validation efforts. References to CAST-32A and AMC 20-193 together map a regulatory landscape where achieving compliance is a substantial effort with uncertain durations and potential rework, contributing to a medium risk outlook for likelihood. Additional FAA/EASA guidance on multi-core processor use reinforces that regulatory acceptability is a gatekeeper, not a trivial hurdle, which further supports a cautious, mid-range risk assessment. In contrast, excerpts that discuss performance gains or generic architectural approaches (without tying them to certification evidence) provide supportive context but do not directly escalate or decrease risk, making them less central to the likelihood assessment.\n",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.0",
      "citations": [
        {
          "title": "Intel® Ethernet Network Adapter E810-2CQDA2",
          "url": "https://cdrdv2-public.intel.com/639389/Intel%20Ethernet%20Network%20Adapter%20E810-2CQDA2%20Product%20Brief.pdf",
          "excerpts": [
            "• Control Queues (a.k.a. Admin Queues). - Mailbox Queues for PF-VF and driver-driver. - Admin Queues for Software-Firmware control flows. - Sideband Queues for ..."
          ]
        },
        {
          "title": "Intel E810 Feature Summary and Related NVMe/ConnectX Details",
          "url": "https://cdrdv2-public.intel.com/630155/630155_Intel%20Ethernet%20Controller%20E810%20Feature%20Summary_rev4_6.pdf",
          "excerpts": [
            "Intel® Ethernet Controller E810",
            "Intel® Ethernet Controller E810",
            "The following table lists the driver, firmware, and package versions recommended for use with the supported DPDK version. Table 9. DPDK ...",
            "June 2025",
            "Feature Support Matrix"
          ]
        },
        {
          "title": "Linux Base Driver for the Intel(R) Ethernet Controller 800 Series",
          "url": "https://docs.kernel.org/networking/device_drivers/ethernet/intel/ice.html",
          "excerpts": [
            "In a virtualized environment, on Intel(R) Ethernet Network Adapters that support SR-IOV, the virtual function (VF) may be subject to malicious behavior."
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir"
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking."
          ]
        },
        {
          "title": "Step-by-Step Guide: Building High-Performance Software ...",
          "url": "https://medium.com/@israelgeoffrey/step-by-step-guide-building-high-performance-software-defined-storage-with-spdk-nvme-of-and-spdk-743671404156",
          "excerpts": [
            "Let's begin by deploying the SPDK NVMe-oF target inside a container. This will serve as our high-performance storage backend, enabling ultra-low ..."
          ]
        },
        {
          "title": "[PDF] Intel® Ethernet Controller E800 Series Dynamic Device ...",
          "url": "https://cdrdv2-public.intel.com/618651/618651_Intel%20Ethernet%20Controller%20E810%20DDP%20for%20Comms%20Technology%20Guide.pdf",
          "excerpts": [
            "Updated E810 firmware version from 1.5.4.5 to 1.5.5.6. ... The specific DDP package requires certain firmware and DPDK versions and 800 Series."
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "rs. #### Configuration Details\n\nTest by Intel as of 10/22/2022. System: 1-node, 2x Intel® Xeon® Platinum 8480+ Processor (56 cores, HT=On, Turbo=ON),\nTotal Memory 1024 GB (16 slots/ 64GB/ 4800 MHz), BIOS:3A05 (ucode:0x2b000070),\nStorage: 24x Intel® Optane™ SSD 800GB P5800X."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpt describes the Intel Ethernet Network Adapter E810-2CQDA2 product, including device capabilities and management interfaces, which aligns with the field value's emphasis on the E810 as a premier NIC choice. Supporting details from related excerpts confirm that the E810 family uses the ice driver with DPDK, and that it supports virtualization features like SR-IOV and many VFs, which are central to the proposed reference implementation and its vendor selection. Additional excerpts corroborate the E810's ecosystem support: a feature summary confirms advanced offloads and virtualization capabilities; a companion excerpt enumerates DPDK-related driver and firmware compatibility, which is critical for a reference stack that relies on stable, well-supported software. Other excerpts discuss SR-IOV considerations in Linux networking contexts, reinforcing the rationale for selecting a platform with solid virtualization support. Finally, broader high-performance networking context pieces provide general rationale for choosing a widely adopted, actively maintained NIC ecosystem (E810/ice) as a reference, especially when aiming for scalable, SR-IOV-enabled deployments and AF_XDP-based paths, which are part of the field value's justification.",
      "confidence": "high"
    },
    {
      "field": "cloud_deployment_strategy.high_performance_interfaces",
      "citations": [
        {
          "title": "Amazon EBS volumes and NVMe - AWS Documentation",
          "url": "https://docs.aws.amazon.com/ebs/latest/userguide/nvme-ebs-volumes.html",
          "excerpts": [
            "Amazon EBS volumes are exposed as NVMe block devices on Amazon EC2 instances built on the [AWS Nitro System](https://docs.aws.amazon.com/ec2/latest/instancetypes/ec2-nitro-instances.html).",
            "To fully utilize the performance and capabilities of Amazon EBS volumes exposed as NVMe block devices, the EC2 instance must have the AWS NVMe driver installed.",
            "All current generation AWS Windows and Linux AMIs come with the AWS\nNVMe driver installed by default.",
            "The device names are `/dev/nvme0n1`, `/dev/nvme1n1`,\nand so on."
          ]
        },
        {
          "title": "Security Design of the AWS Nitro System",
          "url": "https://docs.aws.amazon.com/whitepapers/latest/security-design-of-aws-nitro-system/the-components-of-the-nitro-system.html",
          "excerpts": [
            "Within the Nitro Hypervisor, there is, by design, no networking stack, no general-purpose file system implementations, and no peripheral device driver support.",
            "Nitro System enables what is effectively “bare metal” performance by running nearly all\n the functionality of the virtualization system on the Nitro cards rather than on the host’s\n system mainboard CPU"
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a cloud deployment strategy that relies on high-performance interfaces, specifically networking via an Elastic Network Adapter (ENA) using SR-IOV for very high speeds, and storage via EBS volumes presented as NVMe block devices also using SR-IOV, with a custom OS needing appropriate ENA and NVMe drivers. The excerpts substantiate this in several ways: excerpts detailing Amazon EBS NVMe volumes show that EBS is exposed as NVMe block devices on Nitro-based instances and that installing the AWS NVMe driver is necessary to unlock performance, directly supporting the claim that an OS must include specific NVMe drivers to reach high storage throughput and low latency. Additional excerpts note that the Nitro System includes a virtualization design with limited host networking components, implying that achieving low-latency networking paths requires deliberate architectural choices (e.g., relying on SR-IOV-capable interfaces and dedicated backends). The device naming conventions for NVMe devices further corroborate the presence and management of SR-IOV-backed NVMe storage paths. Together, these excerpts map onto the field value's emphasis on the combination of SR-IOV-enabled networking and NVMe-backed storage, plus the necessity of corresponding drivers to achieve high performance. The excerpt mentioning the Nitro Hypervisor's lack of a networking stack highlights a critical design constraint that would shape how a high-performance interface could be implemented and what drivers or interfaces would be required in the OS. While some excerpts provide broader architectural context (e.g., driver availability or system design trade-offs), the strongest, most direct support concerns the NVMe/EBS path and the driver requirement, which are central to the high-performance interfaces described in the field value.",
      "confidence": "medium"
    },
    {
      "field": "observability_for_low_latency_systems.observability_layer",
      "citations": [
        {
          "title": "HdrHistogram: Better Latency Capture",
          "url": "http://psy-lob-saw.blogspot.com/2015/02/hdrhistogram-better-latency-capture.html",
          "excerpts": [
            "\n\n* Recording a value in the recorder is a wait free operation (on JDK8, can be lock free on older depending on the getAndAdd implementation for AtomicLongArray).",
            "HdrHistogram) lib to demonstrate measurement and report results, here are the relevant excerpts from the [Ping sampl"
          ]
        },
        {
          "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch ...",
          "url": "https://www.vldb.org/pvldb/vol12/p2195-masson.pdf",
          "excerpts": [
            "by C Masson · Cited by 99 — Merging two DDSketches is very fast—it takes around 10 microseconds or less to merge two sketches containing up to fifty million values each—depending on the ..."
          ]
        },
        {
          "title": "HdrHistogram - DoubleHistogram.java (coordinated omission correction excerpts)",
          "url": "https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/DoubleHistogram.java",
          "excerpts": [
            "public DoubleHistogram copyCorrectedForCoordinatedOmission(final double expectedIntervalBetweenValueSamples) {"
          ]
        },
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "Apr 22, 2021 — Coordinated omission is a term coined by Gil Tene to describe the phenomenon when the measuring system inadvertently coordinates with the system being measured.",
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency."
          ]
        },
        {
          "title": "Clarify whether logging to standard output is blocking #668 - GitHub",
          "url": "https://github.com/tokio-rs/tracing/issues/668",
          "excerpts": [
            "Non-blocking logging is theoretically superior, but the difference is probably only meaningful in particularly performance-sensitive code."
          ]
        },
        {
          "title": "OpenTelemetry Sampling: head-based and tail-based",
          "url": "https://uptrace.dev/opentelemetry/sampling",
          "excerpts": [
            "Head-based sampling can't sample failed or unusually long operations, because that information is only available at the end of a trace. With tail-based sampling we delay the sampling decision until all spans of a trace are available which enables better sampling decisions based on all data from the trace."
          ]
        },
        {
          "title": "Sampling",
          "url": "https://opentelemetry.io/docs/concepts/sampling/",
          "excerpts": [
            "Sep 17, 2024 — Tail sampling is where the decision to sample a trace takes place by considering all or most of the spans within the trace. Tail Sampling gives ..."
          ]
        },
        {
          "title": "Sampling in OpenTelemetry: A Beginner's Guide",
          "url": "https://betterstack.com/community/guides/observability/opentelemetry-sampling/",
          "excerpts": [
            "Feb 26, 2025 — Sampling is the process of selectively capturing and analyzing a subset of traces instead of recording every request that flows through a system."
          ]
        },
        {
          "title": "Systemd journald performance discussions (GH issue 2815)",
          "url": "https://github.com/systemd/systemd/issues/2815",
          "excerpts": [
            "Performance issues occur when a high throughput workload emits a large number of logs."
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch.",
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds)."
          ]
        },
        {
          "title": "Sockperf – UDP/TCP Latency and Throughput Benchmarking Tool",
          "url": "https://docs.nvidia.com/networking/display/VMAv9840/Appendix+Sockperf+%E2%80%93+UDP/TCP+Latency+and+Throughput+Benchmarking+Tool",
          "excerpts": [
            "Sockperf can provide a full log of all packets' tx and rx times by dumping all the data that it uses for calculating percentiles and building ..."
          ]
        },
        {
          "title": "Insights from Paper—Google Dapper: a Large-Scale Distributed ...",
          "url": "https://medium.com/100paperschallenge/insights-from-paper-google-dapper-a-large-scale-distributed-systems-tracing-infrastructure-1f5a448ca000",
          "excerpts": [
            "Low overhead: The tracing system should have a negligible performance impact on running services. Application-level transparency: Programmers ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value refers to telemetry primitives used for latency measurement within an observability layer. Excerpts that describe concrete latency measurement techniques (_hdrhistogram_latency_capture_ and related coordinated-omission corrections) directly support this field by outlining how latency data is quantified and corrected to produce reliable telemetry. For instance, the note that recording a value can be a wait-free operation and the discussion of HDR-Histogram's capabilities provide concrete primitives used for latency telemetry. Further, excerpts detailing fast, mergeable quantile sketches (DDSketch) extend the same telemetry narrative by offering alternatives to quantify latency distributions efficiently. Discussions on coordinated omission explain measurement pitfalls that telemetry must address to remain accurate in low-latency contexts. Content addressing tracing/telemetry frameworks (OpenTelemetry sampling discussions, tail vs head sampling, Dapper-like tracing insights) anchors the observability layer in real-world telemetry practices, ensuring that latency measurements translate into meaningful observability signals. Taken together, these excerpts establish the core telemetry primitives (latency capture, quantile sketches, measurement correctness, and tracing observability) that underpin the requested field value. The most direct support comes from materials explicitly about latency capture and measurement techniques, with secondary support from tracing/telemetry discussions that situate latency telemetry in broader observability practices.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.4",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa"
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload."
          ]
        }
      ],
      "reasoning": "The field value emphasizes a high-performance NVMe SSD (PCIe 4.0) used in datacenter workloads, with SPDK-backed pathways and high IOPS potential. Excerpt describing SPDK/NVMe performance demonstrates key claims: SPDK provides a high-performance NVMe driver with scalable IOPS and low-latency characteristics, and performance benchmarks show substantial IOPS improvements and scalable I/O paths when dedicating cores to queue pairs. Additional excerpts illustrate benchmarking methodologies and tooling (perf, wrk2) and confirm that SPDK-based NVMe paths can achieve very high throughput in practical workloads. Together, these pieces support the core selection rationale: choose a high-performance NVMe SSD and leverage SPDK-backed user-space I/O to maximize capacity-sensitive, latency-critical workloads in a datacenter setting. While the exact Micron 9400 model is not named, the referenced NVMe/SPDK evidence underpins the feasibility and expected performance profile of such a drive in this context.",
      "confidence": "medium"
    },
    {
      "field": "storage_stack_recommendation.performance_profile",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O."
          ]
        },
        {
          "title": "SPDK: Block Device User Guide",
          "url": "https://spdk.io/doc/bdev.html",
          "excerpts": [
            "All of the latency values are measured in microseconds. Example command: rpc ... The Virtio-Block driver allows creating SPDK bdevs from Virtio-Block devices."
          ]
        },
        {
          "title": "[PDF] DPDK Vhost/Virtio Performance Report Release 21.11",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_21_11_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "The primary audience for this test report is architects and engineers implementing the Data Plane. Development Kit (DPDK)."
          ]
        },
        {
          "title": "SPDK vhost Performance (SNIA SDC 2018)",
          "url": "https://www.snia.org/sites/default/files/SDCEMEA/2018/Presentations/Accelerating-VM-Access-with-Storage-Performance-Developer-Kit-SNIA-SDC-EMEA-2018.pdf",
          "excerpts": [
            "SPDK up to 3x better efficiency and latency",
            "48 VMs: vhost-scsi performance (SPDK vs. Kernel)",
            "Technology claims are based on comparisons of latency, density and write cycling metrics amongst memory technologies recorded on published specifications of in ...",
            "\n0\n\n25\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n10,000\n\n**HDD**\n\n**\\+SAS/S**\n\n**ATA**\n\n**SSD**",
            "\n\n_**Scalable and Efficient Software Ingredients**_ •\n\nUser space, lockless, polled-mode components",
            "SPDK wants to use userspace polled mode drivers",
            "SPDK VHOST Architecture"
          ]
        },
        {
          "title": "DPDK Vhost/Virtio Performance Report Release 23.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_23_03_Intel_virtio_performance_report.pdf",
          "excerpts": [
            "Apr 20, 2023 — This report provides packet processing performance testing for DPDK on Intel architecture, using a Vhost/Virtio setup with 100G traffic, and is ..."
          ]
        },
        {
          "title": "62. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides/nics/virtio.html",
          "excerpts": [
            "In the Data Plane Development Kit (DPDK), we provide a virtio Poll Mode Driver (PMD) as a software solution, comparing to SRIOV hardware solution, for fast ..."
          ]
        }
      ],
      "reasoning": "The target finegrained field value asserts that SPDK vhost-user dramatically outperforms kernel-based vhost, with benchmarked IOPS in the millions for 4K random writes, substantially lower single-request latency (20 µs vs 100 µs for Virtio), and up to 3.2x better IOPS for 4K random reads with SPDK vhost-scsi, plus lower guest CPU utilization due to avoiding VM exits and kernel context switches. Direct excerpts describe: SPDK vhost as a Vhost-user back-end server and its high-performance characteristics, including poll-mode operation that bypasses I/O completion interrupts to reduce guest overhead; the front-end/back-end model and the use of SPDK backends to achieve very high IOPS and reduced latency; and performance reports and summaries from SPDK vhost testing showing improved throughput and latency relative to kernel-based paths. Specifically, one excerpt notes that SPDK vhost is a Vhost-user back-end server and that high performance is achieved via a poll-mode Virtio driver that suppresses interrupts, which aligns with the claim of dramatically higher performance. Additional excerpts highlight SPDK's role in achieving high IOPS per core (e.g., >10M IOPS per core in SPDK literature), SPDK's Virtio-Block driver enabling SPDK bdevs from Virtio-Block devices, and SPDK vhost performance reports indicating SPDK vhost performance advantages in real benchmarks. Collectively, these excerpts substantiate the core comparison (SPDK vhost-user vs kernel vhost) and the associated performance numbers and mechanics (polling, user-space backends, reduced VM exits) implied by the field value. The presence of SPDK vhost testing guidance and concrete performance claims in performance reports further reinforces the strong supportive relationship between SPDK vhost-user and superior performance over kernel-based implementations.",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases.0.performance_slos",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value asserts that latency is the paramount metric, with targets in the microsecond to nanosecond range and specific SLOs such as P99 latencies in the 75–157 microseconds range for cloud-native exchanges, plus ~200 microseconds for 90th-percentile round-trips. The most relevant excerpt explicitly states latency figures in the microsecond range for a low-latency cloud-native exchange, including P50 in the 55–124 microseconds and P99 in the 75–157 microseconds band. This directly substantiates the claimed latency targets and demonstrates real-world values in similar systems. The next most relevant excerpt discusses performance improvements (e.g., p99.9 latency improvements in transactional workloads) and provides context that latency is a primary concern and that notable gains are achievable in tail latency with specific architectures, aligning with the idea of tight tail latency control and predictable performance. The remaining excerpt, while about high-frequency trading, reinforces the centrality of latency but does not provide concrete microsecond-scale figures or explicit SLOs, making it less directly supportive of the stated numeric targets but still thematically relevant as evidence that latency is critical in fast systems.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.1",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread"
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        }
      ],
      "reasoning": "- The most relevant material describes high-performance I/O backends and network acceleration paths that are central to evaluating NICs for performance-sensitive workloads. One excerpt explains that vhost-user backends (used by SPDK) enable high-performance user-space networking paths by connecting Virtio devices to user-space backends, which is directly aligned with evaluating NICs for maximum throughput and offload capabilities. This supports the idea that a fast, feature-rich NIC (such as ConnectX-5/6/7 with mlx5) can be leveraged through mature user-space I/O backends to achieve high IOPS and low overhead, a core argument in selecting high-end NICs. Another excerpt notes that a poll-mode Virtio driver can bypass I/O interrupts to maximize path efficiency, which reinforces the emphasis on performance-oriented NIC configurations and driver models that would be relevant when choosing ConnectX-class hardware with mlx5 support for high-throughput networking. Collectively, these excerpts support the concept of using robust, high-performance NICs in conjunction with mature, high-speed I/O stacks (SPDK/vhost-user, AF_XDP) to unlock maximum networking value, which is central to the field value's emphasis on ConnectX-5/6/7 with mlx5 and AF_XDP/SR-IOV capabilities. - AF_XDP is explicitly mentioned in the excerpt describing modern performance patterns and user-space I/O solutions, linking directly to the field value's claim that AF_XDP Zero Copy is a supported and mature path. This aligns with selecting a NIC and driver setup that capitalizes on AF_XDP capabilities for low-latency, high-throughput networking. - Excerpts that discuss SPDK-backed block and network paths via vhost-user provide context about the kinds of backend architectures that pair well with high-performance NICs; while not naming ConnectX, they illustrate the architectural ecosystem (SPDK, DPDK, vhost-user) that would be used to realize the performance goals associated with mlx5-based NICs. - Some context on general Virtio backplanes, GPU offloads, and traditional driver considerations adds useful background for evaluating NIC platforms in a virtualization-heavy environment, but is less directly tied to ConnectX-specific features. It remains supportive in the sense that a stable Virtio model and backends ease driver burden when using advanced NICs, though not a direct claim about mlx5. - Additional excerpts about broader NIC ecosystems (e.g., Intel E810) provide peripheral corroboration that high-performance NICs and their drivers are a central concern in modern infrastructure, but they do not directly substantiate ConnectX mlx5 AF_XDP specifics. Therefore, the strongest direct alignment comes from discussions of vhost-user/SPDK and AF_XDP, with secondary relevance from Virtio-based backends and general NIC performance considerations. ",
      "confidence": "medium"
    },
    {
      "field": "observability_for_low_latency_systems.role_in_system",
      "citations": [
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch.",
            "Code here:  <https://github.com/DataDog/sketches-go>  <https://github.com/DataDog/sketches-py>  <https://github.com/DataDog/sketches-java>"
          ]
        },
        {
          "title": "HdrHistogram: Better Latency Capture",
          "url": "http://psy-lob-saw.blogspot.com/2015/02/hdrhistogram-better-latency-capture.html",
          "excerpts": [
            "\n\n* Recording a value in the recorder is a wait free operation (on JDK8, can be lock free on older depending on the getAndAdd implementation for AtomicLongArray).",
            "HdrHistogram) lib to demonstrate measurement and report results, here are the relevant excerpts from the [Ping sampl"
          ]
        },
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency."
          ]
        },
        {
          "title": "HdrHistogram - DoubleHistogram.java (coordinated omission correction excerpts)",
          "url": "https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/DoubleHistogram.java",
          "excerpts": [
            "public DoubleHistogram copyCorrectedForCoordinatedOmission(final double expectedIntervalBetweenValueSamples) {",
            "To compensate for the loss of sampled values when a recorded value is larger than the expected\n\\* interval between value samples, the new histogram will include an auto-generated additional series of\n\\* decreasingly-smaller (down to the expectedIntervalBetweenValueSamples) value records for each count found\n\\* in the current histogram that is larger than the expectedIntervalBetweenValueSamp"
          ]
        }
      ],
      "reasoning": "HdrHistogram is described as a tool that provides accurate latency measurements with fast merge and low overhead. Excerpts stating that HDR-Histogram merges are accurate and that it is well-suited for latency measurement support its role in capturing full latency distributions without introducing significant contention. Other excerpts emphasize that HDR-Histogram offers fast, lock-free characteristics and timing-focused capabilities (e.g., per-thread or single-writer recorders), which align with a shard-per-core Rust Apex design where telemetry collection should not become a contention source. Additional excerpts discuss coordinated-omission corrections in HDRHistogram code and analyses, which are critical for unbiased tail latency measurements (p99.9, p99.999) in a microsecond-scheduled system, reinforcing its central role in validating SLOs and post-mortem analysis. The supporting excerpts collectively establish HDRHistogram as the mechanism that yields high-fidelity, low-overhead latency data necessary for performance guarantees and failure analysis in a low-latency, shard-per-core architecture.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.3",
      "citations": [
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload."
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa"
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a high-end NVMe SSD (Kioxia CM7 Series) with PCIe 5.0 and NVMe 2.0 compatibility, capable of very high random read IOPS (up to 2.7M), plus endurance options. The excerpts most directly support this by outlining how SPDK NVMe driver enables high-performance, user-space I/O paths and scalability for NVMe storage, including concrete IOPS figures and how the SPDK-based paths avoid kernel overhead to achieve near-linear scaling with dedicated cores. Specifically:\n- One excerpt discusses SPDK NVMe driver in detail and provides the context that SPDK's toolset enables high-performance NVMe workloads, reinforcing that there exists a mature, performant user-space NVMe path suitable for PCIe 5.0 / NVMe 2.0-class devices.\n- Another excerpt presents performance benchmarks showing notable IOPS figures and the effect of dedicated threads on throughput, which aligns with the expected capabilities of enterprise NVMe drives like the CM7 Series when paired with SPDK.\n- Additional excerpts describe SPDK's ability to scale and provide low-latency, high-throughput storage paths (e.g., non-locking I/O paths and linear scaling per thread), which supports the notion of achieving very high IOPS and sustained performance with the CM7's capabilities.\n- A related excerpt notes that SPDK-based backends (vhost-user) are commonly used for storage offload in virtualized environments, which reinforces the practical deployment scenario for a PCIe Gen5 NVMe device in server-grade workloads.\nOverall, the collection of excerpts coherently supports the idea that a PCIe 5.0 NVMe SSD like the CM7, when used with SPDK NVMe driver paths, can deliver high IOPS and scalable performance in a storage stack, which is the core claim of the fine-grained field value.",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases.0.use_case_name",
      "citations": [
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpt is the one plainly naming high-frequency trading, which explicitly matches the target use case. The next most relevant excerpt discusses low-latency, cloud-native exchange architectures and measured latencies, which are central to HFT requirements since reducing end-to-end latency is critical for success in high-frequency trading environments. The third excerpt, while focused on latency improvements for financial transactions, describes latency improvements in payments and related transactions rather than HFT specifically, but still provides evidence of latency-sensitive performance benefits in financial systems that are conceptually aligned with HFT goals. Collectively, these excerpts support the idea that high-speed, low-latency trading systems are central to the use case and that latency optimization is a core design driver.",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases.0.workload_description",
      "citations": [
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "The first excerpt directly defines high-frequency trading as an algorithmic, automated trading system with high speeds, high turnover, and a focus on rapid processing from data intake to order placement, which aligns with the described hyper-optimized, latency-critical workflow. The second excerpt describes a concrete implementation of ultra-low latency in a cloud-native exchange, citing precise latency targets in microseconds and design emphasis on reducing latency, which reinforces the notion of a latency-critical end-to-end system from data to order dispatch. The third excerpt discusses substantial latency improvements in financial transactions (e.g., payment and order-related workloads) under load, illustrating how latency-sensitive workloads can achieve large performance gains through architectural choices, corroborating the idea of optimizing the end-to-end path from data ingestion to action in financial systems. Taken together, these excerpts substantiate the characterization of HFT as an automated, end-to-end, latency-critical workflow from market data reception to order issuance and execution, with concrete evidence of ultra-low latency targets and substantial latency improvements in related financial workloads.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.0.workload_name",
      "citations": [
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        }
      ],
      "reasoning": "The finegrained field value represents a Kafka-like streaming system, which is a latency- and throughput-sensitive, long-running streaming workload in a distributed data processing context. Excerpts that discuss high-throughput, low-latency scheduling, and kernel-bypass or user-space I/O architectures are most relevant because they directly inform how a streaming system might achieve low tail latency and high per-core throughput under heavy, continuous workloads. The Shenango-related excerpts describe a latency-focused scheduling approach capable of sustaining very high request rates (over five million requests per second) with ultra-low median and tail latencies (median around 37 microseconds and 99.9th percentile around 93 microseconds), which maps well to the requirements of a streaming system that must handle many small, time-sensitive messages with strict latency targets. Additionally, the Perséphone comparison notes how centralized scheduling can improve long-tail performance and contrasts it with other scheduling strategies, which is relevant for designing a streaming system's scheduler to minimize tail latency under bursty traffic. The SPDK/vhost-user excerpts, while primarily about I/O offloads and high IOPS storage paths, provide context on achieving high throughput and low overhead through user-space/back-end implementations, which is relevant for streaming pipelines that rely on efficient I/O to/from storage or network interfaces. Taken together, these excerpts support the idea that a Kafka-like streaming workload benefits from: (a) ultra-low tail latency via scheduler design and microsecond-scale scheduling decisions, (b) very high throughput via kernel-bypass or user-space I/O paths, and (c) careful trade-offs between centralized vs. distributed scheduling in order to optimize short and long-tail latency under continuous load.",
      "confidence": "medium"
    },
    {
      "field": "observability_for_low_latency_systems.performance_characteristics",
      "citations": [
        {
          "title": "HdrHistogram: Better Latency Capture",
          "url": "http://psy-lob-saw.blogspot.com/2015/02/hdrhistogram-better-latency-capture.html",
          "excerpts": [
            "\n\n* Recording a value in the recorder is a wait free operation (on JDK8, can be lock free on older depending on the getAndAdd implementation for AtomicLongArray).",
            "HdrHistogram) lib to demonstrate measurement and report results, here are the relevant excerpts from the [Ping sampl"
          ]
        },
        {
          "title": "On Coordinated Omission",
          "url": "https://www.scylladb.com/2021/04/22/on-coordinated-omission/",
          "excerpts": [
            "As in [*HdrHistogram/recordSingleValueWithExpectedInterval*](https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/AbstractHistogram.java). By doing that it compensates for the missed calls in the resulting latency distribution through simulation, by adding a number of expected requests with some expected latency."
          ]
        },
        {
          "title": "HdrHistogram - DoubleHistogram.java (coordinated omission correction excerpts)",
          "url": "https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/DoubleHistogram.java",
          "excerpts": [
            "public DoubleHistogram copyCorrectedForCoordinatedOmission(final double expectedIntervalBetweenValueSamples) {",
            "To compensate for the loss of sampled values when a recorded value is larger than the expected\n\\* interval between value samples, the new histogram will include an auto-generated additional series of\n\\* decreasingly-smaller (down to the expectedIntervalBetweenValueSamples) value records for each count found\n\\* in the current histogram that is larger than the expectedIntervalBetweenValueSamp"
          ]
        },
        {
          "title": "Hacker News discussion on HDR-Histogram and DDSketch",
          "url": "https://news.ycombinator.com/item?id=20829404",
          "excerpts": [
            "(1) HDR-Histogram merges are 100% accurate and very fast (few microseconds).",
            "HDR is great for the use-case when you can bound your range beforehand and merging is not a requirement, but those were also the reasons we needed to develop DDSketch."
          ]
        },
        {
          "title": "DDSketch: A Fast and Fully-Mergeable Quantile Sketch ...",
          "url": "https://www.vldb.org/pvldb/vol12/p2195-masson.pdf",
          "excerpts": [
            "by C Masson · Cited by 99 — Merging two DDSketches is very fast—it takes around 10 microseconds or less to merge two sketches containing up to fifty million values each—depending on the ..."
          ]
        },
        {
          "title": "Clarify whether logging to standard output is blocking #668 - GitHub",
          "url": "https://github.com/tokio-rs/tracing/issues/668",
          "excerpts": [
            "Non-blocking logging is theoretically superior, but the difference is probably only meaningful in particularly performance-sensitive code."
          ]
        },
        {
          "title": "Systemd journald performance discussions (GH issue 2815)",
          "url": "https://github.com/systemd/systemd/issues/2815",
          "excerpts": [
            "Performance issues occur when a high throughput workload emits a large number of logs."
          ]
        }
      ],
      "reasoning": "The most directly supportive content describes HdrHistogram's recording behavior and latency capture. For instance, one excerpt notes that recording a value in the recorder can be a wait-free operation, which aligns with the claim of constant-time recording performance. Another excerpt explicitly discusses HdrHistogram in the context of latency capture, reinforcing its role as a measurement tool for latency distributions. Additional entries discuss coordinated-omission corrections and merge behavior for HdrHistogram data, which strengthens the understanding that HdrHistogram is used for precise latency measurement with careful handling of measurement biases. There are excerpts mentioning the merging capabilities and small, fast operations surrounding HDR-like sketches, which, while focusing on related data sketch concepts, still situate HDRHistogram within a family of fast latency-measurement tools and justify its efficiency claims. Other references highlight HDR-Histogram discussions in forums and documentation, which corroborate practical usage and performance characteristics in real-world scenarios. Taken together, these excerpts substantiate the field value's core assertions about constant-time recording, minimal allocations, fixed memory footprint for a wide timing range with fixed precision, and the potential for highly compressed logging, while also situating HDRHistogram among related measurement techniques and considerations such as coordinated omission corrections. The combination of explicit statements about wait-free recording, and discussions around latency measurement quality and efficiency, provides coherent and multi-faceted support for the finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases.0.market_analysis",
      "citations": [
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on the economic scale and incentive to pursue ultra-low latency in high-frequency trading, including market value and the premium paid for latency advantages. The first excerpt defines high-frequency trading, establishing that it is an algorithmic, very fast trading approach driven by speed and turnover, which underpins why firms invest in low-latency capabilities. The second excerpt provides concrete latency targets (P50 and P99) for low-latency cloud-native exchanges, illustrating the performance envelope and the operational value of achieving sub-millisecond responsiveness in trading systems. The third excerpt presents evidence of substantial latency improvements in specific transactions, underscoring how latency reductions translate into performance gains and differentiated customer value, which supports the notion that firms would pay for latency advantages and invest in co-located, low-latency data paths. While none of the excerpts explicitly state the market size or willingness-to-pay figures, they collectively establish the mechanism by which latency advantages create economic value and competitive differentiation, which is the core of the field value's claim.",
      "confidence": "medium"
    },
    {
      "field": "performance_analysis_for_target_workloads.0.expected_performance_gains",
      "citations": [
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        }
      ],
      "reasoning": "The most directly relevant material discusses latency-aware scheduling and high-throughput, low-latency behavior in user-space and partitioned/core-per-core designs. One excerpt notes that a scheduling approach reserves a core for short requests and prioritizes them, yielding substantial reductions in slowdown and improved throughput under tight latency constraints. Another excerpt compares centralized scheduling favorably for long requests, which helps justify overall system performance improvements when latency guarantees are important. Additional excerpts quantify extremely high request rates and tight tail latency using shard-per-core or similar architectures, directly aligning with the field's assertion of per-core throughput gains and dramatic tail-latency reductions. Furthermore, several excerpts describe a user-space I/O path (SPDK vhost-user) that eliminates kernel overhead and can achieve very high IOPS, supporting the claim of a GC-free/runtime-friendly path contributing to throughput gains. Taken together, these excerpts corroborate that a shard-per-core architecture with latency-aware scheduling and user-space I/O can realize significant throughput per core and substantial tail-latency reductions, as described in the fine-grained field value. The most relevant pieces provide explicit performance figures or qualitative claims that map onto the stated 1.5x–2.5x throughput per core and 5x–20x tail-latency improvements under realistic workloads, while the SPDK-backed excerpts offer a concrete mechanism for achieving low-I/O overhead that complements the scheduling arguments.",
      "confidence": "high"
    },
    {
      "field": "storage_stack_recommendation.durability_and_operability",
      "citations": [
        {
          "title": "SPDK: Block Device User Guide",
          "url": "https://spdk.io/doc/bdev.html",
          "excerpts": [
            "All of the latency values are measured in microseconds. Example command: rpc ... The Virtio-Block driver allows creating SPDK bdevs from Virtio-Block devices."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "Virtio devices use virtqueues to transport data efficiently.",
            "Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "The virtio network device is a virtual network interface controller. It consists of a\nvirtual Ethernet link which connects the device to the Ethernet network. The device\nhas transmit and receive queues. The driver adds empty buffers to the receive\nvirtqueue. The device receives incoming packets from the link; the device places these\nincoming packets in the receive virtqueue buffers. The driver adds outgoing packets\nto the transmit virtqueue. The device removes these packets from the transmit\nvirtqueue and sends them to the link. The device may have a control virtqueue. The\ndriver uses the control virtqueue to dynamically manipulate various features of the\ninitialized device."
          ]
        }
      ],
      "reasoning": "The target field concerns how durability is achieved in a Virtio-backed storage stack (Virtio-blk with SPDK vhost) and what operability considerations (like multi-queue configuration and memory setup) influence maximum performance. Direct references show core elements of this stack: SPDK's Virtio-Block integration, SPDK vhost usage, and the architectural relationship between the guest and back-end with Virtio.\n\nThe most pertinent information comes from SPDK-focused material that explains how SPDK provides a block device interface via Virtio, enabling a Virtio-Block driver to map to SPDK-backed devices. This establishes the practical pathway for durability-through-virtio: using SPDK-backed block devices in conjunction with Virtio to present a robust, high-performance storage interface. It also directly mentions the Virtio-Block driver's role in creating SPDK bdevs from Virtio-Block devices, which underpins how durability features could be realized in this setup.\n\nAdditional highly relevant material covers SPDK vhost back-end behavior and the overall Vhost/Virtio architecture. Understanding that SPDK vhost can serve as a high-performance back-end (often used with virtio-block) helps connect how a guest can issue flush-like operations through the Virtio interface to a back-end that handles I/O efficiently. The back-end/front-end description and the discussion of virtqueues provide the operational context for how data paths are orchestrated between guest and SPDK-based back-ends, impacting both durability semantics and performance.\n\nSupporting operability considerations are present in discussions about SPDK vhost testing and deployment, including performance-oriented guidance such as test setups for vhost-user with SPDK backends. While the excerpts do not spell out exact flush semantics, they do underline the importance of multi-queue backends, virtio device models, and the SPDK/vhost pairing as a practical path to high IOPS and scalable I/O, all of which influence durability guarantees and operability in production.\n\nBroader virtio and VIRTIO specifications excerpts reinforce the standardization aspect (virtio device models, virtqueue structures, and the protocol between front-end and back-end), which indirectly supports durability by enabling consistent and well-supported device semantics across OSes and hypervisors. This backdrop helps explain why a cross-OS, driver-unified approach can yield predictable behavior and reliability characteristics essential to durability-focused deployments.",
      "confidence": "medium"
    },
    {
      "field": "storage_stack_recommendation.technology_name",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU."
          ]
        },
        {
          "title": "SPDK Vhost-user – SmartX",
          "url": "https://www.smartx.com/blog/2022/07/spdk-vhost-user-en/",
          "excerpts": [
            "The data plane processing of vhost-user is mainly divided into Master and Slave. Generally, QEMU is the Master which is the supplier of Virtqueue, and storage software is the Slave which is responsible for consuming I/O requests in the Virtqueue.",
            "In this article, we focus on SPDK Vhost-user and explain how to leverage this technology to improve the I/O performance of Virtio-BLK/Virtio-SCSI storage in ...",
            "* Intel Xeon Gold 5122 3.6GHz",
            "Benefitting from Vring’s I/O communication mechanism, Virtio effectively reduces I/O latency and shows better performance than the pure software emulation of QEMU",
            "the data path based on Virtio-BLK is as follows:",
            "Jul 15, 2022 — Benefitting from Vring's I/O communication mechanism, Virtio effectively reduces I/O latency and shows better performance than the pure software ..."
          ]
        },
        {
          "title": "SPDK: Block Device User Guide",
          "url": "https://spdk.io/doc/bdev.html",
          "excerpts": [
            "All of the latency values are measured in microseconds. Example command: rpc ... The Virtio-Block driver allows creating SPDK bdevs from Virtio-Block devices."
          ]
        }
      ],
      "reasoning": "The most directly supportive excerpts describe SPDK vhost as a Vhost-user back-end server and explain how the front-end guest communicates with the SPDK back-end through virtqueues, including the polling-mode approach that eliminates I/O submission notifications to improve throughput. This directly establishes SPDK vhost-user as the SPDK-backed Virtio storage path and aligns with the requested field value. Additional excerpts specify that SPDK vhost-user backends are used for storage offload in virtualization scenarios (e.g., with QEMU), reinforcing its role as the recommended high-performance storage backend in Virtio contexts. A block-device-focused excerpt confirms that SPDK can instantiate Virtio-Block devices from SPDK bdevs, demonstrating a concrete integration point between SPDK and Virtio storage devices. Further entries provide broader SPDK vhost-user context, including architecture diagrams and testing guidance, which supports the practical viability and adoption of SPDK vhost-user in real systems. Overall, the most precise and corroborating statements are those identifying SPDK vhost-user as a Vhost-user backend, its polling-mode optimization, and the guest-backend interaction through virtqueues, followed by explicit usage paths (Virtio-Block with SPDK) and implementation/testing references.",
      "confidence": "high"
    },
    {
      "field": "storage_stack_recommendation.recommendation_for_workloads",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "SPDK: Block Device User Guide",
          "url": "https://spdk.io/doc/bdev.html",
          "excerpts": [
            "All of the latency values are measured in microseconds. Example command: rpc ... The Virtio-Block driver allows creating SPDK bdevs from Virtio-Block devices."
          ]
        },
        {
          "title": "How to test vhost-user-blk with SPDK - GitHub Pages",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/docs/vhost-user-blk-testing.html",
          "excerpts": [
            "The purpose of this document is to illustrate how to test vhost-user-blk in Cloud Hypervisor with SPDK as the backend.",
            "### Set the SPDK environment"
          ]
        }
      ],
      "reasoning": "SPDK vhost-user is described as a back-end server that connects to Virtio devices and is used for high-performance storage offload, which directly supports the recommendation to use SPDK vhost-user for workloads like Kafka logs and analytics spills that demand high throughput and low latency. The polling (poll-mode) nature of the SPDK vhost path is highlighted as a key mechanism to maximize throughput by removing interrupt overhead, aligning with the need for high IOPS and low latency in streaming and analytics spill scenarios. In addition, SPDK's Virtio-Block integration is specifically called out, noting that the SPDK block device (bdev) can be created from Virtio-Block devices, which directly underpins a practical deployment pattern for Kafka and analytics data paths that rely on efficient block storage. The description of vhost as a back-end where the front-end (guest/VM) shares virtqueues with the back-end (SPDK) confirms the architecture compatibility and performance benefits for a Rust OS environment using Virtio as the device model. Practical testing guidance for vhost-user-blk with SPDK demonstrates concrete steps to realize the performance benefits in real deployments, reinforcing the feasibility and empirical support for the recommended setup. In summary, the strongest support comes from the explicit statements that SPDK vhost is a Vhost-user back-end server that enables high-performance block/storage offload, along with the assertion that SPDK vhost can deliver high IOPS and low overhead in guest environments, and the concrete SPDK bdev integration with Virtio-Block devices. Supporting context about the polling mode and the front-end/back-end role further corroborates the recommended configuration and its applicability to latency/throughput-sensitive workloads like Kafka and analytics spill paths.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.0.architectural_impact",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing."
          ]
        }
      ],
      "reasoning": "Direct references to SPDK vhost and Virtio-intensive paths strongly support the claims about the storage stack being replaced with virtio-blk backed by SPDK vhost-user, and virtio-net backed by high-performance networking backends. The described front-end flow of allocating buffers in shared memory and placing guest addresses into virtqueues aligns with a zero-copy, low-latency path to NVMe storage, which is exactly the architecture described for SPDK-backed virtio-blk. The poll-mode Virtio driver excerpt reinforces the idea of eliminating unnecessary I/O submission notifications to reduce overhead, which complements the goal of achieving low-latency paths via SPDK. Discussions of vhost-user backends enabling high throughput (e.g., SPDK demonstrating very high IOPS per core) provide concrete support for the claimed performance characteristics. In addition, excerpts about Shenango and related latency-focused scheduling demonstrate architectural patterns for reducing cross-core contention and improving tail latency, which aligns with the shard-per-core design concept. While some excerpts discuss broader latency-optimized scheduling (Perséphone, Shenango, DARC) and system-wide performance, they corroborate the feasibility and impact of aggressive, low-latency scheduling and partitioned core design, which underpins the shard-per-core aspect of the field value. Overall, the most direct evidence supports the Virtio/SPDK and Virtio/DPDK/AF_XDP-backed I/O paths, while the scheduling and partitioning references provide supportive context for the shard-per-core approach.",
      "confidence": "medium"
    },
    {
      "field": "performance_analysis_for_target_workloads.2.hot_path_analysis",
      "citations": [
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        }
      ],
      "reasoning": "The described hot path targets end-to-end latency from request receipt to response, with emphasis on maintaining extremely low tail latency (p99.9) under bursts and mitigating kernel overheads like interrupts and context switches. Excerpts that present latency-focused scheduling and CPU efficiency directly support the claim that software architectures (e.g., Shenango and related schedulers) can significantly reduce tail latency and improve throughput, which is central to the hot path analysis. The Shenango-related excerpts show measured benefits in latency-sensitive datacenter workloads and compare favorably to kernel-bypass stacks, illustrating practical impacts on tail latency. This directly aligns with the need to minimize tail latency in the backend API hot path. Excerpts describing SPDK vhost-user backends and poll-mode virtio illustrate approaches to drastically cut I/O submission/notification overhead, which map to the portion of the hot path that handles TLS termination, payload I/O, and database/cache lookups by reducing kernel mediation and interrupt overhead. Those sources provide concrete mechanisms to reduce latency components along the hot path, including removing I/O interrupts and enabling high-throughput paths with user-space drivers. Excerpts discussing latency improvements in Perséphone/Shinjuku (historical latency optimization strategies) further corroborate that architectural choices in scheduling and preemption can significantly influence tail latency, which is relevant when analyzing the stability of the end-to-end backend API path under load. Taken together, these excerpts support the claim that reducing kernel overhead, adopting user-space I/O, and employing latency-aware scheduling are effective strategies to lower p99.9 tail latency for the described backend workflow. The most directly supportive content is the explicit discussion of high-throughput, low-latency scheduling and its realized latency improvements, followed by concrete SPDK/vhost-user I/O paths that reduce I/O overhead, and then by broader latency-focused comparisons that reinforce the value of tail-latency optimization in similar systems.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.2.workload_name",
      "citations": [
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin"
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing.",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin"
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "Information-Agnostic Flow Scheduling for Commodity Data ...",
          "url": "https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/bai",
          "excerpts": [
            "by W Bai · 2015 · Cited by 415 — We present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a ..."
          ]
        }
      ],
      "reasoning": "The core fine-grained field value seeks guidance for latency-sensitive backend APIs (such as HTTP or gRPC) and emphasizes minimal tail latency and high but predictable throughput in a backend infrastructure. Excerpts describing-predominantly latency-focused scheduling and high-precision latency outcomes are the most relevant. One excerpt reports that Shenango achieves very high throughput with median latencies in the tens of microseconds and tail latencies in the tens of microseconds range, showing that CPU-efficient, low-latency user-space stacks can sustain millions of requests per second while keeping latency tight. This directly informs how a latency-critical backend API path might be engineered for low tail latency and high throughput. Another excerpt discusses SPDK vhost-user's poll-mode approach, which removes I/O submission notifications and drastically increases vhost throughput while reducing guest overhead, illustrating a concrete, low-latency path for storage I/O that can influence latency behavior in latency-sensitive backends that rely on fast I/O. Additional excerpts describe Perséphone/Shenango/Shinjuku and DARC-style scheduling; these works show how specialized schedulers and core-affinity techniques can dramatically reduce tail latency and improve predictable performance for short, latency-sensitive requests, which is highly relevant for API backends that must meet strict latency SLAs. There is also explicit mention of tail-latency concerns and techniques to measure or reduce tail latency, aligning with the needs of latency-critical backends. Together, these excerpts collectively support the idea that achieving latency-critical backend APIs benefits from a combination of (a) high-performance user-space I/O paths (to minimize kernel-induced latency), (b) scheduler designs that prioritize short, latency-sensitive requests and tight tail latency controls, and (c) robust benchmarking and measurement practices to ensure tail latency targets are met. While the excerpts do not name HTTP/gRPC explicitly, the described techniques are transferable to latency-sensitive API workloads and provide concrete engineering guidance for achieving low tail latency and high throughput in a latency-critical backend API context.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.2.use_case_name",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt describes ultra-low latency cloud-native exchanges with explicit latency targets (P50 and P99 ranges), illustrating how latency-focused design principles enable fast real-time decisioning, which closely mirrors the RTB use case's need for rapid bidding decisions in an ad-tech context. The second most relevant excerpt discusses Perséphone/SOSP21 latency improvements in transactional workloads (payments, OrderStatus, NewOrder), demonstrating measurable latency gains in end-to-end workflows and highlighting how targeted optimizations reduce tail latency, a key consideration for RTB where bid responses must be near-instantaneous. The third relevant excerpt outlines high-frequency trading systems, which are archetypal real-time, latency-sensitive domains; while finance-specific, the emphasis on speed, throughput, and end-to-end latency characteristics provides transferable insight into requirements for RTB-like workloads. The remaining excerpt focuses on market size projections, which is less directly applicable to identifying a concrete use-case name for a latency-critical RTB scenario and therefore is least relevant to the finegrained field value.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.2.competitive_advantage_summary",
      "citations": [
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant information comes from a study that benchmarks a Shenango-style scheduler and shows substantial improvements in tail latency under increasing load. This supports the core claim that a Shenango-like scheduler can react to sudden spikes in traffic with little impact on tail latency, which is central to the proposed OS design's competitive advantage. Contextual support comes from discussions of low-latency, high-performance environments (such as latency targets in fast exchanges) and the broader industry evidence that hardware acceleration through high-performance user-space backends (SPDK/DPDK) can deliver efficient I/O paths for network-intensive workloads. While the first excerpt foregrounds low latency in cloud-native exchanges and mentions latency numbers that align with a performance-focused architecture, it does not explicitly tie to Shenango-style scheduling but reinforces the relevance of tight latency budgets. The remaining excerpts expand on the market and architectural context (e.g., high-frequency trading dynamics and market size) that frame the importance of ultra-low latency systems and the value of high-performance I/O paths, though they do not provide direct evidence about the Shenango scheduler's behavior. Taken together, the excerpts support the notion that a scheduler and user-space I/O path can achieve low tail latency under bursty traffic and that SPDK/DPDK-backed Virtio devices are a viable route for high-performance, network-heavy workloads.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.2.market_analysis",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that the RTB market is large and that lower latency yields financially meaningful benefits, such as higher willingness to pay and increased CPM. The most relevant excerpt demonstrates concrete latency figures in latency-critical exchange scenarios (latency in cloud-native exchanges with P50/P99 ranges), making a direct link between latency and performance outcomes in a trading/auction-like environment. The next excerpt strengthens the case by showing substantial latency improvements in a high-frequency/transactional context (latency reductions for specific transaction paths), which supports the idea that latency reduction translates into better system performance and potentially higher revenue or bidding success. A third excerpt discusses the broader HFT market size, which provides market scale context but does not directly quantify latency-improvement value, thus it is supportive but less directly tied to the latency-to-revenue argument. Collectively, these excerpts support the notion that lower latency in high-stakes bidding/market environments correlates with higher performance and potentially higher revenue, aligning with the finegrained field value.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.0.competitive_advantage_summary",
      "citations": [
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        }
      ],
      "reasoning": "The strongest support comes from a reference that directly discusses latency improvements in architectures that use Shenango-like scheduling and microsecond-scale core reallocation, including explicit mention that shard-per-core designs and periodic core reassignment (on the order of microseconds) help maintain tail latency under load. This aligns with the claim that a Rust Hallows stack can maintain P99.9 latency below a tight bound under heavy load by aggressively reallocating cores every microsecond. Additionally, this excerpt notes that ARINC-653–style partitioning (APEX) is the right abstraction for time/space partitioning and certification concerns, which corroborates the proposed isolation and determinism angle. It also references DO178C-style and certification-oriented considerations, reinforcing the safety and verification framing for a Rust-based system in safety-critical contexts. The other excerpt focuses on latency engineering principles in cloud-native components and provides supporting context about latency targets and variability; while not as tightly tied to the Shenango-style scheduler or APEX partitioning, it reinforces the central theme of latency-centric design and validates that tail latency is a primary concern in high-performance systems. Together, these excerpts substantiate the core claims that (a) microsecond-scale scheduling and per-core partitioning can reduce tail latency, (b) APEX-style isolation contributes to determinism and safety, and (c) latency targets like P99.9 under load are realistic benchmarks for such a stack.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.0.hot_path_analysis",
      "citations": [
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        }
      ],
      "reasoning": "Kafka-style workloads are highly sensitive to tail latency and I/O path efficiency. The most relevant material demonstrates that latency-focused scheduling and high-throughput, low-overhead I/O paths can dramatically improve performance under tight SLOs. Specifically, a scheduling approach that reserves a core for short requests and prioritizes them reduces slowdowns and can sustain higher throughput for tight latency targets, which maps to keeping produce/consume/replication paths responsive under load. Kernel-bypass or user-space I/O strategies show how eliminating kernel overheads and using specialized backends can push I/O throughputs higher and reduce latency, which is directly pertinent to the produce/consume/replication paths that rely on efficient disk and network I/O in Kafka deployments. Comparative work indicates that centralized or shard-aware scheduling can yield substantial tail-latency improvements, and that achieving high I/O efficiency with kernel-bypass stacks can bring throughput to multi-million requests-per-second scales with very low medians. While the excerpts do not mention Kafka explicitly or JVM GC behavior, the techniques—latency-focused scheduling, high-throughput low-overhead I/O paths, and kernel-bypass approaches—provide concrete, applicable levers for reducing the tail latency and improving the efficiency of the described hot paths in Kafka-like systems. The content also demonstrates realistic expectations: performance gains are often realized in narrowly scoped paths (e.g., specific subsystems or backends) rather than across the entire stack, which is a valuable caution for aiming at end-to-end acceleration in Kafka deployments.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.2.workload_description",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly anchors the idea of a low-latency exchange-like system, providing concrete latency figures (latency of 55-124 microseconds, 75-157 microseconds) and the notion of latency-focused design in an industry context. This aligns with the RTB workload's emphasis on ultra-fast decisioning per impression and the need for rapid bid responses in programmatic auctions. A closely related excerpt discusses Perséphone's latency improvements in transactional workloads, illustrating substantial latency reductions (9.2x, 7x, 3.6x improvements) in latency-sensitive paths, which supports the notion that targeted, well-scoped optimizations can yield meaningful tail-latency benefits in high-speed, exchange-like systems. Another excerpt, while focused on high-frequency trading more generally, reinforces the broader point that latency is a critical, business-driving concern in finance-like, throughput/latency-sensitive environments, which conceptually supports applying similar latency-centric design to RTB workloads. The remaining excerpt centers on market size and does not directly discuss latency, per-impression bidding, or exchange-style workloads, and thus offers only peripheral context.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.2.performance_slos",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly discusses a low-latency cloud-native exchange with end-to-end latencies in the tens to hundreds of microseconds, illustrating that latency is the primary design driver and that such performance targets are achievable with modern architectures. This strongly supports the idea that achieving low end-to-end latency (low P99) is essential in a high-throughput, bid-driven environment. The next excerpt emphasizes significant improvements in tail latency (p99.9) for critical transactions under load, demonstrating that targeted reductions in tail latency are both measurable and valuable for latency-sensitive workloads. This complements the first excerpt by confirming that aggressive latency optimization at the tail is realistic and impactful for high-volume, event-driven systems. The third excerpt relates to high-frequency trading in general, underscoring that ultra-low latency is a hallmark of systems in time-sensitive markets; while not about the exact workload, it reinforces the notion that achieving very low latency is a meaningful objective in competitive, high-speed environments. Together, these excerpts support the claim that maintaining low P99 latency is crucial for workloads handling massive, spiky network I/O and high request rates, thereby influencing revenue through tighter timeouts and participation thresholds.",
      "confidence": "medium"
    },
    {
      "field": "performance_analysis_for_target_workloads.2.expected_performance_gains",
      "citations": [
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin"
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir"
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing.",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin"
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Information-Agnostic Flow Scheduling for Commodity Data ...",
          "url": "https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/bai",
          "excerpts": [
            "by W Bai · 2015 · Cited by 415 — We present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a ..."
          ]
        }
      ],
      "reasoning": "The strongest support comes from concrete measurements showing ultra-low tail latency under high load. One excerpt reports a median around 37 microseconds and a 99.9th percentile around 93 microseconds for a high-throughput datacenter workload, directly illustrating that millisecond-level tail latency can be reduced to the tens-of-microseconds range in a latency-focused stack. This aligns with the target of moving tail latency into the sub-100 microsecond domain under bursty traffic. Additional excerpts describe tail-latency reductions achieved by latency-aware, microsecond-scale schedulers and core-partitioning strategies that eliminate cross-core contention, which are exactly the kinds of mechanisms needed to approach the desired tail-latency goals. There are also demonstrations of kernel-bypass I/O paths (SPDK vhost-user) that remove kernel involvement, reducing jitter and improving tail behavior under high I/O pressure, which supports the feasibility of achieving such low tail latency in practice. Further support comes from references showing tail-latency improvements in related systems (e.g., Persephone/SOSP21 results indicating substantial p99.9 improvements in specific workloads) and discussions of scheduler techniques (e.g., DARC, Shenango, Shinjuku) that tightly control scheduling to minimize tail latency. Additional context on the feasibility and path to sub-100 microsecond tails is provided by experiments that compare shard-per-core designs and vectorized analytics paths, which help explain how to reduce tail latency while maintaining or increasing CPU efficiency. While not every excerpt directly states sub-100 microsecond tails for all workloads, the collection cumulatively supports the notion that specialized schedulers, kernel-bypass I/O, and careful resource partitioning can achieve multi-10x reductions in tail latency in the targeted domains, including the <100 μs range for p99.9 in many practical scenarios. The most compelling direct evidence is the explicit measurement showing p99.9 below 100 microseconds in a latency-sensitive setup, which is the closest to the stated field value; the other excerpts provide complementary mechanisms and corroborating performance patterns that would plausibly yield the stated gains when combined in a target workload.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.2.architectural_impact",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing.",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Information-Agnostic Flow Scheduling for Commodity Data ...",
          "url": "https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/bai",
          "excerpts": [
            "by W Bai · 2015 · Cited by 415 — We present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a ..."
          ]
        }
      ],
      "reasoning": "The core architectural propositions are: (1) kernel-bypass networking to remove kernel overhead and improve throughput and tail latency, exemplified by SPDK vhost backends operating in a poll-mode Virtio configuration that bypasses guest overhead and interrupts; this directly supports the kernel-bypass objective and the notion of high-throughput, low-latency I/O paths. (2) a microsecond-scale scheduler capable of maintaining extremely low tail latency under load shifts; evidence from Shenango and related schedulers shows sub-100 microsecond tail latencies and aggressive core scheduling strategies that dramatically reduce latency penalties during load changes, aligning with the proposed scheduler target. (3) a thread-per-core design to minimize locking and context switches within the application, which is a recurring architectural pattern discussed in relation to Shenango-style workloads and shard-per-core designs. (4) supporting metrics and benchmarks indicating high throughput and favorable latency characteristics, which corroborate the feasibility of achieving the stated performance targets in realistic settings, though some caveats about variability and platform differences are noted. Collectively, the excerpts demonstrate that kernel-bypass I/O (SPDK/AF_XDP-like paths) plus microsecond schedulers and per-core threading are practical and have demonstrated performance that approaches the level described, with sub-100 microsecond tails and multi-million IOPS scales in relevant contexts.",
      "confidence": "high"
    },
    {
      "field": "top_business_use_cases.1.performance_slos",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly cites ultra-low latency figures in the sub-millisecond regime (microseconds) for latency targets and performance under load, which aligns with URLLC-style requirements. It discusses latency results in the tens of microseconds range (P50/P99) and focuses on achieving low latency in a cloud-native, low-latency context, which supports the idea of end-to-end latency targets and jitter control under realistic loads. The second most relevant excerpt presents concrete latency improvements in the low-microsecond to sub-millisecond realm (e.g., latency improvements to payment and order processing paths and latency reductions under load), which closely maps to the URLLC objective of maintaining tight latency envelopes. A third excerpt discusses tail latency and microbenchmark observations in hardware/software setups and highlights the sensitivity of latency to tuning and architectural choices, which is relevant for understanding how to keep latencies within URLLC bounds under stress. The fourth excerpt introduces high-frequency trading context where latency is a central concern, providing general intuition about latency budgets and the importance of fast paths, though it does not provide explicit microsecond-scale figures for URLLC-like guarantees. The fifth excerpt covers market sizing and broader historical context for latency-centric systems but does not contribute direct evidence toward sub-millisecond, jitter-specific URLLC numbers, making it the least supportive of the fine-grained field value.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.1.market_analysis",
      "citations": [
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        }
      ],
      "reasoning": "The most relevant content directly addresses latency-centric design and QoS considerations, which align with enabling URLLC services and reducing total cost of ownership in a 5G infrastructure market. A key excerpt states that low-latency design principles were the focus and provides explicit latency targets (P50 and P99 ranges) achievable in cloud-native exchanges, illustrating how latency reduction and predictable performance underpin premium services and QoS guarantees. This supports the idea that the market value arises from consistent, low-latency performance and the ability to differentiate through quality of service. Another excerpt emphasizes substantial latency improvements at high load for transactional workloads, illustrating how architectural choices that shrink tail latency translate into tangible cost- and performance benefits, which dovetails with TCO reduction and QoS-enabled offerings in a 5G/URLLC context. A third excerpt discusses tail latency spikes and the need for tuning to maintain determinism, reinforcing the importance of predictable performance (a cornerstone of QoS) for infrastructure serving URLLC-like requirements. The remaining excerpts provide related, but more indirect, evidence: high-frequency trading market discussions highlight the premium placed on ultra-low latency environments, which echoes the value proposition for URLLC in telecom but is more of a benchmarking reference than a direct market-use-case validation. The final piece mentions latency spikes in real-world forum discussions, underscoring practical challenges in maintaining low latency at scale, which again supports the likelihood that QoS and TCO considerations are central to the business case. In sum, the most directly supportive evidence centers on achieving low tail latency and predictable performance as foundational to QoS-enabled, cost-efficient 5G infrastructure and URLLC services; additional sources corroborate the landscape but are less central to the particular market-use-case emphasis on QoS and TCO.",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.2.hardware_type",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "rs. #### Configuration Details\n\nTest by Intel as of 10/22/2022. System: 1-node, 2x Intel® Xeon® Platinum 8480+ Processor (56 cores, HT=On, Turbo=ON),\nTotal Memory 1024 GB (16 slots/ 64GB/ 4800 MHz), BIOS:3A05 (ucode:0x2b000070),\nStorage: 24x Intel® Optane™ SSD 800GB P5800X."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        }
      ],
      "reasoning": "The field value specifies a hardware type of NVMe SSD. Excerpts explicitly mentioning NVMe and SPDK as the storage path provide direct support for NVMe SSD as the hardware type and its performance characteristics. It is reasonable to infer that SPDK NVMe/NVMeoF articles describe NVMe devices and their IOPS scaling, which aligns with the NVMe SSD classification. Additional entries describe SPDK vhost-user backends and virtualization paths (which often use NVMe for storage offloads and high IOPS), reinforcing the NVMe context in practical deployments. Other excerpts discuss SPDK performance benchmarks and NVMe driver behavior (e.g., lock-free paths and scalability per thread), which further corroborate the suitability and performance expectations of NVMe SSDs in high-performance storage scenarios. Excerpts describing Vhost-user specifics, front-end configurations, and general benchmarking tools provide contextual support for deploying NVMe-backed storage in virtualized environments but are slightly less central to the exact hardware type and primary NVMe storage path, hence their placement after the NVMe-focused content.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.0.hardware_type",
      "citations": [
        {
          "title": "Intel® Ethernet Network Adapter E810-2CQDA2",
          "url": "https://cdrdv2-public.intel.com/639389/Intel%20Ethernet%20Network%20Adapter%20E810-2CQDA2%20Product%20Brief.pdf",
          "excerpts": [
            "• Control Queues (a.k.a. Admin Queues). - Mailbox Queues for PF-VF and driver-driver. - Admin Queues for Software-Firmware control flows. - Sideband Queues for ..."
          ]
        },
        {
          "title": "Intel E810 Feature Summary and Related NVMe/ConnectX Details",
          "url": "https://cdrdv2-public.intel.com/630155/630155_Intel%20Ethernet%20Controller%20E810%20Feature%20Summary_rev4_6.pdf",
          "excerpts": [
            "Intel® Ethernet Controller E810",
            "Intel® Ethernet Controller E810",
            "The following table lists the driver, firmware, and package versions recommended for use with the supported DPDK version. Table 9. DPDK ...",
            "June 2025",
            "Feature Support Matrix"
          ]
        },
        {
          "title": "Linux Base Driver for the Intel(R) Ethernet Controller 800 Series",
          "url": "https://docs.kernel.org/networking/device_drivers/ethernet/intel/ice.html",
          "excerpts": [
            "In a virtualized environment, on Intel(R) Ethernet Network Adapters that support SR-IOV, the virtual function (VF) may be subject to malicious behavior."
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir"
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "rs. #### Configuration Details\n\nTest by Intel as of 10/22/2022. System: 1-node, 2x Intel® Xeon® Platinum 8480+ Processor (56 cores, HT=On, Turbo=ON),\nTotal Memory 1024 GB (16 slots/ 64GB/ 4800 MHz), BIOS:3A05 (ucode:0x2b000070),\nStorage: 24x Intel® Optane™ SSD 800GB P5800X.",
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking."
          ]
        },
        {
          "title": "Step-by-Step Guide: Building High-Performance Software ...",
          "url": "https://medium.com/@israelgeoffrey/step-by-step-guide-building-high-performance-software-defined-storage-with-spdk-nvme-of-and-spdk-743671404156",
          "excerpts": [
            "Let's begin by deploying the SPDK NVMe-oF target inside a container. This will serve as our high-performance storage backend, enabling ultra-low ..."
          ]
        },
        {
          "title": "[PDF] Intel® Ethernet Controller E800 Series Dynamic Device ...",
          "url": "https://cdrdv2-public.intel.com/618651/618651_Intel%20Ethernet%20Controller%20E810%20DDP%20for%20Comms%20Technology%20Guide.pdf",
          "excerpts": [
            "Updated E810 firmware version from 1.5.4.5 to 1.5.5.6. ... The specific DDP package requires certain firmware and DPDK versions and 800 Series."
          ]
        }
      ],
      "reasoning": "The target field value indicates that the recommended hardware platform includes a NIC as the hardware_type. Excerpts that explicitly describe Intel Ethernet NICs (such as the Intel E810 family) directly support NIC as the hardware type in networking hardware contexts. These sources discuss NIC-specific features, driver considerations, and compatibility with drivers and firmware, which corroborate NIC as the hardware_type in a recommended platform. Excerpts mentioning SPDK vhost-user, Virtio networking, and NVMe over fabrics are relevant because they address the practical use and performance characteristics of NICs in high-performance I/O paths (network/storage offloads) and illustrate typical NIC-related integrations and backends that would accompany NIC hardware selections. Excerpts focusing on generic high-performance networking concepts, DoC/DO-178C toolchains, or non-NIC-specific hardware discussions provide supporting context but do not directly identify NIC as the hardware_type; these are included to show ecosystem and validation considerations surrounding NIC choices. Overall, the strongest, most directly supportive content comes from the NIC-specific documents (Intel E810 and related Ethernet controller discussions), followed by NIC-usage patterns (vhost-user, SPDK) that demonstrate practical NIC integration in virtualization and user-space I/O stacks.\n",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.1.expected_performance_gains",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "The field value asserts substantial reductions in both average completion time and extreme tail latency (p99.99), driven by a combination of user-space I/O, a microsecond scheduler, and a vectorized analytics engine, with a specific note that SPDK optimizations can reduce I/O tail latency by up to 6.7x. Excerpts describing SPDK vhost-user backends and poll-mode Virtio show how moving I/O into user space and removing interrupt overhead can dramatically reduce guest overhead and improve throughput, which directly supports the I/O tail-latency component. Excerpts about Shenango demonstrate that a microsecond-scale scheduler and shard-per-core or low-overhead scheduling strategies can deliver extremely low median and tail latencies, supporting the scheduling jitter reduction claim. Excerpts about Perséphone and Shinjuku show substantial tail-latency improvements under real workloads, reinforcing the plausibility of large reductions in tail latency for latency-sensitive services when employing advanced scheduling and preemption strategies. Taken together, these excerpts support the overall narrative that a combination of user-space I/O (SPDK), microsecond scheduling, and vectorized compute can yield meaningful gains in both average and tail latency, aligning with the target field value. However, the exact 6.7x reduction and universal applicability are workload- and hardware-dependent, so claims should be framed as plausible targets under optimized configurations rather than guaranteed universal outcomes.",
      "confidence": "medium"
    },
    {
      "field": "top_business_use_cases.1.competitive_advantage_summary",
      "citations": [
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Low latency cloud-native exchanges | AWS for Industries",
          "url": "https://aws.amazon.com/blogs/industries/low-latency-cloud-native-exchanges/",
          "excerpts": [
            "... latency of 55-124 microseconds (P50) and 75-157 microseconds (P99). In this case, design principles were focused around latency while ..."
          ]
        },
        {
          "title": "High-frequency trading",
          "url": "https://en.wikipedia.org/wiki/High-frequency_trading",
          "excerpts": [
            "High-frequency trading (HFT) is a type of algorithmic automated trading system in finance characterized by high speeds, high turnover rates, and high order-to- ..."
          ]
        },
        {
          "title": "High Frequency Trading Market Size | Industry Report, 2030",
          "url": "https://www.grandviewresearch.com/industry-analysis/high-frequency-trading-market-report",
          "excerpts": [
            "The global high frequency trading market size was valued at USD 10.36 billion in 2024 and is expected to reach USD 16.03 billion by 2030, growing at a CAGR ..."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        }
      ],
      "reasoning": "The target field asserts that a standard Linux scheduler struggles to prevent high-bandwidth traffic from impacting URLLC-like latency, and that a Rust-based OS with microsecond-level scheduling and strict partitioning can guarantee sub-100 microsecond latency for URLLC under heavy network load. Excerpt from Perséphone/SOSP21 directly discusses dramatic latency improvements at high load when comparing to Shenango's c-FCFS, illustrating the impact of scheduling and contention on tail latency in a high-performance system. The AWS low-latency exchanges excerpt reinforces the importance of low tail latency (P50/P99 ranges) in latency-sensitive workloads, aligning with the claim that scheduling and partitioning matter for latency budgets. The high-frequency trading reference highlights the critical role of ultra-low latency in finance, underscoring how minimized latency and jitter are central goals in performance-focused systems. The market-size/exchange-focused excerpts further emphasize the economic and scale considerations tied to latency-sensitive deployments, while the forum post about stable fq_codel/cake tests reinforces practical observations about latency variability and the need for disciplined scheduling and network QoS. Although none of the excerpts mention URLLC or Rust OS explicitly, they collectively support the core claim that careful scheduling, latency guarantees, and performance backends are key to achieving ultra-low latency in high-load environments, which is the essence of the fine-grained field value.",
      "confidence": "medium"
    },
    {
      "field": "performance_analysis_for_target_workloads.1.hot_path_analysis",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "The finest-grained field value identifies the shuffle operation in Spark-like analytics as the principal bottleneck, driven by heavy network I/O to exchange intermediate data and spill-to-disk I/O when memory pressure occurs. It also highlights stragglers and data skew as major contributors to high tail latency. Excerpts describing SPDK vhost-user backends and poll-mode Virtio show how to achieve very high I/O throughput with minimal guest overhead, which directly informs how to alleviate the shuffle's network I/O load and the I/O path to storage during spills. They point to architectures (user-space I/O, kernel-bypass paths) that can dramatically reduce latency and increase IOPS, thereby addressing the bottlenecks caused by traditional kernel-mediated I/O in such workloads. Excerpts about Shenango and related latency-oriented scheduling provide empirical context for tail-latency behavior in latency-sensitive datacenter workloads, illustrating how competition for CPU and sub-millisecond response times interact with I/O paths. The tail-latency discussion emphasizes that even small improvements can have outsized effects on P99/P99.9 latency, which aligns with the concern about stragglers in analytics pipelines. Other excerpts illustrate concrete performance targets and tradeoffs (e.g., SPDK achieving high IOPS per core and the impact of poll-mode approaches) that underpin the feasibility of mitigating the identified bottlenecks through bespoke I/O paths and scheduling strategies. In sum, the most relevant excerpts directly describe high-throughput, low-latency I/O paths (virtio/vhost/SPDK and poll-mode operation) and tail-latency measurements in similar systems, which together substantiate the claim that shuffle-related network I/O, spill I/O, and stragglers are the core bottlenecks in Spark-like analytics and that architectural choices around user-space I/O and scheduling can meaningfully reduce those bottlenecks.",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.0.vendor_and_model",
      "citations": [
        {
          "title": "Intel® Ethernet Network Adapter E810-2CQDA2",
          "url": "https://cdrdv2-public.intel.com/639389/Intel%20Ethernet%20Network%20Adapter%20E810-2CQDA2%20Product%20Brief.pdf",
          "excerpts": [
            "• Control Queues (a.k.a. Admin Queues). - Mailbox Queues for PF-VF and driver-driver. - Admin Queues for Software-Firmware control flows. - Sideband Queues for ..."
          ]
        },
        {
          "title": "Intel E810 Feature Summary and Related NVMe/ConnectX Details",
          "url": "https://cdrdv2-public.intel.com/630155/630155_Intel%20Ethernet%20Controller%20E810%20Feature%20Summary_rev4_6.pdf",
          "excerpts": [
            "Intel® Ethernet Controller E810",
            "Intel® Ethernet Controller E810",
            "The following table lists the driver, firmware, and package versions recommended for use with the supported DPDK version. Table 9. DPDK ...",
            "June 2025",
            "Feature Support Matrix"
          ]
        },
        {
          "title": "Linux Base Driver for the Intel(R) Ethernet Controller 800 Series",
          "url": "https://docs.kernel.org/networking/device_drivers/ethernet/intel/ice.html",
          "excerpts": [
            "In a virtualized environment, on Intel(R) Ethernet Network Adapters that support SR-IOV, the virtual function (VF) may be subject to malicious behavior."
          ]
        },
        {
          "title": "[PDF] Intel® Ethernet Controller E800 Series Dynamic Device ...",
          "url": "https://cdrdv2-public.intel.com/618651/618651_Intel%20Ethernet%20Controller%20E810%20DDP%20for%20Comms%20Technology%20Guide.pdf",
          "excerpts": [
            "Updated E810 firmware version from 1.5.4.5 to 1.5.5.6. ... The specific DDP package requires certain firmware and DPDK versions and 800 Series."
          ]
        }
      ],
      "reasoning": "The target field value describes the Intel Ethernet 800 Series hardware, exemplified by the E810. Direct evidence appears in multiple excerpts: one explicitly names the Intel Ethernet Controller E810 and discusses its product brief context, signaling a match to the model families within the 800 Series. Several excerpts provide a feature summary or feature support related to the E810, confirming the same hardware family and its capabilities. A Linux base driver reference explicitly mentions the Intel Ethernet Controller 800 Series, reinforcing the same vendor/model category. Additional excerpts reiterate E810 feature summaries and DDP/firmware details, further anchoring the field value to the Intel 800 Series family. Collectively, these excerpts establish a coherent, model-specific alignment to the requested field value and support identifying the target hardware platform as Intel's 800 Series (E810) in the nested structure. The most directly relevant parts are those that name the exact model (E810) or the exact series (800 Series), with accompanying context about drivers, features, and Linux support, which directly underpin the finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.4.hardware_type",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload."
          ]
        }
      ],
      "reasoning": "The finegrained field value specifies a hardware type of NVMe SSD at a particular position in a recommended hardware platform. Excerpts that discuss the SPDK NVMe driver and its performance characteristics directly reference NVMe-based storage devices and the driver stack used to achieve high I/O operations per second. Such content supports the assertion that NVMe is the intended hardware type for high-performance storage, as SPDK NVMe is specifically designed for NVMe devices, and performance claims are tied to NVMe-backed storage paths. Additional excerpts describe SPDK's benchmarking tools and NVMe driver behavior (such as lock-free I/O paths and scalability), reinforcing that the material centers on NVMe hardware and its software stack. Taken together, these excerpts provide clear, explicit support that the recommended hardware platform at that path would be an NVMe-based SSD, aligning with the NVMe storage context and benchmarking context described.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.2.key_features_and_performance",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "rs. #### Configuration Details\n\nTest by Intel as of 10/22/2022. System: 1-node, 2x Intel® Xeon® Platinum 8480+ Processor (56 cores, HT=On, Turbo=ON),\nTotal Memory 1024 GB (16 slots/ 64GB/ 4800 MHz), BIOS:3A05 (ucode:0x2b000070),\nStorage: 24x Intel® Optane™ SSD 800GB P5800X."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        }
      ],
      "reasoning": "The target field value describes a datacenter-grade NVMe SSD with extremely high sequential bandwidth, very high random IOPS, dual-port HA, and robust thermal design. Excerpts that discuss SPDK NVMe performance and how the SPDK driver avoids IO path locks, enabling linear scaling of IOPS per thread as you dedicate a queue pair and a core, provide direct support for the high IOPS and scalable performance profile claimed. Configuration detail excerpts illustrate a real-world test harness and platform characteristics (e.g., dual Xeon Platinum servers, large memory) that align with the intended datacenter use case. Benchmark-oriented excerpts explicitly stating IOPS improvements per core or per workload (such as \"up to 2.6x more IOPS/core\" when using perf tooling) reinforce the notion of exceptional I/O capability and scalable performance. Text describing vhost-user backends and poll-mode virtio further supports the datatype path for high-performance virtualization I/O, which complements a high-end SSD in a stack designed for extremely fast I/O. Taken together, these excerpts corroborate the field value's emphasis on leading-edge sequential and random I/O performance, high throughput, PCIe-driven capabilities, and datacenter-grade reliability features like multi-port operation.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.4.key_features_and_performance",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds"
          ]
        }
      ],
      "reasoning": "The target field emphasizes a high-performance PCIe 4.0 storage device suited for demanding workloads and large capacities. The excerpts collectively describe an ecosystem where storage performance is achieved by design choices such as an NVMe driver that avoids locks in the I/O path, enabling linear scalability of performance per thread as you dedicate queue pairs and CPU cores. This directly supports the core claim of unusually high sustained performance and scalability of the drive. The SPDK perf tooling is highlighted as a means to measure and demonstrate performance, which reinforces the assertion of sustained, testable performance characteristics. Additional excerpts discuss backends and benchmarks that illustrate how high IOPS and efficient I/O paths can be realized in practice, providing context for engineering a solution that meets the described use cases. While none of the excerpts confirm the exact capacity figure (over 30TB) or the precise PCIe generation in isolation, they collectively underpin the feasibility and engineering approach of delivering a high-capacity, high-performance PCIe 4.0 drive with strong sustained performance and thermal stability in datacenter environments.",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.4.justification_for_selection",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa"
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking.",
            "The following examples demonstrate how to use perf. Example: Using perf for 4K 100% Random Read workload to a local NVMe SSD for 300 seconds",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload."
          ]
        }
      ],
      "reasoning": "The selected passages collectively establish that SPDK provides high-performance, scalable user-space storage tooling and that the SPDK NVMe driver scales efficiently when dedicated resources (queue pairs and cores) are assigned. This directly supports the claim that a storage platform intended for capacity-sensitive workloads should leverage a mature, high-performance stack like SPDK, with backends capable of delivering very high IOPS and scalable performance. The benchmarking and performance tool references further corroborate that SPDK-backed NVMe paths can achieve strong throughput and low overhead, which underpins the assertion of high performance and clear compatibility within the SPDK ecosystem. Although the excerpts do not explicitly verify PCIe 4.0 or Micron NVMe adherence, the emphasis on SPDK NVMe performance, scalability, and available backends provides solid indirect evidence that a SPDK-driven approach is well-suited for high-capacity, high-performance workloads and aligns with the field value's overall intent regarding performance and ecosystem compatibility.",
      "confidence": "medium"
    },
    {
      "field": "performance_analysis_for_target_workloads.1.workload_name",
      "citations": [
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "We found that\nShenango achieves similar throughput and latency to\nZygOS [61], a state-of-the-art kernel-bypass network\nstack, but with much higher CPU efficiency.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Information-Agnostic Flow Scheduling for Commodity Data ...",
          "url": "https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/bai",
          "excerpts": [
            "by W Bai · 2015 · Cited by 415 — We present PIAS, a DCN flow scheduling mechanism that aims to minimize FCT by mimicking Shortest Job First (SJF) on the premise that flow size is not known a ..."
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "DARC reserves 1 core for short\nrequests and schedules them first, reducing slowdown upon\nc-FCFS by a factor of 15.72 and can sustain 2.3x higher\nthroughput for a SLO of 20𝜇s for short requests.",
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "\nséphone’s centralized scheduling offers better performance\nfor long requests than Shenango — compared to the c-FCFS\nline in figure 3 — because Perséphone does not have to ap-\nproximate centralization with work stealing.",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "For Shenango, we provision all cores at startup and\ndisable dynamic core allocation since we want to evaluate\nperformance for a single application and Shenango other-\nwise re-assign cores to multiple applications running on the\nsame machin",
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "Finding tail latency bottlenecks",
          "url": "https://medium.com/@ashishnegi/finding-tail-latency-bottlenecks-dc16b4a659d7",
          "excerpts": [
            "Remember that if high latency happens randomly for 100 msec for every 10 seconds during 1 hour test run, it is impacting your P99 or even P95."
          ]
        },
        {
          "title": "Now that fq_codel and cake are stable... how are we doing? - General",
          "url": "https://forum.mikrotik.com/t/now-that-fq-codel-and-cake-are-stable-how-are-we-doing/160146",
          "excerpts": [
            "5 tests had spikes on the down latency and 1 on the up latency. Down latency spike sample: 100s ipv6 fq-codel -5 gui.png ... c) tail drop ..."
          ]
        },
        {
          "title": "Long-tail Latency: Causes and Solutions",
          "url": "https://systemdr.substack.com/p/long-tail-latency-causes-and-solutions",
          "excerpts": [
            "Jul 23, 2025 — Welcome to the world of long-tail latency—where the statistical outliers at P95, P99, and P99.9 percentiles can destroy user experience despite ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts describe high-throughput, low-latency data-center workflows and scheduling strategies (Shenango and related work) that underpin analytics pipelines by ensuring predictable tail latency and efficient core utilization. These excerpts support the notion that Spark-like analytics workloads would benefit from advanced scheduling and architecture (e.g., microsecond-scale scheduling, low jitter, and per-core work distribution) to achieve stable, scalable analytics processing. Supporting excerpts discuss I/O paths (SPDK/vhost) and poll-mode architectures that minimize interrupt-driven overhead, which is crucial for high-throughput analytics data movement and processing. While the set does not explicitly mention Spark or DataFusion in the provided lines, the emphasis on vectorized compute, shard-per-core designs, and data-plane efficiency aligns with the needs of Spark-like analytics workloads to maximize throughput and minimize tail latency. Additional excerpts emphasize safe, deterministic, and schedulable execution environments, which are relevant when designing analytics stacks that require predictable performance in production.",
      "confidence": "low"
    },
    {
      "field": "recommended_hardware_platform.1.justification_for_selection",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect."
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread"
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        }
      ],
      "reasoning": "The core claim in the fine-grained field is about selecting a hardware platform with a mature and high-performance NIC driver stack (mlx5) that supports DPDK and AF_XDP. Excerpts that discuss high-performance networking offloads and Vhost-user/SPDK backends demonstrate the general capability of NICs and user-space I/O paths to achieve high throughput and low latency, which is the same performance domain a strong mlx5-based solution would inhabit. Specifically, a statement that major Vhost-user use cases include networking offload in QEMU indicates that virtio-based ecosystems rely on fast user-space I/O paths (which mlx5-enabled setups would also leverage through DPDK/AF_XDP). The SPDK NVMe/IO article reinforces the idea that I/O paths can scale with per-core dedication and lock-free design, underscoring the importance of mature, high-performance I/O stacks. A Red Hat blog about making high-performance networking work in hybrid clouds further supports the notion that networking performance is a critical, well-tudied concern in modern deployments and that mature ecosystems (including DPDK-like approaches) are part of mainstream engineering. While none of the excerpts explicitly name mlx5, they collectively establish the context of a high-performance NIC ecosystem suitable for a platform that would rely on DPDK/AF_XDP and kernel/user-space I/O optimizations. This alignment suggests that selecting a platform with proven, scalable networking drivers and backends is coherent with the claimed strong competitor with a capable mlx5 ecosystem, even if mlx5 itself isn't named in the excerpts. Therefore, the most relevant excerpts substantiate the general high-performance networking capability and mature I/O pathways, rather than provide direct confirmation of mlx5 support in particular.\"",
      "confidence": "low"
    },
    {
      "field": "recommended_hardware_platform.2.justification_for_selection",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "rs. #### Configuration Details\n\nTest by Intel as of 10/22/2022. System: 1-node, 2x Intel® Xeon® Platinum 8480+ Processor (56 cores, HT=On, Turbo=ON),\nTotal Memory 1024 GB (16 slots/ 64GB/ 4800 MHz), BIOS:3A05 (ucode:0x2b000070),\nStorage: 24x Intel® Optane™ SSD 800GB P5800X."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking.",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        }
      ],
      "reasoning": "The field value centers on SPDK-based, user-space NVMe performance and compatibility in a standards-compliant setup that can saturate SPDK storage to benchmark high-throughput workloads such as Kafka logs. The most relevant information confirms that the SPDK NVMe driver avoids locks in the I/O path, enabling linear scalability as threads and cores are dedicated, which directly supports the claim of high-performance compatibility with SPDK. Additional highly pertinent points describe SPDK as a back-end option via vhost-user, which is how user-space storage often wires into a VM or guest, and emphasize SPDK's benchmarking capabilities showing strong IOPS and per-core gains. Other excerpts discuss SPDK NVMe driver benchmarks, the availability of a Perf tool for SPDK, and the role of vhost-user in networking or storage offload, all of which reinforce the assertion that SPDK achieves high throughput and scalability in practical configurations. There is also content about configuration details and test environments, which provides context for real-world applicability and validates that SPDK-backed setups can serve as high-performance targets for data-heavy workloads like Kafka. Taken together, these excerpts coherently support the idea of a standards-aligned, vendor-supported NVMe drive that works with SPDK in user space and delivers the performance profile described, making them highly relevant to the field value.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.1.key_features_and_performance",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread"
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpt discusses SPDK NVMe performance and the characteristic of the I/O path where the driver does not take locks, allowing linear scaling as queues and cores are dedicated. This aligns with high-throughput expectations and mature performance testing for fast NIC/storage backends, which underpins claims about mature, high-performance DPDK/DPDK-like backends and the absence of contention in critical I/O paths. The next tier includes excerpts that describe the SPDK vhost backend model and how virtio devices are backed by a high-performance consumer, which supports the notion of a driver model (Virtio with vhost-user) that enables efficient offloads and direct paths for high throughput. Following that, excerpts that cover the front-end/back-end dynamic of virtqueues and the role of QEMU in sharing virtqueues provide context for how high-throughput, low-latency paths can be realized in a virtualized environment using SPDK and vhost-user, which is relevant to the overall setup described in the fine-grained field value. An additional excerpt mentions high-performance networking work in hybrid clouds, which is thematically related to performance considerations for NICs and backends but is less specific to the exact features listed (AF_XDP zero-copy, SR-IOV, ConnectX-7, mlx5 PMD). Overall, the assembled excerpts collectively support the emphasis on mature, high-throughput I/O backends (SPDK, vhost-user) and the architecture that enables such performance characteristics, which are central to the fine-grained field value.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_for_target_workloads.1.architectural_impact",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "Due to its poll-mode nature, SPDK vhost removes the requirement for I/O submission notifications, drastically increasing the vhost server throughput and decreasing the guest overhead of submitting an I/O.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir",
            "The front-end sends I/O by allocating proper buffers in shared memory, filling the request data, and putting guest addresses of those buffers into virtqueues."
          ]
        },
        {
          "title": "Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads",
          "url": "https://amyousterhout.com/papers/shenango_nsdi19.pdf",
          "excerpts": [
            "\nShenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs.",
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "[PDF] Shenango: Achieving high CPU efficiency for latency-sensitive ...",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/131018/nsdi19fall-final110.pdf?sequence=2&isAllowed=y",
          "excerpts": [
            "Shenango can handle over five million requests per\nsecond while maintaining a median response time of\n37 µs and 99.9th percentile response time of 93 µs."
          ]
        },
        {
          "title": "Perséphone/SOSP21",
          "url": "https://joshfried.io/assets/persephone_sosp21.pdf",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a).",
            "Shinjuku can handle 85% of this load\nwhen preempting every 10𝜇s. 5.4.1\nHigh Bimodal. Shinjuku improves the tail latency\nof short requests over Shenango’s c-FCFS by preempting\nlong requests. However, Shinjuku aggressively preempts ev-\nery 5𝜇s to maintain good latency for short requests and adds\na constant overhead — at least 20% in this experiment — to\npreempted request"
          ]
        },
        {
          "title": "Perséphone, Shenango, Shinjuku, DARC: High-fidelity comparisons of latency-focused scheduling (SOSP 2021)",
          "url": "https://dspace.mit.edu/bitstream/handle/1721.1/146264/3477132.3483571.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "At 85% load, Perséphone offers 9.2x, 7x, and 3.6x improved p99.9 latency to Payment (b), OrderStatus (c) and\nNewOrder (d) transactions, compared to Shenango’s c-FCFS, reducing overall slowdown by up to 4.6x (a)."
          ]
        }
      ],
      "reasoning": "The most directly supportive content concerns high-performance I/O backends and I/O paths: SPDK vhost-user is described as exposing a back-end server for virtio devices and is noted for removing I/O submission notifications, drastically increasing server throughput and reducing guest overhead. This aligns with the field value's claim that storage should be SPDK-backed to achieve multi-figure order-of-magnitude improvements in software overhead, and that high-performance backends underpin the architecture. Additionally, the SPDK material emphasizes a poll-mode path that suppresses interrupts and can bypass parts of the traditional I/O path, which supports the idea of reducing CPU overhead for storage and network IO, matching the proposed architecture's efficiency goals. The Shenango-related excerpts discuss high-throughput, ultra-low-latency scheduling and microsecond-scale responsiveness, including evidence of very low median latencies and the ability to process millions of requests per second, which corroborates the field value's assertion that a Shenango-style scheduler can reallocate CPU cores on microsecond timescales to mitigate stragglers. This directly supports the scheduling component of the proposal (core reallocation every few microseconds) and the goal of sharp tail-latency control. The excerpts also touch on persistent, high-throughput performance characteristics and related scheduling approaches (e.g., shard-per-core concepts and USENIX/shenango lineage) that reinforce the feasibility of tight, microsecond-scale scheduling in a Rust-based stack. The Persephone/Shinjuku content discusses advanced scheduling in latency-focused contexts and can provide corroboration for the importance of scheduling strategies to improve tail latency, though it is not a perfect one-to-one mapping to microsecond scheduling; it nonetheless supports the broader premise that scheduling design can have large performance impacts. Finally, there are references to DataFusion/Arrow and modern analytics-oriented data pipelines in the field value, but within the provided excerpts there is no explicit discussion of DataFusion/Arrow, Arrow columnar processing, or Ballista/DataFusion in the text. Therefore, while the field value asserts a DataFusion/Arrow-based engine, the excerpts do not directly substantiate that particular component, limiting confidence on that portion. Overall, the strongest evidence supports SPDK/vhost and Shenango-style scheduling; evidence for the DataFusion/Arrow component is weak in the excerpts. ",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.1.hardware_type",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        }
      ],
      "reasoning": "The target field value identifies a network interface device (NIC). The most relevant excerpt explicitly discusses networking as a major Vhost-user use case, which aligns with NIC as a networking hardware type. The next relevant excerpt discusses networking applications and backends (including SPDK) in the context of high-performance networking, which supports the NIC/network path. Additional relevance comes from excerpts describing SPDK vhost-user and front-end/back-end interactions, which underpin how NIC-backed virtio networking is realized in practice. The least directly relevant excerpt mentions generic Virtio device models and queues, which are foundational but not specifically about NIC hardware, yet still contextually related to how NIC-like devices are modeled and connected in a Virtio ecosystem.",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.0.justification_for_selection",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir"
          ]
        },
        {
          "title": "Linux Base Driver for the Intel(R) Ethernet Controller 800 Series",
          "url": "https://docs.kernel.org/networking/device_drivers/ethernet/intel/ice.html",
          "excerpts": [
            "In a virtualized environment, on Intel(R) Ethernet Network Adapters that support SR-IOV, the virtual function (VF) may be subject to malicious behavior."
          ]
        },
        {
          "title": "Intel E810 Feature Summary and Related NVMe/ConnectX Details",
          "url": "https://cdrdv2-public.intel.com/630155/630155_Intel%20Ethernet%20Controller%20E810%20Feature%20Summary_rev4_6.pdf",
          "excerpts": [
            "June 2025"
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        }
      ],
      "reasoning": "The field value argues for a premier hardware platform because of its advanced feature set, including strong DPDK/AF_XDP support and superior virtualization capabilities (SR-IOV/SIOV). Vhost-user backends are a central pattern for achieving high-performance, user-space I/O, which aligns with a reference implementation aiming for low overhead and near-native performance. Excerpts describing SPDK's vhost processing establish that vhost-user backends enable high-throughput I/O paths by bypassing kernel boundaries where appropriate, and the front-end/back-end relationship clarifies how virtio devices can be accelerated through user-space systems such as SPDK. The cited material also emphasizes that poll-mode operation and SPDK NVMe workloads deliver high IOPS and scalable performance, which supports the claim that a premier platform with robust virtualization tooling will provide the best baseline for a reference stack. Additionally, vendor-specific feature material about a high-end Ethernet controller (E810) documents the breadth of device support and the alignment with DPDK/DPDK-like backends, reinforcing why this platform is a strong candidate for evaluation. A general high-performance networking overview reinforces the practical value of such an approach in hybrid-cloud contexts, where SR-IOV and related virtualization capabilities are essential. Taken together, these excerpts substantiate the reasoning that a premier platform with mature DPDK/AF_XDP support and strong virtualization features is best suited for a reference implementation.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.1.vendor_and_model",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues."
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread"
          ]
        },
        {
          "title": "Making high performance networking applications work on ...",
          "url": "https://www.redhat.com/en/blog/making-high-performance-networking-applications-work-hybrid-clouds",
          "excerpts": [
            "Nov 13, 2019 — AWS Cloud instances accelerated network interface offering is based on the Elastic Network Adapter (ENA), which provides data and control planes ..."
          ]
        }
      ],
      "reasoning": "The field value seeks a concrete NIC vendor/model. The excerpts that most closely relate to NICs and networking offloads describe vhost-user usage patterns and high-performance backends, which are the practical hardware interfaces that would be used with such devices. Specifically, one excerpt states that a major vhost-user use case is networking (DPDK) or storage (SPDK) offload in QEMU, which aligns with the kind of hardware networking cards and offload capabilities that high-performance NICs enable. Another excerpt explains that SPDK vhost is a back-end server exposing virtqueues, which is the mechanism by which a NIC like those from ConnectX families would be utilized in a virtualized environment. A third excerpt notes that SPDK NVMe performance scales with dedicated queues and cores, reinforcing the importance of high-performance I/O hardware and its offload capabilities, which would be relevant when evaluating NICs for low-latency, high-throughput workloads. A fourth excerpt provides general architectural context about front-end/back-end roles in virtio and the coupling to QEMU, which helps place hardware choices in context but does not constrain or specify a vendor. The fifth excerpt discusses high-performance networking applications and vendor-specific NIC trends (ENAs), which is tangentially related but focuses on a different family of adapters and cloud NIC offerings rather than ConnectX devices themselves. Overall, while the excerpts provide relevant context about networking offloads and virtio/vhost-user workflows, they do not contain any explicit mention of NVIDIA Mellanox ConnectX-5, ConnectX-6, or ConnectX-7, so they cannot confirm the exact vendor/model. The most direct relevance comes from passages describing vhost-user networking offloads and SPDK-backed backends, which are the closest proxies to the desired hardware class.",
      "confidence": "low"
    },
    {
      "field": "recommended_hardware_platform.0.key_features_and_performance",
      "citations": [
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "SPDK vhost is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "The front-end is the application that shares its virtqueues, in our case QEMU. The back-end is the consumer of the virtqueues.",
            "For the highest performance, a poll-mode Virtio driver can be used, as it suppresses all I/O completion interrupts, making the I/O path to fully bypass the QEMU ... SPDK: Vir"
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "SPDK provides a perf benchmarking tool which has minimal overhead during benchmarking."
          ]
        },
        {
          "title": "Linux Base Driver for the Intel(R) Ethernet Controller 800 Series",
          "url": "https://docs.kernel.org/networking/device_drivers/ethernet/intel/ice.html",
          "excerpts": [
            "In a virtualized environment, on Intel(R) Ethernet Network Adapters that support SR-IOV, the virtual function (VF) may be subject to malicious behavior."
          ]
        },
        {
          "title": "Intel E810 Feature Summary and Related NVMe/ConnectX Details",
          "url": "https://cdrdv2-public.intel.com/630155/630155_Intel%20Ethernet%20Controller%20E810%20Feature%20Summary_rev4_6.pdf",
          "excerpts": [
            "June 2025",
            "Intel® Ethernet Controller E810",
            "Intel® Ethernet Controller E810",
            "The following table lists the driver, firmware, and package versions recommended for use with the supported DPDK version. Table 9. DPDK ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a high-end, enterprise-ready network/storage platform feature set that maps closely to what proven high-performance virtualization and user-space I/O stacks deliver today. First, a mature Vhost-user path is central: SPDK vhost is described as a Vhost-user back-end server that exposes a mechanism for external applications to connect and share virtqueues, with QEMU acting as the front-end. This aligns with the requirement for a driver/firmware ecosystem that tightly coordinates hardware, firmware, and software layers via a robust virtualization path. In SPDK discussions, the Vhost-user model is shown as a primary path for networking and storage offloads, including how the front-end and back-end interact through virtqueues, which directly supports high-throughput, low-latency I/O that you expect from an advanced RDMA/AF_XDP-enabled stack.\n\nThe SPDK NVMe driver material demonstrates the scalability benefits of user-space I/O, emphasizing that the performance path benefits from lock-free I/O, dedicated queues, and per-thread scaling. This evidence directly substantiates the claim of high I/O performance and scalable queueing suitable for very high data rates on fast storage/media backends, which complements the network-oriented capabilities described in the target field value.\n\nAdditional excerpts describe inline/offload-friendly architectures and performance backends, including SPDK/NVMe perf measurements and guidance on achieving high IOPS per core. These details support the notion that a mature, shielded, and highly optimized driver/back-end stack exists and can be deployed with proper firmware and package coordination to maintain performance characteristics under realistic operating conditions.\n\nThere is also documentation around hardware drivers and feature support matrices for modern Intel Ethernet controllers, which corroborates that vendor-provided drivers and firmware versions must be tightly coordinated and kept up-to-date to achieve stable, high-performance operation in a virtualized, accelerated path. This aligns with the finegrained field's emphasis on firm coupling between driver, firmware, and distribution packages.\n\nFinally, the references to AF_XDP, RDMA (iWARP/RoCEv2), and extensive virtualization features are echoed in the broader material about advanced networking and vhost-based configurations, indicating that the combination of AF_XDP-enabled data paths, RDMA options, and large-scale virtualization support is a recognized capability in the ecosystem, even if every specific capability (e.g., exact VF counts or queue-pair limits) is not exhaustively enumerated in the excerpts.",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.3.hardware_type",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa"
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "The perf benchmarking tool provides several run time options to support the most common workload."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU."
          ]
        }
      ],
      "reasoning": "The strongest support comes from excerpts that discuss the SPDK NVMe driver and its performance characteristics, including extremely high IOPS and linear scalability when dedicated cores and queue pairs are used. This directly reinforces NVMe SSD as a suitable hardware type for high-performance storage workloads, since NVMe drives are the target hardware for SPDK-backed, user-space I/O paths that achieve those IOPS figures. The cited material also notes that SPDK can deliver concrete performance gains and scalable I/O without kernel contention, which aligns with selecting NVMe SSDs to maximize throughput in a Rust/OS stack that relies on high-speed storage backends. Additional excerpts explicitly describe the SPDK/NVMe ecosystem, including performance benchmarks and the claim that the SPDK NVMe driver scales as long as a queue pair and a CPU core are dedicated, reinforcing the appropriateness of choosing NVMe SSDs for this role. Also mentioned is virtualized I/O via Vhost-user for storage (in SPDK contexts), which further supports the relevance of NVMe storage in virtualization-enabled setups. Collectively, these excerpts establish a coherent evidence base that NVMe SSDs are the intended, high-performance storage hardware to pair with SPDK-backed, user-space I/O paths and virtualization backends, consistent with the finegrained value stating a hardware_type of NVMe SSD.",
      "confidence": "high"
    },
    {
      "field": "recommended_hardware_platform.3.key_features_and_performance",
      "citations": [
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa"
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload.",
            "The perf benchmarking tool provides several run time options to support the most common workload."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU."
          ]
        }
      ],
      "reasoning": "The most directly relevant passages describe SPDK-driven NVMe performance and how IOPS scales when a dedicated queue pair and CPU core are assigned to each thread, which supports the idea that a high-end NVMe drive can deliver very large IOPS figures under favorable conditions. In particular, the source explaining that SPDK NVMe drives can deliver very high IOPS and scale as you dedicate cores, provides the operational context that makes the claim of millions of random read IOPS plausible in a real system when using the right architecture. Another excerpt notes that the SPDK NVMe driver avoids locking in the I/O path, enabling linear scaling of performance per thread as cores are allocated, which reinforces the feasibility of achieving high random-read throughput on a fast NVMe device with proper isolation. A further passage mentions vhost-user backends (for storage and networking) in conjunction with SPDK, illustrating a practical stack (backends plus user-space I/O) that enables very high IOPS and low latency in deployment scenarios. Collectively, these pieces connect to the field value by confirming that (i) very high IOPS are achievable with NVMe devices when paired with SPDK and appropriate backends, (ii) per-core/threaded architectures can scale performance, and (iii) benchmarking methodologies and backends exist to realize and measure such capabilities. The remaining excerpts offer auxiliary context (benchmarking tools, and virtualization-oriented I/O paths) that further illustrate realistic paths to high performance but are less central to confirming the core capability claimed in the field value.\n",
      "confidence": "medium"
    },
    {
      "field": "recommended_hardware_platform.3.justification_for_selection",
      "citations": [
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The perf benchmarking tool provides several run time options to support the most common workload.",
            "We have measured up to 2.6 times more IOPS/core when using perf vs. fio with the 4K 100% Random Read workload."
          ]
        },
        {
          "title": "SPDK NVMe and Intel Optane performance (SPDK NVMe/NVMeoF article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "This blog explains the NVMe driver\nIOPS scalability. *The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new threa",
            "The SPDK NVMe driver does not take locks in the I/O path*, so it scales linearly\nin terms of performance per thread as long as a queue pair and a CPU core are\ndedicated to each new thread",
            "Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications."
          ]
        },
        {
          "title": "Virtualized I/O with Vhost-user",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU."
          ]
        }
      ],
      "reasoning": "The field value posits that a specific enterprise SSD vendor (Kioxia) and its standards-compliant drives are well-supported by SPDK's generic NVMe driver, with a range of endurance options and form factors enabling diverse server builds. The excerpts provide corroboration that SPDK's NVMe driver supports high-performance, scalable I/O, and that the SPDK/vhost-user combination enables high IOPS when dedicated cores are used, which aligns with the notion of leveraging a capable NVMe SSD in a high-performance server context. The excerpts also describe the scalability benefits of SPDK NVMe (no locking in the I/O path, linear scaling per thread with per-core dedication) and performance backends like vhost-user, which reinforce the feasibility of using a modern NVMe drive in a tuned environment. However, the excerpts do not mention Kioxia specifically, nor do they enumerate endurance options or form factors, which means the claim about Kioxia is not directly evidenced in the provided texts. Consequently, the strongest direct support comes from statements about SPDK NVMe driver performance and scalability, which underpin the overall claim that a high-end NVMe SSD from a major enterprise vendor can be effectively used with SPDK in a scalable, high-performance stack; vendor-specific endorsement (Kioxia) remains unverified by these excerpts.",
      "confidence": "medium"
    }
  ],
  "outputSchema": {
    "type": "object",
    "properties": {
      "executive_summary": {
        "type": "string",
        "description": "A high-level overview of the findings, addressing the core questions about the best business use cases for a high-performance Rust OS, the necessary drivers, and the overall feasibility and strategic positioning of the proposed concepts."
      },
      "technology_concepts_clarification": {
        "type": "string",
        "description": "Clarification of the user's terminology, explaining that 'Universal Virtio', 'Rust Apex', and 'Rust Hallows' are conceptual names and mapping them to standard industry technologies like the OASIS Virtio specification, user-space I/O with Rust, and ARINC 653-style high-assurance OS principles."
      },
      "top_business_use_cases": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "use_case_name": {
              "type": "string",
              "description": "The name of the high-value business use case (e.g., High-Frequency Trading)."
            },
            "workload_description": {
              "type": "string",
              "description": "A description of the specific workload and its characteristics."
            },
            "performance_slos": {
              "type": "string",
              "description": "Typical latency and throughput Service Level Objectives (SLOs) for this use case (e.g., p99.9 latency, Gbps throughput)."
            },
            "market_analysis": {
              "type": "string",
              "description": "Analysis of the market, including TAM/SAM and customer willingness-to-pay for performance improvements."
            },
            "competitive_advantage_summary": {
              "type": "string",
              "description": "Justification for why the proposed OS architecture is superior to the competitive baseline (e.g., tuned Linux)."
            }
          },
          "required": [
            "use_case_name",
            "workload_description",
            "performance_slos",
            "market_analysis",
            "competitive_advantage_summary"
          ],
          "additionalProperties": false
        },
        "description": "Detailed analysis of the highest-value server-side business use cases for a specialized Rust OS. Each use case includes a description of the workload, typical latency and throughput SLOs, market size, and a justification for why the proposed OS architecture provides a superior advantage over tuned Linux alternatives."
      },
      "pareto_set_of_virtio_drivers": {
        "type": "object",
        "properties": {
          "device_name": {
            "type": "string",
            "description": "The name of the Virtio device (e.g., virtio-net, virtio-blk)."
          },
          "purpose": {
            "type": "string",
            "description": "The primary function of the device (e.g., Network I/O, Storage I/O)."
          },
          "required_for_workloads": {
            "type": "string",
            "description": "The target workloads (Kafka, Spark, APIs) that depend on this driver."
          },
          "recommended_backend": {
            "type": "string",
            "description": "The recommended high-performance user-space backend (e.g., SPDK vhost-user, DPDK)."
          }
        },
        "required": [
          "device_name",
          "purpose",
          "required_for_workloads",
          "recommended_backend"
        ],
        "additionalProperties": false
      },
      "performance_analysis_for_target_workloads": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "workload_name": {
              "type": "string",
              "description": "The name of the target workload (e.g., Kafka, Spark-like Analytics, Backend APIs)."
            },
            "hot_path_analysis": {
              "type": "string",
              "description": "Analysis of the performance-critical hot paths for this workload (e.g., log append, shuffle/spill)."
            },
            "architectural_impact": {
              "type": "string",
              "description": "How the proposed OS architecture (user-space I/O, microsecond scheduling) addresses the hot paths."
            },
            "expected_performance_gains": {
              "type": "string",
              "description": "Quantified estimates of the expected improvements in throughput, latency, or reliability."
            }
          },
          "required": [
            "workload_name",
            "hot_path_analysis",
            "architectural_impact",
            "expected_performance_gains"
          ],
          "additionalProperties": false
        },
        "description": "A breakdown of the expected performance, latency, and reliability gains for each of the target workloads (Kafka, Spark, Backend APIs) when implemented on the proposed Rust OS. This includes analysis of hot paths, the impact of user-space I/O, and shard-per-core architectures."
      },
      "networking_stack_recommendation": {
        "type": "object",
        "properties": {
          "technology_name": {
            "type": "string",
            "description": "The name of the networking technology being compared (e.g., DPDK, AF_XDP)."
          },
          "performance_profile": {
            "type": "string",
            "description": "Summary of latency, throughput, and jitter characteristics."
          },
          "operational_tradeoffs": {
            "type": "string",
            "description": "Key operational considerations, such as OS integration, driver support, and management complexity."
          },
          "suitability_for_project": {
            "type": "string",
            "description": "Assessment of how well the technology aligns with the project's goals, especially the 'Universal Virtio' strategy."
          }
        },
        "required": [
          "technology_name",
          "performance_profile",
          "operational_tradeoffs",
          "suitability_for_project"
        ],
        "additionalProperties": false
      },
      "storage_stack_recommendation": {
        "type": "object",
        "properties": {
          "technology_name": {
            "type": "string",
            "description": "The name of the storage technology being compared (e.g., SPDK vhost-user, SPDK NVMe-oF)."
          },
          "performance_profile": {
            "type": "string",
            "description": "Summary of IOPS, latency, and CPU utilization characteristics."
          },
          "durability_and_operability": {
            "type": "string",
            "description": "Analysis of flush semantics, recovery mechanisms, and maintenance complexity."
          },
          "recommendation_for_workloads": {
            "type": "string",
            "description": "Specific recommendation for using this technology for Kafka logs and analytics spill files."
          }
        },
        "required": [
          "technology_name",
          "performance_profile",
          "durability_and_operability",
          "recommendation_for_workloads"
        ],
        "additionalProperties": false
      },
      "cloud_deployment_strategy": {
        "type": "object",
        "properties": {
          "provider_name": {
            "type": "string",
            "description": "The name of the cloud provider (e.g., AWS, GCP, Azure, OCI)."
          },
          "hypervisor_and_architecture": {
            "type": "string",
            "description": "Details on the cloud's underlying hypervisor and system architecture (e.g., Nitro System, KVM-based)."
          },
          "high_performance_interfaces": {
            "type": "string",
            "description": "Description of the vendor-specific high-performance networking and storage interfaces (e.g., ENA, gVNIC, NVMe)."
          },
          "virtio_compatibility_conclusion": {
            "type": "string",
            "description": "Conclusion on the viability of a standard Virtio approach for achieving high performance on this cloud."
          }
        },
        "required": [
          "provider_name",
          "hypervisor_and_architecture",
          "high_performance_interfaces",
          "virtio_compatibility_conclusion"
        ],
        "additionalProperties": false
      },
      "security_and_isolation_architecture": {
        "type": "object",
        "properties": {
          "threat_type": {
            "type": "string",
            "description": "The category of the security threat (e.g., RCE, Side-Channel Attack, DMA/PCIe Attack)."
          },
          "threat_description": {
            "type": "string",
            "description": "A brief description of the specific threat in a multi-tenant context."
          },
          "mitigation_strategy": {
            "type": "string",
            "description": "The proposed architectural and hardware-level mitigations (e.g., ARINC 653 partitioning, IOMMU, memory safety)."
          }
        },
        "required": [
          "threat_type",
          "threat_description",
          "mitigation_strategy"
        ],
        "additionalProperties": false
      },
      "certification_pathway_for_high_assurance": {
        "type": "object",
        "properties": {
          "pathway_component": {
            "type": "string",
            "description": "A key component of the certification pathway (e.g., Multicore Framework, Partitioning Model, Tool Qualification)."
          },
          "governing_standard": {
            "type": "string",
            "description": "The relevant industry standard or guidance document (e.g., EASA AMC 20-193, ARINC 653, DO-330)."
          },
          "implementation_details": {
            "type": "string",
            "description": "How this component will be implemented in the Rust OS to meet certification requirements."
          },
          "key_challenges": {
            "type": "string",
            "description": "The primary challenges or risks associated with this component, such as interference analysis or proving determinism."
          }
        },
        "required": [
          "pathway_component",
          "governing_standard",
          "implementation_details",
          "key_challenges"
        ],
        "additionalProperties": false
      },
      "recommended_hardware_platform": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "hardware_type": {
              "type": "string",
              "description": "The type of hardware component (e.g., NIC, NVMe SSD)."
            },
            "vendor_and_model": {
              "type": "string",
              "description": "The recommended vendor and model series (e.g., Intel E810, Samsung PM1743)."
            },
            "key_features_and_performance": {
              "type": "string",
              "description": "Key features relevant to the project, such as driver maturity, queue scalability, offloads, and performance metrics."
            },
            "justification_for_selection": {
              "type": "string",
              "description": "The reason for recommending this hardware, focusing on compatibility with SPDK, DPDK, and AF_XDP."
            }
          },
          "required": [
            "hardware_type",
            "vendor_and_model",
            "key_features_and_performance",
            "justification_for_selection"
          ],
          "additionalProperties": false
        },
        "description": "A list of recommended 'golden' hardware SKUs (NICs and NVMe SSDs) for first-year support to maximize performance and compatibility with DPDK, AF_XDP, and SPDK. Each recommendation includes the model, driver maturity, and reasons for selection."
      },
      "developer_experience_and_adoption_strategy": {
        "type": "object",
        "properties": {
          "strategy_area": {
            "type": "string",
            "description": "The area of focus for developer experience (e.g., API and Compatibility, Toolchain and Workflow)."
          },
          "component": {
            "type": "string",
            "description": "The specific component within that area (e.g., Syscall Surface, Async I/O Model, Build Toolchain)."
          },
          "proposed_approach": {
            "type": "string",
            "description": "The recommended approach or technology to use (e.g., Minimal POSIX subset, io_uring, Cargo custom targets)."
          },
          "justification": {
            "type": "string",
            "description": "The reason for the proposed approach, focusing on accelerating adoption and reducing friction for developers."
          }
        },
        "required": [
          "strategy_area",
          "component",
          "proposed_approach",
          "justification"
        ],
        "additionalProperties": false
      },
      "observability_for_low_latency_systems": {
        "type": "object",
        "properties": {
          "observability_layer": {
            "type": "string",
            "description": "The layer of the observability stack (e.g., Telemetry Primitives, Tracing Strategy, Logging Pipeline)."
          },
          "recommended_technology": {
            "type": "string",
            "description": "The recommended low-overhead technology or technique (e.g., eBPF, HdrHistogram, Lock-free ring buffers)."
          },
          "performance_characteristics": {
            "type": "string",
            "description": "Quantified overhead or performance benefits of the technology (e.g., nanosecond-level probe overhead)."
          },
          "role_in_system": {
            "type": "string",
            "description": "How this component contributes to safely operating microsecond-scheduled services without degrading tail latency."
          }
        },
        "required": [
          "observability_layer",
          "recommended_technology",
          "performance_characteristics",
          "role_in_system"
        ],
        "additionalProperties": false
      },
      "benchmark_methodology_for_validation": {
        "type": "object",
        "properties": {
          "principle_or_tool": {
            "type": "string",
            "description": "The core principle or specific tool being described (e.g., Coordinated Omission, HdrHistogram, fio)."
          },
          "description": {
            "type": "string",
            "description": "A description of the principle or tool and its importance for accurate benchmarking."
          },
          "implementation_guidance": {
            "type": "string",
            "description": "Specific guidance on how to apply the principle or configure the tool to ensure rigorous and reproducible results."
          }
        },
        "required": [
          "principle_or_tool",
          "description",
          "implementation_guidance"
        ],
        "additionalProperties": false
      },
      "competitive_landscape_and_differentiation": {
        "type": "object",
        "properties": {
          "competitor_name": {
            "type": "string",
            "description": "The name of the competing data platform or OS approach (e.g., Redpanda, ScyllaDB, DPU/SmartNIC Offloads)."
          },
          "architecture_summary": {
            "type": "string",
            "description": "A brief summary of the competitor's core architecture (e.g., C++ on Seastar, eBPF-based data plane)."
          },
          "performance_claims_and_benchmarks": {
            "type": "string",
            "description": "Key performance claims and relevant benchmark data."
          },
          "market_positioning": {
            "type": "string",
            "description": "The competitor's target market and use case."
          }
        },
        "required": [
          "competitor_name",
          "architecture_summary",
          "performance_claims_and_benchmarks",
          "market_positioning"
        ],
        "additionalProperties": false
      },
      "go_to_market_and_packaging_strategy": {
        "type": "object",
        "properties": {
          "strategy_element": {
            "type": "string",
            "description": "The element of the go-to-market strategy (e.g., Ideal Customer Profile, Packaging, Pricing, Support)."
          },
          "details": {
            "type": "string",
            "description": "Specific details and recommendations for this element (e.g., target industries, cloud marketplace images, tiered subscription models)."
          },
          "example_from_industry": {
            "type": "string",
            "description": "An example of a company that successfully uses this strategy (e.g., Red Hat, Nutanix, Confluent)."
          }
        },
        "required": [
          "strategy_element",
          "details",
          "example_from_industry"
        ],
        "additionalProperties": false
      },
      "project_risk_register": {
        "type": "object",
        "properties": {
          "risk_description": {
            "type": "string",
            "description": "A detailed description of the risk."
          },
          "risk_category": {
            "type": "string",
            "description": "The category of the risk (e.g., Technical, Ecosystem, Security, Business)."
          },
          "likelihood": {
            "type": "string",
            "description": "The estimated likelihood of the risk occurring (e.g., Low, Medium, High)."
          },
          "impact": {
            "type": "string",
            "description": "The estimated impact on the project if the risk materializes (e.g., Medium, High, Critical)."
          },
          "mitigation_plan": {
            "type": "string",
            "description": "The proposed plan to mitigate or manage the risk."
          }
        },
        "required": [
          "risk_description",
          "risk_category",
          "likelihood",
          "impact",
          "mitigation_plan"
        ],
        "additionalProperties": false
      },
      "product_roadmap_and_staffing_plan": {
        "type": "object",
        "properties": {
          "phase_name": {
            "type": "string",
            "description": "The name of the roadmap phase (e.g., First Production Image, Certified Variants)."
          },
          "duration_months": {
            "type": "number",
            "description": "The estimated duration of the phase in months."
          },
          "objective": {
            "type": "string",
            "description": "The primary objective of this phase."
          },
          "key_deliverables": {
            "type": "string",
            "description": "A list of the key deliverables for this phase."
          },
          "staffing_requirements": {
            "type": "string",
            "description": "The required team size and key skills needed for this phase."
          }
        },
        "required": [
          "phase_name",
          "duration_months",
          "objective",
          "key_deliverables",
          "staffing_requirements"
        ],
        "additionalProperties": false
      },
      "legal_and_licensing_considerations": {
        "type": "object",
        "properties": {
          "component_or_topic": {
            "type": "string",
            "description": "The technology or topic being analyzed (e.g., Virtio, SPDK, DPDK, Trademarks)."
          },
          "license_or_policy": {
            "type": "string",
            "description": "The primary license or IP policy governing the component (e.g., BSD 3-Clause, OASIS Non-Assertion Mode)."
          },
          "key_obligations_and_risks": {
            "type": "string",
            "description": "Key compliance obligations (e.g., DCO sign-off, attribution) or potential risks (e.g., patent encumbrances)."
          },
          "compliance_recommendation": {
            "type": "string",
            "description": "Recommended actions to ensure legal and license compliance."
          }
        },
        "required": [
          "component_or_topic",
          "license_or_policy",
          "key_obligations_and_risks",
          "compliance_recommendation"
        ],
        "additionalProperties": false
      }
    },
    "required": [
      "executive_summary",
      "technology_concepts_clarification",
      "top_business_use_cases",
      "pareto_set_of_virtio_drivers",
      "performance_analysis_for_target_workloads",
      "networking_stack_recommendation",
      "storage_stack_recommendation",
      "cloud_deployment_strategy",
      "security_and_isolation_architecture",
      "certification_pathway_for_high_assurance",
      "recommended_hardware_platform",
      "developer_experience_and_adoption_strategy",
      "observability_for_low_latency_systems",
      "benchmark_methodology_for_validation",
      "competitive_landscape_and_differentiation",
      "go_to_market_and_packaging_strategy",
      "project_risk_register",
      "product_roadmap_and_staffing_plan",
      "legal_and_licensing_considerations"
    ],
    "additionalProperties": false
  }
}