

# **An Architectural and Strategic Analysis of the OpenSearch Ecosystem for the Advanced Open Source Contributor**

### **Part I: Project Deconstruction: History, Architecture, and Ecosystem**

#### **1.0 The Genesis of a Fork: Unpacking the OpenSearch Origin Story**

The OpenSearch project did not emerge in a vacuum; it was born from a foundational schism in one of the most successful open-source projects of the last decade. Understanding this origin is critical to grasping its technical roadmap, community dynamics, and strategic position in the market. The project's existence is a direct consequence of the complex and often fraught relationship between corporate stewardship and the principles of free and open-source software.

##### **1.1 The Licensing Schism: From Apache 2.0 to SSPL**

On January 21, 2021, Elastic NV, the company behind Elasticsearch and Kibana, announced a pivotal change to its licensing strategy.1 For years, the core products had been distributed under the Apache License, Version 2.0 (ALv2), a permissive open-source license highly favored in the industry for its grant of broad freedoms. The ALv2 license allows users to freely use, modify, extend, monetize, and resell the software with minimal restrictions, fostering a vibrant ecosystem of both community and commercial usage.3

However, starting with version 7.11, Elastic transitioned new releases to a dual-license model: the Server Side Public License (SSPL) and the Elastic License v2.4 This was a profound departure. The SSPL, in particular, introduced a controversial clause requiring that any entity offering the software as a service must also release the source code of all supporting and management software under the SSPL. This provision was widely seen as a direct response to the business practices of major cloud providers, most notably Amazon Web Services (AWS), which offered a popular managed service based on open-source Elasticsearch.4

The SSPL is not recognized as an open-source license by the Open Source Initiative (OSI), a key arbiter in the community.6 This reclassification effectively moved Elasticsearch and Kibana from the realm of open-source software to a "source-available" or proprietary model for many use cases. The event ignited a significant debate about the sustainability of the "open core" business model, where a company provides a free, open-source core product while selling proprietary features and services. Elastic's move was a clear signal that it intended to more aggressively protect its commercial interests, even at the cost of alienating a segment of its open-source user base.4

##### **1.2 AWS's Fork and the Community Rally**

The response from the community, and particularly from AWS, was swift and decisive. As the operator of the Amazon Elasticsearch Service, AWS had built a significant business on the open-source version of the software. Elastic's licensing change posed a direct threat to this service, as it would prevent AWS from offering future versions of Elasticsearch without complying with the restrictive SSPL terms.

To ensure the continued availability of a fully open-source, permissively licensed search and analytics suite, AWS announced the creation of the OpenSearch project on April 12, 2021\.3 OpenSearch is a direct fork of the last ALv2-licensed versions of the software: Elasticsearch 7.10.2 and Kibana 7.10.2.2 AWS committed to a long-term investment in the project, pledging to maintain it as a secure, high-quality, and community-driven alternative.3 The initial work involved a substantial effort to scrub the codebase of all Elastic trademarks, telemetry, and proprietary code.3

This move was not undertaken in isolation. The industry's reaction demonstrated a widespread unease with Elastic's licensing shift. A consortium of other major technology companies and service providers, including SAP, Uber, Red Hat, Logz.io, and CrateDB, quickly announced their support for the OpenSearch initiative.3 This broad backing underscored a collective desire within the industry for a foundational search technology that would remain unencumbered by restrictive licensing and single-vendor control, ensuring a level playing field for innovation and commercial offerings.

##### **1.3 Governance Evolution: The OpenSearch Software Foundation**

A primary and persistent criticism leveled against OpenSearch, particularly by Elastic, was that it was not a true community project but rather a proprietary tool governed by and for AWS.4 In its initial phase, this critique held some weight. While AWS invited pull requests from the community, it retained sole ownership and write access to the core source code repositories.8 For the project to achieve long-term viability and attract a diverse set of contributors, it needed to transition to a more neutral and open governance model.

This transition occurred on September 16, 2024, with the announcement of the OpenSearch Software Foundation, an open technical project established within the Linux Foundation.8 This was a landmark moment for the project. Ownership of the OpenSearch software was formally transferred from Amazon to the new foundation, signaling a definitive move toward vendor-neutral governance. The foundation launched with premier members AWS, SAP, and Uber, solidifying its status as a multi-stakeholder initiative.8

This move directly addresses the core governance concerns. The Linux Foundation is a globally respected, neutral entity that hosts some of the world's most critical open-source projects, including Linux and Kubernetes. Placing OpenSearch under its umbrella provides a clear signal to the market that the project's roadmap and technical direction will be driven by a diverse community, not solely by the product needs of AWS.1 Furthermore, the project's policy of not requiring a Contributor License Agreement (CLA) lowers the barrier to entry for developers and reinforces its commitment to open collaboration.3 This strategic evolution is essential for fostering the trust required to build a sustainable, long-term ecosystem capable of competing with its well-established predecessor.

The table below provides a comparative summary of the key strategic and technical differences between the OpenSearch and Elasticsearch ecosystems as they stand today.

| Feature | OpenSearch | Elasticsearch |
| :---- | :---- | :---- |
| **Licensing** | Apache License 2.0 (ALv2) \- a permissive, OSI-approved open-source license.7 | Dual License: Server Side Public License (SSPL) and Elastic License v2. Not OSI-approved.4 |
| **Governance Model** | Community-driven through the OpenSearch Software Foundation, a Linux Foundation project.8 | Corporate-led by Elastic NV, which controls the roadmap and core codebase.4 |
| **Core Security Features** | Security features like fine-grained access control, encryption, and audit logging are included by default and are fully open source.7 | Basic security is included. Advanced features are part of the proprietary X-Pack extensions under a paid subscription.7 |
| **Vector Search** | Integrated vector search capabilities (k-NN) are a core, open-source feature, supporting engines like NMSLIB and FAISS.12 | Advanced vector search capabilities are a key area of development, built on proprietary optimizations to Apache Lucene.4 |
| **Performance Claims (Pro-Elastic)** | Elastic claims its product is 40-140% faster for text queries and aggregations, and 2-12x faster for vector search, due to ongoing proprietary optimizations.4 | Superior performance in various benchmarks, attributed to deep expertise and continuous investment in core engine optimization.4 |
| **Performance Claims (Pro-OpenSearch)** | An independent assessment showed OpenSearch 2.17.1 to be 1.6x faster on the "Big5" workload and 11% faster on a vector search workload than Elasticsearch 8.15.4.16 | Performance can be workload-dependent; claims of superiority are contested by third-party benchmarks.16 |
| **Primary Backers** | AWS, SAP, Uber, and a growing community of organizations and individuals under the Linux Foundation umbrella.8 | Elastic NV is the sole corporate entity driving development and commercialization.4 |

#### **2.0 Core Architecture: A Deep Dive into the Lucene-Based Engine**

To contribute meaningfully to OpenSearch or to design a superior alternative, a deep understanding of its core architecture is essential. The system is a complex, multi-layered distributed platform, but its functionality is ultimately rooted in the capabilities of the foundational Apache Lucene library.

##### **2.1 The Apache Lucene Foundation: Inverted Indexes, Segments, and Scoring**

At its heart, OpenSearch is a user-friendly, distributed wrapper around Apache Lucene.1 Lucene is a mature, high-performance search engine library written entirely in Java that provides the core indexing and search functionality.13 It is not a standalone server but a library that developers can embed in their applications. Both OpenSearch and Elasticsearch leverage Lucene to do the heavy lifting of data storage and retrieval.1

The fundamental data structure that enables Lucene's remarkable speed is the **inverted index**.20 Unlike a traditional database index that maps a primary key to a row of data, an inverted index maps terms (e.g., words) to a list of documents in which they appear. When a document is indexed, its text fields are subjected to an analysis process, which includes tokenization (breaking text into terms), lowercasing, and removing stop words. The resulting terms are then added to the inverted index.20

When a search query is executed, Lucene looks up the query terms in the inverted index to quickly retrieve a list of matching documents. This process is significantly faster than scanning every document for the search terms, especially with large datasets.18 Lucene also calculates a relevance score for each document based on statistical models like TF-IDF (Term Frequency-Inverse Document Frequency) or, more recently, Okapi BM25.17 These models consider factors such as how often a term appears in a document (term frequency) and how rare the term is across the entire collection of documents (inverse document frequency) to rank the most relevant results first.21 As the search landscape evolves, Lucene continues to incorporate advanced features, including native support for nearest-neighbor vector search, which is critical for modern AI and semantic search applications.19

##### **2.2 The Distributed System: Clusters, Nodes, and Shard Mechanics**

While Lucene provides the core search library, OpenSearch's primary value proposition is its ability to scale this functionality across a fleet of servers. OpenSearch is, by design, a distributed system built for horizontal scalability and high availability.21

The highest-level abstraction is the **cluster**, which is a collection of one or more interconnected **nodes** (physical or virtual servers) working together.21 Each node in a cluster can be assigned specific roles to optimize resource usage. Key roles include:

* **Cluster Manager (Master) Node:** Responsible for managing the overall cluster state, tracking which nodes are part of the cluster, and deciding where to allocate shards. In production environments, it is a best practice to have dedicated master nodes that do not handle data or search queries.23  
* **Data Node:** Stores data in the form of shards and handles data-related operations like indexing, searching, and aggregations. These nodes are the workhorses of the cluster and require sufficient CPU, RAM, and fast storage.23  
* **Coordinating Node:** A node that receives client requests, forwards them to the appropriate data nodes, and consolidates the results before sending them back to the client. This offloads the work of request coordination from data and master nodes.24

To distribute data across the cluster, an OpenSearch index is partitioned into one or more **shards**. Each shard is a fully independent, self-contained Lucene index.21 For example, a 1 TB index can be split into ten 100 GB shards, which can then be distributed across ten different data nodes, allowing for parallel processing of queries. This sharding mechanism is the key to OpenSearch's ability to scale horizontally.20

For fault tolerance and increased read capacity, OpenSearch creates one or more copies of each primary shard, known as **replica shards** or replicas. Replicas are never placed on the same node as their primary shard, ensuring that if a node fails, a copy of its data exists elsewhere in the cluster.21 Replicas can also serve read requests, thereby increasing the overall search throughput of the cluster.

##### **2.3 Data Organization: Indices, Documents, and Mappings**

Within the distributed architecture, data is organized logically. The fundamental unit of information in OpenSearch is a **document**, which is represented as a JSON object.20 A document is analogous to a row in a relational database table.

Documents are stored within an **index**. An index is a logical collection of documents that typically share a similar structure.20 For example, in an e-commerce application, there might be a

products index and a customers index.

The structure and data types of the fields within the documents of an index are defined by a **mapping**.20 The mapping is akin to a schema in a traditional database. It specifies, for each field, its type (e.g.,

text, keyword, integer, date, knn\_vector), how it should be analyzed (if it is text), and other properties. While OpenSearch can dynamically generate mappings based on the first document it sees, this is often inefficient. For production workloads, defining an explicit mapping is a critical best practice. An unoptimized mapping can lead to wasted storage, slower query performance, and unexpected search results.25

##### **2.4 The OpenSearch Stack: Core Engine, Dashboards, and Data Prepper**

The OpenSearch project is more than just the core search engine; it is a comprehensive suite of tools designed to provide an end-to-end solution for search and analytics.8 The primary components of the stack are:

* **OpenSearch:** The core distributed search and analytics engine described above. It is written primarily in Java and built on Apache Lucene.8  
* **OpenSearch Dashboards:** The primary user interface for visualization, data exploration, and cluster management. It is a fork of Kibana and is developed using TypeScript and JavaScript.8 It allows users to create interactive dashboards, analyze log data, and manage index settings.  
* **Data Prepper:** A server-side data pipeline component used to filter, enrich, transform, and aggregate data before it is ingested into OpenSearch.1 It is particularly useful for observability use cases, such as processing logs and traces from various sources into a standardized format.5  
* **Client Libraries:** The project maintains a suite of official client libraries to enable developers to interact with the OpenSearch API from their applications. These clients are available for a wide range of popular programming languages, including Python, Java, Go,.NET, and Rust.26

This modular architecture provides multiple distinct areas for potential contribution. A developer does not need to be an expert in Java and distributed systems to contribute to the TypeScript-based frontend of OpenSearch Dashboards or to improve the developer experience by enhancing one of the client libraries. This separation of concerns, however, also introduces challenges. The overall user experience depends on the seamless integration of these components. A significant opportunity for innovation lies not just in improving individual components, but in strengthening the "seams" between them. For instance, creating a unified configuration management system that spans Data Prepper, the core engine's index templates, and Dashboards' index patterns would represent a major leap forward in usability and operational efficiency by reducing the complexity and potential for error inherent in configuring each piece separately.

#### **3.0 The Apple Inquiry: Corporate Contributions in Open Source**

The inquiry regarding Apple's contributions to OpenSearch is a proxy for a deeper question about the project's health, strategic importance, and long-term viability. The engagement of multiple, independent, large-scale technology companies is often a strong indicator of a project's maturity and its path toward becoming an industry standard.

##### **3.1 Direct Contributions to OpenSearch: An Evidence Review**

A thorough review of the project's documentation, foundation members, and community discussions yields no evidence of a formal, strategic contribution to the OpenSearch project by Apple Inc..12 The list of premier members of the OpenSearch Software Foundation at its launch included Amazon Web Services, SAP, and Uber.8 While it is common for employees of large tech companies to contribute to open-source projects under their personal accounts or as part of their work without explicit corporate branding 29, there is no indication of a corporate-level, strategic investment from Apple in the OpenSearch ecosystem akin to its involvement in other major projects. The answer to the direct query is, therefore, that Apple does not appear to be a significant contributor to OpenSearch.

##### **3.2 A Parallel Case Study: Apple's Foundational Impact on Apache Iceberg**

To understand what a high-impact corporate contribution looks like, it is instructive to examine a case where Apple is deeply involved: the Apache Iceberg project. Iceberg is an open table format for huge analytic datasets, designed to bring the reliability and simplicity of SQL tables to big data environments. Apple is not merely a user of Iceberg; it is a key contributor, driving the project's evolution based on its internal needs for managing data at a petabyte scale.31

Apple's contributions to Iceberg are foundational and technically profound. They include:

* **Core Functionality:** The implementation of row-level operations through Copy-on-Write (CoW) and Merge-on-Read (MoR) strategies, which are critical for handling data updates and deletes efficiently in large-scale data lakes.31  
* **Scalability Enhancements:** The development of distributed maintenance procedures within Apache Spark and the introduction of write distribution modes to manage high-frequency data ingestion without performance degradation.31  
* **Performance and Interoperability:** The pioneering of storage-partitioned joins to minimize expensive data shuffling during query processing and improvements to vectorized readers.31  
* **Ecosystem Enablement:** The contribution of zero-copy migration tools, which significantly lower the barrier to adoption for organizations with existing large datasets by allowing them to register data with Iceberg without rewriting it.31  
* **Roadmap Influence:** Actively driving the development of future features such as secondary indexes, materialized views, and native encryption to bring data warehouse-like capabilities to the data lake.31

This case study provides a clear blueprint for what constitutes a "Strategic Contributor" model of corporate engagement in open source. The contributions are problem-driven, technically deep, and create a positive feedback loop where solving internal challenges directly benefits the broader community.

##### **3.3 Models of Corporate Engagement in Modern OSS**

Apple's deep involvement with Apache Iceberg exemplifies a powerful model of corporate open-source engagement. It highlights the difference between various modes of participation. OpenSearch, in its current form, is dominated by the "Originator" model, where AWS initiated the project and remains its primary driving force and largest contributor. While this provides strong initial momentum, the long-term health of a foundational open-source project depends on its ability to evolve beyond this single-vendor-led phase.

The project's success will be measured by its capacity to attract more "Strategic Contributors"—organizations like Apple with Iceberg—that are willing to invest significant engineering resources to solve their own unique, large-scale problems on the OpenSearch platform and contribute those solutions back to the community. The presence of companies like SAP and Uber as premier foundation members is a positive first step, but the emergence of deep, technically-driven contributions from a diverse set of such stakeholders will be the true indicator of OpenSearch's transition from an AWS-led project to a multi-stakeholder, industry-standard platform. This framework provides a valuable lens through which to evaluate the project's ongoing health and strategic trajectory.

### **Part II: A Practitioner's Guide to Contributing to OpenSearch**

For an experienced Open Source Software (OSS) contributor, navigating a new project involves understanding not only the technical landscape but also the community norms, development workflows, and strategic areas where contributions can have the most impact. This section provides a practical guide to engaging with and contributing to the OpenSearch ecosystem.

#### **4.0 Navigating the Contribution Landscape**

The OpenSearch project has established a well-defined and welcoming process for new contributors, recognizing that a healthy community is the lifeblood of a successful open-source initiative.

##### **4.1 The Onboarding Process: From CONTRIBUTING.md to Your First Pull Request**

The project's central hub on GitHub serves as the primary entry point. The main organization page prominently features links to key documents designed to orient newcomers.32 The recommended first steps are:

1. **Read the Onboarding Guide:** The project maintains a [step-by-step onboarding guide](https://github.com/opensearch-project/.github/blob/main/ONBOARDING.md) that covers setting up a development environment, understanding the repository structure, and familiarizing oneself with the project's tools and processes.32  
2. **Review the Contributing Guide:** A general [contributing guide](https://github.com/opensearch-project/.github/blob/main/CONTRIBUTING.md) outlines the development workflow, including branching strategies, commit message formatting, and the expectations for pull requests (PRs).32 Each sub-repository (e.g.,  
   OpenSearch, OpenSearch-Dashboards) may also have its own specific CONTRIBUTING.md file with additional details.  
3. **Adhere to the Code of Conduct:** The project operates under a Code of Conduct that outlines expectations for respectful and collaborative participation in the community.33

A crucial piece of advice for those starting out is to begin with small, well-defined tasks. Submitting large, sweeping pull requests as a first contribution can be difficult for maintainers to review and may require extensive revisions. Starting with a small bug fix or a minor documentation improvement is an excellent way to learn the process and build trust within the community.34

##### **4.2 Identifying Impactful Work: Finding "Good First Issues" and Project Needs**

The OpenSearch project makes a concerted effort to flag issues that are suitable for new contributors. The primary mechanism for finding actionable work is through GitHub issues. Several key repositories use labels like good first issue, help wanted, or untriaged to identify tasks that are well-scoped and provide a good entry point.

Repositories that frequently have issues needing assistance include:

* OpenSearch (the core engine) 32  
* index-management (plugin for automating data operations) 32  
* sql (plugin for SQL/PPL query support) 32  
* flow-framework (plugin for building AI applications) 32  
* oui (OpenSearch UI Framework) 32  
* job-scheduler (plugin for running periodic jobs) 32

Before starting work on an issue, it is standard practice to leave a comment expressing intent to work on it. This prevents duplicated effort and allows maintainers to provide initial guidance or context.

##### **4.3 Engaging the Community: Effective Communication on Slack, GitHub, and Forums**

Technical proficiency is only one aspect of successful OSS contribution; effective communication is equally important. The OpenSearch project provides several channels for community interaction, and using them strategically can significantly accelerate a contributor's integration and impact.10

* **Slack:** The([https://opensearch.org/slack.html](https://opensearch.org/slack.html)) is the best channel for real-time, informal communication. It is ideal for asking quick setup questions, getting clarification on a specific piece of code, or having informal discussions with other developers and maintainers.32  
* **Forums:** The([https://forum.opensearch.org/](https://forum.opensearch.org/)) is designed for more complex, asynchronous discussions. This is the appropriate venue for proposing new features, discussing architectural changes, or asking detailed questions that may benefit from longer, more thoughtful answers. Forum threads are indexed and searchable, creating a valuable knowledge base for the entire community.32  
* **GitHub Issues:** This is the formal channel for tracking specific, actionable work. Bug reports, feature requests, and discussions directly related to a piece of code or a planned change should take place within a GitHub issue.  
* **Community Meetings:** The project holds regular public meetings, including triage meetings for new issues and special interest group meetings.32 Participating in these can provide valuable insight into the project's priorities and decision-making processes.

A common pitfall for new contributors is working in isolation and not asking for help when they get stuck.34 The community channels are there to be used. Engaging with maintainers early in the process—for example, by outlining a proposed solution in a GitHub issue before writing any code—can save a significant amount of time and ensure that a contribution is aligned with the project's direction.

#### **5.0 Technical Contribution Pathways**

The modular nature of the OpenSearch stack offers a variety of technical pathways for contribution, catering to a wide range of skills and interests.

##### **5.1 Core Engine (Java): Bug Fixes, Performance Enhancements, and New Features**

The most impactful and challenging contributions are often made to the core engine, which is located in the opensearch-project/OpenSearch repository.32 This codebase is written primarily in Java and deals with the complexities of distributed systems, data indexing, query processing, and cluster management.8 Contributions in this area require a strong background in Java and a willingness to delve into the intricacies of Apache Lucene. Potential areas of contribution include:

* Fixing bugs related to cluster stability or query correctness.  
* Optimizing the performance of specific query types or indexing operations.  
* Implementing new API endpoints or features as outlined in the project roadmap.  
* Improving the resilience and fault tolerance of the distributed coordination layer.

##### **5.2 Plugin Ecosystem: Extending Functionality in Kotlin, Java, and More**

The OpenSearch plugin architecture provides a powerful mechanism for extending the engine's functionality without modifying the core code. This is an excellent area for contributing significant, self-contained features. The plugin ecosystem uses a variety of languages. For example, the index-management plugin is written in Kotlin, while the sql and flow-framework plugins are written in Java.32

These plugins are not trivial add-ons; they provide critical capabilities such as SQL and Piped Processing Language (PPL) support, anomaly detection, machine learning commons, and alerting.1 Contributing to an existing plugin or creating a new one is a high-impact way to add value to the ecosystem.

##### **5.3 Client Libraries: Improving Developer Experience Across Languages (Python, Go, Rust)**

For developers who use OpenSearch, the client libraries are their primary interface with the system. A high-quality, ergonomic, and well-documented client can significantly improve the developer experience and drive adoption of the platform. OpenSearch maintains official clients for a wide array of languages, including Python, Go,.NET, and, most relevantly for the user query, **Rust**.26

The history of these clients is tied to the project's schism with Elastic. After Elastic added license and version checks to its official clients to prevent them from connecting to OpenSearch clusters, the OpenSearch project forked the last compatible versions and now maintains them independently.6 This makes the quality of the OpenSearch-maintained clients a strategic priority and a key competitive differentiator.

For a contributor with expertise in Rust, focusing on the opensearch-rust client is a direct and highly leveraged pathway to making an impact. The Rust ecosystem is renowned for its emphasis on performance, memory safety, and excellent developer tooling. Creating a best-in-class Rust client—one that is not only feature-complete but also idiomatic and highly performant—would be a significant contribution. It would not only serve the growing community of Rust developers but also act as a "halo" project, showcasing the quality and developer-friendliness of the entire OpenSearch ecosystem.

##### **5.4 Documentation and Web Presence: The Non-Code Contribution Backbone**

High-quality documentation is as critical to an open-source project's success as high-quality code. The OpenSearch project actively encourages and values non-code contributions. There are two primary repositories for this:

* documentation-website: For contributions to the official technical documentation.32  
* project-website: For contributions to the project's blog and general web content.32

The project has a detailed style guide to ensure consistency in its written content.35 For an expert practitioner, writing a deep-dive blog post on a complex topic (e.g., performance tuning for a specific workload, advanced aggregation techniques) or significantly improving a section of the official documentation is a highly visible and valuable form of contribution.

### **Part III: Beyond OpenSearch: Innovating the Next Generation of Search**

While contributing to an established project like OpenSearch offers a clear path to impact, it is also valuable to critically analyze its architectural limitations and explore novel approaches. This section examines the inherent challenges in the OpenSearch/Elasticsearch model and evaluates the potential for a new generation of search engines, including the proposed "Partitioned Rust Kernel."

#### **6.0 A Critical Analysis of OpenSearch's Architectural Limitations**

The architectural design of OpenSearch, inherited from Elasticsearch, is a product of its time. While incredibly powerful, it carries trade-offs that present significant challenges in terms of operational complexity, performance tuning, and cost at scale. These limitations are not necessarily flaws but rather inherent characteristics of its tightly-coupled, JVM-based architecture.

##### **6.1 The JVM Bottleneck: Memory Pressure, Garbage Collection, and Tuning Complexity**

OpenSearch is a large, stateful, distributed application running on the Java Virtual Machine (JVM). This reliance on the JVM is a double-edged sword. It provides a mature, cross-platform runtime environment, but it also introduces a significant layer of abstraction and complexity that is a primary source of performance issues and operational overhead.

High JVM memory pressure is one of the most common and critical problems encountered when operating OpenSearch clusters.36 The service has well-defined pressure thresholds: when heap usage consistently reaches 75%, a garbage collection (GC) cycle is initiated; if it exceeds 92% for an extended period, write operations are blocked to protect the cluster's stability.37 This pressure can be caused by a variety of factors, including insufficient heap size, memory-intensive aggregations, large field data caches, or simply a workload that exceeds the capacity of the nodes.37

Managing this requires deep expertise in JVM tuning. Administrators must carefully configure heap size (typically setting the initial (-Xms) and maximum (-Xmx) sizes to be equal and no more than 50% of the node's RAM, up to a maximum of \~32 GB) and monitor garbage collection logs to identify long "stop-the-world" pauses that can cripple cluster performance.39 This complexity represents a significant operational burden and a key area where an alternative architecture, particularly one built in a language with explicit memory management like Rust, could offer substantial benefits by eliminating GC pauses and providing more predictable performance.

##### **6.2 Scalability Challenges: Shard Strategy, Indexing Speed, and Query Performance**

The scalability of OpenSearch is fundamentally tied to its sharding strategy, and this is another area of immense complexity. The number of shards, their size, and their distribution across nodes are the most critical tuning parameters for performance, yet they are notoriously difficult to get right.41

Common failure modes include:

* **Over-sharding:** Creating too many small shards. Each shard consumes memory and CPU resources for metadata management. A cluster with thousands of tiny shards can become overwhelmed by the overhead of managing them, leading to slow query performance and instability.25  
* **Under-sharding:** Creating too few, overly large shards. This limits parallelism, as a query can only be processed by as many threads as there are shards. A single massive shard can become a bottleneck, and its recovery or relocation after a node failure can be extremely slow.41

The generally accepted best practice is to aim for a shard size between 10 GB and 50 GB for search workloads.25 However, achieving this in practice requires careful capacity planning and the use of tools like Index State Management (ISM) to roll over indices based on size or age. Beyond sharding, slow performance can be caused by unoptimized index mappings (e.g., relying on dynamic mappings) or executing expensive queries that involve leading wildcards, deep aggregations, or scripts.36 The public discourse on performance is contentious, with both Elastic and independent parties publishing benchmarks that show conflicting results depending on the specific workload, highlighting the sensitivity of these systems to configuration and use case.4

##### **6.3 The Monolithic vs. Composable Debate in Search Architecture**

The challenges with JVM tuning and shard management point to a more fundamental architectural characteristic. The OpenSearch model is relatively monolithic, with indexing, storage, and query processing tightly coupled within a single, integrated system. Data must be ingested into OpenSearch's proprietary format and stored on disks directly attached to the data nodes.20 This means that compute resources (the nodes) and storage resources are inextricably linked.

This tight coupling leads to scaling inefficiencies. To add more storage capacity, one must add more data nodes, which also adds more compute capacity, whether it is needed or not. Conversely, to add more query processing power, one must add more nodes, which also brings more storage. This can be a cost-inefficient model, particularly in cloud environments where resources can be provisioned on demand.

The broader trend in modern, cloud-native data platforms is toward **disaggregation**, or the separation of compute and storage. Systems like Snowflake, BigQuery, and data lakehouse platforms built on Apache Iceberg store their data in open, standardized formats (like Apache Parquet) on cheap, highly scalable object storage (like Amazon S3). The query engines are a stateless compute layer that can be scaled up or down independently of the storage layer. This architectural pattern offers superior elasticity, cost-effectiveness, and flexibility. The most significant opportunity for innovating in the search space may not be simply to rebuild OpenSearch in a different language, but to apply this proven, disaggregated architectural pattern to the domain of search and analytics.

#### **7.0 Evaluating the "Partitioned Rust Kernel" Concept**

The proposal to create a "custom Partitioned Rust kernel" is an ambitious and forward-thinking idea that correctly identifies the core limitations of the current architecture. By deconstructing this concept and grounding it in established architectural principles, it can be refined into a viable and potentially transformative project.

##### **7.1 Deconstructing the Proposal: Interpreting "Partitioned Kernel" in a Search Context**

While "Partitioned Rust kernel" is not standard industry terminology, its intent is clear. "Kernel" suggests a new, core engine component responsible for the most performance-critical operations. "Rust" indicates the choice of implementation language to overcome the limitations of the JVM. "Partitioned" points to the distributed nature of the system, handling the partitioning and distribution of data, analogous to OpenSearch's sharding.

In essence, the proposal is for a new, high-performance, distributed storage and retrieval engine, written in Rust, and optimized for search workloads. This is a sound and compelling vision that directly targets the primary pain points of the existing OpenSearch architecture.

##### **7.2 The Merits of Rust: Memory Safety, Concurrency, and Performance Gains**

The choice of Rust as the implementation language is technically astute and aligns with modern trends in high-performance systems programming. Rust is increasingly the language of choice for building databases, storage engines, and other infrastructure software where performance and reliability are paramount.43 Its key advantages relative to Java in this context are:

* **Memory Safety without a Garbage Collector:** Rust's ownership and borrowing system provides compile-time guarantees of memory safety, eliminating entire classes of bugs (like null pointer dereferences and data races) without the need for a runtime garbage collector. This completely sidesteps the JVM GC problem, eliminating unpredictable "stop-the-world" pauses and leading to more consistent, low-latency performance.45  
* **Fearless Concurrency:** The same compile-time checks that ensure memory safety also make it much safer to write highly concurrent code. This is critical for a distributed search engine that must process many requests in parallel.  
* **Low-Level Control:** Rust provides low-level control over memory layout and system resources, similar to C++, allowing developers to write highly optimized code that can take full advantage of modern hardware features like SIMD instructions.

##### **7.3 Architectural Feasibility: A Custom Storage Engine vs. a Full System Rewrite**

Rewriting a system with the feature breadth and maturity of OpenSearch from scratch is a monumental undertaking that would likely take a team of engineers many years. A more pragmatic and ultimately more impactful approach is to identify a strategic component that can be replaced or extended.

A talk at a past OpenSearchCon provides a clue to a viable path forward: extending the Apache Lucene core storage layer via an external writer mechanism, which could be implemented in a native language like Rust.46 This points to a highly surgical and powerful strategy. The Lucene library has a pluggable API called the

Codec, which controls every aspect of how index data—the inverted index, stored fields, term vectors, etc.—is written to and read from disk.

The most feasible and high-leverage path for the "Rust Kernel" idea is to implement a custom Lucene Codec in Rust. This component would interface with the Java-based Lucene via a Foreign Function Interface (FFI), such as JNI. This approach would allow the new Rust component to focus on one thing: making the on-disk storage and retrieval layer as fast and efficient as possible. It could implement more advanced compression algorithms, novel data structures, or better utilize modern hardware. Crucially, it would be able to leverage the entire existing, mature Lucene and OpenSearch ecosystem for everything else: query parsing, the aggregation framework, analysis chains, and distributed cluster coordination. This de-risks the project enormously while still allowing for a fundamental, performance-oriented innovation at the very core of the engine.

##### **7.4 Potential Pitfalls and a Phased Implementation Strategy**

Even this more focused approach is highly ambitious and carries significant technical challenges. The Lucene Codec API is complex and deeply intertwined with the library's internal workings. The performance overhead of the Java-to-Rust FFI must be carefully managed to ensure it does not negate the performance gains of the Rust implementation. Finally, the level of testing required to guarantee correctness and bit-for-bit compatibility with the standard Lucene format would be immense.

A realistic, phased implementation strategy would be:

1. **Phase 1: Ecosystem Immersion.** Begin by contributing to the opensearch-rust client library. This provides a low-risk way to become deeply familiar with the OpenSearch API, data structures, and community development process.  
2. **Phase 2: Standalone Library.** Develop a standalone Rust library that can correctly read and write standard Lucene index segment files. This would be a valuable project in its own right and would prove mastery of the on-disk format.  
3. **Phase 3: Custom Codec Implementation.** With the experience gained from the first two phases, embark on the implementation of the custom Rust Codec and integrate it into a fork of OpenSearch for benchmarking and testing.

This methodical approach turns a grand vision into an actionable and plausible project plan, mitigating risk by building foundational components and expertise incrementally.

#### **8.0 Alternative and Complementary Innovation Vectors**

Beyond augmenting the existing OpenSearch/Lucene architecture, there is a vibrant and growing ecosystem of search solutions being built from the ground up in Rust. These projects offer both inspiration and potential building blocks for a next-generation search engine.

##### **8.1 The Rise of Rust-Native Search Libraries: A Review of Tantivy and MeiliSearch**

The Rust ecosystem already contains several notable search projects that demonstrate the viability of building high-performance search solutions without the JVM.

* **Tantivy:** This is an open-source search engine library written entirely in Rust, with its design heavily inspired by Apache Lucene.43 Like Lucene, Tantivy is a library, not a standalone server. It is designed to be embedded within other applications to provide full-text search capabilities. It has a growing ecosystem, including C++ wrappers to allow integration with systems like ClickHouse.48  
* **MeiliSearch:** This is an open-source, standalone search engine that is also written in Rust. Its primary focus is on developer experience, ease of use, and "search-as-you-type" performance for application search use cases.49 It is often positioned as a simpler, more lightweight alternative to Elasticsearch/OpenSearch for scenarios where the primary goal is a fast and relevant search bar on a website or in an application, rather than large-scale log analytics.51  
* **Other Projects:** The broader Rust data ecosystem includes many other relevant projects, such as **Qdrant**, a high-performance vector search engine, and **ParadeDB**, which aims to be an Elasticsearch alternative built directly on PostgreSQL.44

These projects prove that there is significant developer interest in search solutions that offer the performance and safety benefits of Rust. The table below provides a comparative overview of the key players in this space.

| Project | Primary Use Case | Architectural Style | Data Model | Key Differentiator |
| :---- | :---- | :---- | :---- | :---- |
| **Tantivy** | Search Engine Library | Embedded Library | Documents (JSON-like) | A pure-Rust, Lucene-inspired library for building custom search solutions.43 |
| **MeiliSearch** | Application Search | Client-Server | Documents (JSON) | Ease of use, out-of-the-box relevance, and sub-50ms "search-as-you-type" performance.49 |
| **Qdrant** | Vector Search | Client-Server | High-dimensional Vectors | A specialized vector database with advanced filtering capabilities, optimized for AI/ML similarity search.44 |
| **ParadeDB** | Search & Analytics | PostgreSQL Extension | Relational \+ Text | An Elasticsearch alternative built directly on Postgres, unifying transactional and search workloads.44 |

Analyzing this landscape reveals a potential market gap. While Tantivy provides a library and MeiliSearch excels at application search, there is not yet a mature, distributed, Rust-native engine designed for the large-scale log analytics and observability use cases that are OpenSearch's bread and butter.

##### **8.2 Disaggregated Architecture: Separating Compute and Storage with Cloud-Native Formats**

A truly next-generation search engine would likely embrace the disaggregated architecture discussed in Section 6.3. Such a system would be designed from the ground up to separate compute and storage. The primary, source-of-truth data would reside in an open, columnar format like Apache Parquet on a cloud object store like Amazon S3.46 The search engine itself would consist of a stateless, elastic compute layer responsible for:

1. **Indexing:** Reading data from the object store and building search indexes (e.g., inverted indexes, vector indexes) which could be stored back on the object store or on a high-performance caching layer.  
2. **Querying:** Loading the necessary index segments into memory to serve search queries, with the ability to scale the number of query nodes up or down based on demand.

This architecture, which is the standard for modern cloud data warehouses, would fundamentally solve the cost and scaling challenges of the tightly-coupled OpenSearch model.

##### **8.3 Beyond the Inverted Index: Exploring Alternative Data Structures**

While the inverted index is unparalleled for traditional full-text search, a modern analytics engine must handle a wider variety of query patterns. A forward-looking design might not be limited to a single data structure. It could be a multi-modal system capable of leveraging:

* **Inverted Indexes** for text search.  
* **Columnar Storage** for fast analytical scans and aggregations.  
* **Vector Indexes** (using algorithms like HNSW) for semantic and similarity search.13

The query planner in such a system would be intelligent enough to choose the optimal data structure to satisfy a given query, providing the best of all worlds.

##### **8.4 AI-Native Search: Integrating Vector Search and LLMs at the Core Engine Level**

Finally, any new search engine project must treat AI and Machine Learning capabilities as a first-class concern, not a bolt-on feature. OpenSearch is rapidly adding features for vector search to power semantic search and integrations with Large Language Models (LLMs) for Retrieval-Augmented Generation (RAG) applications.10 This is a key battleground in its competition with Elasticsearch.14

Building a new engine in Rust from the ground up provides an opportunity to design for these workloads natively. This could involve more efficient and tightly integrated vector indexing and search algorithms than are possible when layering them on top of a decades-old, text-first architecture like Lucene's. An AI-native engine might have features like automated embedding generation and native RAG pipelines built into its core.

#### **9.0 Strategic Recommendations for the OSS Contributor**

Based on this comprehensive analysis, three distinct strategic paths present themselves. Each offers a different balance of risk, impact, and alignment with the contributor's stated skills and interests.

##### **9.1 Path A: Deepening Expertise and Impact within the OpenSearch Project**

This is the most direct and lowest-risk path. The recommendation is to focus on the opensearch-rust client library. The goal would be to elevate this client to be the undisputed best-in-class client in the entire OpenSearch ecosystem. This would involve not just achieving feature parity, but also creating a highly ergonomic, idiomatic, and performant API that becomes the gold standard for Rust developers interacting with OpenSearch. This path builds directly on the contributor's existing strengths, addresses a clear strategic need for the project, and offers a high-impact, visible contribution that will be valued by the community.

##### **9.2 Path B: Building a Niche, High-Performance Component in Rust**

This path pursues the "Partitioned Rust Kernel" concept, but reframes it into a more feasible and surgically precise project: the creation of a custom Apache Lucene Codec written in Rust. This is a highly ambitious, "deep tech" project that requires a profound understanding of both Rust and the internals of search engine storage formats. If successful, however, the impact could be immense, potentially providing a significant performance boost to the entire Lucene ecosystem, including OpenSearch, Elasticsearch, and Solr. This is a high-risk, high-reward path for a contributor interested in fundamental, low-level systems innovation.

##### **9.3 Path C: Launching a New, Architecturally Distinct Open Source Search Project**

This is the most ambitious path. It involves synthesizing the architectural ideas from Sections 7 and 8 to design and launch a new, open-source search engine. This project would be differentiated by being:

* **Rust-native:** For performance, safety, and modern developer ergonomics.  
* **Architecturally Disaggregated:** Separating compute and storage to align with modern cloud-native principles.  
* **AI-native:** Treating vector search and LLM integration as core, first-class features.

This engine would not aim to be a feature-for-feature clone of OpenSearch. Instead, it would target a specific, high-value niche, such as large-scale, cost-effective search and analytics on data lakes. This path carries the highest risk and requires the most significant long-term commitment, but it also offers the greatest potential for creating a truly disruptive and innovative new platform in the search and analytics space.

#### **Works cited**

1. What is OpenSearch? \- Open Source Search Engine Explained \- AWS, accessed on August 15, 2025, [https://aws.amazon.com/what-is/opensearch/](https://aws.amazon.com/what-is/opensearch/)  
2. opensearch.org, accessed on August 15, 2025, [https://opensearch.org/about/\#:\~:text=The%20OpenSearch%20Project%20was%20first,with%20a%20rich%20feature%20roadmap.](https://opensearch.org/about/#:~:text=The%20OpenSearch%20Project%20was%20first,with%20a%20rich%20feature%20roadmap.)  
3. Introducing OpenSearch | AWS Open Source Blog, accessed on August 15, 2025, [https://aws.amazon.com/blogs/opensource/introducing-opensearch/](https://aws.amazon.com/blogs/opensource/introducing-opensearch/)  
4. What is OpenSearch and the OpenSearch Dashboard? \- Elastic, accessed on August 15, 2025, [https://www.elastic.co/elasticsearch/opensearch](https://www.elastic.co/elasticsearch/opensearch)  
5. Frequently Asked Questions \- OpenSearch, accessed on August 15, 2025, [https://opensearch.org/faq/](https://opensearch.org/faq/)  
6. Elasticsearch vs. OpenSearch: 6 Key Differences and How to Choose \- Coralogix, accessed on August 15, 2025, [https://coralogix.com/guides/elasticsearch/elasticsearch-vs-opensearch-key-differences/](https://coralogix.com/guides/elasticsearch/elasticsearch-vs-opensearch-key-differences/)  
7. OpenSearch vs. Elasticsearch: Similarities and 6 key differences \- NetApp Instaclustr, accessed on August 15, 2025, [https://www.instaclustr.com/education/opensearch/opensearch-vs-elasticsearch-similarities-and-6-key-differences/](https://www.instaclustr.com/education/opensearch/opensearch-vs-elasticsearch-similarities-and-6-key-differences/)  
8. OpenSearch (software) \- Wikipedia, accessed on August 15, 2025, [https://en.wikipedia.org/wiki/OpenSearch\_(software)](https://en.wikipedia.org/wiki/OpenSearch_\(software\))  
9. Keeping clients of OpenSearch and Elasticsearch compatible with open source \- AWS, accessed on August 15, 2025, [https://aws.amazon.com/blogs/opensource/keeping-clients-of-opensearch-and-elasticsearch-compatible-with-open-source/](https://aws.amazon.com/blogs/opensource/keeping-clients-of-opensearch-and-elasticsearch-compatible-with-open-source/)  
10. OpenSearch: Home, accessed on August 15, 2025, [https://opensearch.org/](https://opensearch.org/)  
11. Understanding the difference between OpenSearch and Elasticsearch \- Graylog, accessed on August 15, 2025, [https://graylog.org/post/understanding-the-difference-between-opensearch-and-elasticsearch/](https://graylog.org/post/understanding-the-difference-between-opensearch-and-elasticsearch/)  
12. Give back and go forward: Driving community contributions from vendor led to vendor neutral \- OpenSearch, accessed on August 15, 2025, [https://opensearch.org/blog/driving-community-contributions/](https://opensearch.org/blog/driving-community-contributions/)  
13. Apache Solr vs OpenSearch \- Comparison and Key Differences \- BigData Boutique Blog, accessed on August 15, 2025, [https://bigdataboutique.com/blog/apache-solr-vs-opensearch-comparison-and-key-differences-d7c790](https://bigdataboutique.com/blog/apache-solr-vs-opensearch-comparison-and-key-differences-d7c790)  
14. Elasticsearch vs. OpenSearch: Vector Search Performance Comparison, accessed on August 15, 2025, [https://www.elastic.co/search-labs/blog/elasticsearch-opensearch-vector-search-performance-comparison](https://www.elastic.co/search-labs/blog/elasticsearch-opensearch-vector-search-performance-comparison)  
15. Elasticsearch vs. OpenSearch: Performance and resource utilization analysis | Elastic Blog, accessed on August 15, 2025, [https://www.elastic.co/blog/elasticsearch-opensearch-performance-gap](https://www.elastic.co/blog/elasticsearch-opensearch-performance-gap)  
16. Benchmarking OpenSearch and Elasticsearch \- The Trail of Bits Blog, accessed on August 15, 2025, [https://blog.trailofbits.com/2025/03/06/benchmarking-opensearch-and-elasticsearch/](https://blog.trailofbits.com/2025/03/06/benchmarking-opensearch-and-elasticsearch/)  
17. OpenSearch Core, accessed on August 15, 2025, [https://opensearch.org/platform/opensearch-core/](https://opensearch.org/platform/opensearch-core/)  
18. A foundational Technology behind OpenSearch and Elasticsearch | by Achanandhi M, accessed on August 15, 2025, [https://medium.com/@achanandhi.m/a-foundational-technology-behind-opensearch-and-elasticsearch-3d12539c0464](https://medium.com/@achanandhi.m/a-foundational-technology-behind-opensearch-and-elasticsearch-3d12539c0464)  
19. Lucene™ Core News, accessed on August 15, 2025, [https://lucene.apache.org/core/corenews.html](https://lucene.apache.org/core/corenews.html)  
20. A Comprehensive Guide to OpenSearch Architecture \- Logit.io, accessed on August 15, 2025, [https://logit.io/blog/post/opensearch-architecture/](https://logit.io/blog/post/opensearch-architecture/)  
21. Intro to OpenSearch, accessed on August 15, 2025, [https://docs.opensearch.org/docs/latest/getting-started/intro/](https://docs.opensearch.org/docs/latest/getting-started/intro/)  
22. Mastering OpenSearch at Scale: A Practical Guide for Enterprise Search Solutions | by Whitespectre \- Medium, accessed on August 15, 2025, [https://medium.com/whitespectre/mastering-opensearch-at-scale-a-practical-guide-for-enterprise-search-solutions-403450bf6eb9](https://medium.com/whitespectre/mastering-opensearch-at-scale-a-practical-guide-for-enterprise-search-solutions-403450bf6eb9)  
23. AWS OpenSearch Deep Dive: Architecture, Pricing, and Best Practices \- Cloudchipr, accessed on August 15, 2025, [https://cloudchipr.com/blog/aws-opensearch](https://cloudchipr.com/blog/aws-opensearch)  
24. Software and architecture \- Amazon OpenSearch Service Lens, accessed on August 15, 2025, [https://docs.aws.amazon.com/wellarchitected/latest/amazon-opensearch-service-lens/software-and-architecture.html](https://docs.aws.amazon.com/wellarchitected/latest/amazon-opensearch-service-lens/software-and-architecture.html)  
25. The \#1 Mistake Killing Your AWS OpenSearch Performance, accessed on August 15, 2025, [https://aws.plainenglish.io/the-1-mistake-killing-your-aws-opensearch-performance-394ee18c2a8a](https://aws.plainenglish.io/the-1-mistake-killing-your-aws-opensearch-performance-394ee18c2a8a)  
26. Language clients \- 《OpenSearch v2.6 Documentation》 \- 书栈网· BookStack, accessed on August 15, 2025, [https://www.bookstack.cn/read/opensearch-2.6-en/2abfea90b102c74f.md](https://www.bookstack.cn/read/opensearch-2.6-en/2abfea90b102c74f.md)  
27. Language clients \- OpenSearch Documentation, accessed on August 15, 2025, [https://docs.opensearch.org/latest/clients/](https://docs.opensearch.org/latest/clients/)  
28. OpenSearch language clients \- Eliatra, accessed on August 15, 2025, [https://eliatra.com/docs/2.11/clients/](https://eliatra.com/docs/2.11/clients/)  
29. Apple's contributions to open source? : r/opensource \- Reddit, accessed on August 15, 2025, [https://www.reddit.com/r/opensource/comments/1axc7ym/apples\_contributions\_to\_open\_source/](https://www.reddit.com/r/opensource/comments/1axc7ym/apples_contributions_to_open_source/)  
30. Fresno OKs $56 million bond, 21-year police headquarters lease \- Fresnoland, accessed on August 15, 2025, [https://fresnoland.org/2025/08/15/fresno-oks-56-million-bond/](https://fresnoland.org/2025/08/15/fresno-oks-56-million-bond/)  
31. How Apple Uses Apache Iceberg to Power Its Lakehouse at Scale \- Data Engineer Things, accessed on August 15, 2025, [https://blog.dataengineerthings.org/how-apple-uses-apache-iceberg-to-power-its-lakehouse-at-scale-f19b27a64c62](https://blog.dataengineerthings.org/how-apple-uses-apache-iceberg-to-power-its-lakehouse-at-scale-f19b27a64c62)  
32. OpenSearch Project · GitHub, accessed on August 15, 2025, [https://github.com/opensearch-project](https://github.com/opensearch-project)  
33. opensearch-project/OpenSearch: Open source distributed and RESTful search engine. \- GitHub, accessed on August 15, 2025, [https://github.com/opensearch-project/OpenSearch](https://github.com/opensearch-project/OpenSearch)  
34. How To Start Contributing To OpenSearch Without Any Prior Knowledge on Op... Abdul Muneer Kolarkunnu \- YouTube, accessed on August 15, 2025, [https://www.youtube.com/watch?v=Bvm1vJmZAcQ](https://www.youtube.com/watch?v=Bvm1vJmZAcQ)  
35. OpenSearch Project Style Guidelines, accessed on August 15, 2025, [https://opensearch.isharkfly.com/STYLE\_GUIDE/](https://opensearch.isharkfly.com/STYLE_GUIDE/)  
36. High JVM memory pressure | Elastic Docs, accessed on August 15, 2025, [https://www.elastic.co/docs/troubleshoot/elasticsearch/high-jvm-memory-pressure](https://www.elastic.co/docs/troubleshoot/elasticsearch/high-jvm-memory-pressure)  
37. Troubleshoot high JVM memory pressure in Amazon OpenSearch Service | AWS re:Post, accessed on August 15, 2025, [https://repost.aws/knowledge-center/opensearch-high-jvm-memory-pressure](https://repost.aws/knowledge-center/opensearch-high-jvm-memory-pressure)  
38. Elasticsearch Out of Memory Error \- Common Causes & Fixes \- Pulse, accessed on August 15, 2025, [https://pulse.support/kb/elasticsearch-out-of-memory-error](https://pulse.support/kb/elasticsearch-out-of-memory-error)  
39. Elasticsearch heap size usage and JVM garbage collection, accessed on August 15, 2025, [https://www.elastic.co/search-labs/blog/elasticsearch-heap-size-jvm-garbage-collection](https://www.elastic.co/search-labs/blog/elasticsearch-heap-size-jvm-garbage-collection)  
40. elasticsearch \- Elastic search high memory consumption \- Stack Overflow, accessed on August 15, 2025, [https://stackoverflow.com/questions/52421232/elastic-search-high-memory-consumption](https://stackoverflow.com/questions/52421232/elastic-search-high-memory-consumption)  
41. Buyer Beware\! Three Challenges with Elasticsearch and OpenSearch \- ChaosSearch, accessed on August 15, 2025, [https://www.chaossearch.io/blog/elasticsearch-opensearch-challenges](https://www.chaossearch.io/blog/elasticsearch-opensearch-challenges)  
42. Scaling OpenSearch for Optimal Performance | by Savan Nahar | Jun, 2025 \- Medium, accessed on August 15, 2025, [https://savannahar68.medium.com/scaling-opensearch-for-optimal-performance-6b0b50ffe696](https://savannahar68.medium.com/scaling-opensearch-for-optimal-performance-6b0b50ffe696)  
43. 11 Alternatives to Elasticsearch, OpenSearch, and Solr \- Sematext, accessed on August 15, 2025, [https://sematext.com/blog/elasticsearch-opensearch-solr-alternatives/](https://sematext.com/blog/elasticsearch-opensearch-solr-alternatives/)  
44. rust-unofficial/awesome-rust: A curated list of Rust code and resources. \- GitHub, accessed on August 15, 2025, [https://github.com/rust-unofficial/awesome-rust](https://github.com/rust-unofficial/awesome-rust)  
45. Building a search engine from scratch, in Rust: introduction \- jdrouet, accessed on August 15, 2025, [https://jdrouet.github.io/posts/202503161800-search-engine-intro/](https://jdrouet.github.io/posts/202503161800-search-engine-intro/)  
46. Samuel Herman – Lucene And Beyond \- Core Storage Extension In OpenSearch \- YouTube, accessed on August 15, 2025, [https://www.youtube.com/watch?v=-\_mXJqvPw0o](https://www.youtube.com/watch?v=-_mXJqvPw0o)  
47. Tantivy \- Database of Databases, accessed on August 15, 2025, [https://dbdb.io/db/tantivy](https://dbdb.io/db/tantivy)  
48. myscale/tantivy-search: Tantivy-search is a C++ wrapper for Tantivy, a full-text search engine written in Rust. It is designed to integrate with MyScaleDB and ClickHouse. \- GitHub, accessed on August 15, 2025, [https://github.com/myscale/tantivy-search](https://github.com/myscale/tantivy-search)  
49. Meilisearch: Open-source AI search engine, accessed on August 15, 2025, [https://www.meilisearch.com/](https://www.meilisearch.com/)  
50. Meilisearch vs Elasticsearch, accessed on August 15, 2025, [https://www.meilisearch.com/blog/meilisearch-vs-elasticsearch](https://www.meilisearch.com/blog/meilisearch-vs-elasticsearch)  
51. Is OpenSearch overkill for my usecase? : r/aws \- Reddit, accessed on August 15, 2025, [https://www.reddit.com/r/aws/comments/1arziid/is\_opensearch\_overkill\_for\_my\_usecase/](https://www.reddit.com/r/aws/comments/1arziid/is_opensearch_overkill_for_my_usecase/)  
52. Comparison to alternatives \- Meilisearch Documentation, accessed on August 15, 2025, [https://www.meilisearch.com/docs/learn/resources/comparison\_to\_alternatives](https://www.meilisearch.com/docs/learn/resources/comparison_to_alternatives)  
53. OpenSearch Dashboards, accessed on August 15, 2025, [https://docs.opensearch.org/latest/dashboards/](https://docs.opensearch.org/latest/dashboards/)