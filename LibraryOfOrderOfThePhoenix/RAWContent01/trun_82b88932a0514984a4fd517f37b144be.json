{
  "input": "I am thinking of writing a new open source operating system in Rust from scratch to tap into its fearless concurrency. What if instead of writing drivers I merely pointed to the Linux existing drivers codebase using Foreign Function Interfaces or something else? How would it impact the technical architecture choices for this new open source operating system",
  "output": {
    "executive_summary": {
      "key_findings": "Directly reusing Linux kernel drivers in a new Rust-based OS by 'pointing' to them via Foreign Function Interfaces (FFI) is technically and legally infeasible. The primary barriers are the Linux kernel's deliberately unstable internal ABI/API, which requires drivers to be recompiled for each kernel version, and the deep, complex dependencies drivers have on a vast array of kernel subsystems (memory management, locking, device model, etc.). Furthermore, the GPLv2 license of the Linux kernel would legally obligate the new Rust OS kernel to also be licensed under GPLv2, as such integration would create a 'derivative work'. However, several alternative strategies for leveraging the Linux driver ecosystem exist, primarily centered around virtualization and compatibility layers, which avoid these critical issues.",
      "primary_recommendation": "The recommended architectural path is a phased, hybrid strategy that prioritizes rapid development and security. The initial and primary strategy should be virtualization-based reuse: run the new Rust OS as a guest in a virtual machine, relying on a mature host OS like Linux to manage the physical hardware. The Rust OS would only need to implement stable, standardized paravirtualized drivers (e.g., Virtio). In the long term, this should be supplemented by the selective development of native Rust drivers, preferably in user-space (e.g., using frameworks like SPDK/DPDK) for performance-critical devices or for platforms where virtualization is not viable. This approach provides the fastest path to a functional system while maintaining a clear path toward full native capability.",
      "strategic_impact": "Adopting a virtualization-first strategy has a profound strategic impact on the OS architecture, licensing, and development roadmap. It decouples the new OS from the complexities and maintenance burden of physical hardware drivers, allowing development to focus on the core OS features and Rust's safety benefits. Architecturally, it favors a hypervisor or microkernel-like design over a traditional monolithic kernel. From a licensing perspective, it creates a clean separation from the GPLv2-licensed Linux drivers, permitting the new Rust OS to adopt a more permissive license (e.g., MIT/Apache 2.0). The development roadmap is significantly de-risked, enabling a usable system early in the lifecycle without the monumental, multi-year effort of writing a complete driver ecosystem from scratch."
    },
    "feasibility_of_direct_ffi_reuse": {
      "technical_barriers": "The primary technical barrier is the Linux kernel's deliberate lack of a stable in-kernel Application Programming Interface (API) or Application Binary Interface (ABI) for its modules. This is a core design philosophy that prioritizes rapid development and refactoring over backward compatibility for out-of-tree components. Consequently, drivers are tightly coupled to the specific kernel version they were compiled against and often break between minor releases. Furthermore, Linux drivers are not self-contained programs; they are deeply integrated with and dependent on a vast ecosystem of kernel subsystems. A simple FFI call cannot satisfy these dependencies, which include specific memory allocators (`kmalloc`, `vmalloc`), a rich suite of concurrency primitives (`spinlocks`, `mutexes`, RCU), deferred work mechanisms (`workqueues`, `tasklets`), interrupt and DMA handling APIs, and foundational frameworks like the Linux Device Model, VFS, and the networking stack. To make a Linux driver function, a new OS would have to re-implement a substantial portion of the Linux kernel's internal architecture, a task far beyond the scope of a simple FFI bridge.",
      "legal_and_licensing_barriers": "The Linux kernel is licensed under the GNU General Public License, version 2 (GPLv2). The prevailing legal interpretation from organizations like the Free Software Foundation (FSF) is that linking any code with the kernel, whether statically or dynamically, creates a 'combined work' that is considered a 'derivative work' of the kernel. As such, the entire combined work must be licensed under the GPLv2. Attempting to link a new Rust OS kernel with Linux drivers would almost certainly obligate the new OS to adopt the GPLv2 license. This creates a significant legal risk and imposes a restrictive licensing model on the new project, which may conflict with the developers' intentions. The kernel community technically enforces this with mechanisms like `EXPORT_SYMBOL_GPL`, further solidifying the legal barrier.",
      "conclusion": "Directly invoking Linux kernel drivers from a non-Linux kernel via a simple Foreign Function Interface (FFI) is not a technically feasible or sustainable strategy. The combination of the unstable, deeply integrated nature of the kernel's internal APIs and the stringent 'derivative work' interpretation of the GPLv2 license makes this approach impractical. The effort required to overcome the technical barriers would be tantamount to reimplementing a significant portion of the Linux kernel, while the legal risks associated with the GPLv2 are substantial. Therefore, alternative strategies such as virtualization or creating a comprehensive compatibility layer are the only viable paths for leveraging the Linux driver ecosystem."
    },
    "recommended_architectural_strategy": {
      "short_term_strategy": "The recommended initial approach is to build and run the Rust OS as a guest within a virtualized environment, leveraging a mature host OS like Linux to manage all physical hardware. The Rust OS will not interact with physical drivers directly. Instead, it will implement a minimal set of client drivers for the standardized Virtio paravirtualization interface. This includes drivers for essential devices such as `virtio-net` (networking), `virtio-blk` (storage), `virtio-console` (serial I/O), `virtio-gpu` (graphics), and `virtio-rng` (entropy). This strategy abstracts away the immense complexity of physical hardware, providing a stable, consistent, and secure platform for the OS to be developed and tested on from day one.",
      "long_term_strategy": "The long-term vision is a hybrid architecture that combines the broad compatibility of virtualization with the performance and control of native drivers. The OS should evolve to support running on bare metal, potentially acting as its own hypervisor. For most commodity hardware, it can continue to use a dedicated Linux 'driver VM' with device passthrough (VFIO) for robust support. For performance-critical I/O, the OS should develop a framework for native, user-space drivers, similar to DPDK and SPDK, allowing applications to bypass the kernel for maximum throughput and low latency. Finally, for a select few core devices or on platforms where virtualization is not an option (e.g., embedded systems), a small number of native, in-kernel Rust drivers can be developed.",
      "rationale": "This dual-phase strategy is recommended because it provides the optimal balance of trade-offs. The short-term virtualization approach maximizes development velocity and time-to-first-boot by offloading the driver problem. It enhances security through strong isolation between the driver domain (host) and the Rust OS (guest). Critically, it avoids the legal and licensing complexities of the GPLv2 by communicating over a standardized, non-licensed interface. The long-term hybrid strategy provides a clear and pragmatic path to achieving native performance and full system control where it matters most, without requiring the infeasible task of rewriting the entire Linux driver ecosystem from scratch. This allows the project to be useful and usable at every stage of its development."
    },
    "driver_strategy_decision_matrix": [
      {
        "strategy_name": "Direct Reuse of Linux Drivers (via Rewriting/Adaptation)",
        "complexity": "Very High. This strategy is exceptionally complex due to the Linux kernel's explicit policy of not maintaining a stable in-kernel API or ABI. Drivers are deeply dependent on a vast array of internal kernel subsystems (memory management, locking, device model) that would need to be reimplemented. This approach requires a profound understanding of the Linux driver model and its constant evolution, making it almost infeasible for any project outside the mainline kernel itself.",
        "performance": "Potentially High. A well-executed native Rust driver, even one based on a Linux C driver, could achieve performance on par with or exceeding the original. Rust's strong type system and features like the `Drop` trait for resource management can help prevent bugs and lead to more robust, performant code over time.",
        "security": "High. This is a primary motivation for using Rust in kernel development. Rust's memory safety guarantees, enforced at compile time, eliminate entire classes of common C vulnerabilities such as buffer overflows, use-after-free, and data races in the 'safe' portions of the code, significantly improving the security posture compared to a pure C implementation.",
        "time_to_first_boot": "Very Long. Rewriting a substantial number of drivers from C to Rust is a monumental and time-consuming effort. The `Rust for Linux` project, which aims to enable this within the kernel, is still considered experimental and not yet suitable for production use, underscoring the long timeline required to achieve a usable system with this approach.",
        "licensing": "Restrictive. The Linux kernel is licensed under GPL-2.0 only. Any driver code ported to Rust would be considered a derivative work and would almost certainly need to be licensed under GPLv2 as well. This would impose the GPLv2 license on a core part of the new operating system, constraining future licensing choices.",
        "maintenance": "Extremely High. Described as a 'Sisyphean task' and a 'nightmare' by kernel developers. Due to the lack of a stable in-kernel API, the rewritten driver code would constantly break with each new kernel release from which it was originally derived. The maintenance burden of tracking and adapting to these upstream changes would be enormous and unsustainable for a separate OS project."
      },
      {
        "strategy_name": "Shim-based Porting (C-to-Rust FFI)",
        "complexity": "Medium to High. While tools like `bindgen` can automate the generation of FFI bindings, creating a robust and safe shim is a complex undertaking. It requires extensive use of `unsafe` Rust, where the compiler's guarantees are suspended. The developer must manually translate C macros and inline functions and meticulously manage memory ownership, lifetimes, and control flow across the language boundary.",
        "performance": "Good, but with overhead. FFI calls are not zero-cost and introduce a performance penalty. While this overhead is often negligible for infrequent calls, it can become a significant bottleneck in high-frequency I/O paths, impacting both latency and throughput.",
        "security": "Mixed. The Rust components of the OS benefit from memory safety, but the shim itself creates a large and complex `unsafe` attack surface. It directly interfaces with the original C driver, which may contain its own vulnerabilities. A bug in either the shim's `unsafe` logic or the underlying C driver can compromise the entire system.",
        "time_to_first_boot": "Medium. This approach is significantly faster than a full rewrite because it leverages existing, mature C driver code. It offers a pragmatic path to enabling specific, critical hardware without starting from scratch, making it a viable option for targeted enablement.",
        "licensing": "Restrictive. The license of the original driver (e.g., GPLv2 for Linux drivers) applies directly. The shim acts as a linking mechanism, which, under the FSF's interpretation, creates a 'combined work'. This extends the GPL's requirements to the Rust code that interacts with the shim, effectively forcing it to be GPL-compatible.",
        "maintenance": "High. The shim is inherently brittle. It is tightly coupled to the specific version and internal structure of the C driver it wraps. Any update to the original driver, or changes in the assumptions it makes about the underlying OS, can break the shim. It also suffers from the unstable Linux API problem by proxy, requiring constant updates."
      },
      {
        "strategy_name": "Virtualization-based Reuse",
        "complexity": "Medium. This strategy requires implementing or porting a hypervisor. However, the primary task for the new Rust OS is to implement client drivers for the stable and well-documented `virtio` specification. This is significantly less complex than writing drivers for a vast and constantly changing ecosystem of physical hardware devices.",
        "performance": "High, with manageable overhead. This approach offers multiple performance tiers. VFIO passthrough provides near-native performance, with benchmarks showing it can achieve 97-99% of native speeds. SR-IOV also offers very high performance and low latency. Standard `virtio` has measurable overhead (e.g., a `virtio-blk` operation might take ~200 µs), but modern implementations with `vhost` and packed rings are highly optimized and can saturate 10G+ network links.",
        "security": "High. This model provides strong, hardware-enforced isolation between the drivers (running in the host OS or a dedicated driver VM) and the Rust OS (running as a guest). The attack surface is limited to the hypervisor and the well-defined `virtio` interface, which can be rigorously audited and secured.",
        "time_to_first_boot": "Short. This is the standout advantage of the virtualization strategy. By leveraging a mature host OS like Linux, the new Rust OS gains immediate access to its vast and stable driver ecosystem. This allows development to focus on the core OS logic and features, rather than being consumed by the monumental task of driver development.",
        "licensing": "Favorable. The Rust OS is completely decoupled from the host's driver licenses. The host (e.g., Linux) is under GPLv2, but the guest OS communicates via a standardized virtual hardware interface. This means the guest OS can be developed and distributed under any license without being considered a derivative work.",
        "maintenance": "Low. The host OS community (e.g., the Linux community) handles the difficult task of maintaining the physical device drivers. The `virtio` specification is stable and backward-compatible, meaning the Rust OS's `virtio` client drivers will require infrequent updates, leading to a very low long-term maintenance burden."
      },
      {
        "strategy_name": "Native/User-space Drivers",
        "complexity": "High. Writing any driver from scratch is a difficult and time-consuming task that requires deep hardware knowledge and reverse-engineering. Developing in user-space can reduce some complexity compared to in-kernel development by allowing the use of standard libraries, debuggers, and tools, but the core challenge of interfacing with hardware remains.",
        "performance": "Can be Extremely High. For specific workloads, this approach is the fastest available. User-space frameworks like DPDK (networking) and SPDK (storage) bypass the kernel entirely to achieve massive performance gains (e.g., DPDK achieving 9.0 million packets per second vs. the kernel's 300,000). However, this performance often comes at the cost of dedicating CPU cores that run at 100% utilization in a busy-polling loop.",
        "security": "Good. Running drivers in isolated user-space processes is a major security benefit. A crash in a driver will only terminate its process and will not crash the kernel. Sandboxing mechanisms like seccomp, capabilities, and IOMMU-backed memory protection can be used to enforce the principle of least privilege and contain malicious or buggy drivers.",
        "time_to_first_boot": "Very Long. This is the slowest approach to achieving a usable system, as every single driver for every piece of hardware (timer, serial, storage, networking, display) must be written from the ground up. This requires a massive, long-term engineering investment.",
        "licensing": "Flexible. The developers have complete and total control over the licensing of the drivers and the operating system. This provides maximum flexibility for the project's goals, whether they are commercial or community-oriented.",
        "maintenance": "Medium to High. The OS development team is solely responsible for maintaining the entire driver stack. While this is a large burden, they have the advantage of controlling their own internal APIs. This makes the APIs stable and avoids the maintenance nightmare of tracking unpredictable changes in an external project like the Linux kernel."
      }
    ],
    "virtualization_based_reuse_deep_dive": {
      "architectural_patterns": "Several architectural patterns enable driver reuse through virtualization. The primary model is the 'Driver VM' or 'Driver Domain,' where a minimal, dedicated OS (typically Linux) runs in an isolated virtual machine (like a Xen DomU or on a microkernel like seL4) with exclusive control over physical hardware. The main Rust OS then runs as a separate guest, interacting with the hardware via standardized interfaces. A related pattern is using static partitioning hypervisors like Jailhouse, which offer lower overhead by avoiding resource emulation. The most common implementation pattern involves a combination of QEMU for device emulation and KVM for hardware acceleration, where the guest OS uses paravirtualized (PV) drivers to communicate efficiently with the hypervisor, bypassing slower emulation paths. This is often referred to as a PVHVM setup. An older, less secure pattern known as 'cohosting,' which uses time-division multiplexing, is considered unsuitable for modern systems due to its lack of strong isolation.",
      "key_technologies": "The key technology for this approach is the Virtio standard, which defines a suite of paravirtualized devices (e.g., `virtio-net`, `virtio-blk`, `virtio-gpu`). Communication is handled via `virtqueues`, which are shared memory ring buffers, and notifications are managed through mechanisms like `eventfd`. To optimize performance, the `vhost` protocol offloads data plane processing from the main VMM (e.g., QEMU) to a more efficient handler. This comes in two main variants: `vhost-net`, a kernel module that accelerates network I/O, and `vhost-user`, which allows a userspace process (like a DPDK application) to serve as the backend over a Unix socket. For achieving near-native performance, direct hardware access is enabled by VFIO (Virtual Function I/O), a secure, IOMMU-protected framework for device passthrough, and SR-IOV (Single Root I/O Virtualization), a hardware feature that allows a single device to appear as multiple virtual devices that can be assigned to different VMs. The `rust-vmm` project provides a suite of Rust-based components for building custom VMMs and `virtio` device backends.",
      "performance_implications": "Virtualization introduces performance overhead, but modern technologies make it manageable and often highly performant. Direct device passthrough using VFIO offers the best performance, approaching native speeds (97-99% in some benchmarks) for devices like NVMe drives. Paravirtualization with Virtio is also highly efficient, though it has measurable overhead. For networking, `vhost-net` significantly improves throughput over pure QEMU emulation (e.g., from 19.2 Gbits/sec to 22.5 Gbits/sec in one test). However, this can come at the cost of higher CPU utilization on the host (1.46x to 2.3x in one study). For storage, `virtio-blk` is effective but showed a 33% overhead in one benchmark. For graphics, `virtio-gpu` with the Venus backend (for Vulkan) offers performance significantly better than older VirGL (OpenGL) but still trails native hardware. Overall, while not zero-cost, the performance is high enough for a wide range of server and desktop workloads.",
      "security_and_licensing_benefits": "This strategy offers significant benefits in both security and licensing. From a security perspective, running drivers in an isolated VM provides strong fault isolation; a crash in a driver is contained within its VM and will not affect the main Rust OS. The use of a hardware IOMMU, managed by the hypervisor and exposed via VFIO, is critical as it prevents a compromised driver from performing malicious DMA attacks on the rest of the system. This compartmentalization is a core principle of security-focused systems like Qubes OS. From a licensing perspective, virtualization provides a clean separation that avoids the complexities of the GPLv2. The FSF generally considers a host OS running a guest VM to be 'mere aggregation.' The communication occurs at 'arm's length' through standardized interfaces like Virtio or hardware passthrough. This means the Rust OS, running as a guest, is not considered a 'derivative work' of the Linux host kernel and is not encumbered by its GPLv2 license, allowing the new OS to be developed under any license."
    },
    "shim_based_porting_deep_dive": {
      "approach_description": "This approach involves creating a compatibility layer, or 'shim,' within the new Rust OS kernel that emulates the Linux Kernel Programming Interface (KPI). This allows existing Linux drivers, written in C, to be compiled and run with minimal modifications. The most prominent real-world example is FreeBSD's `LinuxKPI` (`sys/compat/linuxkpi`), which is a collection of header and source files that translate Linux kernel API calls to their FreeBSD equivalents. This has been successfully used to port complex and critical drivers, such as the `drm-kmod` package for Intel and AMD graphics and the `iwlwifi` driver for modern wireless networking, making it essential for FreeBSD's desktop usability. Other operating systems, like Haiku, have adopted a similar strategy, though they chose to create a compatibility layer for FreeBSD's more stable driver API rather than Linux's.",
      "maintenance_challenges": "The single greatest challenge of the shim-based approach is the immense and perpetual maintenance burden caused by 'upstream churn.' The Linux kernel community deliberately maintains an unstable internal API to facilitate rapid development and refactoring. This means internal functions, data structures, and entire subsystems are constantly changing. Consequently, any shim layer is incredibly fragile and requires continuous, expert-level updates to keep pace with the kernel versions that new drivers depend on. This effort is described as a 'nightmare' by kernel developers and is estimated to require multiple engineer-years of effort annually just to maintain parity for critical subsystems. Without this constant investment, the shim quickly becomes obsolete, and the ability to port new drivers is lost.",
      "licensing_risks": "This strategy carries significant legal risk due to the Linux kernel's GPLv2 license. The Free Software Foundation (FSF) and other copyright holders argue that linking a non-GPL kernel to a GPL driver, even through a shim, creates a 'combined work' or 'derivative work.' This would obligate the entire Rust OS kernel to be licensed and distributed under the GPLv2. The intent of the shim is a key factor; if its purpose is seen as an attempt to circumvent the GPL, it is unlikely to be considered a legally sound separation. Precedents like the `ZFS-on-Linux` controversy reinforce this view. While the `LinuxKPI` project is less contentious because it facilitates open-source to open-source porting, the risk for a new OS with a non-GPL license is substantial. To avoid creating a derivative work, developers might have to perform costly and time-consuming 'clean room' reimplementations of GPL-licensed components."
    },
    "native_and_userspace_driver_architectures": {
      "native_in_kernel_drivers": "This strategy involves writing device drivers from scratch directly in Rust to be integrated into the new OS's kernel. The 'Rust for Linux' project serves as the primary model, demonstrating how Rust code can coexist with C in a kernel environment. The main advantage of this approach is security; Rust's ownership model and borrow checker eliminate entire classes of memory safety vulnerabilities (e.g., buffer overflows, use-after-free) and data races at compile time. Performance is generally on par with equivalent C code due to Rust's zero-cost abstractions. However, this approach has very high complexity and a long development timeline. It requires a deep understanding of kernel internals and hardware, and the FFI boundary for interoperating with any C components is a source of complexity and potential `unsafe` bugs. While this path offers the most control and the highest potential for a secure, robust native driver ecosystem, it represents a monumental, long-term effort.",
      "userspace_drivers": "A modern and increasingly popular architectural pattern is to run device drivers as standard processes in user-space, isolated from the kernel. This provides excellent fault isolation, as a crash in a driver only terminates its process, not the entire system. Security is enhanced by using a hardware IOMMU (Input/Output Memory Management Unit) via a framework like Linux's VFIO (Virtual Function I/O), which confines a device's DMA access to memory explicitly owned by its driver process. For extreme performance, frameworks like DPDK (for networking) and SPDK (for storage) use this model to bypass the kernel entirely. They employ poll-mode drivers (PMDs) that continuously poll the hardware, avoiding the overhead of interrupts and system calls, and enabling zero-copy data transfers. This can yield performance an order of magnitude higher than kernel-based stacks, though often at the cost of dedicating CPU cores to busy-polling.",
      "microkernel_approach": "Microkernel architectures, such as those of Redox OS, seL4, and Fuchsia, take the user-space driver model to its logical conclusion. In this design, the kernel itself is kept minimal, providing only core services like scheduling, IPC, and basic memory management. All device drivers, along with other system services, are mandated to run as separate, unprivileged processes in isolated user-space protection domains. This prioritizes security and modularity above all else. Communication between components is handled via secure Inter-Process Communication (IPC). Formally verified microkernels like seL4 can provide mathematical proof of the isolation between these components. While this offers the highest degree of security and reliability, it can introduce performance overhead due to the cost of IPC for all interactions between drivers and applications, a trade-off that modern microkernels work to mitigate with highly optimized IPC paths."
    },
    "licensing_implications_of_gplv2": {
      "derivative_work_analysis": "Under the GPLv2, any software that links with the Linux kernel's internal functions, whether statically at compile-time or dynamically at runtime, is generally considered to form a 'combined work'. This combined work is legally viewed as a 'derivative work' of the original GPLv2-licensed kernel. The consequence is that the entire derivative work must be licensed under the GPLv2. The Free Software Foundation (FSF) and the Software Freedom Conservancy (SFC) have consistently upheld this interpretation, arguing that the substance of the communication between the components is key. If the components are designed to run together and share complex internal data structures, they are effectively one program, regardless of how they are linked. This interpretation is supported by precedents like the `ZFS-on-Linux` case, where distributing the CDDL-licensed ZFS module for use with the GPLv2 kernel is considered a license violation by the SFC.",
      "technical_enforcement_mechanisms": "The Linux kernel community technically enforces the spirit of the GPLv2 through the `EXPORT_SYMBOL_GPL()` macro. Core kernel functions and data structures that are considered fundamental to the kernel's operation are exported using this macro. The kernel's module loader will only resolve these symbols for modules that have explicitly declared a GPL-compatible license via the `MODULE_LICENSE()` tag in their source code. This creates a technical barrier that prevents proprietary or non-GPL-compatible modules from accessing essential kernel functionality, thereby inhibiting their development and use. This mechanism serves as a clear statement and enforcement of the community's position on what constitutes a derivative work.",
      "virtualization_as_a_compliance_strategy": "Virtualization provides a widely accepted method for maintaining a clean legal separation between a new OS and the Linux driver ecosystem, thereby avoiding the GPLv2's derivative work obligations. When a new OS runs as a guest in a virtual machine on top of a Linux host, the FSF considers this 'mere aggregation'. The two operating systems communicate at 'arm's length' through standardized, well-defined interfaces like Virtio or via hardware passthrough (VFIO), rather than sharing internal kernel data structures. This clear separation means the guest OS is not considered a derivative work of the host's kernel or its drivers. This allows the new Rust OS to be developed and distributed under any license (e.g., MIT, Apache 2.0) while still benefiting from the complete and mature hardware support provided by the underlying Linux host, making it the safest and most recommended compliance strategy."
    },
    "performance_analysis_by_strategy": {
      "native_kernel_performance": "Native kernel drivers, particularly with modern APIs, offer a powerful balance of performance and integration. For networking, the Linux kernel's eXpress Data Path (XDP) and its user-space interface AF_XDP enable packet processing directly in the NIC driver, bypassing much of the traditional stack. This can achieve throughput of 186 Gb/s (18.8 Mpps) with 1200-byte packets using 8 CPUs, with server-to-server latency around 6.5µs. For storage, the `io_uring` asynchronous I/O interface is state-of-the-art. With optimizations like Submission Queue Polling, it can deliver over 2.0 million 4K random read IOPS at a queue depth of 16, with latencies as low as 567 nanoseconds. These APIs are designed to reduce context-switch and interrupt overhead, but performance can still be limited by per-core efficiency and lock contention within the kernel.",
      "userspace_framework_performance": "User-space frameworks like DPDK (networking) and SPDK (storage) offer the highest possible I/O performance by completely bypassing the kernel. They use poll-mode drivers (PMDs) on dedicated CPU cores, eliminating the overhead of interrupts and context switches. DPDK can achieve full line rate on 100GbE NICs, processing 148.81 Mpps with 64-byte frames. SPDK is highly CPU-efficient, achieving up to 1.3 million IOPS per core (compared to the kernel's 370 KIOPS/core) and can reduce software overhead for NVMe-over-Fabrics by up to 10x. This extreme performance comes at the cost of dedicating CPU cores that run at 100% utilization and requires applications to be specifically written for these frameworks. The architecture is inherently zero-copy, as applications directly access I/O buffers.",
      "virtualized_driver_performance": "Virtualized drivers, primarily using the `virtio` standard, provide high performance suitable for most workloads, with manageable overhead. Modern `virtio` implementations use `vhost` to offload the data path into the host kernel or a user-space process, and use `packed rings` to reduce cache contention. A `virtio-net` device can achieve over 8 Mpps for 64B packets on a 100GbE link. For maximum performance, VFIO passthrough allows a VM to directly control a physical device with an IOMMU providing security, achieving 97-99% of native bare-metal performance. While there is inherent latency from hypervisor involvement (e.g., a `virtio-blk` operation might take ~200µs), the performance is excellent for a general-purpose system and provides a clear path to near-native speeds with passthrough for critical devices."
    },
    "security_architecture_for_drivers": {
      "threat_model": "The threat model for device drivers is extensive. Key threats include memory corruption vulnerabilities (e.g., buffer overflows, use-after-free), which can be exploited for arbitrary code execution; logical flaws leading to privilege escalation from user-space to kernel-space; Denial of Service (DoS) attacks caused by malformed input from hardware or userspace; and Direct Memory Access (DMA) attacks, where a malicious peripheral bypasses OS protections to read or write arbitrary system memory. Additionally, supply chain attacks can compromise a driver's integrity before it is ever installed.",
      "isolation_strategies": "Modern OS security relies on strong isolation to contain faulty or malicious drivers. The most effective strategies move drivers out of the kernel's address space. This can be achieved by running drivers as sandboxed user-space processes, a model used by Fuchsia, Redox, and seL4. The Linux VFIO framework is a key enabler for this, using the hardware IOMMU to securely grant a user-space process direct access to a device. The strongest form of isolation is using dedicated Driver VMs, where each driver runs in its own lightweight virtual machine (e.g., Qubes OS, crosvm), ensuring that a full compromise of the driver is contained within the VM and cannot affect the host or other components.",
      "key_defenses": "A multi-layered defense is critical. The hardware IOMMU is the primary defense against DMA attacks, ensuring a device can only access memory explicitly mapped for it. At the software level, a capability-based security model (as seen in Fuchsia and seL4) enforces the principle of least privilege, preventing a component from performing any action for which it does not hold an explicit, unforgeable token of authority. System integrity is ensured through a chain of trust starting with Measured Boot (using a TPM) to create an immutable log of all boot components, and runtime integrity is maintained with tools like IMA/EVM to prevent the execution of tampered files.",
      "attack_surface_analysis": "The choice of driver strategy directly impacts the attack surface. Using a Foreign Function Interface (FFI) to integrate C drivers expands the attack surface at the language boundary. This requires `unsafe` Rust code, where the compiler's safety guarantees are suspended, making the developer responsible for preventing memory errors and data races. In contrast, virtualization significantly reduces the attack surface. By isolating drivers in a separate VM, the primary OS is protected. Using VFIO passthrough further reduces risk by bypassing complex and often-buggy software device emulation layers. The attack surface is then shifted to the hypervisor and the well-defined `virtio` interface, which are smaller and more easily audited than an entire driver ecosystem."
    },
    "concurrency_models_and_driver_api": {
      "rcu_like_model": "Inspired by the Linux kernel's Read-Copy-Update (RCU) mechanism, this model is optimized for read-mostly workloads to achieve exceptional scalability on SMP systems. It allows numerous readers to access a shared data structure concurrently without acquiring any locks, thus avoiding contention and cache-line bouncing. Updaters create a copy of the data, modify it, and then atomically publish a new pointer to the updated version. The old data is reclaimed only after a 'grace period' ensures no readers are still using it. Key primitives include `rcu_read_lock()`/`rcu_read_unlock()` for defining lightweight read-side critical sections, `rcu_dereference()` for safely accessing pointers, and `synchronize_rcu()` or `call_rcu()` for managing grace periods. A Rust-native implementation could leverage the type system for compile-time safety, using libraries like `crossbeam-epoch` as a reference.",
      "per_cpu_data_model": "This pattern is a powerful technique to eliminate lock contention by providing each CPU core with its own private copy of a variable. Instead of a single global variable protected by a lock, a per-CPU variable is an array of variables, one for each core. When code on a specific CPU needs to access the data, it accesses its local copy, inherently avoiding race conditions without explicit locking. This dramatically improves performance for frequently updated, core-local state such as statistics, counters, or hardware queue state pinned to a CPU. Operations like `this_cpu_inc()` can be optimized to use segment registers or dedicated pointers for efficient access. A Rust API could adapt the `thread_local!` macro or provide safe wrappers around a kernel allocator for dynamic per-CPU structures.",
      "message_passing_model": "This model shifts from managing shared mutable state to a paradigm of explicit communication between isolated components, which aligns well with Rust's ownership principles and `async`/`await` syntax. Inspired by systems like Fuchsia and seL4, drivers are implemented as asynchronous, event-driven tasks. Communication with other components and the kernel occurs over channels, with protocols defined using an Interface Definition Language (IDL) like Fuchsia's FIDL to ensure type-safe messaging. In this model, a hardware interrupt is not a function call but a message delivered to the driver's event loop, which can be represented as a `Stream` of events that the driver `await`s. This encourages a non-blocking architecture that can reduce I/O tail latencies and simplify reasoning about concurrency."
    },
    "hardware_discovery_and_configuration_layer": {
      "x86_64_platform_bringup": "For modern x86_64 systems, the bring-up process is managed through UEFI and ACPI standards. The OS loader must locate the Root System Description Pointer (RSDP) in the EFI Configuration Table, which is the entry point to all ACPI data. From the RSDP, the OS finds and parses the eXtended System Description Table (XSDT) to get pointers to other critical tables. Key tables include the MADT (Multiple APIC Description Table) for enumerating interrupt controllers, the MCFG (PCI Express Memory Mapped Configuration Table) to find the base address for the ECAM region used for PCIe configuration, and the FADT (Fixed ACPI Description Table). The OS must also parse SMBIOS tables, also found via the EFI Configuration Table, to gather static hardware information like manufacturer, product name, and memory configuration.",
      "arm64_platform_bringup": "On ARM64 platforms, hardware discovery primarily relies on the Device Tree (DT), a data structure that describes non-discoverable hardware components. The bootloader or UEFI firmware passes a Flattened Device Tree blob (FDT/DTB) to the OS kernel. The OS must parse this FDT to discover devices and their properties, such as the `compatible` string for driver matching, `reg` for memory-mapped register blocks, and `interrupts` for IRQ connections. In addition to parsing the DT, the OS must initialize core ARM64 subsystems like the Generic Interrupt Controller (GIC) and use the Power State Coordination Interface (PSCI) for CPU power management.",
      "common_bus_enumeration": "The OS must implement protocols to discover and configure devices on common peripheral buses. For PCI Express (PCIe), this involves using the Enhanced Configuration Access Mechanism (ECAM), a memory-mapped region whose address is found via the ACPI MCFG table or Device Tree. The OS performs a recursive scan of all buses, devices, and functions to identify hardware and must also discover and program MSI/MSI-X capabilities for modern interrupt handling. For USB, the OS must initialize the eXtensible Host Controller Interface (xHCI) by setting up command and event rings in memory. When a device is connected, the OS performs a standardized enumeration sequence: resetting the port, assigning a unique address, and reading the device's descriptors to identify its capabilities and load the appropriate driver."
    },
    "maintenance_cost_of_tracking_linux_apis": {
      "the_upstream_churn_problem": "The Linux kernel community's explicit policy is to not provide a stable internal API or ABI for kernel modules. This philosophy, detailed in the kernel's 'stable-api-nonsense.rst' documentation, prioritizes the freedom to refactor, optimize, and fix bugs in the kernel's core over providing backward compatibility for external, out-of-tree drivers. As a result, internal interfaces in all major subsystems—networking, storage, DRM, USB—are in a constant state of flux. This 'upstream churn' means that any out-of-tree driver must be re-evaluated and often rewritten for each new kernel release to remain compatible. Kernel developers describe the task of maintaining such drivers as a 'nightmare,' and the only officially recommended way to ensure long-term stability is to have the driver merged into the mainline kernel tree, where it is maintained as part of the whole.",
      "impact_on_shims_and_ports": "The high rate of API churn has a direct and severe impact on any attempt to create a compatibility layer or 'shim' to port Linux drivers to another OS. Projects like FreeBSD's LinuxKPI, while successful, face a massive and continuous engineering investment to keep their shims synchronized with the upstream Linux kernel. This is not a one-time porting effort but a perpetual maintenance task. The effort to keep compatibility for just critical subsystems like graphics and networking is estimated to be in the range of multiple engineer-years annually. Any lag in this maintenance risks rendering the shim obsolete, breaking support for newer hardware that relies on the latest kernel APIs.",
      "mitigation_through_automation": "Given the immense scale and rapid pace of Linux kernel development, heavy automation is not optional but essential for managing the maintenance burden of tracking API changes. The kernel community itself relies on massive Continuous Integration (CI) systems like KernelCI.org and Intel's 0-day CI to test every change. For projects maintaining compatibility, a similar level of automation is required. This includes using tools like `libabigail` to automatically detect ABI/KMI breakage between kernel versions, employing semantic patching tools like `Coccinelle` to perform large-scale, automated refactoring to adapt to API changes, and using code generation tools like `bindgen` to automatically update FFI bindings and flag compilation failures. This infrastructure is critical to making the maintenance task manageable."
    },
    "phased_hardware_enablement_roadmap": {
      "phase_number": 1,
      "phase_name": "Foundational System with Virtio",
      "goal": "To establish a minimal, bootable, and functional Rust OS by relying entirely on standardized, paravirtualized Virtio devices within a controlled virtual machine environment. This phase completely avoids the complexity of physical hardware drivers, focusing instead on bringing up the core OS and its interaction with a stable, virtual hardware interface.",
      "key_activities": "The primary activities involve implementing client-side drivers within the Rust OS for the core Virtio device specifications. This includes: 1. Developing a `virtio-console` driver for basic serial I/O, debugging, and shell access. 2. Implementing a `virtio-blk` driver to mount and interact with a virtual block device, which will serve as the root filesystem. 3. Creating a `virtio-net` driver to provide network connectivity. 4. Implementing a `virtio-rng` driver to supply the OS with a source of entropy for cryptographic functions. 5. Developing a `virtio-gpu` driver to enable a basic graphical user interface. 6. Running the Rust OS as a guest on a standard hypervisor like QEMU/KVM, configured to provide these Virtio devices to the guest.",
      "exit_criteria": "The phase is complete when the Rust OS can reliably demonstrate the following capabilities within a virtual machine: 1. The OS successfully boots from a `virtio-blk` root filesystem. 2. An interactive shell is accessible and stable via the `virtio-console`. 3. The OS can obtain an IP address via DHCP and successfully ping an external network address using the `virtio-net` interface. 4. A simple graphical application or window manager can be rendered to the screen via `virtio-gpu`. 5. The system's cryptographic primitives are successfully seeded by the `virtio-rng` device."
    },
    "gpu_and_display_stack_strategy": {
      "native_stack_challenge": "Building a native display stack from the ground up is a monumental task. The modern Linux graphics stack, centered on the Direct Rendering Manager (DRM) and Kernel Mode Setting (KMS), is an exceptionally complex kernel subsystem. It involves managing a sophisticated object model (Framebuffers, Planes, CRTCs, Encoders), intricate memory management schemes (GEM, DMA-BUF), and complex locking protocols to prevent deadlocks. Developing a DRM driver requires deep, specialized expertise in both kernel internals and the specific target GPU hardware, making it an extremely resource-intensive and time-consuming endeavor for a new operating system.",
      "shim_based_reuse_challenge": "Reusing Linux DRM drivers via a compatibility layer (shim) presents two major challenges. First, the maintenance overhead is substantial and perpetual. The Linux DRM subsystem evolves at a very rapid pace, meaning the shim must be constantly updated to keep pace with upstream API changes, a task described as a 'nightmare' by kernel developers. Second, most Linux drivers are licensed under GPLv2, which creates significant legal and engineering hurdles for projects with incompatible licenses. This often requires a 'clean-room' reimplementation of interfaces rather than direct code reuse, which is a complex and risky process.",
      "recommended_strategy": "The most effective and pragmatic strategy is to target a virtualized environment and use `virtio-gpu`. Specifically, using the Venus backend provides a thin, efficient transport layer for the modern Vulkan API, offering performance close to native for accelerated graphics. This approach allows the new OS to benefit from a hardware-accelerated graphical environment early in its development cycle without writing any hardware-specific drivers. It completely avoids the complexity of native driver development and the high maintenance and legal costs of a shim-based approach, allowing development to focus on the OS and its application ecosystem."
    },
    "storage_stack_strategy": {
      "block_layer_design": "Adopt a multi-queue block layer model inspired by the Linux kernel's `blk-mq` architecture. This design replaces a single, globally-locked I/O queue with multiple, per-CPU software queues and hardware-mapped dispatch queues. This approach is essential for performance on modern multi-core systems as it eliminates lock contention and scales to match the parallelism of high-speed NVMe devices, enabling significantly higher IOPS.",
      "userspace_driver_integration": "Utilize a userspace driver framework like the Storage Performance Development Kit (SPDK) for direct device access. By using kernel mechanisms like VFIO to map the NVMe device's hardware registers into a userspace process, the OS can bypass the kernel stack entirely. This enables a zero-copy, polled-mode driver model that drastically reduces latency and system call overhead, achieving maximum performance while minimizing the kernel's driver surface area.",
      "filesystem_and_consistency": "Develop a new, Rust-native filesystem that prioritizes provable crash consistency. While a simple log-structured filesystem is a good starting point, a more advanced approach inspired by SquirrelFS, which uses Rust's typestate pattern, is recommended. This allows the filesystem to enforce crash-consistency invariants (like update ordering) at compile time, providing a higher level of reliability and leveraging the core safety strengths of the Rust language."
    },
    "networking_stack_strategy": {
      "tcp_ip_stack_choice": "Adopt a mature, safety-focused TCP/IP stack written in Rust. A strong candidate is `smoltcp`, which is a standalone, event-driven stack designed for `no_std` environments. Alternatively, for a general-purpose OS, designing a new user-space stack inspired by Fuchsia's Netstack3 is recommended to fully leverage Rust's safety guarantees and build a more robust and secure networking foundation from the ground up.",
      "performance_architecture": "Integrate with high-performance userspace frameworks like DPDK or AF_XDP to enable kernel-bypass for latency-sensitive applications. This allows for zero-copy data transfers and avoids kernel context-switching overhead. Critically, the OS kernel must be designed to support and manage both kernel-level TLS (kTLS) and hardware TLS offload, which are essential for achieving high-throughput, secure networking with minimal CPU cost on modern hardware.",
      "api_design": "Provide a dual API to serve both existing and new applications. A POSIX-compatible sockets API is essential for portability and running the vast ecosystem of existing network applications. In parallel, a native, modern `async` API should be exposed, leveraging Rust's `async/await` syntax. This provides an ergonomic, efficient, and type-safe interface for new, high-concurrency network services developed specifically for the OS."
    },
    "developer_experience_and_reliability_pipeline": {
      "ci_cd_and_build_infrastructure": "A foundational requirement is establishing reproducible, or deterministic, builds to ensure identical source code produces bit-for-bit identical binaries. This is achieved by controlling build timestamps, user/host information, and file paths using environment variables and compiler flags, and can be managed with tools like Nix or Guix. The CI pipeline must integrate both emulation harnesses and Hardware-in-the-Loop (HIL) testing. Emulation with QEMU/KVM allows for scalable testing of device models (e.g., NVMe, gigabit Ethernet) and hypervisor functionality using frameworks like QTest and kvm-unit-tests. For real hardware validation, a HIL framework like LAVA (Linaro Automated Validation Architecture), as used by KernelCI, is essential for orchestrating large-scale automated testing across a diverse device farm.",
      "testing_and_verification_strategies": "A multi-pronged strategy is required to ensure driver safety. Coverage-guided fuzzing is critical, using tools like Syzkaller with KCOV to fuzz kernel syscalls and driver interfaces in a VM environment. For Rust-specific code, `cargo-fuzz` can be enhanced with selective instrumentation to focus on high-risk `unsafe` blocks. Static analysis and formal methods provide stronger guarantees. The pipeline should integrate advanced Rust verification tools like Kani (bounded model checking), Prusti (deductive verification with pre/post-conditions), and Miri (an interpreter for detecting undefined behavior in `unsafe` code). Property-based testing frameworks like `proptest` should also be used to automatically generate a wide range of inputs to validate function invariants.",
      "crash_triage_and_telemetry": "To manage the high volume of data from automated testing, a robust crash triage and telemetry system is essential. This should be modeled on systems like Syzbot, the public, continuously running instance of Syzkaller. Syzbot automates the entire workflow: fuzzing, detecting crashes, analyzing kernel output, deduplicating reports, generating high-quality bug reports with reproducers, and attempting to automatically bisect the bug-introducing commit. For aggregating and analyzing results from all testing sources (HIL, emulation, fuzzing), a centralized dashboard like SQUAD (Software Quality Dashboard) or KCIDB-ng (KernelCI's database) is necessary to provide a unified view of system health and track regressions over time."
    }
  },
  "outputBasis": [
    {
      "field": "maintenance_cost_of_tracking_linux_apis",
      "citations": [
        {
          "title": "Stable Kernel Interfaces and API Nonsense (stable-api-nonsense.rst)",
          "url": "https://www.kernel.org/doc/Documentation/process/stable-api-nonsense.rst",
          "excerpts": [
            "The Linux Kernel Driver Interface"
          ]
        },
        {
          "title": "Linux Kernel API churn and Android drivers (Greg Kroah-Hartman discussion)",
          "url": "https://lwn.net/Articles/372419/",
          "excerpts": [
            "The kernel community has for years been telling these companies to get their code merged, so that they can take advantage of the security fixes, and handle the rapid API churn automatically."
          ]
        },
        {
          "title": "The Linux Kernel Driver Interface",
          "url": "https://docs.kernel.org/process/stable-api-nonsense.html",
          "excerpts": [
            "This is being written to try to explain why Linux does not have a binary kernel interface, nor does it have a stable kernel interface."
          ]
        },
        {
          "title": "Ask HN: Why no stable binary kernel interface for drivers?",
          "url": "https://news.ycombinator.com/item?id=14533398",
          "excerpts": [
            "Jun 11, 2017 — Linux does not have a stable binary kernel interface for drivers. This enforces either that all the drivers must be in the kernel source ..."
          ]
        },
        {
          "title": "Hacker News discussion on Linux driver API stability",
          "url": "https://news.ycombinator.com/item?id=30868346",
          "excerpts": [
            "This is by design. Linux doesn't *want* to pay the maintenance and performance costs of guaranteeing a stable in-kernel API/ABI: [https://www.kernel.org/doc/Documentation/process/stable-api-"
          ]
        }
      ],
      "reasoning": "The upstream Linux community explicitly states that there is no stable internal API or ABI for kernel modules, emphasizing the freedom to refactor and optimize rather than preserve long-term compatibility for out-of-tree drivers. This underpins the idea that maintaining external shims or ports will always face churn and require ongoing adaptation as the kernel evolves. The discussion framing stability as a deliberate \"stable API nonsense\" further clarifies that compatibility baselines drift over time, increasing the maintenance cost of any porting effort. Excerpts that describe the churn problem and its consequences for ports and shims corroborate the field value by illustrating why a long-term, largely automated approach is necessary to keep bindings and adapters aligned with the upstream kernel. Direct commentary on kernel API churn and Android driver churn reinforces that churn is not only real but central to planning for compatibility layers. The Linux kernel driver interface and related discourse reinforce that this churn is a core characteristic of Linux's engineering model, which has concrete implications for porting work. Together, these excerpts map closely to the notion that ongoing maintenance cost is inherent to tracking Linux APIs and that automation and formal tooling will be essential to manage it. A few excerpts emphasize automation and tooling (e.g., ABI/KMI tooling) as mitigation strategies, aligning with the mitigation part of the field. Overall, the strongest connection lies in a documented policy and community discourse that Linux maintains no stable internal API, which directly drives the maintenance burden for any shim/port strategy. The remaining excerpts provide supportive color on the churn and its practical impact on porting efforts.",
      "confidence": "high"
    },
    {
      "field": "concurrency_models_and_driver_api",
      "citations": [
        {
          "title": "Linux RCU Documentation",
          "url": "https://www.kernel.org/doc/Documentation/RCU/whatisRCU.txt",
          "excerpts": [
            ". There are at least three flavors of RCU usage in the Linux kernel."
          ]
        },
        {
          "title": "RCU (Read-Copy Update) - Linux kernel documentation",
          "url": "https://docs.kernel.org/RCU/rcu.html",
          "excerpts": [
            "These counters allow limited types of blocking within\n  RCU read-side critical sections.",
            "SRCU also uses CPU-local\n  counters, and permits general blocking within RCU read-side\n  critical sections."
          ]
        },
        {
          "title": "Symmetric Multi-Processing – Linux Kernel Labs Lecture",
          "url": "https://linux-kernel-labs.github.io/refs/heads/master/lectures/smp.html",
          "excerpts": [
            "Per CPU data avoids race conditions by avoiding to use shared data.",
            "Per CPU data avoids race conditions by avoiding to use shared data.",
            "Instead, an array sized to the maximum possible CPU cores is\nused and each core will use its own array entry to read and write\ndata.",
            "Instead, an array sized to the maximum possible CPU cores is\nused and each core will use its own array entry to read and write\ndata."
          ]
        },
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode.",
            "Note that to build a system from scratch on a microkernel, you will need basic OS services such as drivers, file system and networking. Some of the tools and ...",
            "Capabilities are an OS abstraction for managing access rights."
          ]
        },
        {
          "title": "Simple IPC cycle costs and microkernel overview",
          "url": "https://www.researchgate.net/figure/Simple-IPC-cycle-costs_tbl1_3694140",
          "excerpts": [
            "Microkernel has many intriguing features like security, fault-tolerance, modularity and customizability, which recently stimulate a resurgent interest in both academia and industry (including seL4, QNX and Google's Fuchsia OS). However, IPC (inter-process communication), which is known as the Achilles' Heel of microkernels, is still the major factor for the overall (poor) OS performance."
          ]
        }
      ],
      "reasoning": "For the rcU_like_model, excerpts that explain Read-Copy-Update (RCU) as a read-optimized synchronization mechanism, including how readers access data without locking and how grace periods manage reclamation, directly support the described model. Specific passages describe RCU's read-side critical sections, access patterns, and grace period concepts, which align with the notion of a read-mostly design that minimizes locking contention in SMP systems. The per-CPU data model is supported by excerpts detailing per-CPU variables as a means to avoid contention by giving each CPU its own copy of state, enabling per-core locality and high-throughput operation. The discussion of per-CPU access primitives, and the idea of avoiding shared data contention through per-CPU storage, maps cleanly to the described per_cpu_data_model. For the message-passing model, excerpts discuss shifting from shared mutable state to explicit IPC/messaging paradigms and reference microkernel-like IPC as a driver model, which supports thinking about drivers and devices as communicating components rather than sharing memory blindly. The cited material also contrasts IPC-based approaches (e.g., endpoints, channels, and FIDL-like semantics) with traditional shared-state designs, illustrating how a message-passing architecture can improve isolation and composability in driver stacks. The combination of these excerpts provides a cohesive set of evidence across three complementary concurrency/architecture patterns. Overall, the rcU_like_model and per_cpu_data_model claims have strong direct support, while the message_passing_model has solid but somewhat broader coverage, including microkernel-oriented discussions and IPC-focused excerpts.",
      "confidence": "high"
    },
    {
      "field": "shim_based_porting_deep_dive",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "Jul 5, 2022 — LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification. The drm-kmod package, ...",
            "The drm-kmod package, for example, includes Intel and AMD Linux graphics driver code, but is able to compile and run properly on FreeBSD using LinuxKPI.",
            "there exist FreeBSD specific bugs, performance hits, and missing features as a result of LinuxKPI's small codebase.",
            "drm-debugfs support is one of many missing features.",
            "This drm-debugfs support is essential to igt-gpu-tools, so my next task was compiling drm-kmod with the DEBUG\\_FS configuration flag enabled",
            "GPLv2 licensed Linux code must be rewritten under FreeBSD's 2-clause BSD License.",
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification.",
            "Jul 5, 2022 — LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "LinuxKPI - FreeBSD Wiki",
          "url": "https://wiki.freebsd.org/LinuxKPI",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "FreeBSD LinuxKPI documentation",
          "url": "https://freebsdfoundation.org/blog/sharing-dual-licensed-drivers-between-linux-and-freebsd/",
          "excerpts": [
            "or the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD.",
            "The Linux Kernel “license rules” documentation <https://www.kernel.org/doc/html/v5.0/process/license-rules.html> has explict discussion of dual licensing: “individual files can be provided under a dual license, e.g. one of the compatible GPL variants and alternatively under a permissive license like BSD, MIT etc",
            "**Interface Compatibility**",
            "The second issue with code reuse between operating systems relates to the interfaces provided by and/or required by the code in question. This may be achieved in several ways; one is to implement core functionality in a reusable library, and augment it with an operating-system-dependent layer. It may be that the core library can be shared between Linux, Windows, and FreeBSD, providing for significant reuse. For the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD",
            "### **Intel i915 graphics drivers**",
            "**License:** MIT"
          ]
        }
      ],
      "reasoning": "The shim-based porting approach described in the field value centers on inserting a compatibility layer that translates or bridges APIs so that drivers written for one kernel (e.g., Linux) can be compiled for and run on another kernel (e.g., FreeBSD or Haiku) with minimal changes. The most directly relevant passages explicitly discuss the idea of a compatibility layer (LinuxKPI) that provides API translations and allows Linux drivers to run under another OS, and they name concrete driver stacks that have been ported or facilitated by such layers (e.g., drm-kmod for Intel/AMD graphics and iwlf wifi). The rationale is that a single, shared interface layer reduces porting effort by reusing existing Linux driver code, rather than rewriting drivers from scratch. The cited material also notes real-world outcomes, such as DRM-related ports that enable desktop usability on FreeBSD, and the Haiku approach of porting FreeBSD drivers via a compatibility layer rather than porting Linux's native API. The combination of these excerpts demonstrates the core concept: a shim/compat layer that presents a stable or translator surface (Linux KPI) so Linux drivers can be compiled and executed in a non-Linux kernel, with examples and consequences discussed. The maintenance concerns and licensing implications are also central to the field value; several excerpts explicitly describe the maintenance burden (upstream churn) and the GPL-derived licensing risks of linking GPL code to non-GPL kernels via shims, which is precisely the risk highlighted in the field description. Taken together, these excerpts support the field value by illustrating (1) what the shim approach looks like in practice, (2) its concrete historical precedents (FreeBSD's LinuxKPI, drm-kmod, iwlfwi), (3) the operational challenges of keeping such a shim viable across kernel evolutions, and (4) the licensing pitfalls that can arise from using GPL-covered Linux APIs through a shim. The most directly relevant parts are about the existence and use of LinuxKPI/drm-kmod as ports, the explicit FFI-like bridging described in the Rust/FFI excerpts, and the licensing implications discussed in the GPL/license-focused excerpts. The surrounding material on SPDK, VFIO, and virtualization tooling provides contextual background but is less central to the specific 'shim-based porting' theme and is therefore ranked lower in relevance.",
      "confidence": "high"
    },
    {
      "field": "networking_stack_strategy",
      "citations": [
        {
          "title": "redox-os / smoltcp · GitLab",
          "url": "https://gitlab.redox-os.org/redox-os/smoltcp/-/tree/redox",
          "excerpts": [
            "smoltcp* is a standalone, event-driven TCP/IP stack that is designed for bare-metal,\nreal-time systems. Its design goals are simplicity and robustness",
            "smoltcp, being a freestanding networking stack, needs to be able to transmit and receive raw frames. For testing purposes, we will use a regular OS, and run ...",
            "IPv4 time-to-live value is configurable per socket, set to 64 by default."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "5. Security Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/rte_security.html",
          "excerpts": [
            "The library defines generic APIs to create and free security sessions which can support full protocol offload as well as inline crypto operation with NIC or ..."
          ]
        },
        {
          "title": "Kernel TLS, NIC Offload and Socket Sharding in Modern Linux/SDN Context",
          "url": "https://dev.to/ozkanpakdil/kernel-tls-nic-offload-and-socket-sharding-whats-new-and-who-uses-it-4e1f",
          "excerpts": [
            "NIC TLS Offload\n---------------\n\nSome NICs (Mellanox/NVIDIA ConnectX, Chelsio T6, Intel QAT) can offload encryption entirely. The kernel sets up the session keys, while the NIC performs the crypto inline. * **Netflix (FreeBSD + Mellanox CX6-DX)**:\n\n  + In production, serving **400–800 Gb/s per server**. + Reported **~50% reduction in memory bandwidth use** vs CPU crypto. * **NVIDIA/Mellanox (netdev conf)**:\n\n  + Inline TLS offload gave **~2× throughput** vs software kTLS. + Example: **8.8 Gb/s vs 4.4 Gb/s** in test environments. * **Chelsio T6**:\n\n  + Vendor benchmarks show **~44.8 Gb/s TLS throughput** with very low CPU use. * **Academic 2024 SmartNIC study**:\n\n  + Inline mode: **~9.3 Gb/s** throughput under parallel load. + Coprocessor mode: less throughput but **lowest CPU cost**. --",
            "Socket Sharding\n---------------\n\nTraditionally, only one process could `accept()` connections from a TCP socket. With Linux **4.5 (2016)** and `SO_REUSEPORT` enhancements, multiple processes can share a listening socket. This “socket sharding” lets the kernel distribute connections efficiently across worker processes. * Adopted in **NGINX** and **HAProxy**. * **FreeBSD** has similar support. * **Windows** does not provide this functionality. ✅ Result: Better scalability on multi-core systems with no user-space load balancer needed",
            "Windows Support? ----------------\n\nUnlike Linux and FreeBSD, **Windows does not support kernel TLS**. TLS remains in user space via SChannel or libraries like OpenSSL/BoringSSL, with no NIC offload integration. ---",
            "Summary\n-------\n\n* **Socket sharding** (Linux 4.5+) spreads load across processes → higher scalability. * **kTLS** (Linux 4.13+, FreeBSD 13+) avoids copies → **20–40% CPU savings**, ~**2× throughput** for NGINX. * **NIC TLS offload** (Netflix, NVIDIA, Chelsio):\n  + **2× throughput**, huge CPU savings,\n  + **400–800 Gb/s per server** possible in production.\n* **Windows**: No kernel TLS or NIC offload support today. These optimizations matter most at hyperscaler scale, but are increasingly available to anyone running **NGINX**, **HAProxy**, or workloads on modern NICs. ---"
          ]
        },
        {
          "title": "Fuchsia Netstack3 - Rust-based netstack and related networking stack strategy",
          "url": "https://fuchsia.dev/fuchsia-src/contribute/roadmap/2021/netstack3",
          "excerpts": [
            "Netstack3 - A Fuchsia owned rust based netstack",
            "The Fuchsia Netstack team will design\nand implement a Rust-based netstack that achieves functional parity with the\nexisting netstack, while leveraging the type and memory safety of rust.",
            "Risks and mitigations"
          ]
        }
      ],
      "reasoning": "- Excerpts describing smoltcp and Rust-based networking stacks provide direct support for adopting a Rust-native or Rust-inspired stack in a new OS. They illustrate a Rust-focused networking approach (smoltcp as a standalone, no_std-capable stack; references to Rust-based netstacks in Fuchsia and Redox ecosystems) that aligns with the proposed Rust-first strategy and the idea of building a Rust-centric networking foundation from the ground up. Specifically, discussions about smoltcp existing as a standalone stack and the Rust-based Netstack1/Netstack3 plans illustrate concrete architectures for a Rust-first networking layer. This directly substantiates the field's emphasis on a Rust-native or Rust-informed stack rather than exclusively reusing C-based stacks. - Excerpts highlighting high-performance user-space networking paths such as AF_XDP, DPDK, and vhost-user map closely to the field's performance architecture requirement. They describe mechanisms for kernel-bypass data paths, zero-copy data movement, and user-space device access, which are essential for low-latency networking and scale. This supports the second pillar of the value: leveraging high-performance user-space frameworks to minimize kernel overhead and maximize throughput. - TLS offload and kTLS discussions in excerpts about NIC offload and kernel TLS (kTLS) illustrate concrete security-performance tradeoffs for networking. They justify the need to consider both software TLS in the kernel and NIC-based TLS offloads, which is a critical dimension of a robust, secure high-throughput networking stack. - The API-design-oriented excerpts discuss dual interfaces: POSIX sockets for ecosystem compatibility and a modern async API in Rust. This aligns with the field value's call for a POSIX-compatible surface plus a dedicated, ergonomically designed async API to enable high-concurrency server workloads while remaining accessible to existing software. - Collectively, the strongest support comes from passages that directly describe Rust-native stacks (smoltcp, Netstack3) and high-performance user-space networking paths (AF_XDP, DPDK, vhost). Subsequent excerpts provide context about TLS offloads and API design patterns, reinforcing the overall strategy rather than contradicting it.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_reliability_pipeline",
      "citations": [
        {
          "title": "Reproducible builds - Linux Kernel documentation",
          "url": "https://docs.kernel.org/kbuild/reproducible-builds.html",
          "excerpts": [
            "It is generally desirable that building the same source code with\nthe same set of tools is reproducible, i.e. the output is always\nexactly the same.",
            "The kernel embeds timestamps in three places:",
            "This must\nbe overridden using the [KBUILD\\_BUILD\\_TIMESTAMP](kbuild.html) variabl",
            "If you are building from a git commit, you could use its commit date. The kernel does *not* use the `__DATE__` and `__TIME__` macros,\nand enables warnings if they are use",
            "If you incorporate external\ncode that does use these, you must override the timestamp they\ncorrespond to by setting the [SOURCE\\_DATE\\_EPOCH](https://reproducible-builds.org/docs/source-date-epoch/) environment\nvariabl",
            "User, host",
            "The kernel embeds the building user and host names in\n`/proc/version`. These must be overridden using the\n[KBUILD\\_BUILD\\_USER and KBUILD\\_BUILD\\_HOST](kbuild.html) variab",
            "Absolute filenames",
            "When the kernel is built out-of-tree, debug information may include\nabsolute filenames for the source files. This must be overridden by\nincluding the `-fdebug-prefix-map` option in the [KCFLAGS](kbuild.html) variable.",
            "Depending on the compiler used, the `__FILE__` macro may also expand\nto an absolute filename in an out-of-tree build. Kbuild automatically\nuses the `-fmacro-prefix-map` option to prevent this, if it is\nsupported.",
            "Generated files in source packages",
            "The build processes for some programs under the `tools/`\nsubdirectory do not completely support out-of-tree builds. This may\ncause a later source package build using e.g. `make rpm-pkg` to\ninclude generated files.",
            "You should ensure the source tree is\npristine by running `make mrproper` or `git clean -d -f -x` before\nbuilding a source package.",
            "Module signing",
            "If you enable `CONFIG_MODULE_SIG_ALL`, the default behaviour is to\ngenerate a different temporary key for each build, resulting in the\nmodules being unreproducible.",
            "One approach to this is to divide up the build process so that the\nunreproducible parts can be treated as sources:",
            "1. Generate a persistent signing key. Add the certificate for the key\n   to the kernel sour",
            "2. Set the `CONFIG_SYSTEM_TRUSTED_KEYS` symbol to include the\n   signing key’s certificate, set `CONFIG_MODULE_SIG_KEY` to an\n   empty string, and disable `CONFIG_MODULE_SIG_ALL`. Build the kernel and mo",
            "3. Create detached signatures for the modules, and publish them as\n   sourc"
          ]
        },
        {
          "title": "KernelCI Documentation",
          "url": "https://docs.kernelci.org/",
          "excerpts": [
            "Welcome to the KernelCI documentation website! *\n\nThe KernelCI project’s mission is to ensure the quality, stability and long-term maintenance of the Linux kernel. To achieve that, we are building an ecosystem to foster collaboration around Linux kernel testing and validation, facilitating the process of executing tests for a given subsystem or the whole kernel and following up on the results, **helping you catch issues early and maintain kernel qua",
            "documentation",
            "The upstream Linux kernel test project\nKernelCI[KernelCI](/)"
          ]
        },
        {
          "title": "BlueOS Kernel – Written in Rust, compatible with POSIX",
          "url": "https://news.ycombinator.com/item?id=44669026",
          "excerpts": [
            "It would be momentous to simply get Linux drivers to comply with a kernel-agnostic ABI. ... operating system written in the Rust language from the ..."
          ]
        },
        {
          "title": "Fuzzing with cargo-fuzz - Rust Fuzz Book",
          "url": "https://rust-fuzz.github.io/book/cargo-fuzz.html",
          "excerpts": [
            "cargo-fuzz is the recommended tool for fuzz testing Rust code. cargo-fuzz is itself not a fuzzer, but a tool to invoke a fuzzer."
          ]
        },
        {
          "title": "Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation",
          "url": "https://arxiv.org/html/2505.02464v1",
          "excerpts": [
            "However, Rust builds are not reproducible in all use cases and depend on multiple factors, for example, the absolute build path.",
            "To fuzz the target programs, FourFuzz utilizes afl.rs in version 0.15.3 which is based on AFL + + version 4.10c.",
            " \nWe implement partial instrumentation to focus the fuzzing efforts on unsafe Rust code to significantly increase the efficiency and probability of triggering and detecting vulnerabilities in Rust programs.",
            "This is challenging as Rust uses a complex compilation toolchain:\nThis includes multiple intermediate representations and several compiler stages facilitating Rust-specific code analysis.",
            " as MirChecker  [ 42 ] are static analysis frameworks that utilize intermediate code representations to detect different bug type"
          ]
        }
      ],
      "reasoning": "The field value centers on a robust development and verification pipeline for a Rust-/kernel-oriented OS project. Reproducible builds are central to the pipeline; excerpts describing methods to achieve reproducible builds, including build timestamp control, environment-variable management, and deterministic toolchains, directly support the claim. KernelCI and LKFT/LAVA references describe how large-scale automated testing across hardware platforms can be organized and automated, which aligns with the requested focus on CI/CD integration and hardware-in-the-loop-like validation. The fuzzing-focused excerpts discuss coverage-guided fuzzing, both in kernel contexts (Syzkaller/KCOV, KCOV usage) and Rust-specific verification (Kani, Prusti, Miri), which ties into testing strategies for safety, correctness, and reliability. Excerpts about vhost/VFIO/VM-related performance testing and virtualization infrastructure illustrate how emulation and virtualization environments complement hardware testing in a scalable CI pipeline. Overall, the strongest direct support comes from the reproducible-builds and CI/fuzzing/telemetry-oriented excerpts; additional context is provided by excerpts describing large-scale testing infrastructures and Rust verification tooling. The content of these excerpts consistently demonstrates practices and concepts that would map to the fine-grained field values for ci_cd_and_build_infrastructure and testing_and_verification_strategies, while crash_triage_and_telemetry is supported by notes on kernel crash triage tooling and telemetry dashboards like syzbot/KCIDB in the surrounding discussions.",
      "confidence": "medium"
    },
    {
      "field": "feasibility_of_direct_ffi_reuse",
      "citations": [
        {
          "title": "The proper use of EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/769471/",
          "excerpts": [
            "In the case of EXPORT_SYMBOL_GPL(), only modules that declare a GPL-compatible license will be able to see the symbol."
          ]
        },
        {
          "title": "Linux Kernel Licensing Rules and Precedents",
          "url": "https://docs.kernel.org/process/license-rules.html",
          "excerpts": [
            "The Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), as provided in LICENSES/preferred/GPL-2.0,\nwith an explicit syscall exception described in\nLICENSES/exceptions/Linux-syscall-note, as described in the COPYING file.",
            "\n\n> Loadable kernel modules also require a `MODULE_LICENSE()` tag.\nThis tag is\n> neither a replacement for proper source code license information\n> (SPDX-License-Identifier) nor in any way relevant for expressing or\n> determining the exact license under which the source code of the module\n> is provided. > \n> The sole purpose of this tag is to provide sufficient information\n> whether the module is free software or proprietary for the kernel\n> module loader and for user space tools. > \n> The valid license strings for `MODULE_LICENSE()` are:\n> \n> > | “GPL” | ",
            "This documentation file provides a description of how each source file should be annotated to make its license clear and unambiguous.",
            "Aside from that, individual files can be provided under a dual license,\ne.g. one of the compatible GPL variants and alternatively under a\npermissive license like BSD, MIT etc.",
            "The Linux kernel requires the precise SPDX identifier in all source files."
          ]
        },
        {
          "title": "Linux kernel licensing rules",
          "url": "https://www.kernel.org/doc/html/v4.19/process/license-rules.html",
          "excerpts": [
            "The Linux kernel is under GPL-2.0, with individual files compatible with GPL-2.0 and using SPDX identifiers. UAPI files have a special syscall exception.",
            "The files in this directory contain the full exception text and the required Exception Metatags. Examples: LICENSES/exceptions/Linux-syscall-note. Contains the ...",
            "\n\nThe Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), ",
            "The User-space API (UAPI) header files, which describe the interface of\nuser-space programs to the kernel are a special case. According to the\nnote in the kernel COPYING file, the syscall interface is a clear boundary,\nwhich does not extend the GPL requirements to any software which uses it to\ncommunicate with the kernel."
          ]
        },
        {
          "title": "FSF: Interpreting, enforcing and changing the GNU GPL, as applied ...",
          "url": "https://lwn.net/Articles/683352/",
          "excerpts": [
            "... linking creates derivative works. The FSF's own FAQ on the GPLv2 also takes this position by stating that linking to a GPL'd work requires ..."
          ]
        },
        {
          "title": "GNU General Public License",
          "url": "https://en.wikipedia.org/wiki/GNU_General_Public_License",
          "excerpts": [
            "The foundation asserts that an executable that uses a dynamically linked library is indeed a derivative work. This assertion does not, however, apply to ..."
          ]
        },
        {
          "title": "Questioning EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/603131/",
          "excerpts": [
            "No, it can not. The court does not give a damn about the opinion of the author, nor whether the accused was or was not aware of that opinion.",
            "Jun 23, 2014 — EXPORT_SYMBOL_GPL() is meant to mark kernel interfaces that are deemed to be so low-level and specific to the kernel that any software that uses them must ... It was requested to be added after the fact by the maintainer and by now the discussion has had negative impact - all it would take is printing out the discussion and submitting it as evidence that the author was coerced into making the change and it is quite reasonable to assume that the flag is not a technical indicator, but merely a social statement. "
          ]
        },
        {
          "title": "Tainted kernels — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/tainted-kernels.html",
          "excerpts": [
            "Modules without a MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by insmod as GPL compatible are assumed to be proprietary. F if any module ..."
          ]
        },
        {
          "title": "Re: incorrect taint of ndiswrapper — Linux Kernel",
          "url": "https://lists.linuxcoding.com/kernel/2006-q4/msg08685.html",
          "excerpts": [
            "EXPORT_SYMBOL_GPL is a technical mechanism of enforcing GPL against non-free code, but ndiswrapper is free. The non-free NDIS drivers are not using those ..."
          ]
        },
        {
          "title": "Making life (even) harder for proprietary modules",
          "url": "https://lwn.net/Articles/939842/",
          "excerpts": [
            "Aug 3, 2023 — Distributing a proprietary module might be a copyright violation, though, if the module itself is a derived work of the kernel code. But \" ..."
          ]
        },
        {
          "title": "Linux Kernel GPL and ZFS CDDL License clarifications in ...",
          "url": "https://github.com/openzfs/zfs/issues/13415",
          "excerpts": [
            "... kernel module \"could\" be considered a DERIVATIVE WORK. It does NOT mean that if you use those then your kernel module \"IS\" a derivative work.See more"
          ]
        },
        {
          "title": "Proprietary kernel modules - the boundary shifts?",
          "url": "https://lwn.net/Articles/13398/",
          "excerpts": [
            "Oct 22, 2002 — The Linux kernel's license states explicitly that programs running in user mode and making use of kernel system calls are not derived works, and ...",
            "Oct 22, 2002 — Linus Torvalds's ... The _only_ thing that allows for non-GPL modules is copyright law, and in particular the \"derived work\" issue."
          ]
        },
        {
          "title": "Linux Syscall Note (SPDX)",
          "url": "https://spdx.org/licenses/Linux-syscall-note.html",
          "excerpts": [
            "the only valid version of the GPL as far as the kernel\nis concerned is \\_this\\_ particular version of the license (ie v2, not\nv2.2 or v3.x or whatever), unless explicitly otherwise stated. Linus Torvalds"
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        },
        {
          "title": "Is it allowed to dynamically link a GPL licensed library for ...",
          "url": "https://opensource.stackexchange.com/questions/2157/is-it-allowed-to-dynamically-link-a-gpl-licensed-library-for-commercial-use",
          "excerpts": [
            "It answer is that it doesn't matter whether you link statically or dynamically, it's all governed by the GPL. The second, can you make an LGPL ..."
          ]
        },
        {
          "title": "Interpreting, enforcing and changing the GNU GPL, as applied to ...",
          "url": "https://www.fsf.org/licensing/zfs-and-linux",
          "excerpts": [
            "Apr 7, 2016 — The specific occasion for this article is the violation of combining Linux with ZFS, and that concerns specifically GNU GPL version 2; however, ..."
          ]
        },
        {
          "title": "2.5. Hello World (part 4): Licensing and Module Documentation",
          "url": "https://tldp.org/LDP/lkmpg/2.6/html/x279.html",
          "excerpts": [
            "In kernel 2.4 and later, a mechanism was devised to identify code licensed under the GPL (and friends) so people can be warned that the code is non open-source."
          ]
        },
        {
          "title": "Free Software Foundation Issues Statement in Support of ...",
          "url": "https://sfconservancy.org/blog/2016/apr/11/fsf-zfs/",
          "excerpts": [
            "Apr 11, 2016 — Without such permission, users who seek to combine Linux and ZFS live in a precarious situation without the necessary copyright permissions to ..."
          ]
        },
        {
          "title": "FreeBSD Licensing Policy",
          "url": "https://docs.freebsd.org/en/articles/license-guide/",
          "excerpts": [
            "The FreeBSD project discourages completely new licenses and variations on the standard licenses. New licenses require the approval of core@FreeBSD.org to ..."
          ]
        },
        {
          "title": "Linux Networking and Network Devices APIs",
          "url": "https://docs.kernel.org/networking/kapi.html",
          "excerpts": [
            "All the pointers pointing into skb header may change and must be reloaded after call to this function. struct sk_buff *skb_expand_head(struct sk_buff *skb, ..."
          ]
        },
        {
          "title": "NAPI",
          "url": "https://docs.kernel.org/networking/napi.html",
          "excerpts": [
            "NAPI¶. NAPI is the event handling mechanism used by the Linux networking stack. The name NAPI no longer stands for anything in particular [1]."
          ]
        },
        {
          "title": "Linux USB API",
          "url": "https://docs.kernel.org/driver-api/usb/index.html",
          "excerpts": [
            "Linux USB API¶ · Introduction to USB on Linux · USB Host-Side API Model · USB-Standard Types · Host-Side Data Types and Macros · USB Core APIs · Host Controller APIs ..."
          ]
        },
        {
          "title": "5. EXPORT_SYMBOL - Linux Kernel Workbook - Read the Docs",
          "url": "http://lkw.readthedocs.io/en/latest/doc/04_exporting_symbols.html",
          "excerpts": [
            "5.2. Export Symbol¶ · EXPORT_SYMBOL() helps you provide APIs to other modules/code. · The functions which you EXPORT are available to the other modules/code."
          ]
        }
      ],
      "reasoning": "Key supports for the fine-grained field value come from explicit notes on license and ABI constraints: a) The EXPORT_SYMBOL_GPL and GPL-symbol licensing indicate that kernel symbols exported to modules are restricted to GPL-licensed modules, which blocks a non-GPL OS from cleanly binding to Linux drivers via FFI. b) The kernel license statements clarify that Linux is GPLv2 with a syscall exception, and that linking to GPL code creates derivative-work obligations, complicating any plan to ship a mixed-licensing OS through an FFI bridge. c) Other excerpts describe the absence of a stable in-kernel ABI for external modules and the need to reimplement substantial kernel functionality to support drivers outside the kernel, reinforcing the conclusion that a straightforward FFI bridge is not realistically feasible. d) Additional excerpts discuss safer alternatives (e.g., VM-based approaches using VFIO, linuxkpi shims) that are suggested as viable, but they do not support direct analogs of a simple FFI bridge to Linux drivers. These connections collectively support the conclusion that the direct FFI reuse approach is not practically feasible, and that licensing barriers are a major showstopper.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_by_strategy",
      "citations": [
        {
          "title": "SPDK Performance Comparison (Harris et al., 2017)",
          "url": "https://files.futurememorystorage.com/proceedings/2017/20170810_FW32_Harris.pdf",
          "excerpts": [
            " \n\nSPDK cuts latency \n\nalmost in "
          ]
        },
        {
          "title": "Mellanox DPDK Performance Report (DPDK 17.05, Mellanox ConnectX NICs)",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_17_05_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            " \n\nDevice Under Test (DUT) is made up of the HPE server and the Mellanox ConnectX®-5 Ex NIC with a dual- \nport (only first port used in this test). The DUT is connected to the IXIA packet generator which generates \ntraffic towards the ConnectX®-5 Ex NIC."
          ]
        },
        {
          "title": "ICPE 2024 SPDK vs Linux storage stack performance",
          "url": "https://research.spec.org/icpe_proceedings/2024/proceedings/p154.pdf",
          "excerpts": [
            "by 7 — Our setup is able to deliver a peak random read performance of 3.4 Million IOPS with the Linux storage stack under Linux v6.3.8 (5.9 Million ... This demonstrates that SPDK\n(1.3 MIOPS/core) is still the state-of-the-art storage stack, which is at\nleast 3.6× more efficient (single core peak IOPS performance with 8\nSSDs, “1p” and “1p SPDK” lines in Figure 1c) than the Linux storage\nstack (370 KIOPS/core) (O-2). SPDK can deliver higher throughput\nthan the Linux storage stack when the CPU is the bottleneck be-\ncause SPDK is more CPU efficient than the Linux storage stack. Previous study shows that SPDK needs fewer CPU instructions to\nprocess each I/O request than the Linux storage stack [51]. 3.3\nSummary\n\nWhat is the key resource bottleneck for the L-app, T-4KiB-app, and T-\n64KiB-app? The key finding (Key Finding, KF1) here is that as the\nNVMe device speeds are improving, the CPU becomes the primary\nbottleneck. Modern fast NVMe storage devices like Samsung 980\nPRO 1TiB, can require more than a single CPU core to saturate\nthe performance of the SSD. On our setup, L-apps (QD=1), and\nT-4KiB-apps (QD=128) are bounded by the CPU performance. We\nreport that the T-4KiB-app needs at least 4 T-4KiB-apps on 4 CPU\ncores to saturate a single SSD. The T-64KiB-app can saturate a\nsingle SSD with only one CPU core (not shown). Hence, the age-old\n\nmantra of “CPU is fast, I/O devices are slow” does not hold anymore\nfor modern fast NVMe SSDs (O-3). We answer RQ1 by identifying\nwhen the CPU or the SSD becomes a bottleneck (the inflection or\nsaturation points)."
          ]
        },
        {
          "title": "Intel Ethernet' s Performance Report with DPDK 23.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_23_03_Intel_NIC_performance_report.pdf",
          "excerpts": [
            "Apr 20, 2023 — Single core test case: Used to check the maximum IO throughput for a single core. The case requires at least 1x 100GbE ports for the ice DPDK ..."
          ]
        },
        {
          "title": "NVIDIA NICs Performance Report with DPDK 24.07",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_24_07_NVIDIA_NIC_performance_report.pdf",
          "excerpts": [
            "Figure 2: Test #1 Results – NVIDIA ConnectX-6 Dx 100GbE Throughput at Zero Packet Loss",
            "Figure 2: Test #1 Results – NVIDIA ConnectX-6 Dx 100GbE Throughput at Zero Packet Loss"
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.07",
          "url": "http://fast.dpdk.org/doc/perf/DPDK_22_07_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "Feb 21, 2023 — The ConnectX-6Dx data traffic is passed through DPDK to the test application l3fwd and is redirected to the opposite direction on the same port."
          ]
        },
        {
          "title": "Packed virtqueue: How to reduce overhead with virtio",
          "url": "https://www.redhat.com/en/blog/packed-virtqueue-how-reduce-overhead-virtio",
          "excerpts": [
            "This layout is relatively easy to implement and to debug thus it's a good entry point for learning the virtio dataplane basics.",
            "Packed virtqueue amends it by merging the three rings in just one location in virtual environment guest memory."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "5. Security Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/rte_security.html",
          "excerpts": [
            "The library defines generic APIs to create and free security sessions which can support full protocol offload as well as inline crypto operation with NIC or ..."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.",
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO.",
            "Devices are the main target of any I/O driver."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.",
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO."
          ]
        }
      ],
      "reasoning": "- When discussing native kernel performance, the excerpts highlight that the Linux kernel provides strong in-kernel networking and storage paths, with mechanisms like DPDK/SPDK-enabled paths not being default kernel tools but viable high-performance routes. In particular, the excerpt about DPDK showing that it can achieve full line rate on 100GbE NICs underlines the capability of kernel- bypass style configurations to push throughput high into the tens or hundreds of Gbps, which aligns with the idea that native kernel APIs (e.g., XDP/AF_XDP) enable high-performance packet processing directly in the kernel or at the interface level. The explicit statement that DPDK can reach full line rate on 100GbE NICs and that SPDK's NVMe driver is a user-space, polled-mode, async, lockless stack supports the claim that native kernel paths plus these user-space accelerators can achieve very high throughput and low latency when properly engineered, albeit with tradeoffs in CPU core dedication and architectural complexity. This directly supports the \"native_kernel_performance\" field value by anchoring expectations in concrete throughput/latency figures and architectures that minimize kernel involvement in hot paths.",
      "confidence": "medium"
    },
    {
      "field": "licensing_implications_of_gplv2",
      "citations": [
        {
          "title": "Linux Kernel Licensing Rules and Precedents",
          "url": "https://docs.kernel.org/process/license-rules.html",
          "excerpts": [
            "Loadable kernel modules also require a `MODULE_LICENSE()` tag.",
            "The Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), as provided in LICENSES/preferred/GPL-2.0,\nwith an explicit syscall exception described in\nLICENSES/exceptions/Linux-syscall-note, as described in the COPYING file.",
            "Aside from that, individual files can be provided under a dual license,\ne.g. one of the compatible GPL variants and alternatively under a\npermissive license like BSD, MIT etc.",
            "According to the\nnote in the kernel COPYING file, the syscall interface is a clear boundary,\nwhich does not extend the GPL requirements to any software which uses it to\ncommunicate with the kernel.",
            "The Linux kernel requires the precise SPDX identifier in all source files. The valid identifiers used in the kernel are explained in the section [License identifiers]() and have been retrieved from the official SPDX\nlicense list at <https://spdx.org/licenses/> along with the license texts."
          ]
        },
        {
          "title": "EXPORT_SYMBOL_GPL() include/linux/export.h",
          "url": "https://www.kernel.org/doc./htmldocs/kernel-hacking/sym-exportsymbols-gpl.html",
          "excerpts": [
            "Similar to EXPORT_SYMBOL() except that the symbols exported by EXPORT_SYMBOL_GPL() can only be seen by modules with a MODULE_LICENSE() that specifies a GPL ..."
          ]
        },
        {
          "title": "Proprietary kernel modules - the boundary shifts?",
          "url": "https://lwn.net/Articles/13398/",
          "excerpts": [
            "Oct 22, 2002 — The Linux kernel's license states explicitly that programs running in user mode and making use of kernel system calls are not derived works, and ...",
            "Oct 22, 2002 — Linus Torvalds's ... The _only_ thing that allows for non-GPL modules is copyright law, and in particular the \"derived work\" issue."
          ]
        },
        {
          "title": "The Linux Kernel documentation",
          "url": "https://docs.kernel.org/",
          "excerpts": [
            "The Linux Kernel documentation includes development guides, internal API manuals, development tools, and user-oriented documentation."
          ]
        },
        {
          "title": "GNU General Public License",
          "url": "https://en.wikipedia.org/wiki/GNU_General_Public_License",
          "excerpts": [
            "The foundation asserts that an executable that uses a dynamically linked library is indeed a derivative work. This assertion does not, however, apply to ..."
          ]
        },
        {
          "title": "Tainted kernels — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/tainted-kernels.html",
          "excerpts": [
            "Modules without a MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by insmod as GPL compatible are assumed to be proprietary. F if any module ..."
          ]
        },
        {
          "title": "Re: incorrect taint of ndiswrapper — Linux Kernel",
          "url": "https://lists.linuxcoding.com/kernel/2006-q4/msg08685.html",
          "excerpts": [
            "EXPORT_SYMBOL_GPL is a technical mechanism of enforcing GPL against non-free code, but ndiswrapper is free. The non-free NDIS drivers are not using those ..."
          ]
        },
        {
          "title": "Making life (even) harder for proprietary modules",
          "url": "https://lwn.net/Articles/939842/",
          "excerpts": [
            "Aug 3, 2023 — Distributing a proprietary module might be a copyright violation, though, if the module itself is a derived work of the kernel code. But \" ..."
          ]
        },
        {
          "title": "Linux Kernel GPL and ZFS CDDL License clarifications in ...",
          "url": "https://github.com/openzfs/zfs/issues/13415",
          "excerpts": [
            "... kernel module \"could\" be considered a DERIVATIVE WORK. It does NOT mean that if you use those then your kernel module \"IS\" a derivative work.See more"
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        }
      ],
      "reasoning": "The most directly relevant content discusses how kernel-facing code is licensed and what that implies for derivative works. Content explaining MODULE_LICENSE tags and the enforcement mechanism around EXPORT_SYMBOL_GPL shows how kernel maintainers delineate what is allowed to be used by non-GPL code, which is central to understanding derivative-work boundaries under GPLv2. Phrases describing that only modules declaring GPL-compatible licenses may access GPL-exported symbols illustrate a formal enforcement mechanism that constrains how non-GPL code can legally interact with the kernel, which is a core aspect of the field value's claims about derivative-work implications and licensing compliance. Discussions about the \"derivative work\" concept in kernel contexts reinforce that whether code is linked statically or dynamically can affect licensing status, and that the kernel community has treated certain combinations as creating a derivative work requiring GPL licensing. The material addressing dual licensing and SPDX identifiers further ground the legal landscape by explaining how licenses may be tagged, identified, and shared, which influences how a new OS might license its components if it interfaces with Linux code or uses Linux-derived APIs. Content discussing situations like ZFS and related license conflicts highlights real-world precedents and reinforces the importance of separation strategies (e.g., virtualization boundaries) to avoid creating derivative-works scenarios. Overall, these excerpts collectively support the claim that GPLv2 interactions (especially via linking, symbol exports, and licensing tags) materially constrain how non-GPL or differently licensed components can interact with the Linux kernel and thus inform licensing strategy for a Rust/open-source OS that wants to interface with or reuse Linux code. The strongest, most direct support comes from explicit kernel licensing enforcement mechanisms (MODULE_LICENSE, EXPORT_SYMBOL_GPL), followed by derivative-work implications of linking and the dual-licensing/SPDX discussions that shape how software ownership and redistribution are viewed in practice.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        },
        {
          "title": "Asterinas: A Linux ABI-Compatible, Rust-Based Framekernel OS ...",
          "url": "https://arxiv.org/html/2506.03876v1",
          "excerpts": [
            "Ostd leverages IOMMU to prevent peripheral devices from writing to unauthorized physical regions. Initially, no part of physical memory is DMA-accessible. Drivers can create DMA mappings ( DmaStream or DmaCoherent ) only over untyped memory ( UFrame or USegment ), so sensitive regions stay protected.",
            "Ostd leverages IOMMU to prevent peripheral devices from writing to unauthorized physical regions. Initially, no part of physical memory is DMA-accessible. Drivers can create DMA mappings ( DmaStream or DmaCoherent ) only over untyped memory ( UFrame or USegment ), so sensitive regions stay protected."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it.",
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer.",
            "swiotlb also provides “sync” APIs that correspond to the dma\\_sync\\_\\*() APIs that\na driver may use when control of a buffer transitions between the CPU and the\ndevice."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region.",
            "16\\.2. How is an I/O Virtual Address (IOVA) generated?",
            "When we turn on DMA translation, DMA to those regions will fail.",
            "OS is expected to setup\nunity mappings for these regions for these devices to access these regions."
          ]
        },
        {
          "title": "iommu.rst",
          "url": "https://www.kernel.org/doc/Documentation/userspace-api/iommu.rst",
          "excerpts": [
            "The primary use cases are guest Shared Virtual Address (SVA) and guest IO virtual address (IOVA), wherein the vIOMMU implementation relies on the physical IOMMU ..."
          ]
        },
        {
          "title": "LKML Discussion: DMA API and IOMMU (March 6, 2025)",
          "url": "https://lkml.org/lkml/2025/3/6/1236",
          "excerpts": [
            "There is a lot of state tied to the struct device lifecycle that the DMA API and iommu implicitly manages."
          ]
        },
        {
          "title": "Capabilities - Fuchsia",
          "url": "https://fuchsia.dev/fuchsia-src/concepts/components/v2/capabilities",
          "excerpts": [
            "A capability combines access to a resource and a set of rights, providing a access control and a means for interacting with the resource."
          ]
        },
        {
          "title": "Computer Laboratory: Capsicum: practical capabilities for UNIX",
          "url": "https://www.cl.cam.ac.uk/research/security/capsicum/",
          "excerpts": [
            "Capsicum is a lightweight OS capability and sandbox framework developed at the University of Cambridge Computer Laboratory, supported by grants from Google."
          ]
        },
        {
          "title": "Unsafe in Rust: Syntactic Patterns",
          "url": "https://cs.stanford.edu/~aozdemir/blog/unsafe-rust-syntax/",
          "excerpts": [
            "This post takes a first step in that direction by laying the basis for syntactic analyses of unsafe in Rust code hosted on crates.io."
          ]
        },
        {
          "title": "Software Sandboxing Basics",
          "url": "https://blog.emilua.org/2025/01/12/software-sandboxing-basics/",
          "excerpts": [
            "Fortunately there’s a well researched model that solves\nthis problem for us: capability-based security.",
            "There’s even a programming\nlanguage based on the actor model and capability-based security:\n[the Pony programming language",
            "A capability isn’t only a reference\nto a resource, but the associated access rights as well. Owning a capability is\nthe same as also having access rights to perform actions",
            "Now that we have messaging solved with the actor model, let’s jump into\nsandboxing (security models) again.",
            ")*” is\none of the most well known phrases within the UNIX culture. If we can send file\ndescriptors then we have a really broad range of resources that we can work with\nfrom sandboxed processes.",
            "The initial versions of the contributed\npatchset were rejected due to the use of the syscall `write` in operations\nthat’d use the credentials of the calling process for permission\nchecks",
            "It’s important to notice that kernel developers will respect the convention\nwhether suid binaries are allowed on our Linux distro or not."
          ]
        },
        {
          "title": "Capsicum, Fuchsia, seL4, and Linux Sandboxing — OCAP-related discussion",
          "url": "https://github.com/dckc/awesome-ocap",
          "excerpts": [
            "Capsicum Capsicum is a lightweight OS capability and sandbox framework that extends the POSIX API, providing several new OS primitives to support object ...",
            "t's based on a\nmicrokernel, Zircon ,\nwith a capability security model. * 2021-06 cr0 blog: A few thoughts on Fuchsia security Julien Tinnes. Sandboxing is trivial. In fact a new process with access to no capabilities",
            "seL4 is the world's first\noperating-system kernel with an end-to-end proof of implementation\ncorrectness and security enforcement; it is available as open\nsource.",
            "2024-03: Fuchsia OS powers all Nest Hub devices – Fuchsia's capability-based component framework is now deployed across all Nest Hub models, replacing the ..."
          ]
        },
        {
          "title": "Capability-based security",
          "url": "https://en.wikipedia.org/wiki/Capability-based_security",
          "excerpts": [
            "Capability-based security refers to the principle of designing user programs such that they directly share capabilities with each other.",
            "Notable research and commercial systems employing capability-based security include the following:",
            "* [FreeBSD](/wiki/FreeBSD \"FreeBSD\") [Capsicum](/wiki/Capsicum_\\(Unix\\) \"Capsicum \\\\(Unix\\\\)\")",
            "* [Genode](/wiki/Genode \"Genode\")",
            "* [Fuchsia](/wiki/Fuchsia_\\(operating_system\\) \"Fuchsia \\\\(operating system\\\\)\")",
            "* [HarmonyOS](/wiki/HarmonyOS \"HarmonyOS\") ( [OpenHarmony](/wiki/OpenHarmony \"OpenHarmony\") )",
            "* [RedoxOS](/wiki/RedoxOS \"RedoxOS\")",
            "* [L4 microkernel family](/wiki/L4_microkernel_family \"L4 microkernel family\") :\n      + OKL4 from Open Kernel Labs\n      + [SeL4](/wiki/SeL4 \"SeL4\") from NICTA"
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr"
          ]
        },
        {
          "title": "OPW, Linux: The block I/O layer, part 2 - The request interface",
          "url": "http://ari-ava.blogspot.com/2014/06/opw-linux-block-io-layer-part-2-request.html",
          "excerpts": [
            "Jun 27, 2014 — The elv_merge() function involves invoking one of the elevator hooks (elevator_allow_merge_fn) to ask if a bio and a request can be merged; it ..."
          ]
        }
      ],
      "reasoning": "The field value outlines four concrete driver-strategy options and evaluates them along dimensions such as complexity, performance, security, time-to-boot, licensing, and maintenance. Several excerpts directly address these strands. For the approach that aims to reuse Linux drivers (via rewriting/adaptation), content that discusses Rust for Linux as a mechanism to port or wrap existing Linux driver code, and the required FFI boundaries, is highly relevant. It covers how to bridge Rust with existing C drivers, the level of effort, and licensing implications when bringing Linux drivers into a new OS via wrappers or reinterpretation. Statements emphasizing that Rust can wrap or interoperate with C through FFI, and that such wrappers introduce an unsafe boundary, map directly to the Shim-based Porting category and its tradeoffs. Moreover, explicit discussions of Linux-specific driver portability concerns (e.g., GPL-2.0 licensing, the need to treat UAPI boundaries carefully) illuminate the licensing and maintenance dimensions of porting work. The excerpts mentioning linuxkpi (Linux Kernel Programming Interface) and VFIO/IOMMU hardware virtualization concepts provide concrete infrastructure options for virtualization-based reuse strategies and the security/isolation guarantees that they entail. Additionally, excerpts describing the virtualization path (reusing drivers via a host, or exporting devices to user-space via VFIO) supply concrete context for \"Virtualization-based Reuse,\" including performance and isolation tradeoffs. Finally, excerpts that describe the broader Linux driver model and stable/unstable interfaces give background on why direct in-kernel reuse is so challenging, supporting the \"Very High complexity\" assessment for Direct Reuse of Linux drivers. Taken together, these excerpts collectively map onto the fine-grained field's four strategy nodes and support the matrix's evaluation criteria. The most directly supportive content centers on Rust-for-Linux FFI boundaries and safety implications, followed by VFIO/IOMMU-based isolation and licensing/maintenance considerations; broader driver-model discussions provide necessary background for evaluating feasibility and long-term maintainability.",
      "confidence": "medium"
    },
    {
      "field": "security_architecture_for_drivers",
      "citations": [
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode."
          ]
        },
        {
          "title": "IMA and EVM overview (Yocto/Yocto-related writeup)",
          "url": "https://ejaaskel.dev/yocto-hardening-ima-and-evm/",
          "excerpts": [
            "IMA is an acronym for integrity measurement architecture. It is a security subsystem in Linux that can be used to ensure the system’s integrity by measuring and appraising files before they are accessed or executed.",
            "The measurement consists of creating hashes of the files in the system and storing the hashes in the measurement list.",
            "IMA appraisal on the other hand is a kind of a Linux extension to the secure boot, where the file usage may be denied if their content doesn’t match the expected values.",
            "In addition to these two options, you most likely want to add `CONFIG_AUDIT=y` so that the IMA appraisal violations get logged to `dmesg`."
          ]
        },
        {
          "title": "Apriorit blog on Interoperability and Unsafe Rust",
          "url": "https://www.apriorit.com/dev-blog/interoperability-unsafe-rust",
          "excerpts": [
            "Rust interoperability allows developers to use the Foreign Function Interface (FFI) to call Rust functions from other languages and call functions written in other languages from Rust."
          ]
        },
        {
          "title": "Rust-ready Driver Security and FFI Considerations",
          "url": "https://www.codethink.co.uk/articles/rust-ready/",
          "excerpts": [
            "FFI is inherently unsafe, and so your developers will need to understand and appreciate all of the safety rules before engaging in this kind of development.",
            "This coexistence is made possible primarily because Rust is designed to be a systems programming language and as such has interoperability with C built in.",
            "The Kernel has significant numbers of very well designed interfaces which are explicit about lifetimes of objects etc. permitting reasonably simple, or at least comprehensible, bindings into the Rust world.",
            "These two worlds, safe and unsafe, live together by means of very constrained boundaries at which the assumptions Rust makes can be asserted back to the compiler.",
            "With all of the above, developers can then use Rust's ability to reference symbols in C code (or export symbols with C linkage) by means of Rust's FFI.",
            "As you'd expect, the Rust ecosystem provides tooling to assist with managing all this, there's [`cargo geiger`](https://crates.io/crates/cargo-geiger) which is a tool to determine how much of, and what kind of, unsafe code exists in your software and its dependency tree.",
            "And there's [`cargo miri`](https://github.com/rust-lang/miri) which can actually interpret the intermediate form of Rust within the compiler and can be used to check for undefined behaviour at runtime (typically via your test suite), allowing you to assert the correctness of the second class of `unsafe` keyword usage mentioned above."
          ]
        },
        {
          "title": "vfio.txt - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ..."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        }
      ],
      "reasoning": "The most relevant content directly argues for relocating drivers from the kernel into user-space or isolated environments to improve security. One excerpt notes that seL4 enables user-mode device drivers and that drivers can be isolated from the trusted computing base, which directly supports the field's emphasis on isolation strategies as a security architecture choice. Additional excerpts describe how Linux VFIO uses the IOMMU to securely expose device access to user-space drivers, which is a hardware-assisted isolation mechanism crucial for limiting DMA-based threats. Together, these excerpts substantiate the core defense-in-depth: (a) architectural separation of drivers (kernel vs user space or sandbox), (b) hardware-enforced memory protection via IOMMU, and (c) use of mediated or containerized driver models (e.g., microkernel-like or VM-based isolation) to constrain the blast radius of driver faults or compromises. Several excerpts also emphasize broader security models that complement isolation, such as Capsicum-style capability permissions, and integrity-measurement approaches (IMA/EVM, TPM) to protect the driver lifecycle and runtime integrity, which align with the field value's emphasis on a multi-layered protection stack. Additional notes discuss the boundary implications of using FFI (unsafe boundaries between Rust and C) and how that boundary can influence the attack surface, which is relevant to the \"FFI boundaries\" portion of the field value. Collectively, these excerpts map well to the threat model and multi-layer defenses described in the field value, illustrating concrete security architectures and mechanisms used to secure drivers in modern systems, including VFIO/IOMMU, sandboxing, and integrity verification.",
      "confidence": "medium"
    },
    {
      "field": "maintenance_cost_of_tracking_linux_apis.the_upstream_churn_problem",
      "citations": [
        {
          "title": "Stable Kernel Interfaces and API Nonsense (stable-api-nonsense.rst)",
          "url": "https://www.kernel.org/doc/Documentation/process/stable-api-nonsense.rst",
          "excerpts": [
            "The Linux Kernel Driver Interface"
          ]
        },
        {
          "title": "The Linux Kernel Driver Interface",
          "url": "https://docs.kernel.org/process/stable-api-nonsense.html",
          "excerpts": [
            "This is being written to try to explain why Linux does not have a binary kernel interface, nor does it have a stable kernel interface."
          ]
        },
        {
          "title": "Hacker News discussion on Linux driver API stability",
          "url": "https://news.ycombinator.com/item?id=30868346",
          "excerpts": [
            "This is by design. Linux doesn't *want* to pay the maintenance and performance costs of guaranteeing a stable in-kernel API/ABI: [https://www.kernel.org/doc/Documentation/process/stable-api-"
          ]
        },
        {
          "title": "Linux Kernel API churn and Android drivers (Greg Kroah-Hartman discussion)",
          "url": "https://lwn.net/Articles/372419/",
          "excerpts": [
            "The kernel community has for years been telling these companies to get their code merged, so that they can take advantage of the security fixes, and handle the rapid API churn automatically."
          ]
        },
        {
          "title": "Ask HN: Why no stable binary kernel interface for drivers?",
          "url": "https://news.ycombinator.com/item?id=14533398",
          "excerpts": [
            "Jun 11, 2017 — Linux does not have a stable binary kernel interface for drivers. This enforces either that all the drivers must be in the kernel source ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that the Linux kernel does not provide a stable internal API or ABI for kernel modules, that this is deliberate to allow refactoring and core improvements, and that maintaining such interfaces for external drivers is not the goal. It also notes that the recommended long-term solution is to have drivers merged into the mainline kernel so they are maintained with the rest of the kernel. Excerpt how-it-is framed in the kernel documentation aligns directly with the claim that there is no stable internal API/ABI and that the stable-api-nonsense documentation explains this philosophy, which supports the idea of upstream churn. Excerpt describing the rationale around churn states that the kernel avoids a stable in-kernel API/ABI to reduce maintenance and performance costs, and explains the ongoing variability across subsystems (networking, storage, DRM, USB), which reinforces the notion of constant flux and the need for drivers to be re-evaluated or merged upstream to stay compatible. Excerpt discussing discussions around stability and the lack of a stable binary interface provides historical and community context that corroborates the overall stance. Excerpt about API churn and Android drivers illustrates real-world implications of churn and the need to adapt or merge to maintain compatibility, further supporting the theme of churn being an active and recognized problem. Excerpt posing questions about why there is no stable interface captures the community inquiry and reinforces the accepted view that stability for external drivers is not provided by design, aligning with the field value. The combination of these excerpts paints a coherent picture: there is intentional absence of a stable kernel API/ABI for modules, upstream churn is a persistent reality, and long-term stability is pursued primarily through upstream maintenance via mainline integration rather than external, out-of-tree compatibility. ",
      "confidence": "high"
    },
    {
      "field": "maintenance_cost_of_tracking_linux_apis.impact_on_shims_and_ports",
      "citations": [
        {
          "title": "Linux Kernel API churn and Android drivers (Greg Kroah-Hartman discussion)",
          "url": "https://lwn.net/Articles/372419/",
          "excerpts": [
            "The kernel community has for years been telling these companies to get their code merged, so that they can take advantage of the security fixes, and handle the rapid API churn automatically."
          ]
        },
        {
          "title": "Stable Kernel Interfaces and API Nonsense (stable-api-nonsense.rst)",
          "url": "https://www.kernel.org/doc/Documentation/process/stable-api-nonsense.rst",
          "excerpts": [
            "The Linux Kernel Driver Interface"
          ]
        },
        {
          "title": "The Linux Kernel Driver Interface",
          "url": "https://docs.kernel.org/process/stable-api-nonsense.html",
          "excerpts": [
            "This is being written to try to explain why Linux does not have a binary kernel interface, nor does it have a stable kernel interface."
          ]
        },
        {
          "title": "Hacker News discussion on Linux driver API stability",
          "url": "https://news.ycombinator.com/item?id=30868346",
          "excerpts": [
            "This is by design. Linux doesn't *want* to pay the maintenance and performance costs of guaranteeing a stable in-kernel API/ABI: [https://www.kernel.org/doc/Documentation/process/stable-api-"
          ]
        },
        {
          "title": "Ask HN: Why no stable binary kernel interface for drivers?",
          "url": "https://news.ycombinator.com/item?id=14533398",
          "excerpts": [
            "Jun 11, 2017 — Linux does not have a stable binary kernel interface for drivers. This enforces either that all the drivers must be in the kernel source ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant material discusses the ongoing, multi-year engineering effort required to keep a port of Linux drivers usable via a shim like LinuxKPI, highlighting that compatibility is not a one-off port but a perpetual maintenance task and that even critical subsystems like graphics and networking require sustained effort. This directly supports the claim that high API churn imposes substantial maintenance costs on shims and ports. The next set of excerpts addresses the broader context of API churn and the absence of a stable binary kernel interface, which reinforces why maintaining cross-OS compatibility layers is costly and fragile: Linux does not commit to a fully stable in-kernel API/ABI, and discussions explain why the kernel maintains churn to avoid heavy maintenance burdens, with consequences for porting layers. Another excerpt emphasizes that the kernel community has long argued for integrating drivers in-tree to ensure security fixes and handle churn, underscoring the architectural tension that makes external shims expensive to sustain. Finally, discussions about the existence or non-existence of a stable kernel API for drivers provide context for the long-term feasibility issues of porting approaches, though they are slightly less specific about maintenance costs than the explicit mention of perpetual shims engineering. Overall, the chain of ideas in these excerpts supports the claim that API churn imposes significant, ongoing maintenance costs on shims and driver-port strategies, and that lack of a stable interface compounds the challenge.",
      "confidence": "high"
    },
    {
      "field": "executive_summary",
      "citations": [
        {
          "title": "Linux ABI description",
          "url": "https://docs.kernel.org/admin-guide/abi.html",
          "excerpts": [
            "We have four different levels of ABI stability, as shown by the four\ndifferent subdirectories in this location. Interfaces may change levels\nof stability according to the rules described below.",
            "## ABI symbols [¶]",
            "## ABI files [¶]",
            "This part of the documentation inside Documentation/ABI directory\nattempts to document the ABI between the Linux kernel and\nuserspace, and the relative stability of these interfaces.",
            " This directory documents the interfaces that the developer has\n>     defined to be stable. Userspace programs are free to use these\n>     interfaces with no restrictions, and backward compatibility for\n>     them will be guaranteed for at least 2 years. Most interfaces\n>     (like syscalls) are expected to never change"
          ]
        },
        {
          "title": "LinuxKPI - FreeBSD Wiki",
          "url": "https://wiki.freebsd.org/LinuxKPI",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "The Linux kernel doesn't provide a stable ABI for modules so they ...",
          "url": "https://news.ycombinator.com/item?id=21243406",
          "excerpts": [
            "Apr 10, 2021 — The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to rebuild ...",
            "The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to ...",
            "The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to ..."
          ]
        },
        {
          "title": "Symbol Namespaces",
          "url": "https://docs.kernel.org/core-api/symbol-namespaces.html",
          "excerpts": [
            "The following document describes how to use Symbol Namespaces to structure the export surface of in-kernel symbols exported through the family of EXPORT_SYMBOL ..."
          ]
        },
        {
          "title": "Symbol Namespaces — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/core-api/symbol-namespaces.html",
          "excerpts": [
            "The following document describes how to use Symbol Namespaces to structure the export surface of in-kernel symbols exported through the family of EXPORT_SYMBOL ...",
            "In addition to the macros EXPORT_SYMBOL() and EXPORT_SYMBOL_GPL(), that allow exporting of kernel symbols to the kernel symbol table, variants of these are ..."
          ]
        },
        {
          "title": "The proper use of EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/769471/",
          "excerpts": [
            "In the case of EXPORT_SYMBOL_GPL(), only modules that declare a GPL-compatible license will be able to see the symbol.",
            "Oct 27, 2018 — In the case of EXPORT_SYMBOL_GPL(), only modules that declare a GPL-compatible license will be able to see the symbol."
          ]
        },
        {
          "title": "CRC value for a symbol in Module.symvers is zero",
          "url": "https://stackoverflow.com/questions/18594368/crc-value-for-a-symbol-in-module-symvers-is-zero",
          "excerpts": [
            "The config option CONFIG_MODVERSIONS=y is turned on. But the CRC value in Module.symvers for the particular symbol is 0x00000000 which leads ..."
          ]
        },
        {
          "title": "DMA-API-HOWTO.txt",
          "url": "https://www.kernel.org/doc/Documentation/DMA-API-HOWTO.txt",
          "excerpts": [
            "This is a guide to device driver writers on how to use the DMA API with example pseudo-code. For a concise description of the API, see DMA-API.txt. CPU and DMA ..."
          ]
        },
        {
          "title": "linux - How are intermodule dependencies resolved when...?",
          "url": "https://stackoverflow.com/questions/20412183/how-are-intermodule-dependencies-resolved-when",
          "excerpts": [
            "Modversioning is used to ensure that binary loadable modules are compatible with the kernels they are loaded upon. It is enabled with .config option CONFIG_ ..."
          ]
        },
        {
          "title": "EXPORT_SYMBOL in Linux Kernel - Linux Device Driver Part 29",
          "url": "https://embetronicx.com/tutorials/linux/device-drivers/export_symbol-in-linux-device-driver/",
          "excerpts": [
            "Another macro is also available to export the symbols like EXPORT_SYMBOL . That is EXPORT_SYMBOL_GPL() . EXPORT_SYMBOL exports the symbol to any loadable module . EXPORT_SYMBOL_GPL exports the symbol only to GPL-licensed modules."
          ]
        },
        {
          "title": "Feasibility of adding drivers to linux kernel",
          "url": "https://softwareengineering.stackexchange.com/questions/423138/feasibility-of-adding-drivers-to-linux-kernel",
          "excerpts": [
            "Mar 8, 2021 — I want to ask how much feasible is to add tons of device driver to Linux kernel , suppose after 30 -40 years the size of Linux kernel will be ..."
          ]
        },
        {
          "title": "Linux in-kernel vs out-of-kernel drivers and plug and play ...",
          "url": "https://www.reddit.com/r/linuxhardware/comments/182uaw7/linux_inkernel_vs_outofkernel_drivers_and_plug/",
          "excerpts": [
            "Linux in-kernel drivers are preferable over out-of-kernel drivers for most users and use cases as problems with locating, installing and maintaining drivers ..."
          ]
        },
        {
          "title": "Linux Syscall Note (SPDX)",
          "url": "https://spdx.org/licenses/Linux-syscall-note.html",
          "excerpts": [
            "This note is used with the Linux kernel to clarify how user space API files should be treated. Text Text in italicized blue is omittable.",
            "Text in italicized blue is omittable (see [Matching Guidelines B.3.5](https://spdx.github.io/spdx-spec/v2.3/license-matching-guidelines-and-templates/)).",
            "the only valid version of the GPL as far as the kernel\nis concerned is \\_this\\_ particular version of the license (ie v2, not\nv2.2 or v3.x or whatever), unless explicitly otherwise stated. Linus Torvalds",
            "NOTE! This copyright does \\*not\\* cover user programs that use kernel\nservices by normal system calls - this is merely considered normal use\nof the kernel, and does \\*not\\* fall under the heading of \"derived work\"."
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        },
        {
          "title": "Confusing Usage-Guide in LICENSES/exceptions/Linux-syscall-note",
          "url": "https://lkml.org/lkml/2020/2/21/2193",
          "excerpts": [
            "For the reference, here is the Linux syscall note: NOTE! This copyright does *not* cover user programs that use kernel"
          ]
        }
      ],
      "reasoning": "The field value describes a core finding: reusing Linux kernel drivers in a new Rust-based OS via FFI is not practical due to two pillars. First, the ABI/API instability of the Linux kernel means that modules in user space tied to kernel internals would face continual recompilation and breakage across kernel versions. This is supported by multiple passages that describe Linux ABI stability issues and lack of a stable binary interface for drivers, including explicit statements that Linux does not have a stable binary kernel interface and that stability is not guaranteed across kernel versions. The executive_summary's conclusion about instability driving infeasibility is directly grounded in these sources, which emphasize ABI instability and frequent changes to internal interfaces. Second, licensing concerns—particularly GPLv2—pose a barrier because linking or integrating Linux kernel code into a new kernel could make the new kernel a derivative work subject to GPL terms; this is a central argument in several excerpts that explain GPL derivative-work implications, the meaning of EXPORT_SYMBOL_GPL, the necessity of MODULE_LICENSE tags, and the way the kernel license governs UAPI boundaries. The excerpts that discuss MODULE_LICENSE, GPL derivative implications, and GPL vs non-GPL symbol exports provide the strongest, most direct support for the licensing barrier claimed in the executive_summary. Additional excerpts discuss the Linux kernel's module-symbol and symbol-versioning concepts ( Module.symvers ) and export symbol behavior (EXPORT_SYMBOL, EXPORT_SYMBOL_GPL), which further substantiate the licensing and ABI complexity involved in direct driver reuse. Beyond the core findings, the field value also notes a strategic pivot: use virtualization/compatibility layers to leverage the Linux driver ecosystem without triggering GPL-derived constraints, and plan for native Rust drivers later. Excerpts that describe LinuxKPI-style compatibility layers, Linux-to-FreeBSD/VM-based compatibility approaches, VFIO/vhost/vfio-user, and virtio-based paravirtualization provide concrete evidence for these alternative strategies and are thus highly relevant for supporting the recommended architectural direction. Some excerpts about symbol namespaces and nuanced licensing details (e.g., how GPL-only symbols affect visibility, and the role of UAPI vs internal APIs) provide partial context but are slightly less central to the executive_summary's primary claims, hence are ranked slightly lower in relevance compared to direct ABI/License points.",
      "confidence": "high"
    },
    {
      "field": "maintenance_cost_of_tracking_linux_apis.mitigation_through_automation",
      "citations": [
        {
          "title": "Linux Kernel API churn and Android drivers (Greg Kroah-Hartman discussion)",
          "url": "https://lwn.net/Articles/372419/",
          "excerpts": [
            "The kernel community has for years been telling these companies to get their code merged, so that they can take advantage of the security fixes, and handle the rapid API churn automatically."
          ]
        },
        {
          "title": "Stable Kernel Interfaces and API Nonsense (stable-api-nonsense.rst)",
          "url": "https://www.kernel.org/doc/Documentation/process/stable-api-nonsense.rst",
          "excerpts": [
            "The Linux Kernel Driver Interface"
          ]
        },
        {
          "title": "The Linux Kernel Driver Interface",
          "url": "https://docs.kernel.org/process/stable-api-nonsense.html",
          "excerpts": [
            "This is being written to try to explain why Linux does not have a binary kernel interface, nor does it have a stable kernel interface."
          ]
        },
        {
          "title": "Ask HN: Why no stable binary kernel interface for drivers?",
          "url": "https://news.ycombinator.com/item?id=14533398",
          "excerpts": [
            "Jun 11, 2017 — Linux does not have a stable binary kernel interface for drivers. This enforces either that all the drivers must be in the kernel source ..."
          ]
        },
        {
          "title": "Hacker News discussion on Linux driver API stability",
          "url": "https://news.ycombinator.com/item?id=30868346",
          "excerpts": [
            "This is by design. Linux doesn't *want* to pay the maintenance and performance costs of guaranteeing a stable in-kernel API/ABI: [https://www.kernel.org/doc/Documentation/process/stable-api-"
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly discusses automating the response to API churn, noting that the kernel community aims to \"handle the rapid API churn automatically\" and that large-scale CI and testing infrastructure are used to test changes. This aligns with the field value's emphasis on heavy automation and CI systems to maintain compatibility across kernel versions. Supporting context comes from the broader discussion of a lack of a stable binary kernel interface, which creates ongoing maintenance pressure and reinforces the need for automation tools to adapt to API changes. Additional excerpts reinforce the theme: discussions of stability (or the absence of it by design) illustrate why automation and tooling are not optional but necessary in this ecosystem, providing context for why automated refactoring and binding updates would be valuable in mitigating maintenance costs. Taken together, these excerpts connect the need for automation to the realities of API churn and the architectural implications for a new OS project that may rely on existing driver codebases. The most informative parts are the explicit statements about automatic handling of API churn and the architectural consequence that a stable in-kernel interface is not guaranteed, thus driving demand for automation tooling and CI/quality pipelines. The remaining excerpts, while related, provide background on stability debates and community practices rather than explicit prescriptions for automation tooling, and thus are less directly supportive but still contextually relevant to the field value.",
      "confidence": "medium"
    },
    {
      "field": "concurrency_models_and_driver_api.per_cpu_data_model",
      "citations": [
        {
          "title": "Symmetric Multi-Processing – Linux Kernel Labs Lecture",
          "url": "https://linux-kernel-labs.github.io/refs/heads/master/lectures/smp.html",
          "excerpts": [
            "Instead, an array sized to the maximum possible CPU cores is\nused and each core will use its own array entry to read and write\ndata.",
            "Instead, an array sized to the maximum possible CPU cores is\nused and each core will use its own array entry to read and write\ndata.",
            "Per CPU data avoids race conditions by avoiding to use shared data.",
            "Per CPU data avoids race conditions by avoiding to use shared data."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt states that the design uses an array sized to the maximum number of CPUs, with each core using its own array entry to read and write, which directly maps to a per-CPU variable model (per-core copies to minimize contention and avoid global locking). This aligns with the concept of a per-CPU data model where data local to a CPU is accessed without cross-core synchronization, enabling efficient access for frequently updated, core-local state. The second most relevant excerpt explicitly notes that per-CPU data avoids race conditions by avoiding shared data, which underpins the rationale for the per-CPU pattern by describing the core advantage of eliminating contention through locality. Together, these excerpts substantiate the idea of representing per-CPU state as a collection of per-core slots (an array indexed by CPU) and leveraging locality to improve performance, consistent with the described pattern and potential Rust API adaptations (e.g., thread_local-like semantics) for safe, dynamic per-CPU structures in a kernel context.",
      "confidence": "high"
    },
    {
      "field": "native_and_userspace_driver_architectures",
      "citations": [
        {
          "title": "Rust — The Linux Kernel documentation",
          "url": "https://docs.kernel.org/rust/index.html",
          "excerpts": [
            "Documentation related to Rust within the kernel. To start using Rust in the kernel, please read the Quick Start guide."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "rust-for-linux kernel crate documentation",
          "url": "https://rust-for-linux.github.io/docs/kernel/",
          "excerpts": [
            "Common clock framework. [cred](cred/index.html \"kernel::cred mod\")"
          ]
        },
        {
          "title": "FFI - The Rustonomicon",
          "url": "https://doc.rust-lang.org/nomicon/ffi.html",
          "excerpts": [
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped with unsafe {} as a promise to the compiler that everything contained within ...",
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped\nwith `unsafe {}` as a promise to the compiler that everything contained within\ntruly is saf",
            "The raw C API needs to be wrapped to provide memory safety and make use of higher-level concepts\nlike vectors.",
            "Unsafe blocks isolate unsafety and are a promise to\nthe compiler that the unsafety does not leak out of the block.",
            "When declaring the argument types to a foreign function, the Rust compiler\ncannot check if the declaration is correct, so specifying it correctly is part\nof keeping the binding correct at runtime."
          ]
        }
      ],
      "reasoning": "- The excerpts that focus on Rust in the Linux kernel and the Rust-for-Linux initiative directly inform the native_in_kernel_drivers field. They discuss porting or integrating Rust alongside C in kernel space, and show concrete examples and policies around building kernel drivers in Rust, which is exactly the scenario of writing native in-kernel drivers in Rust for a new OS. For instance, material describing the Rust for Linux kernel crate and accompanying documentation demonstrates that the Rust language can live inside the kernel and interoperate with existing C components, which underpins native_in_kernel_drivers in a Rust-first kernel.",
      "confidence": "medium"
    },
    {
      "field": "hardware_discovery_and_configuration_layer",
      "citations": [
        {
          "title": "ACPI and UEFI Specifications (excerpt)",
          "url": "https://uefi.org/specs/ACPI/6.5/05_ACPI_Software_Programming_Model.html",
          "excerpts": [
            "The [Root System Description Pointer (RSDP)]() structure is located in the\nsystem’s memory address space and is setup by the platform firmw",
            "This structure contains the address of the [Extended System Description Table (XSDT)]() ,\nwhich references other description tables that provide\ndata to OSPM, supplying it with knowledge of the base system’s\nimplementation and configur"
          ]
        },
        {
          "title": "ACPI/UEFI Specification (ACPI 6.5) and related references",
          "url": "https://uefi.org/sites/default/files/resources/ACPI_Spec_6_5_Aug29.pdf",
          "excerpts": [
            "Root System Description Pointer (RSDP) Structure",
            "Advanced Configuration and Power Interface (ACPI) Specification, Release 6.5",
            ".2.26.5 SMBIOS Management Information Structur"
          ]
        },
        {
          "title": "Device Tree Basics",
          "url": "https://devicetree-specification.readthedocs.io/en/latest/chapter2-devicetree-basics.html",
          "excerpts": [
            "The dma-ranges property is used to describe the direct memory access (DMA) structure of a memory-mapped bus whose devicetree parent can be accessed from DMA ... For\nexample, the architecture of PCI enables a client to probe and detect\nattached devices, and thus devicetree nodes describing PCI devices\nmight not be required. However, a device node is required to describe a\nPCI host bridge device in the system if it cannot be detected by\nprobing. **Example**\n\n[Fig. 2.1]() shows an example representation of a\nsimple devicetree that is nearly\ncomplete enough to boot a simple operating system, with the platform\ntype, CPU, memory and a single UART described. Device nodes are shown\nwith properties and values inside each node. !",
            "Property name: `reg`",
            "Description:\n\n> The *reg* property describes the address of the device’s resources\n> within the address space defined by its parent bus. Most commonly this\n> means the offsets and lengths of memory-mapped IO register blocks, but\n> may have a different meaning on some bus types. Addresses in the address\n> space defined by the root node are CPU real addresses. >\n> The value is a *<prop-encoded-array>*, composed of an arbitrary number\n> of pairs of address and length, *<address length>*. The number of\n> *<u32>* cells required to specify the address and length are\n> bus-specific and are specified by the ** and **\n> properties in the parent of the device node. If the parent node\n> specifies a value of 0 for **, the length field in the value\n> of *reg* shall be omitted.\n",
            "Property name: `ranges`",
            "Value type: `<empty>` or `<prop-encoded-array>` encoded as an arbitrary number of\n(*child-bus-address*, *parent-bus-address*, *length*) triplets. Description:\n\n> The *ranges* property provides a means of defining a mapping or\n> translation between the address space of the bus (the child address\n> space) and the address space of the bus node’s parent (the parent\n> address space). >\n> The format of the value of the *ranges* property is an arbitrary number\n> of triplets of (*child-bus-address*, *parent-bus-address*, *length*)\n>\n> * The *child-bus-address* is a physical address within the child bus’\n>   address spa"
          ]
        },
        {
          "title": "PCI Express - OSDev Wiki",
          "url": "https://wiki.osdev.org/PCI_Express",
          "excerpts": [
            "On x86 and x64 platforms, the address of each memory area is determined by the ACPI 'MCFG' table.",
            "To access a specific register within a device's PCI configuration space, you have to use the device's PCI Segment Group and bus to determine which memory mapped PCI configuration space area to use, and obtain the starting physical address and starting bus number for that memory mapped area.",
            "Physical_Address = MMIO_Starting_Physical_Address + ((Bus) << 20 | Device << 15 | Function << 12)",
            "The enhanced configuration mechanism makes use of memory mapped address space range/s to access PCI configuration space.",
            "or non-x86 systems the method varies, but usually systems provide themselves with a Devicetree which is parsed at runtime.",
            "Enhanced Configuration Mechanism",
            "the address of each memory area is determined by the ACPI 'MCFG' table.",
            "The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes. This extended configuration space cannot be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC).",
            "The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes. This extended configuration space cannot be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC). Instead, an Enhanced Configuration Mechanism is provided."
          ]
        },
        {
          "title": "OSDev PCI Enumeration and Firmware Interfaces",
          "url": "https://wiki.osdev.org/PCI",
          "excerpts": [
            "Enumerating PCI Buses. There are 3 ways to enumerate devices on PCI buses ... However, support for them is mandatory in PCIe devices, so you can be sure ...",
            "The first step for the recursive scan is to implement a function that scans one bus. Pseudo-code might look like this:\nvoid checkBus ( uint8_t bus ) { uint8_t device ; for ( device = 0 ; device < 32 ; device ++ ) { checkDevice ( bus , device ); } "
          ]
        },
        {
          "title": "Retry deferred probe at runtime (device tree) - driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/78863378/retry-deferred-probe-at-runtime-device-tree",
          "excerpts": [
            "I've developed a device tree overlay to describe some pieces of my embedded system. The system is comprised of a Raspberry Pi, a separate custom circuit board ..."
          ]
        }
      ],
      "reasoning": "For x86_64 platform bring-up, the firmware/BIOS interfaces give the OS essential entry points to the platform hardware via ACPI. The Root System Description Pointer located in firmware tables enables the OS to locate the Extended System Description Table, which references critical ACPI tables that drive system bring-up, such as the MADT for interrupt controllers and the MCFG for PCI Express configuration space base. This chain (RSDP -> XSDT -> other ACPI tables) is central to enumerating hardware on x86_64 machines and sets the stage for PCIe/bus initialization. SMBIOS information accessed via firmware tables also provides static hardware and memory configuration data that informs platform bring-up. The OS therefore must parse these tables to understand the platform's topology and capabilities during early boot. From these excerpts, you can quote the guidance that the Root System Description Pointer and XSDT provide pointers to critical platform description tables, and that SMBIOS tables supply static hardware information, which are fundamental for correct hardware bring-up on x86_64 platforms.",
      "confidence": "high"
    },
    {
      "field": "concurrency_models_and_driver_api.rcu_like_model",
      "citations": [
        {
          "title": "Linux RCU Documentation",
          "url": "https://www.kernel.org/doc/Documentation/RCU/whatisRCU.txt",
          "excerpts": [
            ". There are at least three flavors of RCU usage in the Linux kernel."
          ]
        },
        {
          "title": "RCU (Read-Copy Update) - Linux kernel documentation",
          "url": "https://docs.kernel.org/RCU/rcu.html",
          "excerpts": [
            "These counters allow limited types of blocking within\n  RCU read-side critical sections.",
            "SRCU also uses CPU-local\n  counters, and permits general blocking within RCU read-side\n  critical sections."
          ]
        }
      ],
      "reasoning": "The field value describes an RCU-like concurrency model optimized for read-mostly workloads, enabling many readers to access shared data concurrently without locks and using a grace-period-based reclamation cycle. Excerpt 0 explicitly notes that there are multiple flavors of RCU usage in the Linux kernel, which aligns with the idea of adopting an RCU-inspired model with various implementation options. Excerpt 1 highlights that RCU read-side critical sections can involve limited types of blocking via counters, which supports the notion of lightweight read protection without heavy locking. Excerpt 2 discusses a variant (SRCU) that uses CPU-local counters and permits different blocking semantics within read-side critical sections, further reinforcing that there are different RCU-like approaches and their implications for synchronization. Collectively, these excerpts corroborate the core idea of an RCU-inspired mechanism focused on read-dominant access patterns, with distinctions between flavors and blocking characteristics that would inform architectural choices for a Rust-based OS. The field value's mention of specific APIs (rcu_read_lock, rcu_read_unlock, rcu_dereference, synchronize_rcu, call_rcu) and grace periods is not directly quoted in the excerpts, but the references to read-side critical sections and RCU variants map to the conceptual components described in the field value, indicating alignment more with design principles than with exact API coverage in the excerpts.",
      "confidence": "medium"
    },
    {
      "field": "shim_based_porting_deep_dive.approach_description",
      "citations": [
        {
          "title": "FreeBSD LinuxKPI documentation",
          "url": "https://freebsdfoundation.org/blog/sharing-dual-licensed-drivers-between-linux-and-freebsd/",
          "excerpts": [
            "or the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD.",
            "The second issue with code reuse between operating systems relates to the interfaces provided by and/or required by the code in question. This may be achieved in several ways; one is to implement core functionality in a reusable library, and augment it with an operating-system-dependent layer. It may be that the core library can be shared between Linux, Windows, and FreeBSD, providing for significant reuse. For the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD",
            "### **Intel i915 graphics drivers**",
            "**License:** MIT",
            "**Interface Compatibility**",
            "The Linux Kernel “license rules” documentation <https://www.kernel.org/doc/html/v5.0/process/license-rules.html> has explict discussion of dual licensing: “individual files can be provided under a dual license, e.g. one of the compatible GPL variants and alternatively under a permissive license like BSD, MIT etc"
          ]
        },
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "The drm-kmod package, for example, includes Intel and AMD Linux graphics driver code, but is able to compile and run properly on FreeBSD using LinuxKPI.",
            "Jul 5, 2022 — LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification.",
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification.",
            "there exist FreeBSD specific bugs, performance hits, and missing features as a result of LinuxKPI's small codebase.",
            "This drm-debugfs support is essential to igt-gpu-tools, so my next task was compiling drm-kmod with the DEBUG\\_FS configuration flag enabled",
            "GPLv2 licensed Linux code must be rewritten under FreeBSD's 2-clause BSD License.",
            "drm-debugfs support is one of many missing features."
          ]
        },
        {
          "title": "LinuxKPI - FreeBSD Wiki",
          "url": "https://wiki.freebsd.org/LinuxKPI",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes implementing a compatibility layer (a shim) that emulates the Linux KPI inside the new Rust-based OS kernel, allowing existing Linux drivers (written in C) to be compiled and run with minimal modifications. The most directly supporting content states that this interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver sources to another OS. It also notes that LinuxKPI enables Linux drivers to run in a FreeBSD environment with only minor modification, and cites concrete examples like the drm-kmod package for Intel/AMD graphics and the iwifi driver, illustrating the practical success of such an approach. Additional excerpts reinforce the concept by describing LinuxKPI as a small compatibility layer and by highlighting that drivers can be ported by sharing core functionality through a compatibility layer, possibly augmented with an OS-dependent layer. The later excerpts touch on licensing and broader code reuse considerations, which are contextually relevant to a cross-OS shim strategy but do not contradict the central claim. Overall, the selected excerpts consistently support the idea that a shim-based KPI compatibility layer is a viable, real-world approach for porting Linux drivers to a new kernel via minimal modifications, with FreeBSD/LinuxKPI serving as the canonical example and enabling technologies such as drm-kmod and iwlwifi illustrating its practicality.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_reliability_pipeline.testing_and_verification_strategies",
      "citations": [
        {
          "title": "Fuzzing with cargo-fuzz - Rust Fuzz Book",
          "url": "https://rust-fuzz.github.io/book/cargo-fuzz.html",
          "excerpts": [
            "cargo-fuzz is the recommended tool for fuzz testing Rust code. cargo-fuzz is itself not a fuzzer, but a tool to invoke a fuzzer."
          ]
        },
        {
          "title": "BlueOS Kernel – Written in Rust, compatible with POSIX",
          "url": "https://news.ycombinator.com/item?id=44669026",
          "excerpts": [
            "It would be momentous to simply get Linux drivers to comply with a kernel-agnostic ABI. ... operating system written in the Rust language from the ..."
          ]
        },
        {
          "title": "Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation",
          "url": "https://arxiv.org/html/2505.02464v1",
          "excerpts": [
            "However, Rust builds are not reproducible in all use cases and depend on multiple factors, for example, the absolute build path.",
            " \nWe implement partial instrumentation to focus the fuzzing efforts on unsafe Rust code to significantly increase the efficiency and probability of triggering and detecting vulnerabilities in Rust programs.",
            "This is challenging as Rust uses a complex compilation toolchain:\nThis includes multiple intermediate representations and several compiler stages facilitating Rust-specific code analysis.",
            " as MirChecker  [ 42 ] are static analysis frameworks that utilize intermediate code representations to detect different bug type",
            "To fuzz the target programs, FourFuzz utilizes afl.rs in version 0.15.3 which is based on AFL + + version 4.10c."
          ]
        }
      ],
      "reasoning": "The central field value advocates a multi-faceted verification pipeline: coverage-guided fuzzing with a focus on kernel interfaces, Rust-specific fuzzing enhancements, selective instrumentation for unsafe blocks, and formal/static verification tools. Cargo-fuzz is explicitly highlighted as a Rust-focused fuzzing tool, aligning with the Rust-centric testing emphasis. The idea of making drivers Rust-safe and interoperable with kernel drivers is echoed by references to Rust in kernel contexts, which supports the relevance of Rust-based driver/OS development considerations. Additional excerpts discuss fuzzing broader target programs and employing instrumentation strategies to focus fuzzing on risky areas, which directly supports the notion of a selective instrumentation approach. While some excerpts discuss related topics (e.g., reproducible builds, KernelCI) that provide ecosystem context, they do not directly corroborate the specific multi-tool verification stack, but they help frame the environment in which such strategies would operate. Overall, the most directly supportive items are those naming cargo-fuzz and instrumentation-oriented fuzzing approaches, with additional supportive context from Rust/kernel-driver discussions and broader fuzzing references.",
      "confidence": "medium"
    },
    {
      "field": "networking_stack_strategy.tcp_ip_stack_choice",
      "citations": [
        {
          "title": "redox-os / smoltcp · GitLab",
          "url": "https://gitlab.redox-os.org/redox-os/smoltcp/-/tree/redox",
          "excerpts": [
            "smoltcp* is a standalone, event-driven TCP/IP stack that is designed for bare-metal,\nreal-time systems. Its design goals are simplicity and robustness",
            "smoltcp, being a freestanding networking stack, needs to be able to transmit and receive raw frames. For testing purposes, we will use a regular OS, and run ...",
            "IPv4 time-to-live value is configurable per socket, set to 64 by default."
          ]
        },
        {
          "title": "Fuchsia Netstack3 - Rust-based netstack and related networking stack strategy",
          "url": "https://fuchsia.dev/fuchsia-src/contribute/roadmap/2021/netstack3",
          "excerpts": [
            "Netstack3 - A Fuchsia owned rust based netstack",
            "The Fuchsia Netstack team will design\nand implement a Rust-based netstack that achieves functional parity with the\nexisting netstack, while leveraging the type and memory safety of rust.",
            "Risks and mitigations"
          ]
        }
      ],
      "reasoning": "The most directly relevant information comes from excerpts describing smoltcp as a standalone, event-driven TCP/IP stack designed for bare-metal, real-time systems and its goals of simplicity and robustness. This directly supports the idea of adopting a mature, safety-focused TCP/IP stack written in Rust for a new OS, with smoltcp identified as a strong candidate in a no_std environment. Additional support comes from notes about smoltcp being tested in an OS-like context, which reinforces its practicality for integrating into a new OS architecture. The discussion of Fuchsia Netstack3 provides a concrete example of a Rust-based networking strategy at the OS level, aligning with the recommended approach to leverage Rust's safety guarantees to build a robust networking foundation from the ground up. Together, these excerpts substantiate both the specific recommendation to consider smoltcp and a broader Rust-based networking strategy exemplified by Netstack3, which the field value endorses as an architectural direction.",
      "confidence": "medium"
    },
    {
      "field": "shim_based_porting_deep_dive.maintenance_challenges",
      "citations": [
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "there exist FreeBSD specific bugs, performance hits, and missing features as a result of LinuxKPI's small codebase.",
            "Jul 5, 2022 — LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification. The drm-kmod package, ...",
            "The drm-kmod package, for example, includes Intel and AMD Linux graphics driver code, but is able to compile and run properly on FreeBSD using LinuxKPI."
          ]
        },
        {
          "title": "FreeBSD LinuxKPI documentation",
          "url": "https://freebsdfoundation.org/blog/sharing-dual-licensed-drivers-between-linux-and-freebsd/",
          "excerpts": [
            "or the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD.",
            "The second issue with code reuse between operating systems relates to the interfaces provided by and/or required by the code in question. This may be achieved in several ways; one is to implement core functionality in a reusable library, and augment it with an operating-system-dependent layer. It may be that the core library can be shared between Linux, Windows, and FreeBSD, providing for significant reuse. For the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD"
          ]
        }
      ],
      "reasoning": "The strongest support comes from language that the shim layer is a small compatibility layer that enables Linux drivers to run with only minor modification, which is precisely the form of a shim-based approach. However, the same excerpt also explicitly notes that the shim introduces FreeBSD-specific bugs, performance hits, and missing features due to the shim's small codebase, highlighting an ongoing maintenance burden and fragility as upstream kernel changes occur. Additional excerpts reinforce this context by describing the purpose of the linuxkpi layer as providing API-compatible implementations to simplify adaptation, which, while helpful, still implies a dependency on maintaining compatibility against evolving kernel interfaces. Together, these excerpts illuminate both the mechanism of shim-based porting and the inherent maintenance challenges driven by upstream churn, supporting the claim that the greatest challenge of this approach is sustaining parity with a frequently changing kernel API over time.",
      "confidence": "medium"
    },
    {
      "field": "recommended_architectural_strategy",
      "citations": [
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Introduction to virtio-networking and vhost-net - Red Hat",
          "url": "https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net",
          "excerpts": [
            "In this post we will provide a high level solution overview of the virtio-networking architecture, based on establishing an interface between the host kernel ..."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The NVMe driver is a C library that may be linked directly into an application that provides direct, zero-copy data transfer to and from NVMe SSDs.",
            "NVMe Driver Internal Memory Usage. The SPDK NVMe driver provides a zero-copy data transfer path, which means that there are no data buffers for I/O commands.",
            "The library controls NVMe devices by directly mapping the PCI BAR into the local process and performing MMIO. I/O is submitted asynchronously via queue ..."
          ]
        },
        {
          "title": "Userspace vs kernel space driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/15286772/userspace-vs-kernel-space-driver",
          "excerpts": [
            "I'd say go for user space if it is possible and the amount of effort to make things work is less than writing a kernel driver."
          ]
        }
      ],
      "reasoning": "- The VFIO excerpt clearly aligns with the recommended long-term capability to run device access in user space under a secure IOMMU-enabled environment. It states that VFIO provides a secure, IOMMU-protected pathway for exposing device access to userspace and that an IOMMU groups model governs isolation, which underpins safe user-space drivers and VFIO-based device sharing. This directly supports the vision of leveraging VFIO/IOMMU for the long-term native-driver path. - The Virtio/networking reference illustrates the standard virtualization path for device I/O that the short-term strategy relies on: a driver VM and paravirtualized devices (virtio-net, virtio-blk, etc.) using a shared-fabric model (virtqueues) to communicate between guest and host. This provides concrete grounding for a minimal driver surface in the guest and a robust, proven virtualization boundary. - The Rust-and-driver-FFI excerpt discusses how Rust drivers can interoperate with existing kernel or C APIs via FFI, which underpins both the short-term plan (writing Rust-driver components that run in the guest or as user-space companions) and the long-term strategy (bridging Rust with C-based kernel interfaces or with VFIO-backed user-space backends). It justifies a pragmatic incremental path where Rust code can start in a controlled boundary and gradually mature through safe interop patterns. - The SPDK/NVMe user-space driver excerpts provide concrete examples of a high-performance, user-space driver ecosystem (zero-copy, polled-mode NVMe, and a full block stack) that maps well to the long-term vision of bypassing kernel-only paths for performance-critical subsystems. They illustrate how a user-space driver stack can achieve substantial throughput and low latency, which supports the long-term goal of native user-space drivers for critical devices. - The DMA/IOMMU and VFIO-related entries reinforce the architectural feasibility of secure, isolated user-space device access. They provide context for how memory mapping and IOMMU-based protection enable safe user-space drivers, which the long-term plan explicitly targets as a core mechanism. - The Rust-in-kernel and Rust-for-Linux-oriented excerpts (e.g., Rust driver discussions and Rust-in-kernel practice) further support the feasibility of a Rust-centric strategy that can interoperate with existing kernel structures and with user-space driver ecosystems over FFI boundaries, which is consistent with a staged approach from guest/Virtio-based development to more autonomous native-driver implementations over time.",
      "confidence": "high"
    },
    {
      "field": "feasibility_of_direct_ffi_reuse.conclusion",
      "citations": [
        {
          "title": "Linux Kernel Licensing Rules and Precedents",
          "url": "https://docs.kernel.org/process/license-rules.html",
          "excerpts": [
            "The Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), as provided in LICENSES/preferred/GPL-2.0,\nwith an explicit syscall exception described in\nLICENSES/exceptions/Linux-syscall-note, as described in the COPYING file."
          ]
        },
        {
          "title": "FSF: Interpreting, enforcing and changing the GNU GPL, as applied ...",
          "url": "https://lwn.net/Articles/683352/",
          "excerpts": [
            "... linking creates derivative works. The FSF's own FAQ on the GPLv2 also takes this position by stating that linking to a GPL'd work requires ..."
          ]
        },
        {
          "title": "Linux kernel licensing rules",
          "url": "https://www.kernel.org/doc/html/v4.19/process/license-rules.html",
          "excerpts": [
            "The User-space API (UAPI) header files, which describe the interface of\nuser-space programs to the kernel are a special case. According to the\nnote in the kernel COPYING file, the syscall interface is a clear boundary,\nwhich does not extend the GPL requirements to any software which uses it to\ncommunicate with the kernel.",
            "The Linux kernel is under GPL-2.0, with individual files compatible with GPL-2.0 and using SPDX identifiers. UAPI files have a special syscall exception.",
            "\n\nThe Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), "
          ]
        },
        {
          "title": "Linux Syscall Note (SPDX)",
          "url": "https://spdx.org/licenses/Linux-syscall-note.html",
          "excerpts": [
            "the only valid version of the GPL as far as the kernel\nis concerned is \\_this\\_ particular version of the license (ie v2, not\nv2.2 or v3.x or whatever), unless explicitly otherwise stated. Linus Torvalds"
          ]
        },
        {
          "title": "Questioning EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/603131/",
          "excerpts": [
            "Jun 23, 2014 — EXPORT_SYMBOL_GPL() is meant to mark kernel interfaces that are deemed to be so low-level and specific to the kernel that any software that uses them must ... It was requested to be added after the fact by the maintainer and by now the discussion has had negative impact - all it would take is printing out the discussion and submitting it as evidence that the author was coerced into making the change and it is quite reasonable to assume that the flag is not a technical indicator, but merely a social statement. "
          ]
        },
        {
          "title": "5. EXPORT_SYMBOL - Linux Kernel Workbook - Read the Docs",
          "url": "http://lkw.readthedocs.io/en/latest/doc/04_exporting_symbols.html",
          "excerpts": [
            "5.2. Export Symbol¶ · EXPORT_SYMBOL() helps you provide APIs to other modules/code. · The functions which you EXPORT are available to the other modules/code."
          ]
        }
      ],
      "reasoning": "Directly reusing Linux kernel drivers via a simple FFI from another kernel confronts two intertwined constraints: the licensing regime of the Linux kernel and the legal interpretation of what constitutes a derivative work when code from the kernel is used by non-kernel software. The kernel being GPL-2.0 licensed, with explicit license handling in source files and a syscall/UAPI boundary, implies that any substantial use of kernel code could trigger GPL obligations or derivative-work considerations. The FSF and licensing discussions emphasize that linking to a GPL'd work can create derivative works, which would implicate GPLv2 terms for software that integrates or relies on such code. There are also notes clarifying that certain kernel interfaces (like the UAPI headers and syscall boundary) are designed to limit GPL expansion beyond the kernel boundary, which might influence how much of the kernel's license actually applies to user-space or non-kernel components. Taken together, these points strongly suggest that a straightforward FFI approach to reuse Linux driver code from a non-Linux kernel is not technically straightforward or legally clean, and would at minimum face significant licensing and derivative-work considerations that could negate feasibility. Given these constraints, alternatives such as virtualization or a compatibility layer align better with the need to leverage the Linux driver ecosystem without incurring the same GPL-driven integration risks.",
      "confidence": "high"
    },
    {
      "field": "storage_stack_strategy",
      "citations": [
        {
          "title": "Multi-Queue Block IO Queueing Mechanism (blk-mq)",
          "url": "https://docs.kernel.org/block/blk-mq.html",
          "excerpts": [
            "Missing: refactor 2019-2025 lwn.",
            "blk-mq has two group of queues: software staging queues and hardware dispatch\nqueues.",
            "When the request arrives at the block layer, it will try the shortest\npath possible: send it directly to the hardware queue. However, there are two\ncases that it might not do that: if there’s an IO scheduler attached at the\nlayer or if we want to try to merge requests. In both cases, requests will be\nsent to the software queue.",
            "\nThe hardware queue (represented by [`struct blk_mq_hw_ctx`](.blk_mq_hw_ctx \"blk\\_mq\\_hw\\_ctx\") ) is a `struct\nused` by device drivers to map the device submission queues (or device DMA ring\nbuffer), and are the last step of the block layer submission code before the\nlow level device driver taking ownership of the request.",
            "The scheduling\nhappens only between requests in the same queue, so it is not possible to merge\nrequests from different queues, otherwise there would be cache trashing and a\nneed to have a lock for each queue."
          ]
        },
        {
          "title": "SPDK and Its Implementations",
          "url": "https://zhou-diyu.github.io/files/spdkp-hotstorage25.pdf",
          "excerpts": [
            "SPDK also implements a user-space,\n\nasynchronous, polling-based NVMe drive",
            "SPDK employs a block device abstraction\n\ncalled Bdev to perform the same functions as the block de-\n\nvice layer in the ker",
            "SPDK+ utilizes such instructions to construct a\n\nlow-power user-level thread within the I/O process. When\n\nthe working CPU core submits an I/O request, it switches\n\nto the low-power thread, waiting for the incoming MSI-X\n\ninterr",
            "SPDK+ enhances\n\npower efficiency by up to 49.5% with only a negligible latency\n\nincrease compared to SPD"
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: LIBAIO, SPDK, IO_URING: A PRIMER",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            " **is close to SPDK. ** io\\_uring with kernel polling\n\ncan deliver performance close to SPDK (within 10% ), thanks\n\nto the elimination of system calls from the I/O path",
            "\nthis performance needs twice as many CPU cores as SPDK"
          ]
        },
        {
          "title": "SPDK NVMe and high-performance storage (SPDK news article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications. It achieves high performance by moving all of the necessary drivers into userspace and operating in a polled mode instead of relying on interrupts, which avoids kernel context switches and eliminates interrupt handling overhead.",
            "The bedrock of SPDK is a user space, polled-mode, asynchronous, lockless NVMe\ndriver that provides zero-copy, highly parallel access directly to an SSD from\na user space application.",
            "SPDK further provides a full block stack as a user\nspace library that performs many of the same operations as a block stack in\nan operating system.",
            "SPDK provides the flexibility to configure\nstorage software to run on CPU core(s) that are on the same NUMA node as the hardware\nit will access.",
            "Feb 1, 2023 — The graph below demonstrates the I/O throughput scalability of the SPDK NVMe driver with the addition of more CPU cores to perform I/O. We ..."
          ]
        },
        {
          "title": "blk-mq.rst - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/block/blk-mq.rst",
          "excerpts": [
            "blk-mq has two group of queues: software staging queues and hardware dispatch queues. When the request arrives at the block layer, it will try the shortest path ..."
          ]
        },
        {
          "title": "Multi-Queu on block layer in Linux Kernel - Hyunyoung2",
          "url": "https://hyunyoung2.github.io/2016/09/14/Multi_Queue/",
          "excerpts": [
            "The blk_mq API Implements a two-levels block layer design which use two separate sets of request queues. software staging queues, allocated per- ..."
          ]
        },
        {
          "title": "Storage Performance Development Kit driver",
          "url": "https://docs.openstack.org/cinder/2023.1/configuration/block-storage/drivers/spdk-volume-driver.html",
          "excerpts": [
            "Nov 12, 2019 — Storage Performance Development Kit (SPDK) is a user space, polled-mode, asynchronous, lockless NVMe driver. It provides zero-copy, highly ..."
          ]
        },
        {
          "title": "System Configuration User Guide",
          "url": "https://spdk.io/doc/system_configuration.html",
          "excerpts": [
            "VFIO device access is protected with sysfs file permissions and can be configured with chown/chmod. Please note that the VFIO device isolation is based ..."
          ]
        },
        {
          "title": "SPDK on LS2 PLATFORM",
          "url": "https://community.nxp.com/pwmxy87654/attachments/pwmxy87654/qoriq-grl%40tkb/53/1/SPDK_updated.pdf",
          "excerpts": [
            "It controls NVMe devices by directly mapping the PCI BAR into the local process and performing MMIO. ... SPDK integrates DPDK as default running environment."
          ]
        },
        {
          "title": "Log-Structured File Systems | Baeldung on Computer Science",
          "url": "https://www.baeldung.com/cs/log-structured-file-systems",
          "excerpts": [
            "To recover from a system crash, LFS employs two strategies: checkpoints, which define consistent states of the file system , and roll-forward, which is used to recover information saved since the last checkpoint. The checkpoint is a point where all the file system structures are complete and consistent."
          ]
        },
        {
          "title": "Log-structured filesystems (CS 4410, Summer 2017) - CS@Cornell",
          "url": "https://www.cs.cornell.edu/courses/cs4410/2017su/lectures/lec18-lfs.html",
          "excerpts": [
            "This is referred to as a checkpoint; once the superblock has been written, the filesystem now reflects everything that happened before the checkpoint. Note that ..."
          ]
        },
        {
          "title": "SquirrelFS (OSDI 2024)",
          "url": "https://github.com/utsaslab/squirrelfs",
          "excerpts": [
            "SquirrelFS is a file system for persistent memory (PM) written in Rust that uses soft updates for crash consistency. It uses Rust support for the typestate ..."
          ]
        },
        {
          "title": "SquirrelFS: Rust-native PM filesystem with crash-consistency",
          "url": "https://www.usenix.org/system/files/osdi24_slides-leblanc.pdf",
          "excerpts": [
            "Static checks rely only on existing Rust features",
            "Uses the typestate pattern to statically check ordering of durable updates",
            "Atomic metadata-related system calls (including rename)"
          ]
        }
      ],
      "reasoning": "For the storage_stack_strategy field, the strongest support comes from excerpts describing a multi-queue block I/O approach: the blk-mq documentation explains that a system can use per-CPU software queues and separate hardware dispatch queues to achieve high throughput and reduced contention, which directly aligns with a multi-queue block layer model designed for modern CPUs and fast storage devices. This is evidenced by passages detailing per-CPU queues, per-queue hardware contexts, and the goal of avoiding single, globally locked I/O queues. The cited material also notes that the block layer's queueing can merge and dispatch IOs efficiently and that different queues can use pluggable schedulers, which maps well to a design that scales with cores and storage bandwidth. In addition, several excerpts discuss SPDK and VFIO as a pathway to user-space drivers: SPDK is described as a user-space, polled-mode, zero-copy NVMe driver stack, designed to bypass kernel I/O paths for high performance, while VFIO provides an IOMMU-protected path for safe direct device access from userspace. This supports the finegrained field value's emphasis on integrating a userspace driver path to maximize throughput and minimize kernel overhead. The collection also covers the vulnerability of kernel-bypass approaches to device access and how VFIO/IOMMU-based isolation can be used to protect memory while enabling near-native performance, which aligns with the storage_stack_strategy emphasis on performance and safety when using userspace drivers. Finally, the excerpts highlight a Rust-based, crash-consistent filesystem approach inspired by SquirrelFS: the content discusses using Rust to enforce crash-consistency invariants at compile time via typestate and references SquirrelFS's design principles for crash safety, which directly matches the filesystem_and_consistency component of the field value. This supports the idea of building a Rust-native FS with strong, compile-time guarantees for durability ordering and crash safety.\n\nIn summary, the most relevant excerpts concretely support (a) a multi-queue, per-CPU blk-mq style block layer design to reduce contention and scale with cores, (b) a path for userspace, zero-copy, polled-mode devices using SPDK and VFIO with IOMMU-based protection, and (c) a Rust-based filesystem design that emphasizes crash-consistency via compile-time invariants akin to SquirrelFS. Collectively they map well to the requested storage_stack_strategy components and provide concrete, citable backing for the proposed architecture choices.",
      "confidence": "high"
    },
    {
      "field": "networking_stack_strategy.api_design",
      "citations": [
        {
          "title": "Fuchsia Netstack3 - Rust-based netstack and related networking stack strategy",
          "url": "https://fuchsia.dev/fuchsia-src/contribute/roadmap/2021/netstack3",
          "excerpts": [
            "Netstack3 - A Fuchsia owned rust based netstack",
            "The Fuchsia Netstack team will design\nand implement a Rust-based netstack that achieves functional parity with the\nexisting netstack, while leveraging the type and memory safety of rust.",
            "Risks and mitigations"
          ]
        }
      ],
      "reasoning": "The most relevant content points to a Rust-based netstack design within a roadmap context for a modern network stack. This aligns with the idea of using Rust to provide a safe, high-concurrency networking interface. The excerpts describe building a Rust-based netstack and designing toward functional parity with the existing netstack, which supports the notion of a modern native API surface built with Rust's safety and async features. While not every sentence confirms a dual API with a POSIX-compatible sockets API, the emphasis on Rust-based design and modern networking strategies provides clear support for the core philosophy of a Rust-centric, high-concurrency API design. The remaining excerpts discuss broader networking features (TLS offload, kernel networking capabilities, and general netstack strategy) that provide context but do not directly validate the dual API approach or the Rust-centric API surface described in the field value. They are supportive but not central evidence for the exact API design combination requested.\n",
      "confidence": "medium"
    },
    {
      "field": "networking_stack_strategy.performance_architecture",
      "citations": [
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "Kernel TLS, NIC Offload and Socket Sharding in Modern Linux/SDN Context",
          "url": "https://dev.to/ozkanpakdil/kernel-tls-nic-offload-and-socket-sharding-whats-new-and-who-uses-it-4e1f",
          "excerpts": [
            "Summary\n-------\n\n* **Socket sharding** (Linux 4.5+) spreads load across processes → higher scalability. * **kTLS** (Linux 4.13+, FreeBSD 13+) avoids copies → **20–40% CPU savings**, ~**2× throughput** for NGINX. * **NIC TLS offload** (Netflix, NVIDIA, Chelsio):\n  + **2× throughput**, huge CPU savings,\n  + **400–800 Gb/s per server** possible in production.\n* **Windows**: No kernel TLS or NIC offload support today. These optimizations matter most at hyperscaler scale, but are increasingly available to anyone running **NGINX**, **HAProxy**, or workloads on modern NICs. ---",
            "NIC TLS Offload\n---------------\n\nSome NICs (Mellanox/NVIDIA ConnectX, Chelsio T6, Intel QAT) can offload encryption entirely. The kernel sets up the session keys, while the NIC performs the crypto inline. * **Netflix (FreeBSD + Mellanox CX6-DX)**:\n\n  + In production, serving **400–800 Gb/s per server**. + Reported **~50% reduction in memory bandwidth use** vs CPU crypto. * **NVIDIA/Mellanox (netdev conf)**:\n\n  + Inline TLS offload gave **~2× throughput** vs software kTLS. + Example: **8.8 Gb/s vs 4.4 Gb/s** in test environments. * **Chelsio T6**:\n\n  + Vendor benchmarks show **~44.8 Gb/s TLS throughput** with very low CPU use. * **Academic 2024 SmartNIC study**:\n\n  + Inline mode: **~9.3 Gb/s** throughput under parallel load. + Coprocessor mode: less throughput but **lowest CPU cost**. --",
            "Socket Sharding\n---------------\n\nTraditionally, only one process could `accept()` connections from a TCP socket. With Linux **4.5 (2016)** and `SO_REUSEPORT` enhancements, multiple processes can share a listening socket. This “socket sharding” lets the kernel distribute connections efficiently across worker processes. * Adopted in **NGINX** and **HAProxy**. * **FreeBSD** has similar support. * **Windows** does not provide this functionality. ✅ Result: Better scalability on multi-core systems with no user-space load balancer needed"
          ]
        },
        {
          "title": "5. Security Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/rte_security.html",
          "excerpts": [
            "The library defines generic APIs to create and free security sessions which can support full protocol offload as well as inline crypto operation with NIC or ..."
          ]
        }
      ],
      "reasoning": "The target field value describes wiring the OS networking stack to work with high-performance, kernel-bypass technologies (such as AF_XDP) to achieve low latency through zero-copy data transfers, while also ensuring the kernel supports and coordinates TLS offload capabilities (kTLS and hardware offload) to sustain high throughput with minimal CPU overhead. The AF_XDP documentation directly discusses an architecture optimized for high-performance packet processing and is the canonical example of kernel-bypass networking the field value advocates leveraging. The summaries of kernel TLS offload and NIC offload outline concrete benefits (e.g., increased throughput, CPU savings) and show that TLS processing can be offloaded to NICs or offloaded in the kernel, which aligns with the field value's emphasis on TLS/offload integration as part of the architecture. Additional security/offload content describes generic APIs to create and manage security sessions that enable inline crypto and offload, reinforcing the necessity for coordinated TLS offload support in the OS design. A performance-focused excerpt on socket sharding and scalability provides supportive context about the demand for multi-core throughput and efficient distribution, which complements the architectural goal of high-performance networking but is a secondary consideration relative to the explicit TLS offload and kernel-bypass capabilities. Together, these excerpts collectively substantiate the core claim that high-performance, low-latency networking in the OS hinges on kernel-bypass pathways (AF_XDP) and robust TLS offload, both in-kernel and via NICs, to achieve zero-copy transfers and minimal CPU overhead.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_reliability_pipeline.ci_cd_and_build_infrastructure",
      "citations": [
        {
          "title": "Reproducible builds - Linux Kernel documentation",
          "url": "https://docs.kernel.org/kbuild/reproducible-builds.html",
          "excerpts": [
            "It is generally desirable that building the same source code with\nthe same set of tools is reproducible, i.e. the output is always\nexactly the same.",
            "The kernel embeds timestamps in three places:",
            "This must\nbe overridden using the [KBUILD\\_BUILD\\_TIMESTAMP](kbuild.html) variabl",
            "If you are building from a git commit, you could use its commit date. The kernel does *not* use the `__DATE__` and `__TIME__` macros,\nand enables warnings if they are use",
            "If you incorporate external\ncode that does use these, you must override the timestamp they\ncorrespond to by setting the [SOURCE\\_DATE\\_EPOCH](https://reproducible-builds.org/docs/source-date-epoch/) environment\nvariabl",
            "You should ensure the source tree is\npristine by running `make mrproper` or `git clean -d -f -x` before\nbuilding a source package.",
            "Depending on the compiler used, the `__FILE__` macro may also expand\nto an absolute filename in an out-of-tree build. Kbuild automatically\nuses the `-fmacro-prefix-map` option to prevent this, if it is\nsupported.",
            "One approach to this is to divide up the build process so that the\nunreproducible parts can be treated as sources:",
            "1. Generate a persistent signing key. Add the certificate for the key\n   to the kernel sour",
            "2. Set the `CONFIG_SYSTEM_TRUSTED_KEYS` symbol to include the\n   signing key’s certificate, set `CONFIG_MODULE_SIG_KEY` to an\n   empty string, and disable `CONFIG_MODULE_SIG_ALL`. Build the kernel and mo",
            "3. Create detached signatures for the modules, and publish them as\n   sourc",
            "Generated files in source packages",
            "The build processes for some programs under the `tools/`\nsubdirectory do not completely support out-of-tree builds. This may\ncause a later source package build using e.g. `make rpm-pkg` to\ninclude generated files."
          ]
        },
        {
          "title": "KernelCI Documentation",
          "url": "https://docs.kernelci.org/",
          "excerpts": [
            "Welcome to the KernelCI documentation website! *\n\nThe KernelCI project’s mission is to ensure the quality, stability and long-term maintenance of the Linux kernel. To achieve that, we are building an ecosystem to foster collaboration around Linux kernel testing and validation, facilitating the process of executing tests for a given subsystem or the whole kernel and following up on the results, **helping you catch issues early and maintain kernel qua",
            "documentation",
            "The upstream Linux kernel test project\nKernelCI[KernelCI](/)"
          ]
        },
        {
          "title": "BlueOS Kernel – Written in Rust, compatible with POSIX",
          "url": "https://news.ycombinator.com/item?id=44669026",
          "excerpts": [
            "It would be momentous to simply get Linux drivers to comply with a kernel-agnostic ABI. ... operating system written in the Rust language from the ..."
          ]
        },
        {
          "title": "Fuzzing with cargo-fuzz - Rust Fuzz Book",
          "url": "https://rust-fuzz.github.io/book/cargo-fuzz.html",
          "excerpts": [
            "cargo-fuzz is the recommended tool for fuzz testing Rust code. cargo-fuzz is itself not a fuzzer, but a tool to invoke a fuzzer."
          ]
        },
        {
          "title": "Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation",
          "url": "https://arxiv.org/html/2505.02464v1",
          "excerpts": [
            "However, Rust builds are not reproducible in all use cases and depend on multiple factors, for example, the absolute build path.",
            "To fuzz the target programs, FourFuzz utilizes afl.rs in version 0.15.3 which is based on AFL + + version 4.10c.",
            " \nWe implement partial instrumentation to focus the fuzzing efforts on unsafe Rust code to significantly increase the efficiency and probability of triggering and detecting vulnerabilities in Rust programs.",
            "This is challenging as Rust uses a complex compilation toolchain:\nThis includes multiple intermediate representations and several compiler stages facilitating Rust-specific code analysis.",
            " as MirChecker  [ 42 ] are static analysis frameworks that utilize intermediate code representations to detect different bug type"
          ]
        }
      ],
      "reasoning": "The target field describes a foundational requirement for reproducible, or deterministic, builds, achieved by controlling build timestamps, user/host information, and file paths via environment variables and compiler flags, and by using tools like Nix or Guix. This directly maps to the excerpts that explain the principles and mechanics of reproducible builds, such as ensuring identical outputs across builds and discussing how timestamps, build metadata, and source tree state affect reproducibility. It also emphasizes integrating robust CI testing with emulation (QEMU/KVM) and hardware-in-the-loop testing (LAVA) for validating device models and hypervisor functionality, which aligns with establishing a comprehensive CI/CD and build infrastructure. KernelCI-related content reinforces the importance of an ecosystem for testing kernel components and validating builds across configurations, which is highly relevant to the described CI needs. Additional excerpts discuss practical build hygiene (pristine sources, out-of-tree build considerations, and how to handle compiler-generated or generated files) that support designing a deterministic and maintainable pipeline. While some excerpts focus on related topics (e.g., fuzzing, Rust-specific tooling, or driver-ABI considerations), they provide supplementary context about tooling and reliability practices that can integrate into the broader CI/CD strategy described in the field value.",
      "confidence": "high"
    },
    {
      "field": "virtualization_based_reuse_deep_dive",
      "citations": [
        {
          "title": "Driver Domain - Xen Project Wiki",
          "url": "https://wiki.xenproject.org/wiki/Driver_Domain",
          "excerpts": [
            "A driver domain is unprivileged Xen domain that has been given responsibility for a particular piece of hardware. It runs a minimal kernel with only that hardware driver and the backend driver for that device class. Thus, if the hardware driver fails, the other domains (including Dom0) will survive and, when the driver domain is restarted, will be able to use the hardware again. As disk driver domains are not currently supported, this page will describe the setup for network driver domains.",
            "Benefits",
            "Performance",
            "This will eliminate dom0 as a bottleneck. All device backend in dom0 will result dom0 to have bad response latency.",
            "Enhanced reliability",
            "Enhanced security",
            "Requirements",
            "Having a system with a modern IOMMU (either AMD or VT-d version 2) is highly recommended. Without IOMMU support, there's nothing to stop the driver domain from using the network card's DMA engine to read and write any system memory.",
            "setup",
            "Hardware drivers are the most failure-prone part of an operating system. It would be good for safety if you could isolate a driver from the rest of the system so that, when it failed, it could just be restarted without affecting the rest of the machine.",
            "Putting this in a separate, unprivileged domain limits the value of attacking the network stack: even if they succeed, they have no more access than a normal unprivileged VM.",
            "Install the xen-related hotplug scripts",
            "Use PCI passthrough to give the VM access to the hardware NIC",
            "Set up network topology in the VM",
            "Configure guests"
          ]
        },
        {
          "title": "Bridging the Gap between Software and Hardware ...",
          "url": "https://www.usenix.org/legacy/event/usenix08/tech/full_papers/santos/santos_html/index.html",
          "excerpts": [
            "In this paper we show how to significantly narrow the performance gap by improving the performance of the driver domain model."
          ]
        },
        {
          "title": "Our Prototype on Driver Reuse via Virtual Machines (IöTEC KIT)",
          "url": "https://os.itec.kit.edu/844.php",
          "excerpts": [
            "Our prototype's network performance is within 3–8% of a native Linux system.",
            "We evaluated reused drivers for the network,\ndisk and PCI subsystems.",
            "The figure distinguishes between lines specific to the\ntranslation modules added to the server, lines specific to\nthe virtual device drivers added to the client, and addi-\ntional lines that are common (and are counted once",
            "The achieved code reuse ratio is 99.9% for NIC drivers\nin Linux; the translation modules add 0.1% to their code\nbase.",
            "When we additionally include all code required for\nthe virtualization—the L4 microkernel, the VMM, and\nthe paravirtualization modifications—we still achieve a\nreuse ratio of 91% just for Linux’s NIC driver base.",
            "The engineering effort enabled us to successfully reuse\nLinux device drivers with all of our tested lab hardware.",
            "The following drivers were tested: Intel gigabit, Intel\n100 Mbit, Tulip (with a variety of Tulip compatible hard-\nware), Broadcom gigabit, pcnet32, ATA and SATA IDE,\nand a variety of uniprocessor and SMP chipsets for Intel\nPentium 3/4 and AMD Opteron processor",
            " Paravirtualization is an enhanced\nmachine API that relocates some functionality from the\nguest OS to the VMM and hypervisor",
            "For example,\nit permits our DD/OS instances to directly translate their\nvirtual addresses into bus addresses for DMA. It also pro-\nvides performance benefit"
          ]
        },
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ... Virtual I/O Device (VIRTIO) Version 1.3",
            "Virtual I/O Device (VIRTIO) Version 1.3",
            "Any PCI device with PCI Vendor ID 0x1AF4, and PCI Device ID 0x1000 through\n0x107F inclusive is a virtio device.",
            "The PCI Device ID is calculated by adding\n0x1040 to the Virtio Device ID, as indicated in section [5]() .",
            "A Virtio device using Virtio Over PCI Bus MUST expose to guest an interface that\nmeets the specification requirements of the appropriate PCI specification: [[PCI]]() and [[PCIe]]() respectively.",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "5.7 GPU Device. virtio-gpu is a virtio based graphics adapter. It can operate in 2D mode and in 3D mode. 3D mode will offload rendering ops to the host gpu ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "The device MUST allow reading of any device-specific configuration field\nbefore FEATURES\\_OK is set by the drive"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "The PCI Device ID is calculated by adding\n0x1040 to the Virtio Device ID, as indicated in section [5]() .",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "SOF VirtIO design",
          "url": "https://thesofproject.github.io/latest/developer_guides/virtualization/virtualization.html",
          "excerpts": [
            "/) VirtIO design implements an interface to transport IPC (\nInter-Process Communication) commands and audio data between host and\nguests.",
            "irtio) is a platform-agnostic virtualization standard, managed by [OASIS](https://www.oasis-open.org/) . It\ndefines a design and a protocol for implementing device IO in VMs.",
            "This\nvirtualization solution creates its own set of IPC operations to redirect\nIPC messages from the guest over virtual queues to the host.",
            "Typically an IPC message exchange between the driver and the DSP takes place\nover a mailbox and a doorbell. A mailbox is a shared memory region between\nthe DSP and host.",
            "A doorbell is\nasynchronous signaling hardware between DSP and host. This mailbox /\ndoorbell interface can be hardware-specific; thus, every hardware variant\nprovides its own set of mailbox / doorbell handling operations."
          ]
        }
      ],
      "reasoning": "The core fine-grained field centers on how to reuse drivers via virtualization-based patterns and the technologies that enable it. Several excerpts directly describe concrete architectural patterns and mechanisms that enable this reuse and isolation. Driver Domain concepts from the Xen Project Wiki illustrate a pattern where drivers run in a separate domain with isolated memory and privileges, thereby achieving strong fault containment and reduced risk to the host OS. This directly supports the pattern of a dedicated driver-domain model (Driver VM/Driver Domain) as a primary architectural approach to reuse and isolation. Jailhouse discussions and related entries describe static partitioning and the idea of running drivers in a minimal, separated environment (driver domains, or partitioned domains) to improve reliability and security, which aligns with the field's emphasis on isolation boundaries and low-overhead partitioning. The excerpts detailing PVHVM, vhost, VFIO, and SR-IOV describe concrete technologies and protocols that enable efficient, near-native hardware access from guests or user-space drivers while preserving isolation. VFIO, VFIO-mediated devices, and IOMMU groups are recurrent topics explaining how devices can be passed through or shared safely with VMs or user-space drivers, supporting the field's emphasis on security and hardware access patterns. The PVHVM and several notes on para-virtualized driver models (Virtio, vhost-net, vhost-user) describe how virtualization can map to near-native performance, aligning with the field's performance/perimeter considerations. Excerpts also discuss the Rust/vmm ecosystem as a modern, software-level approach to building virtualization components (Rust-based backends and Rust-driven VMM stacks), which complements the architectural reuse pattern by providing safer, high-assurance components for guest-side drivers and virtualization frameworks. The excerpts collectively map to the field value's clusters: (i) Driver Domain/partitioned driver isolation patterns; (ii) Paravirtualization and Virtio-based data paths (vhost, vhost-user, virtio devices) for efficient I/O across VM/device boundaries; (iii) VFIO/IOMMU/SR-IOV for secure PCI device passthrough and isolation; and (iv) Rust-vmm ecosystems and driver-reuse studies that illustrate modern implementation patterns and safety considerations. Direct quotes across these excerpts (e.g., description of Driver Domain architecture, Jailhouse's partitioning approach, PVHVM concepts, Virtio/Vhost/IOMMU/VFIO flow, and Rust-based VMM components) demonstrate how each technology or pattern contributes to the overall architecture choices for the proposed open-source OS with Rust drivers and virtualization-based driver reuse. The strongest support comes from the driver-domain and Jailhouse sections (clearly articulating partitioned driver domains and isolation benefits) and the VFIO/IOMMU sections (detailing secure device passthrough and memory isolation), with additional support from PVHVM discussions and Virtio/Vhost discussions for performance considerations, and from Rust-vmm discussions for architecture in Rust-based virtualization tooling. Overall, the evidence supports a high confidence level that virtualization-based reuse patterns (Driver Domain, Jailhouse, PVHVM, Virtio/Vhost/VFIO/IOMMU) are viable and well-supported, though exact performance figures depend on workload and platform specifics. ",
      "confidence": "high"
    },
    {
      "field": "gpu_and_display_stack_strategy",
      "citations": [
        {
          "title": "Direct Rendering Manager",
          "url": "https://en.wikipedia.org/wiki/Direct_Rendering_Manager",
          "excerpts": [
            "The Direct Rendering Manager (DRM) is a subsystem of the Linux kernel responsible for interfacing with GPUs of modern video cards."
          ]
        },
        {
          "title": "A Look at the Latest Linux KMS Color API Developments on ...",
          "url": "https://melissawen.github.io/blog/2025/05/19/drm-info-with-kms-color-api",
          "excerpts": [
            "May 19, 2025 — With this API in place, compositors will be able to provide better HDR support and advanced color management for Linux users."
          ]
        },
        {
          "title": "AMD Posts 11th Iteration Of Color Pipeline API ...",
          "url": "https://www.phoronix.com/news/AMD-11th-Color-Pipeline-API",
          "excerpts": [
            "Aug 15, 2025 — One of the Linux Direct Rendering Manager (DRM) patch series that has long been in the works is on a standardized Color Pipeline API."
          ]
        },
        {
          "title": "Mode Setting Helper Functions",
          "url": "https://docs.kernel.org/gpu/drm-kms-helpers.html",
          "excerpts": [
            "The DRM mode setting helper functions are common code for drivers to use if they wish. Drivers are not forced to use this code in their implementations."
          ]
        },
        {
          "title": "Apparently if your process name starts with the letter 'X' ...",
          "url": "https://www.reddit.com/r/linux/comments/yo8iu5/apparently_if_your_process_name_starts_with_the/",
          "excerpts": [
            "Apparently if your process name starts with the letter 'X', Linux's DRM subsystem will change its API behavior... Kernel. Apparently, if your ..."
          ]
        },
        {
          "title": "Why is it \"good\" to have all drivers in the Kernel?",
          "url": "https://www.reddit.com/r/linuxquestions/comments/ke76qa/why_is_it_good_to_have_all_drivers_in_the_kernel/",
          "excerpts": [
            "Drivers are in the kernel source tree. At runtime they're modules that are loaded on demand. The kernel has everything AVAILABLE, but only uses what is needed."
          ]
        },
        {
          "title": "Linux Kernel Technical Debt: The Hidden Burden in ...",
          "url": "https://riscstar.com/blog/linux-kernel-technical-debt/",
          "excerpts": [
            "Jul 15, 2025 — Linux Kernel Technical Debt: The Hidden Burden in Embedded Development ... Out-of-tree code paths risk introducing unpredictable vulnerabilities."
          ]
        },
        {
          "title": "Linux kernel documentation - Introduction to DRM/KMS",
          "url": "https://www.kernel.org/doc/html//v6.4/gpu/introduction.html",
          "excerpts": [
            "A note on versions: this guide covers features found in the DRM tree,\nincluding the TTM memory manager, output configuration and mode setting,\nand the new vblank internals, in addition to all the regular features\nfound in current kernels."
          ]
        },
        {
          "title": "Kernel DRM/KMS Documentation",
          "url": "https://docs.kernel.org/gpu/drm-kms.html",
          "excerpts": [
            ")\n\nThe basic object structure KMS presents to userspace is fairly simple. Framebuffers (represented by [`struct drm_framebuffer`](.drm_framebuffer \"drm\\_framebuffer\") ,\nsee [Frame Buffer Abstraction]() ) feed into planes. Planes are represented by [`struct drm_plane`](.drm_plane \"drm\\_plane\") , see [Plane Abstraction]() for more\ndetails. One or more (or even no) planes feed their pixel data into a CRTC\n(represented by [`struct drm_crtc`](.drm_crtc \"drm\\_crtc\") , see [CRTC Abstraction]() )\nfor blending. The precise blending step is explained in more detail in [Plane\nComposition Properties]() and related chapters.",
            " Planes are represented by [`struct drm_plane`](.drm_plane \"drm\\_plane\") , see [Plane Abstraction]() for more\ndetails.",
            "One or more (or even no) planes feed their pixel data into a CRTC\n(represented by [`struct drm_crtc`](.drm_crtc \"drm\\_crtc\") , see [CRTC Abstraction]() )\nfor blending",
            " For the output routing the first step is encoders (represented by [`struct drm_encoder`](.drm_encoder \"drm\\_encoder\") , see [Encoder Abstraction]() ).",
            " The final, and real, endpoint in the display chain is the connector (represented\nby [`struct drm_connector`](.drm_connector \"drm\\_connector\") , see [Connector\nAbstraction]() )."
          ]
        },
        {
          "title": "VirGL — The Mesa 3D Graphics Library latest documentation",
          "url": "https://docs.mesa3d.org/drivers/virgl.html",
          "excerpts": [
            "VirGL is a virtual 3D GPU for use inside QEMU virtual machines, that allows the guest operating system to use the capabilities of the host GPU to accelerate 3D ..."
          ]
        },
        {
          "title": "Venus on QEMU: Enabling the new virtual Vulkan driver",
          "url": "https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/",
          "excerpts": [
            "Nov 26, 2021 — This blog post describes how to enable 3D acceleration of Vulkan applications in QEMU through the Venus experimental Vulkan driver for VirtIO-GPU with a local ..."
          ]
        },
        {
          "title": "solarkraft/awesome-wlroots: A curated list of tools and ...",
          "url": "https://github.com/solarkraft/awesome-wlroots",
          "excerpts": [
            "wlroots is a compositor library and defines a large and diverse ecosystem around wlr-protocols. This list is dedicated to cool stuff/useful tools for wlroots ..."
          ]
        },
        {
          "title": "DRM Internals — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.18/gpu/drm-internals.html",
          "excerpts": [
            "This chapter documents DRM internals relevant to driver authors and developers working to add support for the latest features to existing drivers."
          ]
        },
        {
          "title": "DRM Internals",
          "url": "https://docs.kernel.org/gpu/drm-internals.html",
          "excerpts": [
            "This chapter documents DRM internals relevant to driver authors and developers working to add support for the latest features to existing drivers."
          ]
        },
        {
          "title": "embassy-rs/embassy: Modern embedded framework, using ...",
          "url": "https://github.com/embassy-rs/embassy",
          "excerpts": [
            "Networking - The embassy-net network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically ..."
          ]
        },
        {
          "title": "Stack in embassy_net - Rust - embassy-executor",
          "url": "https://docs.embassy.dev/embassy-net/git/default/struct.Stack.html",
          "excerpts": [
            "Network stack handle. Use this to create sockets. It's Copy, so you can pass it by value instead of by reference."
          ]
        },
        {
          "title": "smoltcp/wire/ mod.rs",
          "url": "https://docs.rs/smoltcp/latest/src/smoltcp/wire/mod.rs.html",
          "excerpts": [
            "1/*! Low-level packet access and construction. 2 3The `wire` module deals with the packet *representation*. It provides two levels 4of functionality."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "5. Security Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/rte_security.html",
          "excerpts": [
            "The library defines generic APIs to create and free security sessions which can support full protocol offload as well as inline crypto operation with NIC or ..."
          ]
        },
        {
          "title": "Introduction — The Linux Kernel documentation - DRI",
          "url": "https://dri.freedesktop.org/docs/drm/gpu/introduction.html",
          "excerpts": [
            "This guide covers features found in the DRM tree, including the TTM memory manager, output configuration and mode setting, and the new vblank internals."
          ]
        },
        {
          "title": "Introduction — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.5/gpu/introduction.html",
          "excerpts": [
            "This guide covers features found in the DRM tree, including the TTM memory manager, output configuration and mode setting, and the new vblank internals."
          ]
        },
        {
          "title": "Building the virtualization stack of the future with rust-vmm",
          "url": "https://opensource.com/article/19/3/rust-virtual-machine",
          "excerpts": [
            "The virtio-bindings are the bindings for Virtio devices generated using the Virtio Linux headers.",
            "\nEach box on the right side of the diagram is a GitHub repository corresponding to one package, which in Rust is called a crate"
          ]
        },
        {
          "title": "Introduction to virtio-networking and vhost-net - Red Hat",
          "url": "https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net",
          "excerpts": [
            "In this post we will provide a high level solution overview of the virtio-networking architecture, based on establishing an interface between the host kernel ..."
          ]
        },
        {
          "title": "How AWS Firecracker Works - a deep dive",
          "url": "https://unixism.net/2019/10/how-aws-firecracker-works-a-deep-dive/",
          "excerpts": [
            "\nWhen you have operating systems that can’t be modified, but come with drivers for some hardware devices, it makes sense to emulate them. That way, it becomes possible to run these operating systems unmodified.",
            "The concept behind VirtIO is very simple. It specifies a way for the guest and host to communicate efficiently. It defines various device types like: network, block, console, entropy, memory balloon and SCSI host.",
            "VirtIO is relatively simple to program compared to programming real hardware and it is also designed to be very high performance.",
            "The performance is also the result of not having to emulate whole, real hardware devices.",
            "Firecracker takes advantage of this. VirtIO was developed initially by Rusty Russell for [LGuest](https://en.wikipedia.org/wiki/Lguest), which made its way into the kernel in 2007, but was removed in 2017.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "* VirtIO Block (emulated by Firecracker in [devices/src/virtio/block.rs](https://github.com/firecracker-microvm/firecracker/blob/master/devices/src/virtio/block.rs))",
            "* VirtIO Net (emulated by Firecracker in [devices/src/virtio/net.rs](https://github.com/firecracker-microvm/firecracker/blob/master/devices/src/virtio/net.rs))",
            "VirtIO needs a bit more explanation. Real hardware devices have all kinds of quirks and are fairly complicated to program.",
            "VirtIO needs a bit more explanation. Real hardware devices have all kinds of quirks and are fairly complicated to program.",
            "The concept behind VirtIO is very simple. It specifies a way for the guest and host to communicate efficiently."
          ]
        },
        {
          "title": "Virtio-net Feature Bits",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2410/Virtio-net+Feature+Bits",
          "excerpts": [
            "Nov 12, 2024 — Per virtio spec, virtio the device negotiates with the virtio driver on the supported features when the driver probes the device."
          ]
        },
        {
          "title": "VFIO virtual device - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/virt/kvm/devices/vfio.html",
          "excerpts": [
            "The created device tracks VFIO files (group or device) in use by the VM and features of those groups/devices important to the correctness and acceleration of ..."
          ]
        },
        {
          "title": "rust-vmm/vhost",
          "url": "https://github.com/rust-vmm/vhost",
          "excerpts": [
            "The vhost workspace hosts libraries related to the vhost and vhost-user protocols. It currently consists of the following crates."
          ]
        },
        {
          "title": "Minijail - Book of crosvm",
          "url": "https://crosvm.dev/book/appendix/minijail.html",
          "excerpts": [
            "On Linux hosts, crosvm uses minijail to sandbox the child devices. The minijail C library is utilized via a Rust wrapper so as not to repeat the intricate ..."
          ]
        },
        {
          "title": "Consider changing the vhost-user-backend API · Issue #279",
          "url": "https://github.com/rust-vmm/vhost/issues/279",
          "excerpts": [
            "Nov 20, 2024 — Usually in rust-vmm a repository can be a workspace and contain multiple crates (e.g. vhost, vhost-device), but this is just a development ..."
          ]
        },
        {
          "title": "syzkaller/docs/contributing.md at master",
          "url": "https://github.com/google/syzkaller/blob/master/docs/contributing.md",
          "excerpts": [
            "To contribute code or syscall descriptions, at the very least you need to be able to build and run syzkaller, see the instructions here."
          ]
        },
        {
          "title": "syzkaller is an unsupervised coverage-guided kernel fuzzer",
          "url": "https://github.com/google/syzkaller",
          "excerpts": [
            "Documentation. Initially, syzkaller was developed with Linux kernel fuzzing in mind, but now it's being extended to support other OS kernels as ..."
          ]
        },
        {
          "title": "Next VirtIO device for Project Stratos? - Stratos-dev - op-lists.linaro.org",
          "url": "https://op-lists.linaro.org/archives/list/stratos-dev@op-lists.linaro.org/thread/VPLKMBWYB4PG2X5MTACUIW5SBGBP5HVF/",
          "excerpts": [
            "Neither of these have up-streamed the specification to OASIS but there is an implementation of the mac80211_hwsim in the Linux kernel. I found evidence of a ..."
          ]
        },
        {
          "title": "Linux Kernel patch submission checklist - No comment",
          "url": "https://www.infradead.org/~mchehab/kernel_docs/process/submit-checklist.html",
          "excerpts": [
            "Linux Kernel patch submission checklist¶. Here are some basic things that developers should do if they want to see their kernel patch submissions accepted ..."
          ]
        },
        {
          "title": "Check if your code follows kernel code style — kw documentation",
          "url": "https://kworkflow.org/tutorials/codestyle.html",
          "excerpts": [
            "The Linux kernel has a very clear and well-defined code style automated by a tool named checkpatch.pl . You can use this script to check a file or a patch, and ..."
          ]
        },
        {
          "title": "Vhost Processing - SPDK Documentation",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "The purpose of virtio and [virtio] specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms. Virtio devices use virtqueues to transport data efficiently. Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O.",
            "\nThis document is intended to provide an overview of how Vhost works behind the scenes. Code snippets used in this document might have been simplified for the sake of readability and should not be used as an API or implementation reference. Reading from the [Virtio specification",
            "interop/vhost-user.html) describes the protocol as follows:\n\n> ‍[Vhost-user protocol] is aiming to complement the ioctl interface used to control the vhost implementation in the Linux kernel. It implements the control plane needed to establish virtqueue sharing with a user space process on the same host. It uses communication over a Unix domain socket to share file descriptors in the ancillary data of the message.",
            "The initial vhost implementation is a part of the Linux kernel and uses ioctl interface to communicate with userspace applications. What makes it possible for SPDK to expose a vhost device is Vhost-user protocol.",
            "Vhost is a protocol for devices accessible via inter-process communication. It uses the same virtqueue layout as Virtio to allow Vhost devices to be mapped directly to Virtio devices.",
            "is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "QEMU-based VM communicates with SPDK Vhost-SCSI device. QEMU/SPDK vhost data flow",
            "The front-end and back-end can be either a client (i.e. connecting) or server (listening) in the socket communication. SPDK vhost is a Vhost-user back-end server."
          ]
        },
        {
          "title": "vhost-user back ends",
          "url": "https://www.qemu.org/docs/master/system/devices/vhost-user.html",
          "excerpts": [
            "vhost-user back ends are way to service the request of VirtIO devices outside of QEMU itself. To do this there are a number of things required.",
            "These are simple stub devices that ensure the VirtIO device is visible\nto the guest.",
            "The code is mostly boilerplate although each device has\na `chardev` option which specifies the ID of the `--chardev` device that connects via a socket to the vhost-user _daemon_ .",
            "The vhost-user-device is a generic development device intended for\nexpert use while developing new backends.",
            "to specify\nall the required parameters including:\n\n> *\n>   Device `virtio-id`\n> *\n>   The `num_vqs` it needs and their `vq_size`\n> *\n>   The `config_size`",
            "This is a separate process that is connected to by QEMU via a socket\nfollowing the [Vhost-user Protocol"
          ]
        },
        {
          "title": "CodeLinaro / le / platform / external / rust-vmm / vhost-device - GitLab",
          "url": "https://git.codelinaro.org/clo/le/platform/external/rust-vmm/vhost-device",
          "excerpts": [
            "Typically they have been tested with QEMU although the rust-vmm project does provide a vhost-user frontend crate for rust based VMMs. While ..."
          ]
        },
        {
          "title": "Timekeeping Virtualization for X86-Based Architectures",
          "url": "https://docs.kernel.org/virt/kvm/x86/timekeeping.html",
          "excerpts": [
            "In general, the HPET is recommended as a high precision (compared to PIT /RTC) time source which is independent of local variation (as there is only one HPET in ..."
          ]
        },
        {
          "title": "cloud-hypervisor/docs/io_throttling.md at main",
          "url": "https://github.com/cloud-hypervisor/cloud-hypervisor/blob/main/docs/io_throttling.md",
          "excerpts": [
            "Cloud Hypervisor now supports I/O throttling on virtio-block and virtio-net devices. This support is based on the rate-limiter module from Firecracker."
          ]
        },
        {
          "title": "Cloud Hypervisor documentation - GitHub Pages",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/README.html",
          "excerpts": [
            "A large part of the Cloud Hypervisor code is based on either the Firecracker or the crosvm projects implementations. Both of these are VMMs written in Rust with ..."
          ]
        },
        {
          "title": "Firecracker: start a VM in less than a second - Julia Evans",
          "url": "https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/",
          "excerpts": [
            "I've been using Firecracker to start relatively large VMs – Ubuntu VMs running systemd as an init system – and it takes maybe 2-3 seconds for them to boot."
          ]
        },
        {
          "title": "Getting Started with Firecracker | Harry Hodge",
          "url": "https://harryhodge.co.uk/posts/2024/01/getting-started-with-firecracker/",
          "excerpts": [
            "We start by manually setting up the TAP network interface for the firecracker VM. This could get fiddly if we want to run multiple VMs. 1# Setup ..."
          ]
        },
        {
          "title": "Paravirtualized KVM features — QEMU documentation",
          "url": "https://www.qemu.org/docs/master/system/i386/kvm-pv.html",
          "excerpts": [
            "Paravirtualized KVM features are represented as CPU flags. The following features are enabled by default for any CPU model when KVM acceleration is enabled."
          ]
        },
        {
          "title": "Chapter 8. KVM Guest Timing Management - Red Hat Documentation",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/chap-kvm_guest_timing_management",
          "excerpts": [
            "KVM avoids these issues by providing guest virtual machines with a paravirtualized clock (kvm-clock). However, it is still important to test timing before ..."
          ]
        },
        {
          "title": "Chapter 14. KVM Guest Timing Management | 6",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/virtualization_host_configuration_and_guest_installation_guide/chap-virtualization_host_configuration_and_guest_installation_guide-kvm_guest_timing_management",
          "excerpts": [
            "KVM avoids these issues by providing guest virtual machines with a paravirtualized clock (kvm-clock). However, it is still important to test timing before ..."
          ]
        },
        {
          "title": "A detailed view of the vhost user protocol and its ...",
          "url": "https://access.redhat.com/solutions/3394851",
          "excerpts": [
            "Jun 14, 2024 — The vhost user protocol consists of a control path and a data path. All control information is exchanged via a Unix socket. This includes ..."
          ]
        },
        {
          "title": "vhost-device-vsock - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/vhost-device-vsock",
          "excerpts": [
            "The crate introduces a vhost-device-vsock device that enables communication between an application running in the guest ie inside a VM and an application ..."
          ]
        },
        {
          "title": "Firecracker GitHub Repository - Getting Started / Design & Features",
          "url": "https://github.com/firecracker-microvm/firecracker",
          "excerpts": [
            "Firecracker\nhas a minimalist design. It excludes unnecessary devices and guest-facing\nfunctionality to reduce the memory footprint and attack surface area of each\nmicroVM",
            " Firecracker is open sourced\nunder [Apache version 2.0",
            "Firecracker consists of a single micro Virtual Machine Manager process that\nexposes an API endpoint to the host once started.",
            "The main component of Firecracker is a virtual machine monitor (VMM) that uses\nthe Linux Kernel Virtual Machine (KVM) to create and run microVMs."
          ]
        },
        {
          "title": "How to write an actually working Linux kernel module in Rust?",
          "url": "https://users.rust-lang.org/t/how-to-write-an-actually-working-linux-kernel-module-in-rust/97293",
          "excerpts": [
            "I've been trying to write a simple hello world kernel module in Rust, but none of the examples available on GitHub appear to be working."
          ]
        },
        {
          "title": "How to call a Rust struct's method from C using FFI? - Stack Overflow",
          "url": "https://stackoverflow.com/questions/54156498/how-to-call-a-rust-structs-method-from-c-using-ffi",
          "excerpts": [
            "I am trying to call a public function (located inside a Rust struct's impl block) from a C program using the FFI. Calling regular pub fn s has not been too ..."
          ]
        },
        {
          "title": "Not matching ABI between C++ and Rust",
          "url": "https://users.rust-lang.org/t/not-matching-abi-between-c-and-rust/112791",
          "excerpts": [
            "Missing: porting drivers"
          ]
        },
        {
          "title": "Licenses",
          "url": "https://www.rust-lang.org/policies/licenses",
          "excerpts": [
            "The Rust Programming Language and all other official projects, including this website, are generally dual-licensed: Apache License, Version 2.0 ..."
          ]
        },
        {
          "title": "Will Rust in the Linux kernel make it non-GPL?",
          "url": "https://www.bleepingcomputer.com/forums/t/782256/will-rust-in-the-linux-kernel-make-it-non-gpl/",
          "excerpts": [
            "Feb 5, 2023 — GPL code, however, can't be used in MIT licensed code. Edit: Here is a ... Rust code can be licensed however the author of the code wants."
          ]
        },
        {
          "title": "PortingCToRust - Wiki",
          "url": "https://wiki.alopex.li/PortingCToRust",
          "excerpts": [
            "I decided to port the minimp3 library from C to Rust. I want a pure Rust MP3 decoder crate to exist under a permissive license."
          ]
        },
        {
          "title": "Linux NVMe-oF Initiator + Linux SPDK NVMe-oF Target - StarWind",
          "url": "https://www.starwindsoftware.com/blog/nvme-part-1-linux-nvme-initiator-linux-spdk-nvmf-target/",
          "excerpts": [
            "Linux SPDK NVMe-oF Target provides 20 000 IOPS gain under all 64KB patterns. Under 4k blocks, things look even better: SPDK Target provides 300 ...",
            "This article examines Linux NVMe-oF initiator and target performance, using a Linux SPDK NVMe-oF target and a Linux NVMe-oF initiator."
          ]
        },
        {
          "title": "QXL vs VirtIO GPU vs VirGL GPU - trivial benchmark on my setup",
          "url": "https://www.reddit.com/r/Proxmox/comments/1auvdlg/qxl_vs_virtio_gpu_vs_virgl_gpu_trivial_benchmark/",
          "excerpts": [
            "So I just tested with Portal 2 and VirtIO GPU & QXL gave me between 2 and 10 fps, while VirGL gave me about 180 fps. VirGL also gave about 600 - ..."
          ]
        },
        {
          "title": "How much performance does VFIO hit.",
          "url": "https://www.reddit.com/r/VFIO/comments/utow7o/how_much_performance_does_vfio_hit/",
          "excerpts": [
            "I've switched to dual GPU VFIO headless setup on Pascal cards and assuming that everything is correctly optimized You'll get between 97-99% ...",
            "I notice a like 2/5% loss compared to bare metal (without any bloat installed). I think this is an acceptable loss. I do use two gpu's instead ..."
          ]
        },
        {
          "title": "io - What is the impact of different Linux I/O APIs on performance ...",
          "url": "https://unix.stackexchange.com/questions/728807/what-is-the-impact-of-different-linux-i-o-apis-on-performance-benchmarking-of-nv",
          "excerpts": [
            "I am benchmarking NVMe SSDs on my Linux server, with the aim to achieve the IOPS, BW and Latency values as mentioned in the product specifications."
          ]
        },
        {
          "title": "A Guide to Porting C/C++ to Rust | Hacker News",
          "url": "https://news.ycombinator.com/item?id=13513807",
          "excerpts": [
            "This is an awesome start for a great manual on the area. I'm very inspired by the experience I had in doing this, and hope that we do more."
          ]
        },
        {
          "title": "Rust support in the Linux kernel - Hacker News",
          "url": "https://news.ycombinator.com/item?id=29485465",
          "excerpts": [
            "Rust should not be used in the Linux kernel until it supports 100% of the architectures supported by Linux itself. > Rust should not be used in ..."
          ]
        },
        {
          "title": "Is it possible to rewrite the Linux kernel in Rust? - Quora",
          "url": "https://www.quora.com/Is-it-possible-to-rewrite-the-Linux-kernel-in-Rust",
          "excerpts": [
            "Sure, it could be done. But the Linux kernel is enormous these days, with 27.8 million lines of code as of 2020, and probably more now."
          ]
        },
        {
          "title": "VFIO: Tuning your Windows gaming VM for optimal performance",
          "url": "https://angrysysadmins.tech/index.php/2022/07/grassyloki/vfio-tuning-your-windows-gaming-vm-for-optimal-performance/",
          "excerpts": [
            "Jul 29, 2022 — This will be a guide on advanced tuning for a VFIO gaming VM. If you're starting from scratch, read through the Arch Wiki guide on PCI passtrhough via OVMF."
          ]
        },
        {
          "title": "I'm shocked the Rust community is pushing an MIT licensed Rust ...",
          "url": "https://users.rust-lang.org/t/im-shocked-the-rust-community-is-pushing-an-mit-licensed-rust-rewrite-of-gnu-coreutils/126110?page=2",
          "excerpts": [
            "The Rust code that is being integrated to the Linux kernel is under GPLv2 as the rest of the kernel. On the other hand most Rust projects (Linux ..."
          ]
        },
        {
          "title": "Rust is Officially in the Linux Kernel : r/programming",
          "url": "https://www.reddit.com/r/programming/comments/1l6y59c/rust_is_officially_in_the_linux_kernel/",
          "excerpts": [
            "Rust was already officially in the Linux kernel for other device drivers before Nova."
          ]
        },
        {
          "title": "Rust for Linux: Rust Version Policy and Kernel Integration",
          "url": "https://rust-for-linux.com/rust-version-policy",
          "excerpts": [
            "The kernel supports a minimum version of Rust, starting with Rust 1.78.0.",
            "The project\n\n1. [Contact](/contact)\n2. [Contributing](/contributing)\n3. [Rust kernel policy](/rust-kernel-policy)\n4. [Branches](/branches)\n5. [Rust reference drivers](/rust-reference-drivers)\n6. [Rust version policy](/rust-version-policy)"
          ]
        },
        {
          "title": "QEMU/KVM with VFIO passthrough kinda slow - Reddit",
          "url": "https://www.reddit.com/r/VFIO/comments/v9eian/qemukvm_with_vfio_passthrough_kinda_slow/",
          "excerpts": [
            "Recently I have set up a Win10 vm for gmaing on linux host system (Debian 11) and its performance is low. My Host Configuration"
          ]
        },
        {
          "title": "[SOLVED] KVM/QEMU GPU Passthrough Performance Issues ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=280628",
          "excerpts": [
            "User experienced poor performance (lagging, 2 fps in PUBG) with KVM/QEMU GPU passthrough, but the issue was later solved."
          ]
        },
        {
          "title": "~15-20% CPU performance penalty under KVM : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/r49an8/1520_cpu_performance_penalty_under_kvm/",
          "excerpts": [
            "I'm getting a Cinebench R23 single core score of ~1580 points, while under QEMU it is reduced to ~1300, with a big variance - between 1220 and 1380.See more"
          ]
        },
        {
          "title": "Introduction to KVM and SR-IOV",
          "url": "https://dzone.com/articles/introduction-to-kvm-sr-iov",
          "excerpts": [
            "Mar 20, 2024 — Improved I/O performance: Direct access for VMs to network hardware speeds up data processing and minimizes latency. · Efficient resource sharing ..."
          ]
        },
        {
          "title": "To FUSE or Not to FUSE: Performance of User-Space File ...",
          "url": "https://www.usenix.org/system/files/conference/fast17/fast17-vangoor.pdf",
          "excerpts": [
            "by BKR Vangoor · 2017 · Cited by 237 — FUSE consists of a kernel part and a user-level dae- mon. The kernel part is implemented as a Linux kernel module that, when loaded, registers a ...",
            "by BKR Vangoor · 2017 · Cited by 237 — Thus, overheads caused by the FUSE's user-kernel communi- cations explain the performance degradation. File Reads. Figure 5 shows different ..."
          ]
        },
        {
          "title": "fuse - A File Systems Geek",
          "url": "https://fsgeek.ca/tag/fuse/",
          "excerpts": [
            "FUSE remains slower than a native file system in many scenarios however (e.g., they use EXT4 as a benchmark comparison) though the performance seems to be much ..."
          ]
        },
        {
          "title": "White Paper",
          "url": "https://www.lannerinc.com/support/download-center/category/38-white-papers?download=46:throughput-improvements-with-the-dpdk",
          "excerpts": [
            "We provide application exam- ples for writing the software to acquire data from the Intel DPDK instead of the Linux kernel. With the Intel DPDK and Lanner ..."
          ]
        },
        {
          "title": "Development of Ethernet Based Real-Time Applications in ...",
          "url": "https://accelconf.web.cern.ch/icalepcs2019/papers/mopha044.pdf",
          "excerpts": [
            "by G Gaio · Cited by 2 — The goal of the test was to validate the mainline Linux kernel together with the DPDK platform as a possible alternative to the present ..."
          ]
        },
        {
          "title": "Improving the performance of a Windows 11 Guest on QEMU",
          "url": "https://leduccc.medium.com/improving-the-performance-of-a-windows-10-guest-on-qemu-a5b3f54d9cf5",
          "excerpts": [
            "This guide is intended for existing KVM/QEMU Libvirt setups and will help you reach near native performance on your Windows 10 or 11 Guest.See more"
          ]
        },
        {
          "title": "RFUSE: Modernizing Userspace Filesystem Framework ...",
          "url": "https://www.usenix.org/system/files/fast24-cho.pdf",
          "excerpts": [
            "by KJ Cho · 2024 · Cited by 8 — However, the complex internal stack of FUSE leads to notable performance overhead, which becomes even more prominent in modern hardware ..."
          ]
        },
        {
          "title": "How to have good graphics performance in KVM-based ...",
          "url": "https://www.reddit.com/r/VFIO/comments/1hexcq6/how_to_have_good_graphics_performance_in_kvmbased/",
          "excerpts": [
            "I'm wondering what should I do to have good VM graphics performance. I thought that it is just enough to buy a GPU with SR-IOV, and my VMs will ..."
          ]
        },
        {
          "title": "Virtio-blk latency measurements and analysis",
          "url": "https://www.linux-kvm.org/page/Virtio/Block/Latency",
          "excerpts": [
            "The results give a 33% virtualization overhead.",
            "The time spent in QEMU servicing a read request made up 29 us or a 23% overhead compared to a host read request.",
            "The following numbers for the layers of the stack are derived from the previous numbers by subtracting successive latency readings:",
            "95 ns latency (13% of total) is high. The guest should be filling in virtio-blk read commands and talking to the virtio-blk PCI device, there isn't much interesting work going on inside the guest.",
            "The QEMU 29393 ns latency (~15% of total) is high."
          ]
        },
        {
          "title": "MODULE_LICENSE and EXPORT_SYMBOL_GPL",
          "url": "https://lwn.net/2001/1025/a/module-license.php3",
          "excerpts": [
            "MODULE_LICENSE() allows kernel developers to identify kernels that have been tainted by modules whose source code is not generally available."
          ]
        },
        {
          "title": "2024 VFIO Performance Loss vs Bare Metal - Virtualization",
          "url": "https://forum.level1techs.com/t/2024-vfio-performance-loss-vs-bare-metal/221503",
          "excerpts": [
            "Dec 11, 2024 — In demanding games (Squad) I see a 30-40% loss in performance versus bare metal. In older titles (CS:Source, World in Conflict) the performance drop is more ..."
          ]
        },
        {
          "title": "Low Latency Storage Optimizations for Proxmox, KVM, & ...",
          "url": "https://kb.blockbridge.com/technote/proxmox-tuning-low-latency-storage/",
          "excerpts": [
            "This technote describes how to optimize I/O latency in a performance-critical virtual environment consisting of KVM, QEMU, and Proxmox."
          ]
        },
        {
          "title": "[PDF] QEMU & KVM Automated Performance Benchmarking @ SUSE",
          "url": "https://kvm-forum.qemu.org/2022/QEMU%20%26%20KVM%20Automated%20Performance%20Benchmarking%20%40%20SUSE-1.pdf",
          "excerpts": [
            "More combinations: — Run benchmarks CPU bench, I/O bench and MEM bench in a 15-SP3 VM, on a 15-SP3 host. — Run benchmarks CPU bench, I/O bench and MEM bench ..."
          ]
        },
        {
          "title": "Virtualization Tuning and Optimization Guide | 7",
          "url": "https://docs.redhat.com/de/documentation/red_hat_enterprise_linux/7/html-single/virtualization_tuning_and_optimization_guide/index",
          "excerpts": [
            "Within this guide, you can find tips and suggestions for making full use of KVM performance features and options for your host systems and virtualized guests."
          ]
        },
        {
          "title": "NVMe over Fabrics Target - SPDK",
          "url": "https://spdk.io/doc/nvmf.html",
          "excerpts": [
            "The Linux kernel also implements an NVMe-oF target and host, and SPDK is tested for interoperability with the Linux kernel implementations."
          ]
        },
        {
          "title": "What is the overhead on a FUSE filesystem compared ...",
          "url": "https://news.ycombinator.com/item?id=43517538",
          "excerpts": [
            "What is the overhead on a FUSE filesystem compared to being implemented in the kernel? The overhead is quite high, because of the additional context ..."
          ]
        },
        {
          "title": "The Linux Kernel Archives",
          "url": "https://www.kernel.org/",
          "excerpts": [
            "The Linux Kernel Archives ; mainline: 6.17-rc3, 2025-08-24, [tarball] ; stable: 6.16.3, 2025-08-23, [tarball], [pgp] ; stable: 6.15.11 [EOL], 2025-08-20, [tarball] ..."
          ]
        },
        {
          "title": "ABI stable symbols - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/admin-guide/abi-stable.html",
          "excerpts": [
            "Documents the interfaces that the developer has defined to be stable. Userspace programs are free to use these interfaces with no restrictions."
          ]
        },
        {
          "title": "Kernel index",
          "url": "https://lwn.net/Kernel/Index/",
          "excerpts": [
            "This index covers articles published in the LWN.net Kernel Page. All articles from the beginning of 2004 have been entered here."
          ]
        },
        {
          "title": "70% game performance in vfio vm compared to bare metal",
          "url": "https://forum.level1techs.com/t/70-game-performance-in-vfio-vm-compared-to-bare-metal/178705",
          "excerpts": [
            "Nov 22, 2021 — The GPU is pegged at 100% on bare metal but only like 40-70% in the VM. In both, there is “normal”/expected cpu usage. I feel like I'm missing something."
          ]
        },
        {
          "title": "Accelerate Your NVMe Drives with SPDK - Intel",
          "url": "https://www.intel.com/content/www/us/en/developer/articles/technical/accelerating-your-nvme-drives-with-spdk.html",
          "excerpts": [
            "When comparing the SPDK userspace NVMe driver to the Linux Kernel 4.4, the latency is up to 10x lower: Figure 3. A comparison of latency between ..."
          ]
        },
        {
          "title": "Linux Kernel Enforcement Statement",
          "url": "https://www.kernel.org/doc/html/v4.17/process/kernel-enforcement-statement.html",
          "excerpts": [
            "As developers of the Linux kernel, we have a keen interest in how our software is used and how the license for our software is enforced. Compliance with the ..."
          ]
        },
        {
          "title": "DDEKit and DDE for Linux",
          "url": "https://os.inf.tu-dresden.de/ddekit/",
          "excerpts": [
            "On this page you find our implementation of a DDE that allows running Linux kernel drivers in Linux user space without requiring any modifications to the driver ..."
          ]
        },
        {
          "title": "The Linux kernel has an incredibly strong userspace API ...",
          "url": "https://news.ycombinator.com/item?id=40816716",
          "excerpts": [
            "Jun 28, 2024 — The Linux kernel has an incredibly strong userspace API stability guarantee. I think in practice this guarantee is unmatched in modern ..."
          ]
        },
        {
          "title": "iwlwifi(4) - FreeBSD Manual Pages",
          "url": "https://man.freebsd.org/cgi/man.cgi?iwlwifi(4)",
          "excerpts": [
            "DESCRIPTION The iwlwifi driver provides support for Intel Wireless network devices. ... iwlwifi driver module parameters and are mapped automatically by linuxkpi."
          ]
        },
        {
          "title": "What is the meaning of \"driver core\" in the context of Linux kernel ...",
          "url": "https://stackoverflow.com/questions/47584527/what-is-the-meaning-of-driver-core-in-the-context-of-linux-kernel-device-drive",
          "excerpts": [
            "The driver core is the generic code that manages drivers, devices, buses, classes, etc. It is not tied to a specific bus or device."
          ]
        },
        {
          "title": "The Biggest Linux Gaslight Of Them All: Stable Driver API",
          "url": "https://www.reddit.com/r/linuxsucks/comments/18s688e/the_biggest_linux_gaslight_of_them_all_stable/",
          "excerpts": [
            "Except this isn't a problem, because the drivers would be out of tree. The whole POINT of a stable driver API is so that drivers can be moved ...",
            "One of the top Linux kernel maintainers wrote this: https://github.com/torvalds/linux/blob/master/Documentation/process/stable-api-nonsense.rst ..."
          ]
        },
        {
          "title": "Possible solution to the drm-kmod kernel mismatch after ...",
          "url": "https://forums.freebsd.org/threads/possible-solution-to-the-drm-kmod-kernel-mismatch-after-upgrade-from-bapt.96058/",
          "excerpts": [
            "Dec 13, 2024 — It's a thorny problem that often comes up after upgrades from a minor version that has not reached EOL yet. bapt@ has posted a solution to freebsd-ports."
          ]
        },
        {
          "title": "When FFI Function Calls Beat Native C",
          "url": "https://news.ycombinator.com/item?id=17171252",
          "excerpts": [
            "I think the main performance are really (1) dynamically linked C/C++ with GOT + PLT, (2) statically linked C/C++ and then (potentially) separate ..."
          ]
        },
        {
          "title": "Compatibility layers are the future of Linux gaming and native builds ...",
          "url": "https://www.reddit.com/r/linux_gaming/comments/11wu0kw/compatibility_layers_are_the_future_of_linux/",
          "excerpts": [
            "Proton or other compatibility layers are far superior to native Linux builds of games. In fact they are even superior to native windows versions of games."
          ]
        },
        {
          "title": "Torvalds' New Patch Improves Linux Security and ...",
          "url": "https://linuxsecurity.com/news/security-projects/torvalds-new-patch-implications-for-security-performance",
          "excerpts": [
            "Nov 11, 2024 — Torvalds' recent patch enhances Linux performance and addresses critical security against Meltdown and Spectre attacks."
          ]
        },
        {
          "title": "And I'm not at all interested in some \"ZFS shim layer\" thing either If ...",
          "url": "https://news.ycombinator.com/item?id=22005901",
          "excerpts": [
            "Linux absolutely is especially hostile. Windows will generally try to support existing drivers, even binary-only ones, and give plenty of notice for API changes ..."
          ]
        },
        {
          "title": "QEMU/virtual machines with GPU pass through even possible on ...",
          "url": "https://www.reddit.com/r/linux4noobs/comments/15vtwgt/qemuvirtual_machines_with_gpu_pass_through_even/",
          "excerpts": [
            "I've been trying to get GPU pass through to work on mint cinnamon 21.2 but everything I found has only worked for arch and fedora."
          ]
        },
        {
          "title": "Linux Kernel 6.14: Enhanced Drivers, Security ...",
          "url": "https://thenewstack.io/linux-kernel-6-14-enhanced-drivers-security-performance-improvements/",
          "excerpts": [
            "Mar 25, 2025 — Drivers help increase the operating system's stability and performance, enabling new development and greater integration of Linux with cloud ..."
          ]
        },
        {
          "title": "NDSS 2024: Linux Driver Runtime (LDR)",
          "url": "https://www.ndss-symposium.org/wp-content/uploads/2024-9-paper.pdf",
          "excerpts": [
            "LDR is a sandbox environment\nthat provides necessary runtime support for driver execution\ninside the secure OS which works as the operating system\ninside the TrustZone secure world (SW).",
            "sium.org\n\ndriver module for a target secure peripheral, which we refer\nto as a SW driver, into LDR and orchestrate and mediate its\ninteractions with the secure OS as well as the normal world\n(NW) Linux kernel so that the driver module can properly\ninitialize and operate on the target peripheral.",
            "To mitigate the\nsecurity concerns introduced by the SW driver’s dependency\non the NW Linux kernel, LDR provides library function (e.g.,\nmemory management, I/O operation, etc.) support for the SW\ndriver by reusing the secure OS library functions. As for the\nremaining ones which the secure OS does not implement, LDR\nredirects them to the NW Linux kernel yet with security checks\non both passed arguments and return values.",
            "Library functions can be directly supported by the secure\nOS and the Linux kernel subsystem functions need to be redirected\nto and handled by the NW Linux kernel.",
            "During runtime, the twin drivers work\ncooperatively to facilitate SW driver support (§IV-D). The\nSW driver is loaded into LDR, interacts with the secure\nperipheral devices and provides secure I/O services to other\nSW components.",
            "ing NW driver is a helper\ndriver running inside the NW Linux kernel to assist with SW\ndriver initialization and subsystem call redirection.",
            "Considering that SW drivers themselves have large code\nbase and may contain potential security flaws, we create\nan isolated execution environment to protect the secure OS\nfrom the untrusted SW drivers and prevent an untrusted SW\ndriver from jeopardizing other drivers’ code integrity and data\nprivacy.",
            "e create an isolated execution domain for\na set of functionally-related SW drivers and prevent such\nSW drivers from accessing memory regions belonging to the\nsecure OS and other IEDs (§IV-E1).",
            "e provide\n\n4\n\na carefully-defined IED call gate that intercepts and mediates\neach dependency function call issued by a SW driver (§IV-",
            "Finally, we propose several driver state maintenance schemes\nthat securely maintain both SW and NW driver states so that no\nsensitive SW driver information is leaked to the Linux kernel\nduring driver initialization and execution (§IV-F)",
            "Redirecting All Function Calls to Linux Kernel.\nAnother\nalternative approach is to only put driver LKMs inside the\nsecure OS and redirect all driver dependency function invoca-\ntions back to the NW Linux kernel, as shown in Figure 1-(b",
            "Such total function call redirection approach does not enlarge\nTCB since the complex Linux kernel subsystems still reside in\nthe NW and can be realized with relatively small engineering\nefforts.",
            "However, such approach may result in performance\ndowngrade as well as security issues. Firstly, redirecting every\nsingle function invocation may result in huge performance\noverhead for drivers with frequent kernel function calls, e.g.,\nsensor devices with bus I/O at a high sampling rate.",
            "e deploy an MMA8451 accelerometer driver[18]\ninside the secure OS following this approach. The results show\nthat the sampling rate drops to 405.89, 49.3% lower than the\ndevice’s default sampling rate of 8",
            "t the kernel I/O library\nfunctions primarily help drivers to interact with the underlying\ndevices.\nTherefore, the untrusted Linux kernel may corrupt the\nraw data by hooking these I/O functions and the SW cannot\nacquire integral and genuine raw data.",
            "LDR\nintroduces minimal performance overhead to real-world\nuse cases."
          ]
        },
        {
          "title": "The Linux Kernel Driver Interface",
          "url": "https://docs.kernel.org/process/stable-api-nonsense.html",
          "excerpts": [
            "This is being written to try to explain why Linux does not have a binary kernel interface, nor does it have a stable kernel interface."
          ]
        },
        {
          "title": "Linux's license is GPLv2 plus a syscall exception. If it was ...",
          "url": "https://news.ycombinator.com/item?id=37320678",
          "excerpts": [
            "Aug 30, 2023 — Linux's license is GPLv2 plus a syscall exception. If it was GPLv2 alone, proprietary modules would probably be infringing."
          ]
        },
        {
          "title": "Tutorial On Rump Kernel Servers and Clients",
          "url": "https://www.netbsd.org/docs/rump/sptut.html",
          "excerpts": [
            "This document acts as a tutorial on how to configure and use unmodified NetBSD kernel drivers as userspace services with utilities available from the NetBSD ..."
          ]
        },
        {
          "title": "The rump kernel: A tool for driver development and a toolkit ...",
          "url": "https://www.netbsd.org/gallery/presentations/justin/2015_AsiaBSDCon/justincormack-abc2015.pdf",
          "excerpts": [
            "by J Cormack · Cited by 6 — The NetBSD rump kernel is a way to run device drivers outside the kernel. Until recently the most common use was as a tool for running tests on NetBSD ..."
          ]
        },
        {
          "title": "The Linux Kernel API",
          "url": "https://docs.kernel.org/core-api/kernel-api.html",
          "excerpts": [
            "The Linux Kernel API¶. Basic C Library Functions¶. When writing drivers, you cannot in general use routines which are from the C Library."
          ]
        },
        {
          "title": "Driver Model",
          "url": "https://docs.kernel.org/driver-api/driver-model/index.html",
          "excerpts": [
            "Contents · Driver Basics · Driver Model · Device links · Device drivers infrastructure · ioctl based interfaces · CPU and Device Power Management."
          ]
        },
        {
          "title": "LWN.net - Non-free drivers",
          "url": "https://lwn.net/Articles/425142/",
          "excerpts": [
            "It's not about \"never\", as there is no point in keeping stable kernel API and ABI forever. It's about keeping them stable for a reasonable amount of time, in a way that administrators and 3rd party developers can actually understand and adapt to.",
            "Locking ABIs down means you just won't innovate that part of the system any longer, and you'll put up with what eventually turn out to be insufficient designs, as new requirements evolve, for the sake of stability."
          ]
        },
        {
          "title": "Hacker News discussion on Linux driver API stability",
          "url": "https://news.ycombinator.com/item?id=30868346",
          "excerpts": [
            "This is by design. Linux doesn't *want* to pay the maintenance and performance costs of guaranteeing a stable in-kernel API/ABI: [https://www.kernel.org/doc/Documentation/process/stable-api-"
          ]
        },
        {
          "title": "Linux kernel licensing rules",
          "url": "https://android.googlesource.com/kernel/common/+/b207caff4176/Documentation/process/license-rules.rst",
          "excerpts": [
            " space API ( UAPI ) header files , which describe the interface of |\n|  | user \\- space programs to the kernel are a special case . According to the |\n|  | note in the kernel COPYING file , the syscall interface is a clear boundary , |\n|  | which does not extend the GPL requirements to any software which uses it to |\n|  | communicate "
          ]
        },
        {
          "title": "DDEKit Approach for Linux User Space Drivers",
          "url": "http://os.inf.tu-dresden.de/papers_ps/beleg_weisbach.pdf",
          "excerpts": [
            "by H Weisbach · 2011 — DDEKit-Linux enables the re-use of device drivers in Linux, although an extensive number of drivers are available for Linux. DDEKit-Linux ..."
          ]
        },
        {
          "title": "The Userspace I/O HOWTO",
          "url": "https://www.kernel.org/doc/html/v5.0/driver-api/uio-howto.html",
          "excerpts": [
            "The main part of the driver will run in user space. This simplifies development and reduces the risk of serious bugs within a kernel module."
          ]
        },
        {
          "title": "Linux device driver ports - Breaking new ground",
          "url": "https://genodians.org/skalk/2021-04-06-dde-linux-experiments",
          "excerpts": [
            "Apr 6, 2021 — In my following blog post(s) I want to elaborate on recent attemtps to renew and enhance the way Linux device drivers are ported to Genode."
          ]
        },
        {
          "title": "Chapter 5. KVM Paravirtualized (virtio) Drivers",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/chap-kvm_para_virtualized_virtio_drivers",
          "excerpts": [
            "Paravirtualized drivers enhance the performance of guests, decreasing guest I/O latency and increasing throughput almost to bare-metal levels. It is recommended ..."
          ]
        },
        {
          "title": "PCI-passthrough: Windows gaming... on Linux - bitgrounds.tech",
          "url": "https://bitgrounds.tech/posts/pci-passthrough/",
          "excerpts": [
            "VFIO allows for near native performance from within the guest system , partly because it provides DMA mapping and isolation using hardware ..."
          ]
        },
        {
          "title": "Can I set the virtio network device to a faster speed?",
          "url": "https://www.reddit.com/r/Proxmox/comments/j2t3or/can_i_set_the_virtio_network_device_to_a_faster/",
          "excerpts": [
            "I think VirtIO goes up to 10G, but it depends on the driver. What OS? Did you install the VirtIO drivers, if necessary?"
          ]
        }
      ],
      "reasoning": "The field gpu_and_display_stack_strategy centers on how to implement or reuse the Linux graphics stack. Direct references to DRM and KMS are highly relevant for the native stack aspect. Excerpt describing the DRM and KMS architecture, including the core DRM subsystem, GEM, DMA-BUF, and mode setting, directly informs the feasibility and complexity of building a native display stack from scratch. Those excerpts show the structure and responsibilities of the DRM/KMS stack, which is essential context for the native_stack_challenge portion of the field value. Content discussing VirGL and Venus, as well as Virtio-GPU integration in QEMU/KVM environments, is highly relevant for the recommended strategy: it outlines a near-native graphics path that avoids full native driver development by leveraging virtualization-backed GPU devices and user-space GPU stacks. The Venus/VirGL discussions illustrate how to map Vulkan/OpenGL workloads through a mediated GPU path, which aligns with recommending a virtualization-based approach to start delivering graphical capabilities without building everything in-kernel. References to vDRM, virglrenderer, and virtio-gpu reinforce this virtualization strategy and the practical design implications of using a standard like VirtIO-GPU to expose GPU resources to guests or user-space drivers. Broader DRM/KMS documentation provides baseline knowledge about how these subsystems fit into the kernel graphics stack, which informs the feasibility and integration considerations for a new OS attempting to adopt a similar strategy via virtualization. In sum, the strongest alignment with the field value comes from explicit DRM/KMS basics (native stack) and virtualization-enabled GPU paths (shim-free, Virtio-GPU/Venus), with related infrastructure discussions offering supportive context for a practical deployment route. The remaining excerpts on licensing, general GPU topics, or peripheral topics provide background but are less central to the gpu_and_display_stack_strategy field itself. ",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_by_strategy.userspace_framework_performance",
      "citations": [
        {
          "title": "ICPE 2024 SPDK vs Linux storage stack performance",
          "url": "https://research.spec.org/icpe_proceedings/2024/proceedings/p154.pdf",
          "excerpts": [
            "by 7 — Our setup is able to deliver a peak random read performance of 3.4 Million IOPS with the Linux storage stack under Linux v6.3.8 (5.9 Million ... This demonstrates that SPDK\n(1.3 MIOPS/core) is still the state-of-the-art storage stack, which is at\nleast 3.6× more efficient (single core peak IOPS performance with 8\nSSDs, “1p” and “1p SPDK” lines in Figure 1c) than the Linux storage\nstack (370 KIOPS/core) (O-2). SPDK can deliver higher throughput\nthan the Linux storage stack when the CPU is the bottleneck be-\ncause SPDK is more CPU efficient than the Linux storage stack. Previous study shows that SPDK needs fewer CPU instructions to\nprocess each I/O request than the Linux storage stack [51]. 3.3\nSummary\n\nWhat is the key resource bottleneck for the L-app, T-4KiB-app, and T-\n64KiB-app? The key finding (Key Finding, KF1) here is that as the\nNVMe device speeds are improving, the CPU becomes the primary\nbottleneck. Modern fast NVMe storage devices like Samsung 980\nPRO 1TiB, can require more than a single CPU core to saturate\nthe performance of the SSD. On our setup, L-apps (QD=1), and\nT-4KiB-apps (QD=128) are bounded by the CPU performance. We\nreport that the T-4KiB-app needs at least 4 T-4KiB-apps on 4 CPU\ncores to saturate a single SSD. The T-64KiB-app can saturate a\nsingle SSD with only one CPU core (not shown). Hence, the age-old\n\nmantra of “CPU is fast, I/O devices are slow” does not hold anymore\nfor modern fast NVMe SSDs (O-3). We answer RQ1 by identifying\nwhen the CPU or the SSD becomes a bottleneck (the inflection or\nsaturation points)."
          ]
        },
        {
          "title": "SPDK Performance Comparison (Harris et al., 2017)",
          "url": "https://files.futurememorystorage.com/proceedings/2017/20170810_FW32_Harris.pdf",
          "excerpts": [
            " \n\nSPDK cuts latency \n\nalmost in "
          ]
        },
        {
          "title": "Intel Ethernet' s Performance Report with DPDK 23.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_23_03_Intel_NIC_performance_report.pdf",
          "excerpts": [
            "Apr 20, 2023 — Single core test case: Used to check the maximum IO throughput for a single core. The case requires at least 1x 100GbE ports for the ice DPDK ..."
          ]
        },
        {
          "title": "Mellanox DPDK Performance Report (DPDK 17.05, Mellanox ConnectX NICs)",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_17_05_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            " \n\nDevice Under Test (DUT) is made up of the HPE server and the Mellanox ConnectX®-5 Ex NIC with a dual- \nport (only first port used in this test). The DUT is connected to the IXIA packet generator which generates \ntraffic towards the ConnectX®-5 Ex NIC."
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.07",
          "url": "http://fast.dpdk.org/doc/perf/DPDK_22_07_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "Feb 21, 2023 — The ConnectX-6Dx data traffic is passed through DPDK to the test application l3fwd and is redirected to the opposite direction on the same port."
          ]
        },
        {
          "title": "NVIDIA NICs Performance Report with DPDK 24.07",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_24_07_NVIDIA_NIC_performance_report.pdf",
          "excerpts": [
            "Figure 2: Test #1 Results – NVIDIA ConnectX-6 Dx 100GbE Throughput at Zero Packet Loss",
            "Figure 2: Test #1 Results – NVIDIA ConnectX-6 Dx 100GbE Throughput at Zero Packet Loss"
          ]
        }
      ],
      "reasoning": "The field value asserts that user-space frameworks like DPDK for networking and SPDK for storage offer the highest possible I/O performance by bypassing the kernel, using poll-mode drivers on dedicated cores, enabling full line-rate throughput on 100 GbE NICs, and achieving specific performance targets (e.g., 148.81 Mpps with 64-byte frames, up to 1.3 million IOPS per core, and substantially higher efficiency than the kernel). The most direct support includes explicit performance figures and comparisons: one excerpt reports a peak random read performance of 3.4 million IOPS with the Linux storage stack versus SPDK being more CPU-efficient and capable of very high IOPS/core, and notes that SPDK can reduce software overhead for NVMe-over-Fabrics by up to 10x, with zero-copy architecture and 100% utilization of dedicated cores. This tightly aligns with the claim that bypassing the kernel and dedicating cores yields maximal throughput, and that SPDK achieves substantially higher IOPS per core than the kernel-based path. Another excerpt explicitly states the concept of SPDK cutting latency, reinforcing the advantage of user-space, low-overhead paths. Additional excerpts describe single-core throughput tests for DPDK and show its use in high-throughput scenarios with NICs, further supporting the idea that these user-space frameworks push performance boundaries through dedicated cores and optimized data paths. The remaining excerpts provide corroborative context about the domains (DPDK performance reports, NIC-specific demonstrations, and related architectural notes) that reinforce the overall argument but contribute less directly to the exact performance figures and mechanisms described in the field value.",
      "confidence": "high"
    },
    {
      "field": "feasibility_of_direct_ffi_reuse.technical_barriers",
      "citations": [
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        },
        {
          "title": "Linux Syscall Note (SPDX)",
          "url": "https://spdx.org/licenses/Linux-syscall-note.html",
          "excerpts": [
            "the only valid version of the GPL as far as the kernel\nis concerned is \\_this\\_ particular version of the license (ie v2, not\nv2.2 or v3.x or whatever), unless explicitly otherwise stated. Linus Torvalds"
          ]
        },
        {
          "title": "FSF: Interpreting, enforcing and changing the GNU GPL, as applied ...",
          "url": "https://lwn.net/Articles/683352/",
          "excerpts": [
            "... linking creates derivative works. The FSF's own FAQ on the GPLv2 also takes this position by stating that linking to a GPL'd work requires ..."
          ]
        },
        {
          "title": "GNU General Public License",
          "url": "https://en.wikipedia.org/wiki/GNU_General_Public_License",
          "excerpts": [
            "The foundation asserts that an executable that uses a dynamically linked library is indeed a derivative work. This assertion does not, however, apply to ..."
          ]
        },
        {
          "title": "Tainted kernels — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/tainted-kernels.html",
          "excerpts": [
            "Modules without a MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by insmod as GPL compatible are assumed to be proprietary. F if any module ..."
          ]
        },
        {
          "title": "Linux kernel licensing rules",
          "url": "https://www.kernel.org/doc/html/v4.19/process/license-rules.html",
          "excerpts": [
            "The User-space API (UAPI) header files, which describe the interface of\nuser-space programs to the kernel are a special case. According to the\nnote in the kernel COPYING file, the syscall interface is a clear boundary,\nwhich does not extend the GPL requirements to any software which uses it to\ncommunicate with the kernel.",
            "\n\nThe Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), ",
            "The Linux kernel is under GPL-2.0, with individual files compatible with GPL-2.0 and using SPDX identifiers. UAPI files have a special syscall exception.",
            "The files in this directory contain the full exception text and the required Exception Metatags. Examples: LICENSES/exceptions/Linux-syscall-note. Contains the ..."
          ]
        },
        {
          "title": "Interpreting, enforcing and changing the GNU GPL, as applied to ...",
          "url": "https://www.fsf.org/licensing/zfs-and-linux",
          "excerpts": [
            "Apr 7, 2016 — The specific occasion for this article is the violation of combining Linux with ZFS, and that concerns specifically GNU GPL version 2; however, ..."
          ]
        },
        {
          "title": "Proprietary kernel modules - the boundary shifts?",
          "url": "https://lwn.net/Articles/13398/",
          "excerpts": [
            "Oct 22, 2002 — The Linux kernel's license states explicitly that programs running in user mode and making use of kernel system calls are not derived works, and ...",
            "Oct 22, 2002 — Linus Torvalds's ... The _only_ thing that allows for non-GPL modules is copyright law, and in particular the \"derived work\" issue."
          ]
        },
        {
          "title": "Free Software Foundation Issues Statement in Support of ...",
          "url": "https://sfconservancy.org/blog/2016/apr/11/fsf-zfs/",
          "excerpts": [
            "Apr 11, 2016 — Without such permission, users who seek to combine Linux and ZFS live in a precarious situation without the necessary copyright permissions to ..."
          ]
        },
        {
          "title": "Linux Kernel Licensing Rules and Precedents",
          "url": "https://docs.kernel.org/process/license-rules.html",
          "excerpts": [
            "\n\n> Loadable kernel modules also require a `MODULE_LICENSE()` tag.\nThis tag is\n> neither a replacement for proper source code license information\n> (SPDX-License-Identifier) nor in any way relevant for expressing or\n> determining the exact license under which the source code of the module\n> is provided. > \n> The sole purpose of this tag is to provide sufficient information\n> whether the module is free software or proprietary for the kernel\n> module loader and for user space tools. > \n> The valid license strings for `MODULE_LICENSE()` are:\n> \n> > | “GPL” | ",
            "The Linux kernel requires the precise SPDX identifier in all source files.",
            "The Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), as provided in LICENSES/preferred/GPL-2.0,\nwith an explicit syscall exception described in\nLICENSES/exceptions/Linux-syscall-note, as described in the COPYING file.",
            "This documentation file provides a description of how each source file should be annotated to make its license clear and unambiguous.",
            "Aside from that, individual files can be provided under a dual license,\ne.g. one of the compatible GPL variants and alternatively under a\npermissive license like BSD, MIT etc."
          ]
        },
        {
          "title": "Questioning EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/603131/",
          "excerpts": [
            "No, it can not. The court does not give a damn about the opinion of the author, nor whether the accused was or was not aware of that opinion.",
            "Jun 23, 2014 — EXPORT_SYMBOL_GPL() is meant to mark kernel interfaces that are deemed to be so low-level and specific to the kernel that any software that uses them must ... It was requested to be added after the fact by the maintainer and by now the discussion has had negative impact - all it would take is printing out the discussion and submitting it as evidence that the author was coerced into making the change and it is quite reasonable to assume that the flag is not a technical indicator, but merely a social statement. "
          ]
        },
        {
          "title": "2.5. Hello World (part 4): Licensing and Module Documentation",
          "url": "https://tldp.org/LDP/lkmpg/2.6/html/x279.html",
          "excerpts": [
            "In kernel 2.4 and later, a mechanism was devised to identify code licensed under the GPL (and friends) so people can be warned that the code is non open-source."
          ]
        },
        {
          "title": "FreeBSD Licensing Policy",
          "url": "https://docs.freebsd.org/en/articles/license-guide/",
          "excerpts": [
            "The FreeBSD project discourages completely new licenses and variations on the standard licenses. New licenses require the approval of core@FreeBSD.org to ..."
          ]
        },
        {
          "title": "Linux Networking and Network Devices APIs",
          "url": "https://docs.kernel.org/networking/kapi.html",
          "excerpts": [
            "All the pointers pointing into skb header may change and must be reloaded after call to this function. struct sk_buff *skb_expand_head(struct sk_buff *skb, ..."
          ]
        },
        {
          "title": "NAPI",
          "url": "https://docs.kernel.org/networking/napi.html",
          "excerpts": [
            "NAPI¶. NAPI is the event handling mechanism used by the Linux networking stack. The name NAPI no longer stands for anything in particular [1]."
          ]
        },
        {
          "title": "Linux USB API",
          "url": "https://docs.kernel.org/driver-api/usb/index.html",
          "excerpts": [
            "Linux USB API¶ · Introduction to USB on Linux · USB Host-Side API Model · USB-Standard Types · Host-Side Data Types and Macros · USB Core APIs · Host Controller APIs ..."
          ]
        },
        {
          "title": "The proper use of EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/769471/",
          "excerpts": [
            "In the case of EXPORT_SYMBOL_GPL(), only modules that declare a GPL-compatible license will be able to see the symbol."
          ]
        },
        {
          "title": "Re: incorrect taint of ndiswrapper — Linux Kernel",
          "url": "https://lists.linuxcoding.com/kernel/2006-q4/msg08685.html",
          "excerpts": [
            "EXPORT_SYMBOL_GPL is a technical mechanism of enforcing GPL against non-free code, but ndiswrapper is free. The non-free NDIS drivers are not using those ..."
          ]
        },
        {
          "title": "Making life (even) harder for proprietary modules",
          "url": "https://lwn.net/Articles/939842/",
          "excerpts": [
            "Aug 3, 2023 — Distributing a proprietary module might be a copyright violation, though, if the module itself is a derived work of the kernel code. But \" ..."
          ]
        },
        {
          "title": "Linux Kernel GPL and ZFS CDDL License clarifications in ...",
          "url": "https://github.com/openzfs/zfs/issues/13415",
          "excerpts": [
            "... kernel module \"could\" be considered a DERIVATIVE WORK. It does NOT mean that if you use those then your kernel module \"IS\" a derivative work.See more"
          ]
        },
        {
          "title": "5. EXPORT_SYMBOL - Linux Kernel Workbook - Read the Docs",
          "url": "http://lkw.readthedocs.io/en/latest/doc/04_exporting_symbols.html",
          "excerpts": [
            "5.2. Export Symbol¶ · EXPORT_SYMBOL() helps you provide APIs to other modules/code. · The functions which you EXPORT are available to the other modules/code."
          ]
        }
      ],
      "reasoning": "Extracts that discuss linking and licensing rules hint at the kernel's design that is not friendly to arbitrary external reuse. For instance, discussions about BPF licensing and module linking indicate there are specific constraints around how modules can interface with the kernel, suggesting that simply reusing or reimplementing kernel functionality from outside the kernel is nontrivial. References to the idea that the kernel has a defined boundary via UAPI headers show that the interface to user-space is treated as a special case, reinforcing the notion that kernel internals are not freely exposed as stable external APIs. The explicit statements about the GPL and derivative-works concepts underscore that any external integration must navigate tightly controlled licensing and derivative-work considerations, which compounds architectural and integration challenges when attempting to reuse kernel code or mirror its internal driver infrastructure via FFI. Quotes noting that the Linux kernel is GPL-2.0 and that symbols, module licensing, and syscall boundaries are purposefully constrained provide concrete evidence that the in-kernel environment is not designed for straightforward foreign-function interfaces. Taken together, these excerpts support the field value by illustrating legal, structural, and architectural constraints that function as a high barrier to direct FFI-based reuse of kernel drivers and internals, thus justifying a cautious assessment of feasibility.",
      "confidence": "low"
    },
    {
      "field": "performance_analysis_by_strategy.virtualized_driver_performance",
      "citations": [
        {
          "title": "Packed virtqueue: How to reduce overhead with virtio",
          "url": "https://www.redhat.com/en/blog/packed-virtqueue-how-reduce-overhead-virtio",
          "excerpts": [
            "This layout is relatively easy to implement and to debug thus it's a good entry point for learning the virtio dataplane basics.",
            "Packed virtqueue amends it by merging the three rings in just one location in virtual environment guest memory."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.",
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO.",
            "Devices are the main target of any I/O driver."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.",
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO."
          ]
        }
      ],
      "reasoning": "The core of the field value is that virtualized drivers using virtio can achieve high performance with manageable overhead, aided by vhost offloading and packed rings to reduce cache contention, and that VFIO passthrough can enable near-native performance with IOMMU security guarantees. Excerpts describing packed virtqueue directly support the claim that combining multiple rings into a single, more cache-friendly structure helps cut overhead in virtualized I/O paths. Excerpts about virtio-related technologies (virtio-net with vhost offloading and the use of packed rings) provide concrete mechanisms by which performance is improved in a virtualized environment. Excerpts addressing AF_XDP and Linux kernel documentation contextualize high-performance packet processing paths that virtio-based virtualization can leverage or contend with in a modern OS. Excerpts detailing VFIO and IOMMU-based device access outline the secure, user-space or host-kernel boundary arrangements that enable direct device control from VMs or user space, which is critical for achieving high throughput with strong isolation guarantees. Together, these excerpts underpin the architectural choices that enable high-performance virtualization-driven I/O and explain how each component contributes to the performance profile described in the field value.",
      "confidence": "medium"
    },
    {
      "field": "licensing_implications_of_gplv2.derivative_work_analysis",
      "citations": [
        {
          "title": "GNU General Public License",
          "url": "https://en.wikipedia.org/wiki/GNU_General_Public_License",
          "excerpts": [
            "The foundation asserts that an executable that uses a dynamically linked library is indeed a derivative work. This assertion does not, however, apply to ..."
          ]
        },
        {
          "title": "Linux Kernel GPL and ZFS CDDL License clarifications in ...",
          "url": "https://github.com/openzfs/zfs/issues/13415",
          "excerpts": [
            "... kernel module \"could\" be considered a DERIVATIVE WORK. It does NOT mean that if you use those then your kernel module \"IS\" a derivative work.See more"
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        },
        {
          "title": "Tainted kernels — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/tainted-kernels.html",
          "excerpts": [
            "Modules without a MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by insmod as GPL compatible are assumed to be proprietary. F if any module ..."
          ]
        }
      ],
      "reasoning": "The core claim in the target field value is that any software that links with the Linux kernel's internal functions forms a combined or derivative work that must be licensed under GPLv2, and that FSF/SFC have supported this interpretation, with precedents like ZFS-on-Linux cited as license-violation examples when distributing such combinations. The most directly relevant piece states that an executable using a dynamically linked library is a derivative work, which establishes the basic principle that linking to kernel interfaces can create derivative status. Further support comes from discussions about GPL/GPL-compatible licensing and how symbols and modules relate to GPL constraints, including explicit notes that linking proprietary modules may be constrained by GPL rules and that the boundary conditions around module licensing exist. A precedent reference to ZFS-on-Linux is relevant because it illustrates real-world enforcement and interpretation by SFC regarding derivative-like concerns in practice. Additional notes about kernel licensing rules and how function usage is checked by the kernel when loading a module reinforce the practical impact of linking decisions on licensing status, even if they don't state the exact derivative-work conclusion in the same terms. Taken together, these excerpts support the idea that linking to kernel internals can create a derivative relationship with GPLv2 and that licensing implications are actively considered in practice, though the exact universal conclusion about \"the entire derivative work must be licensed under GPLv2\" is not exhaustively proven by a single excerpt.",
      "confidence": "medium"
    },
    {
      "field": "concurrency_models_and_driver_api.message_passing_model",
      "citations": [
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode.",
            "Note that to build a system from scratch on a microkernel, you will need basic OS services such as drivers, file system and networking. Some of the tools and ...",
            "Capabilities are an OS abstraction for managing access rights."
          ]
        },
        {
          "title": "Simple IPC cycle costs and microkernel overview",
          "url": "https://www.researchgate.net/figure/Simple-IPC-cycle-costs_tbl1_3694140",
          "excerpts": [
            "Microkernel has many intriguing features like security, fault-tolerance, modularity and customizability, which recently stimulate a resurgent interest in both academia and industry (including seL4, QNX and Google's Fuchsia OS). However, IPC (inter-process communication), which is known as the Achilles' Heel of microkernels, is still the major factor for the overall (poor) OS performance."
          ]
        }
      ],
      "reasoning": "The most directly relevant information comes from excerpts that discuss messaging between components and driver handling in user space. One excerpt states that a microkernel family (seL4) enables user-mode device drivers and the forwarding of IRQs as asynchronous messages, which directly aligns with the proposed model of drivers implemented as asynchronous, event-driven tasks that communicate via messages rather than direct function calls. Another excerpt notes that in a microkernel-like approach, the system can run all device drivers in user mode, reinforcing the separation of concerns and non-blocking, message-based interaction between drivers and the kernel. A third excerpt highlights that devices in such systems can be managed through asynchronous, message-driven interactions, with capabilities and memory mapped in a way that supports user-space drivers, which is consistent with the idea of channel-based communication and IDL-defined protocols for type-safe messaging. These pieces together illustrate a design space where drivers are decoupled from the kernel and interact through non-blocking messages, which is the core of the proposed concurrency/driver-API model.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.2",
      "citations": [
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi",
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A"
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment."
          ]
        }
      ],
      "reasoning": "The strongest support comes from excerpts describing a vm-virtio framework, which provides virtio abstractions and concrete device implementations, signaling a clear virtualization-based reuse path that leverages virtio as the standardized interface to hardware. These excerpts explicitly name the virtio stack and its components (virtio device, queue, bindings, and device abstractions), illustrating how a Rust-based OS could interact with conventional hardware through a mature, vendor-neutral virtualization interface rather than writing each device driver from scratch. Additional excerpts discuss that the virtio ecosystem enables a lightweight guest or driver model and that virtio-related bindings and devices are designed to be stable and interoperable, which aligns with the idea of reusing existing driver ecosystems via virtualization. Supporting context from VFIO and IOMMU-focused excerpts highlights how IOMMU groups and VFIO enable secure, mediated access to devices from user space or a guest, reinforcing the architectural rationale of isolating drivers and providing protected pathways for device access, which complements the virtualization-based reuse strategy. Collectively, these excerpts connect the proposed finegrained field value to a coherent plan where virtualization (via virtio) and secure device access (via VFIO/IOMMU) enable leveraging existing hardware interfaces with controlled isolation, predictable performance characteristics, and lower maintenance compared to implementing broad driver support in the new Rust OS.",
      "confidence": "high"
    },
    {
      "field": "licensing_implications_of_gplv2.virtualization_as_a_compliance_strategy",
      "citations": [
        {
          "title": "GNU General Public License",
          "url": "https://en.wikipedia.org/wiki/GNU_General_Public_License",
          "excerpts": [
            "The foundation asserts that an executable that uses a dynamically linked library is indeed a derivative work. This assertion does not, however, apply to ..."
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        },
        {
          "title": "Tainted kernels — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/tainted-kernels.html",
          "excerpts": [
            "Modules without a MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by insmod as GPL compatible are assumed to be proprietary. F if any module ..."
          ]
        },
        {
          "title": "Linux Kernel Licensing Rules and Precedents",
          "url": "https://docs.kernel.org/process/license-rules.html",
          "excerpts": [
            "According to the\nnote in the kernel COPYING file, the syscall interface is a clear boundary,\nwhich does not extend the GPL requirements to any software which uses it to\ncommunicate with the kernel.",
            "Loadable kernel modules also require a `MODULE_LICENSE()` tag."
          ]
        }
      ],
      "reasoning": "The finegrained field value asserts that virtualization can be used as a compliance strategy to keep a new OS legally separate from the Linux kernel and its drivers, thereby avoiding GPLv2 derivative-work obligations. The most directly supportive information among the excerpts concerns the concept of derivative works and how licensing boundaries can apply to code that interacts with the kernel. One excerpt discusses the foundational idea that a dynamically linked executable can be considered a derivative work, which underpins concerns about whether a guest OS or kernel module would be bound by GPL obligations. Related excerpts discuss the boundary rules in the kernel license, including that the syscall interface does not extend GPL requirements to software that communicates with the kernel, and notes that modules without a recognized GPL-compatible license can be tainted or proprietary, which informs how a separation strategy might preserve non-GPL licensing in a hosted or virtualization scenario. Additional excerpts address licensing of kernel components and how certain licensing identifiers and module export mechanisms (such as GPL-tagged exports) influence what constitutes a derivative work or a permissible private/module boundary. Collectively, these excerpts establish the legal scaffolding that virtualization-based separation would rely on to argue that a guest OS can operate under a different license without becoming a derivative work of the host kernel or its drivers, provided the separation is maintained through clearly defined interfaces and boundaries.",
      "confidence": "medium"
    },
    {
      "field": "licensing_implications_of_gplv2.technical_enforcement_mechanisms",
      "citations": [
        {
          "title": "EXPORT_SYMBOL_GPL() include/linux/export.h",
          "url": "https://www.kernel.org/doc./htmldocs/kernel-hacking/sym-exportsymbols-gpl.html",
          "excerpts": [
            "Similar to EXPORT_SYMBOL() except that the symbols exported by EXPORT_SYMBOL_GPL() can only be seen by modules with a MODULE_LICENSE() that specifies a GPL ..."
          ]
        },
        {
          "title": "Tainted kernels — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/tainted-kernels.html",
          "excerpts": [
            "Modules without a MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by insmod as GPL compatible are assumed to be proprietary. F if any module ..."
          ]
        },
        {
          "title": "Re: incorrect taint of ndiswrapper — Linux Kernel",
          "url": "https://lists.linuxcoding.com/kernel/2006-q4/msg08685.html",
          "excerpts": [
            "EXPORT_SYMBOL_GPL is a technical mechanism of enforcing GPL against non-free code, but ndiswrapper is free. The non-free NDIS drivers are not using those ..."
          ]
        },
        {
          "title": "Linux Kernel Licensing Rules and Precedents",
          "url": "https://docs.kernel.org/process/license-rules.html",
          "excerpts": [
            "Loadable kernel modules also require a `MODULE_LICENSE()` tag.",
            "The Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), as provided in LICENSES/preferred/GPL-2.0,\nwith an explicit syscall exception described in\nLICENSES/exceptions/Linux-syscall-note, as described in the COPYING file.",
            "According to the\nnote in the kernel COPYING file, the syscall interface is a clear boundary,\nwhich does not extend the GPL requirements to any software which uses it to\ncommunicate with the kernel."
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        },
        {
          "title": "GNU General Public License",
          "url": "https://en.wikipedia.org/wiki/GNU_General_Public_License",
          "excerpts": [
            "The foundation asserts that an executable that uses a dynamically linked library is indeed a derivative work. This assertion does not, however, apply to ..."
          ]
        },
        {
          "title": "Proprietary kernel modules - the boundary shifts?",
          "url": "https://lwn.net/Articles/13398/",
          "excerpts": [
            "Oct 22, 2002 — Linus Torvalds's ... The _only_ thing that allows for non-GPL modules is copyright law, and in particular the \"derived work\" issue."
          ]
        }
      ],
      "reasoning": "The claim that the Linux kernel community technically enforces the spirit of the GPLv2 through a symbol-export mechanism is supported by the discussion of the EXPORT_SYMBOL_GPL macro and its relation to MODULE_LICENSE. The assertion that only modules with a GPL-compatible license can access exported symbols is evidenced by notes describing how the GPL-compatible licensing is required for resolution of those symbols, and how the tooling and tainting framework reflect licensing enforcement at load time. Additional sources emphasize that the kernel's licensing constructs define a boundary around what constitutes a derivative work and how non-GPL modules interact with core kernel functionality, reinforcing the idea that the technical barrier is implemented via both symbol export controls and licensing declarations in module code. Collectively, these excerpts support the central idea that licensing enforcement is exercised through explicit exported interfaces and licensing declarations, which function as the practical mechanism to deter proprietary modules from accessing core functionality and to delineate derivative-work boundaries.",
      "confidence": "high"
    },
    {
      "field": "feasibility_of_direct_ffi_reuse.legal_and_licensing_barriers",
      "citations": [
        {
          "title": "Linux kernel licensing rules",
          "url": "https://www.kernel.org/doc/html/v4.19/process/license-rules.html",
          "excerpts": [
            "\n\nThe Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), ",
            "The Linux kernel is under GPL-2.0, with individual files compatible with GPL-2.0 and using SPDX identifiers. UAPI files have a special syscall exception."
          ]
        },
        {
          "title": "Linux Kernel Licensing Rules and Precedents",
          "url": "https://docs.kernel.org/process/license-rules.html",
          "excerpts": [
            "The Linux Kernel is provided under the terms of the GNU General Public\nLicense version 2 only (GPL-2.0), as provided in LICENSES/preferred/GPL-2.0,\nwith an explicit syscall exception described in\nLICENSES/exceptions/Linux-syscall-note, as described in the COPYING file.",
            "\n\n> Loadable kernel modules also require a `MODULE_LICENSE()` tag.\nThis tag is\n> neither a replacement for proper source code license information\n> (SPDX-License-Identifier) nor in any way relevant for expressing or\n> determining the exact license under which the source code of the module\n> is provided. > \n> The sole purpose of this tag is to provide sufficient information\n> whether the module is free software or proprietary for the kernel\n> module loader and for user space tools. > \n> The valid license strings for `MODULE_LICENSE()` are:\n> \n> > | “GPL” | ",
            "The Linux kernel requires the precise SPDX identifier in all source files."
          ]
        },
        {
          "title": "FSF: Interpreting, enforcing and changing the GNU GPL, as applied ...",
          "url": "https://lwn.net/Articles/683352/",
          "excerpts": [
            "... linking creates derivative works. The FSF's own FAQ on the GPLv2 also takes this position by stating that linking to a GPL'd work requires ..."
          ]
        },
        {
          "title": "GNU General Public License",
          "url": "https://en.wikipedia.org/wiki/GNU_General_Public_License",
          "excerpts": [
            "The foundation asserts that an executable that uses a dynamically linked library is indeed a derivative work. This assertion does not, however, apply to ..."
          ]
        },
        {
          "title": "Linux Syscall Note (SPDX)",
          "url": "https://spdx.org/licenses/Linux-syscall-note.html",
          "excerpts": [
            "the only valid version of the GPL as far as the kernel\nis concerned is \\_this\\_ particular version of the license (ie v2, not\nv2.2 or v3.x or whatever), unless explicitly otherwise stated. Linus Torvalds"
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule"
          ]
        },
        {
          "title": "5. EXPORT_SYMBOL - Linux Kernel Workbook - Read the Docs",
          "url": "http://lkw.readthedocs.io/en/latest/doc/04_exporting_symbols.html",
          "excerpts": [
            "5.2. Export Symbol¶ · EXPORT_SYMBOL() helps you provide APIs to other modules/code. · The functions which you EXPORT are available to the other modules/code."
          ]
        },
        {
          "title": "Questioning EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/603131/",
          "excerpts": [
            "Jun 23, 2014 — EXPORT_SYMBOL_GPL() is meant to mark kernel interfaces that are deemed to be so low-level and specific to the kernel that any software that uses them must ... It was requested to be added after the fact by the maintainer and by now the discussion has had negative impact - all it would take is printing out the discussion and submitting it as evidence that the author was coerced into making the change and it is quite reasonable to assume that the flag is not a technical indicator, but merely a social statement. "
          ]
        },
        {
          "title": "Tainted kernels — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/tainted-kernels.html",
          "excerpts": [
            "Modules without a MODULE_LICENSE or with a MODULE_LICENSE that is not recognised by insmod as GPL compatible are assumed to be proprietary. F if any module ..."
          ]
        }
      ],
      "reasoning": "The kernel is licensed under GPLv2, with the implication that software that links to or combines with GPLv2-licensed code can be considered a derivative work, potentially requiring the entire combined work to be GPLv2. This sets a high bar for reusing Linux driver code via FFI in a new OS: while FFI can provide a boundary crossing between user space and kernel or driver code, many interpretations treat such linking as creating a derivative work or at least imposing GPLv2 obligations on the combined artifact. Additionally, kernel licensing notes emphasize that MODULE_LICENSE tags help indicate licensing, SPDX identifiers ensure license clarity, and EXPORT_SYMBOL_GPL marks interfaces as GPL-only, which can constrain how external modules or interfaces are exposed and used. Taken together, these points support the view that directly reusing Linux drivers through FFI would invite significant licensing risk and potentially compel the new OS to align with GPLv2 terms, unless architectural boundaries clearly segregate GPL-covered components from the rest of the system or pursue licensing strategies that avoid derivative-linking conclusions. The exposure of these licensing controls (EXPORT_SYMBOL_GPL, MODULE_LICENSE) and the broader derivative-work interpretation reinforce that licensing barriers are real and technically enforceable aspects of the OS design decision about FFI-based driver reuse.",
      "confidence": "high"
    },
    {
      "field": "shim_based_porting_deep_dive.licensing_risks",
      "citations": [
        {
          "title": "FreeBSD LinuxKPI documentation",
          "url": "https://freebsdfoundation.org/blog/sharing-dual-licensed-drivers-between-linux-and-freebsd/",
          "excerpts": [
            "The Linux Kernel “license rules” documentation <https://www.kernel.org/doc/html/v5.0/process/license-rules.html> has explict discussion of dual licensing: “individual files can be provided under a dual license, e.g. one of the compatible GPL variants and alternatively under a permissive license like BSD, MIT etc",
            "The second issue with code reuse between operating systems relates to the interfaces provided by and/or required by the code in question. This may be achieved in several ways; one is to implement core functionality in a reusable library, and augment it with an operating-system-dependent layer. It may be that the core library can be shared between Linux, Windows, and FreeBSD, providing for significant reuse. For the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD",
            "or the specific case of code sharing with FreeBSD, another option is the use of the “linuxkpi” layer. This interface layer provides API-compatible implementations of many Linux kernel interfaces, greatly simplifying the adaptation of Linux driver source to FreeBSD.",
            "**Interface Compatibility**"
          ]
        },
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "GPLv2 licensed Linux code must be rewritten under FreeBSD's 2-clause BSD License.",
            "Jul 5, 2022 — LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification. The drm-kmod package, ...",
            "The drm-kmod package, for example, includes Intel and AMD Linux graphics driver code, but is able to compile and run properly on FreeBSD using LinuxKPI.",
            "there exist FreeBSD specific bugs, performance hits, and missing features as a result of LinuxKPI's small codebase.",
            "This drm-debugfs support is essential to igt-gpu-tools, so my next task was compiling drm-kmod with the DEBUG\\_FS configuration flag enabled",
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification.",
            "Jul 5, 2022 — LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "LinuxKPI - FreeBSD Wiki",
          "url": "https://wiki.freebsd.org/LinuxKPI",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        }
      ],
      "reasoning": "The central claim is that the licensing risk is significant because the Linux kernel is GPLv2, and linking a non-GPL kernel to a GPL-licensed driver—even through a shim—can create a combined or derivative work that could obligate the entire project to be licensed under GPLv2. One excerpt explicitly discusses dual licensing and the explicit GPL-related licensing rules, providing a framework for how licenses interact when components are combined; this supports the core risk assessment of a non-GPL overall kernel policy being endangered by the shim approach. Another excerpt highlights that the LinuxKPI layer exists to enable porting between Linux and FreeBSD, which is relevant because it suggests a defined boundary or compatibility layer that may influence whether a combined-works argument applies; this helps assess whether a shim-based approach clearly avoids derivative-creation or not. A related excerpt notes how interface compatibility and the possibility of sharing core functionality via a reusable library or a platform-dependent layer could affect reuse and licensing boundaries; this adds nuance to how one might structure a port to mitigate licensing risk, by potentially isolating GPL-licensed code from non-GPL components through careful architecture. Other excerpts reinforce the licensing discussion by explaining that Linux code under GPLv2 generally requires derivative-work considerations when integrated with non-GPL components, and by describing how the LinuxKPI project can be less contentious because it facilitates open-source to open-source porting, which may influence risk assessment for a non-GPL-licensed OS kernel in certain porting scenarios. Taken together, these excerpts support the conclusion that the licensing risk is substantial under a shim-based porting strategy, particularly due to the GPLv2 requirements and derivative-work concerns, while there are nuanced architectural strategies (e.g., hard boundaries via LinuxKPI or clean-room reimplementation) that may offer avenues to reduce or manage that risk.",
      "confidence": "high"
    },
    {
      "field": "developer_experience_and_reliability_pipeline.crash_triage_and_telemetry",
      "citations": [
        {
          "title": "KernelCI Documentation",
          "url": "https://docs.kernelci.org/",
          "excerpts": [
            "Welcome to the KernelCI documentation website! *\n\nThe KernelCI project’s mission is to ensure the quality, stability and long-term maintenance of the Linux kernel. To achieve that, we are building an ecosystem to foster collaboration around Linux kernel testing and validation, facilitating the process of executing tests for a given subsystem or the whole kernel and following up on the results, **helping you catch issues early and maintain kernel qua",
            "The upstream Linux kernel test project\nKernelCI[KernelCI](/)"
          ]
        },
        {
          "title": "Fuzzing with cargo-fuzz - Rust Fuzz Book",
          "url": "https://rust-fuzz.github.io/book/cargo-fuzz.html",
          "excerpts": [
            "cargo-fuzz is the recommended tool for fuzz testing Rust code. cargo-fuzz is itself not a fuzzer, but a tool to invoke a fuzzer."
          ]
        },
        {
          "title": "Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation",
          "url": "https://arxiv.org/html/2505.02464v1",
          "excerpts": [
            "However, Rust builds are not reproducible in all use cases and depend on multiple factors, for example, the absolute build path.",
            "To fuzz the target programs, FourFuzz utilizes afl.rs in version 0.15.3 which is based on AFL + + version 4.10c.",
            " \nWe implement partial instrumentation to focus the fuzzing efforts on unsafe Rust code to significantly increase the efficiency and probability of triggering and detecting vulnerabilities in Rust programs.",
            "This is challenging as Rust uses a complex compilation toolchain:\nThis includes multiple intermediate representations and several compiler stages facilitating Rust-specific code analysis.",
            " as MirChecker  [ 42 ] are static analysis frameworks that utilize intermediate code representations to detect different bug type"
          ]
        }
      ],
      "reasoning": "The finegrained field value describes the need for a robust crash triage and telemetry system for automated testing, modeled on established platforms like Syzbot and using centralized dashboards such as SQUAD or KCIDB-ng. The most directly supporting information comes from kernel testing and validation ecosystems described in the excerpts. Specifically, KernelCI is framed as an organization and project whose mission is to ensure quality, stability, and long-term maintenance of the Linux kernel by building an ecosystem to encourage testing, validation, and result tracking across subsystems or the whole kernel. This aligns with the idea of a centralized, end-to-end testing and telemetry workflow that aggregates results from multiple sources and provides visibility into system health and regressions. Additional support comes from the KernelCI documentation highlighting KernelCI as the upstream Linux kernel test project, reinforcing the existence of an established, centralized testing infrastructure as a model to emulate for crash triage and telemetry pipelines. The fuzzing-related excerpts extend this landscape by detailing practical tooling and approaches for testing and fault discovery in Rust code, which informs the technical options for instrumentation and workflow automation in the crash triage/telemetry system when the target OS is developed in Rust or includes Rust components. They discuss cargo-fuzz as a recommended fuzzing tool, and subsequent discussions describe focused instrumentation strategies for unsafe Rust code, acknowledging the need to optimize testing effectiveness and pinpoint crash reproducers. Taken together, these excerpts describe both the high-level architecture pattern (centralized testing, dashboards, and result aggregation) and concrete tooling strategies (fuzzing, partial instrumentation) that could be adapted to implement a robust crash triage and telemetry system modeled after Syzkaller/Syzbot and integrated with a dashboard like SQUAD or KCIDB-ng. Therefore, the most relevant content centers on KernelCI's mission and function as a centralized test-and-trace platform, followed by concrete fuzzing tooling and instrumentation strategies that would support crash triage workflows in a Rust-based environment.",
      "confidence": "high"
    },
    {
      "field": "security_architecture_for_drivers.attack_surface_analysis",
      "citations": [
        {
          "title": "Rust-ready Driver Security and FFI Considerations",
          "url": "https://www.codethink.co.uk/articles/rust-ready/",
          "excerpts": [
            "FFI is inherently unsafe, and so your developers will need to understand and appreciate all of the safety rules before engaging in this kind of development.",
            "This coexistence is made possible primarily because Rust is designed to be a systems programming language and as such has interoperability with C built in.",
            "The Kernel has significant numbers of very well designed interfaces which are explicit about lifetimes of objects etc. permitting reasonably simple, or at least comprehensible, bindings into the Rust world.",
            "These two worlds, safe and unsafe, live together by means of very constrained boundaries at which the assumptions Rust makes can be asserted back to the compiler.",
            "With all of the above, developers can then use Rust's ability to reference symbols in C code (or export symbols with C linkage) by means of Rust's FFI.",
            "As you'd expect, the Rust ecosystem provides tooling to assist with managing all this, there's [`cargo geiger`](https://crates.io/crates/cargo-geiger) which is a tool to determine how much of, and what kind of, unsafe code exists in your software and its dependency tree.",
            "And there's [`cargo miri`](https://github.com/rust-lang/miri) which can actually interpret the intermediate form of Rust within the compiler and can be used to check for undefined behaviour at runtime (typically via your test suite), allowing you to assert the correctness of the second class of `unsafe` keyword usage mentioned above."
          ]
        },
        {
          "title": "Apriorit blog on Interoperability and Unsafe Rust",
          "url": "https://www.apriorit.com/dev-blog/interoperability-unsafe-rust",
          "excerpts": [
            "Rust interoperability allows developers to use the Foreign Function Interface (FFI) to call Rust functions from other languages and call functions written in other languages from Rust."
          ]
        },
        {
          "title": "vfio.txt - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ..."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode."
          ]
        }
      ],
      "reasoning": "The finegrained field value discusses how using a Foreign Function Interface (FFI) to connect C drivers increases the attack surface at the language boundary and requires unsafe Rust code with the programmer responsible for memory safety and data race prevention. It then contrasts this with virtualization approaches where drivers run in isolated environments, reducing the attack surface, and mentions VFIO passthrough as a way to bypass complex device emulation layers, shifting risk to the hypervisor and a smaller, auditable virtio interface. The most relevant excerpts directly address these contrasts and the mechanisms involved: - FFI is inherently unsafe and demands adherence to safety rules, making developers accountable for safety. - Rust's interoperability with C and the explicit lifetime-aware kernel interfaces enable bindings but still require careful boundary management. - The coexistence of safe and unsafe code is managed by constrained boundaries, enabling explicit assertions back to the compiler. - Rust can reference or export symbols via FFI, enabling cross-language calls, which ties directly to the attack-surface implications of FFI. - Tooling like cargo-geiger and cargo-miri help quantify and verify unsafe usage, which is pertinent to managing the attack surface. - VFIO provides a framework to expose direct device access to userspace within an IOMMU-protected environment, alleviating the need for fragile emulation layers. - VFIO passthrough and the virtio interface shift the remaining risk to the hypervisor, a smaller and more auditable surface than a full driver ecosystem. These points collectively map to the claim that FFI-based driver strategies expand the attack surface at the language boundary, while virtualization-based approaches (including VFIO) reduce it by isolating devices and using well-defined interfaces.",
      "confidence": "high"
    },
    {
      "field": "security_architecture_for_drivers.isolation_strategies",
      "citations": [
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "vfio.txt - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ..."
          ]
        }
      ],
      "reasoning": "The described security architecture advocates strong isolation of drivers from the kernel, including moving drivers into sandboxed user-space processes and, in the strongest form, running each driver in its own lightweight virtual machine. Excerpts that mention user-mode device drivers and forwarding or handling device access from user space support this approach, such as the idea that seL4 can \"map device memory to drivers and forward IRQs as (asynchronous) messages\" and that a system can \"run all device drivers in user mode.\" These statements align with shifting drivers out of the kernel and into isolated user-space contexts. The VFIO excerpts reinforce a hardware-assisted isolation pathway: VFIO is described as \"an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment,\" which provides a concrete mechanism to grant user-space drivers safe and controlled access to devices. This matches the proposed architecture where isolation is achieved through hardware-assisted mediation between drivers and devices, rather than untrusted in-kernel driver execution. Additional VFIO-focused detail, such as VFIO being the mechanism to bind devices to a VFIO driver before operations can proceed, further corroborates the practical steps toward secure user-space driver execution. Taken together, these excerpts support the core claim that modern OS security relies on strong isolation by moving drivers out of the kernel and leveraging VFIO-enabled user-space access, with the strongest form being Driver VMs that encapsulate each driver in its own VM for containment beyond what user-space isolation alone can achieve.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.0",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer.",
            "swiotlb also provides “sync” APIs that correspond to the dma\\_sync\\_\\*() APIs that\na driver may use when control of a buffer transitions between the CPU and the\ndevice."
          ]
        }
      ],
      "reasoning": "The most directly relevant information comes from passages that discuss interoperability between Rust and existing C driver code, particularly through Foreign Function Interfaces (FFI). This aligns with the finegrained field value's emphasis on leveraging Linux drivers via rewriting/adaptation rather than starting from scratch, highlighting how Rust can bridge with C code and how such interoperability would be arranged. Additional highly relevant material notes that Rust's memory safety and security advantages motivate its use in kernel development, supporting the security and reliability claims of the field value. The excerpts that describe the need for DMA-related abstractions and the kernel's DMA-mapping subsystem are relevant because any strategy that reuses Linux drivers must contend with how DMA is set up, mapped, and unmapped, which are critical architectural aspects when porting or adapting drivers. The referenced articles also touch on the complexity and maintenance burden associated with reusing or adapting Linux drivers, which maps to the field value's statements about complexity, time to first boot, and the ongoing maintenance load. Collectively, these excerpts support assertions about feasible integration points (FFI, interop with C), performance expectations, security benefits, and the practical challenges (DMA handling, API stability, maintenance) inherent in this strategy.",
      "confidence": "high"
    },
    {
      "field": "security_architecture_for_drivers.threat_model",
      "citations": [
        {
          "title": "vfio.txt - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ..."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Rust-ready Driver Security and FFI Considerations",
          "url": "https://www.codethink.co.uk/articles/rust-ready/",
          "excerpts": [
            "FFI is inherently unsafe, and so your developers will need to understand and appreciate all of the safety rules before engaging in this kind of development.",
            "This coexistence is made possible primarily because Rust is designed to be a systems programming language and as such has interoperability with C built in.",
            "The Kernel has significant numbers of very well designed interfaces which are explicit about lifetimes of objects etc. permitting reasonably simple, or at least comprehensible, bindings into the Rust world.",
            "These two worlds, safe and unsafe, live together by means of very constrained boundaries at which the assumptions Rust makes can be asserted back to the compiler.",
            "With all of the above, developers can then use Rust's ability to reference symbols in C code (or export symbols with C linkage) by means of Rust's FFI.",
            "As you'd expect, the Rust ecosystem provides tooling to assist with managing all this, there's [`cargo geiger`](https://crates.io/crates/cargo-geiger) which is a tool to determine how much of, and what kind of, unsafe code exists in your software and its dependency tree.",
            "And there's [`cargo miri`](https://github.com/rust-lang/miri) which can actually interpret the intermediate form of Rust within the compiler and can be used to check for undefined behaviour at runtime (typically via your test suite), allowing you to assert the correctness of the second class of `unsafe` keyword usage mentioned above."
          ]
        },
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode."
          ]
        },
        {
          "title": "IMA and EVM overview (Yocto/Yocto-related writeup)",
          "url": "https://ejaaskel.dev/yocto-hardening-ima-and-evm/",
          "excerpts": [
            "IMA is an acronym for integrity measurement architecture. It is a security subsystem in Linux that can be used to ensure the system’s integrity by measuring and appraising files before they are accessed or executed.",
            "The measurement consists of creating hashes of the files in the system and storing the hashes in the measurement list.",
            "IMA appraisal on the other hand is a kind of a Linux extension to the secure boot, where the file usage may be denied if their content doesn’t match the expected values.",
            "In addition to these two options, you most likely want to add `CONFIG_AUDIT=y` so that the IMA appraisal violations get logged to `dmesg`."
          ]
        },
        {
          "title": "Apriorit blog on Interoperability and Unsafe Rust",
          "url": "https://www.apriorit.com/dev-blog/interoperability-unsafe-rust",
          "excerpts": [
            "Rust interoperability allows developers to use the Foreign Function Interface (FFI) to call Rust functions from other languages and call functions written in other languages from Rust."
          ]
        }
      ],
      "reasoning": "Directly relevant excerpts describe how IOMMU-based isolation and VFIO frameworks enable secure device access from userspace, which maps to mitigating DMA attacks and privilege boundary risks in the threat model. For example, VFIO discussions emphasize exposing direct device access to userspace in a secure, IOMMU-protected environment, which directly addresses DMA-based threats by enforcing memory access controls through hardware isolation. A complementary excerpt about the VFIO documentation reinforces this isolation model by describing the same IOMMU-bound framework. Together, these excerpts underpin the architectural choice to place drivers behind isolation boundaries to reduce the risk of arbitrary memory access and privilege escalation due to DMA or direct hardware access. Additional excerpts focus on the interoperability and safety implications of using Foreign Function Interfaces (FFI) with Rust to integrate with C or kernel interfaces. They acknowledge that FFI is inherently unsafe and requires careful discipline and tooling to manage safety boundaries, which aligns with the threat of memory corruption and undefined behavior stemming from unsafe cross-language boundaries. Other excerpts discuss how Rust's design supports safe/unsafe separation and the mechanisms (like explicit lifetimes and C linkage) that help manage these risks, which informs the architectural decision to delineate safe and unsafe regions and to provide tooling to audit unsafe usage. Some content also references integrity and auditing concepts (IMA), which links to maintaining driver integrity and tamper resistance, contributing to the broader threat model by offering a mechanism to detect or log integrity violations, though it is less directly tied to the core DMA/memory-safety threats. Finally, the seL4-related excerpts mentioning user-mode drivers illustrate architectural options for running drivers in user space, which can influence the threat surface by changing where enforcement and isolation boundaries live, though they do not enumerate the threats themselves. Collectively, the strongest support comes from the explicit description of IOMMU/VFIO isolation for DMA protection, followed by the explicit warnings and guidance about safe/unsafe boundaries in FFI usage and Rust-C integration, and then by architectural options for user-mode drivers that affect the threat surface.",
      "confidence": "high"
    },
    {
      "field": "security_architecture_for_drivers.key_defenses",
      "citations": [
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "vfio.txt - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ..."
          ]
        },
        {
          "title": "IMA and EVM overview (Yocto/Yocto-related writeup)",
          "url": "https://ejaaskel.dev/yocto-hardening-ima-and-evm/",
          "excerpts": [
            "IMA appraisal on the other hand is a kind of a Linux extension to the secure boot, where the file usage may be denied if their content doesn’t match the expected values.",
            "IMA is an acronym for integrity measurement architecture. It is a security subsystem in Linux that can be used to ensure the system’s integrity by measuring and appraising files before they are accessed or executed."
          ]
        },
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode."
          ]
        }
      ],
      "reasoning": "The most relevant material establishes that exposing direct device access should occur within an IOMMU-protected environment, which aligns with the primary hardware defense against DMA attacks. It also shows that a secure driver boundary can be maintained through a framework that isolates device access and enforces security boundaries at the hardware level. In parallel, the material describing integrity measurement and attestation explains how runtime and boot-time integrity can be preserved: IMA measures and appraises files, the measurement list is populated to support integrity checks, and there is guidance about enabling audit logging to capture tampering events. Additionally, the discussion of a measured boot approach using a TPM provides the chain-of-trust starting point, ensuring an immutable log of boot components, which complements runtime integrity checks. Finally, references to running drivers in user space (as with seL4) illustrate a security model where drivers operate under constrained, verifiable boundaries rather than unchecked kernel access, reinforcing a defense-in-depth approach that combines hardware protections with software integrity mechanisms.",
      "confidence": "high"
    },
    {
      "field": "executive_summary.key_findings",
      "citations": [
        {
          "title": "The Linux kernel doesn't provide a stable ABI for modules so they ...",
          "url": "https://news.ycombinator.com/item?id=21243406",
          "excerpts": [
            "Apr 10, 2021 — The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to rebuild ...",
            "The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to ..."
          ]
        },
        {
          "title": "Linux ABI description",
          "url": "https://docs.kernel.org/admin-guide/abi.html",
          "excerpts": [
            " This directory documents the interfaces that the developer has\n>     defined to be stable. Userspace programs are free to use these\n>     interfaces with no restrictions, and backward compatibility for\n>     them will be guaranteed for at least 2 years. Most interfaces\n>     (like syscalls) are expected to never change",
            "We have four different levels of ABI stability, as shown by the four\ndifferent subdirectories in this location. Interfaces may change levels\nof stability according to the rules described below."
          ]
        },
        {
          "title": "LinuxKPI - FreeBSD Wiki",
          "url": "https://wiki.freebsd.org/LinuxKPI",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "The proper use of EXPORT_SYMBOL_GPL() - LWN.net",
          "url": "https://lwn.net/Articles/769471/",
          "excerpts": [
            "In the case of EXPORT_SYMBOL_GPL(), only modules that declare a GPL-compatible license will be able to see the symbol.",
            "Oct 27, 2018 — In the case of EXPORT_SYMBOL_GPL(), only modules that declare a GPL-compatible license will be able to see the symbol."
          ]
        }
      ],
      "reasoning": "Direct reuse of Linux kernel drivers via FFI hinges on two core risks: (a) the kernel has no stable ABI for modules, meaning drivers must be recompiled for each kernel version; and (b) integrating Linux drivers into a new OS could create a derivative work under GPLv2. The excerpts clearly support these points: the claim that the Linux kernel does not provide a stable ABI for modules and thus requires frequent recompilation is supported by statements noting instability and the need to recompile; while also noting that some workarounds or mitigations exist (e.g., compatibility layers). Additionally, licensing discussions indicate that exporting GPLv2-licensed kernel interfaces imposes licensing obligations on derivative work, reinforcing the legal infeasibility of a straightforward FFI-based reuse. Beyond these, there are mentions of defined interfaces that are intended to be stable for a period (interfaces may be stable or not), and that certain stability guarantees apply to selected interfaces, which further complicate relying on a static FFI boundary. As viable alternatives, the excerpts point to LinuxKPI-like compatibility layers that allow running Linux drivers in another OS context with limited modification, aligning with the idea of avoiding direct derivative-work integration while still leveraging Linux driver functionality through a compatibility approach rather than direct FFI-based coupling.",
      "confidence": "high"
    },
    {
      "field": "executive_summary.primary_recommendation",
      "citations": [
        {
          "title": "Linux in-kernel vs out-of-kernel drivers and plug and play ...",
          "url": "https://www.reddit.com/r/linuxhardware/comments/182uaw7/linux_inkernel_vs_outofkernel_drivers_and_plug/",
          "excerpts": [
            "Linux in-kernel drivers are preferable over out-of-kernel drivers for most users and use cases as problems with locating, installing and maintaining drivers ..."
          ]
        },
        {
          "title": "LinuxKPI - FreeBSD Wiki",
          "url": "https://wiki.freebsd.org/LinuxKPI",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "The Linux kernel doesn't provide a stable ABI for modules so they ...",
          "url": "https://news.ycombinator.com/item?id=21243406",
          "excerpts": [
            "Apr 10, 2021 — The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to rebuild ..."
          ]
        },
        {
          "title": "Feasibility of adding drivers to linux kernel",
          "url": "https://softwareengineering.stackexchange.com/questions/423138/feasibility-of-adding-drivers-to-linux-kernel",
          "excerpts": [
            "Mar 8, 2021 — I want to ask how much feasible is to add tons of device driver to Linux kernel , suppose after 30 -40 years the size of Linux kernel will be ..."
          ]
        },
        {
          "title": "Linux ABI description",
          "url": "https://docs.kernel.org/admin-guide/abi.html",
          "excerpts": [
            " This directory documents the interfaces that the developer has\n>     defined to be stable. Userspace programs are free to use these\n>     interfaces with no restrictions, and backward compatibility for\n>     them will be guaranteed for at least 2 years. Most interfaces\n>     (like syscalls) are expected to never change"
          ]
        }
      ],
      "reasoning": "The highly recommended phased, hybrid architecture described in the fine-grained field value emphasizes an initial virtualization-based path, reuse of a mature host (via virtualization), and eventual native driver development with a focus on user-space drivers for performance-critical devices. Excerpts that discuss the placement and stability of drivers are most directly aligned with these architectural considerations. In particular, the discussion of Linux in-kernel vs out-of-kernel drivers highlights the tradeoffs between where drivers reside and how they are managed, which informs decisions about reusing or replacing driver code in a new OS design. The LinuxKPI excerpt describes a compatibility layer that allows Linux drivers to operate within another environment with minor modifications, which directly supports the idea of leveraging existing driver ecosystems as part of an incremental strategy rather than reimplementing everything from scratch. The discussions about ABI stability and the need for recompiled modules (and related workarounds like DKMS) underscore the practical challenges of driver portability and the evolution of interfaces, relevant to planning a phased path where early virtualization and compatibility layers reduce risk and accelerate progress. The feasibility-oriented excerpt about adding drivers to the Linux kernel touches on the complexity of driver integration and may inform the difficulty of bridging to native Rust driver implementations later. The remaining excerpts provide broader context about symbol export and ABI discussions but are less directly tied to the architecture-first, virtualization-centric strategy described in the target field value. ",
      "confidence": "medium"
    },
    {
      "field": "native_and_userspace_driver_architectures.userspace_drivers",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "FFI - The Rustonomicon",
          "url": "https://doc.rust-lang.org/nomicon/ffi.html",
          "excerpts": [
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped with unsafe {} as a promise to the compiler that everything contained within ...",
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped\nwith `unsafe {}` as a promise to the compiler that everything contained within\ntruly is saf",
            "Unsafe blocks isolate unsafety and are a promise to\nthe compiler that the unsafety does not leak out of the block.",
            "When declaring the argument types to a foreign function, the Rust compiler\ncannot check if the declaration is correct, so specifying it correctly is part\nof keeping the binding correct at runtime.",
            "The raw C API needs to be wrapped to provide memory safety and make use of higher-level concepts\nlike vectors."
          ]
        },
        {
          "title": "rust-for-linux kernel crate documentation",
          "url": "https://rust-for-linux.github.io/docs/kernel/",
          "excerpts": [
            "Common clock framework. [cred](cred/index.html \"kernel::cred mod\")"
          ]
        },
        {
          "title": "Rust — The Linux Kernel documentation",
          "url": "https://docs.kernel.org/rust/index.html",
          "excerpts": [
            "Documentation related to Rust within the kernel. To start using Rust in the kernel, please read the Quick Start guide."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a modern driver architecture where drivers run as isolated user-space processes with strong fault isolation and use a framework like VFIO for device DMA confinement, as well as performance-oriented approaches that bypass the kernel. To support this, information about interoperation between Rust and C via FFI is highly relevant, since user-space drivers would need a robust boundary between Rust code and existing C driver interfaces or kernel components. The excerpt about enabling Rust to work alongside C through an FFI bridge directly informs how such interoperation could be realized. Additionally, guidance from the Rustonomicon on FFI emphasizes that foreign functions are unsafe by default and require explicit unsafe blocks, which is crucial for maintaining safety guarantees when crossing process or language boundaries in a driver architecture. Further detail on wrapping the raw C API to provide memory safety and to leverage high-level abstractions (like vectors) helps explain how a user-space driver layer could safely interact with low-level device interfaces. The notes on unsafe blocks acting as a containment boundary and ensuring the binding correctness at runtime reinforce how a driver boundary might be engineered to avoid leaks and maintain stability. Finally, discussions on correctly declaring argument types for foreign functions highlight the precision required when building such boundaries, which is essential for reliable user-space driver implementations or bindings in a mixed Rust/C environment. Together, these excerpts illustrate the engineering considerations (FFI boundaries, safety wrappers, and correct declarations) that would underpin a user-space driver architecture that uses existing C kernel interfaces or VFIO-like mechanisms.",
      "confidence": "medium"
    },
    {
      "field": "native_and_userspace_driver_architectures.native_in_kernel_drivers",
      "citations": [
        {
          "title": "rust-for-linux kernel crate documentation",
          "url": "https://rust-for-linux.github.io/docs/kernel/",
          "excerpts": [
            "Common clock framework. [cred](cred/index.html \"kernel::cred mod\")"
          ]
        },
        {
          "title": "Rust — The Linux Kernel documentation",
          "url": "https://docs.kernel.org/rust/index.html",
          "excerpts": [
            "Documentation related to Rust within the kernel. To start using Rust in the kernel, please read the Quick Start guide."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "FFI - The Rustonomicon",
          "url": "https://doc.rust-lang.org/nomicon/ffi.html",
          "excerpts": [
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped with unsafe {} as a promise to the compiler that everything contained within ...",
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped\nwith `unsafe {}` as a promise to the compiler that everything contained within\ntruly is saf",
            "The raw C API needs to be wrapped to provide memory safety and make use of higher-level concepts\nlike vectors.",
            "Unsafe blocks isolate unsafety and are a promise to\nthe compiler that the unsafety does not leak out of the block.",
            "When declaring the argument types to a foreign function, the Rust compiler\ncannot check if the declaration is correct, so specifying it correctly is part\nof keeping the binding correct at runtime."
          ]
        }
      ],
      "reasoning": "The field value centers on adopting a Rust-centric driver strategy within the kernel, leveraging the Rust-for-Linux model where Rust and C coexist. Documentation on Rust in the kernel provides foundational context for starting Rust in kernel space, which supports the feasibility of integrating Rust-based drivers into the kernel. The discussion on Linux driver development with Rust explicitly notes bridging Rust and C via FFI, highlighting a key mechanism for interoperability and the associated challenges that arise at the boundary. The kernel crate documentation for Rust for Linux reinforces the practical model and tooling available for kernel development in Rust, underscoring that such an integration is actively supported and considered in current projects. The Rustonomicon excerpts about FFI describe how foreign functions are unsafe by default and require wrapping, which directly supports the field value's point about potential unsafe bugs at the FFI boundary and the need for careful design (e.g., unsafe blocks, memory safety abstractions, and safe wrappers around raw C interfaces). Additional FFI-focused notes emphasize that the raw C API needs to be wrapped to provide higher-level safety concepts, aligning with the field value's claim about complexity and the importance of safe, high-level abstractions when interfacing with C. Collectively, these excerpts corroborate the core claims: the Rust-driven driver strategy is viable and modeled after proven approaches, but it brings significant complexity, safety guarantees, and careful FFI handling to prevent vulnerabilities while aiming for robust, secure native drivers integrated into the kernel.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.3",
      "citations": [
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ..."
          ]
        },
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr"
          ]
        },
        {
          "title": "OPW, Linux: The block I/O layer, part 2 - The request interface",
          "url": "http://ari-ava.blogspot.com/2014/06/opw-linux-block-io-layer-part-2-request.html",
          "excerpts": [
            "Jun 27, 2014 — The elv_merge() function involves invoking one of the elevator hooks (elevator_allow_merge_fn) to ask if a bio and a request can be merged; it ..."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region.",
            "16\\.2. How is an I/O Virtual Address (IOVA) generated?",
            "When we turn on DMA translation, DMA to those regions will fail.",
            "OS is expected to setup\nunity mappings for these regions for these devices to access these regions."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb also provides “sync” APIs that correspond to the dma\\_sync\\_\\*() APIs that\na driver may use when control of a buffer transitions between the CPU and the\ndevice.",
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer."
          ]
        },
        {
          "title": "iommu.rst",
          "url": "https://www.kernel.org/doc/Documentation/userspace-api/iommu.rst",
          "excerpts": [
            "The primary use cases are guest Shared Virtual Address (SVA) and guest IO virtual address (IOVA), wherein the vIOMMU implementation relies on the physical IOMMU ..."
          ]
        },
        {
          "title": "LKML Discussion: DMA API and IOMMU (March 6, 2025)",
          "url": "https://lkml.org/lkml/2025/3/6/1236",
          "excerpts": [
            "There is a lot of state tied to the struct device lifecycle that the DMA API and iommu implicitly manages."
          ]
        },
        {
          "title": "Unsafe in Rust: Syntactic Patterns",
          "url": "https://cs.stanford.edu/~aozdemir/blog/unsafe-rust-syntax/",
          "excerpts": [
            "This post takes a first step in that direction by laying the basis for syntactic analyses of unsafe in Rust code hosted on crates.io."
          ]
        },
        {
          "title": "Software Sandboxing Basics",
          "url": "https://blog.emilua.org/2025/01/12/software-sandboxing-basics/",
          "excerpts": [
            "Fortunately there’s a well researched model that solves\nthis problem for us: capability-based security.",
            "There’s even a programming\nlanguage based on the actor model and capability-based security:\n[the Pony programming language",
            "A capability isn’t only a reference\nto a resource, but the associated access rights as well. Owning a capability is\nthe same as also having access rights to perform actions",
            "Now that we have messaging solved with the actor model, let’s jump into\nsandboxing (security models) again.",
            ")*” is\none of the most well known phrases within the UNIX culture. If we can send file\ndescriptors then we have a really broad range of resources that we can work with\nfrom sandboxed processes.",
            "The initial versions of the contributed\npatchset were rejected due to the use of the syscall `write` in operations\nthat’d use the credentials of the calling process for permission\nchecks",
            "It’s important to notice that kernel developers will respect the convention\nwhether suid binaries are allowed on our Linux distro or not."
          ]
        },
        {
          "title": "Capsicum, Fuchsia, seL4, and Linux Sandboxing — OCAP-related discussion",
          "url": "https://github.com/dckc/awesome-ocap",
          "excerpts": [
            "Capsicum Capsicum is a lightweight OS capability and sandbox framework that extends the POSIX API, providing several new OS primitives to support object ...",
            "t's based on a\nmicrokernel, Zircon ,\nwith a capability security model. * 2021-06 cr0 blog: A few thoughts on Fuchsia security Julien Tinnes. Sandboxing is trivial. In fact a new process with access to no capabilities",
            "seL4 is the world's first\noperating-system kernel with an end-to-end proof of implementation\ncorrectness and security enforcement; it is available as open\nsource.",
            "2024-03: Fuchsia OS powers all Nest Hub devices – Fuchsia's capability-based component framework is now deployed across all Nest Hub models, replacing the ..."
          ]
        },
        {
          "title": "Capability-based security",
          "url": "https://en.wikipedia.org/wiki/Capability-based_security",
          "excerpts": [
            "Capability-based security refers to the principle of designing user programs such that they directly share capabilities with each other.",
            "Notable research and commercial systems employing capability-based security include the following:",
            "* [FreeBSD](/wiki/FreeBSD \"FreeBSD\") [Capsicum](/wiki/Capsicum_\\(Unix\\) \"Capsicum \\\\(Unix\\\\)\")",
            "* [Genode](/wiki/Genode \"Genode\")",
            "* [Fuchsia](/wiki/Fuchsia_\\(operating_system\\) \"Fuchsia \\\\(operating system\\\\)\")",
            "* [HarmonyOS](/wiki/HarmonyOS \"HarmonyOS\") ( [OpenHarmony](/wiki/OpenHarmony \"OpenHarmony\") )",
            "* [RedoxOS](/wiki/RedoxOS \"RedoxOS\")",
            "* [L4 microkernel family](/wiki/L4_microkernel_family \"L4 microkernel family\") :\n      + OKL4 from Open Kernel Labs\n      + [SeL4](/wiki/SeL4 \"SeL4\") from NICTA"
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment."
          ]
        }
      ],
      "reasoning": "The most helpful evidence directly discusses the core tradeoffs of running drivers in user space and using interoperability mechanisms (FFI) to connect with existing kernel drivers or abstractions. This supports the idea that a Native/User-space Drivers strategy can achieve higher performance in certain workloads by bypassing kernel paths, while still enabling secure and isolated operation through sandboxing and capability/IOMMU-based protections. Concrete mentions of improving security and reliability when using Rust for drivers, and the ability to interoperate with existing C/C++ driver code via FFI, align with the field's claims about architecture choices, performance potential, and security posture. Other excerpts illustrate practical user-space driver ecosystems and abstractions (e.g., virtio-based user-space devices and bindings) that reinforce the feasibility and design implications of such a strategy. In contrast, references focusing primarily on kernel DMA/IOMMU specifics provide necessary constraints but do not directly assert the strategic value of native vs user-space drivers; they serve to contextualize the technical feasibility rather than to advocate the strategy itself.",
      "confidence": "medium"
    },
    {
      "field": "storage_stack_strategy.filesystem_and_consistency",
      "citations": [
        {
          "title": "SquirrelFS: Rust-native PM filesystem with crash-consistency",
          "url": "https://www.usenix.org/system/files/osdi24_slides-leblanc.pdf",
          "excerpts": [
            "Static checks rely only on existing Rust features",
            "Uses the typestate pattern to statically check ordering of durable updates",
            "Atomic metadata-related system calls (including rename)"
          ]
        },
        {
          "title": "SquirrelFS (OSDI 2024)",
          "url": "https://github.com/utsaslab/squirrelfs",
          "excerpts": [
            "SquirrelFS is a file system for persistent memory (PM) written in Rust that uses soft updates for crash consistency. It uses Rust support for the typestate ..."
          ]
        }
      ],
      "reasoning": "The most directly supportive content describes a Rust-native PM filesystem that emphasizes crash-consistency, i.e., a filesystem implemented in Rust designed to ensure durability and ordering guarantees. This aligns with the proposed approach of building a new Rust-native filesystem with provable crash consistency. Additionally, a discussion of using the typestate pattern to statically check the ordering of durable updates provides a concrete mechanism to enforce invariants at compile time, which directly reinforces the notion of compile-time crash-consistency guarantees in the filesystem design. Further supporting material discusses SquirrelFS as a Rust-based filesystem for persistent memory and the idea of crash-consistency through durable updates, which offers practical architectural inspiration and patterns to adapt. Lastly, notes on atomic metadata-related system calls (such as rename) relate to ensuring atomicity and consistency in metadata operations, which are important components of a crash-safe filesystem design, albeit more tangentially than the core Rust-native and typestate discussion. Together, these excerpts collectively support the concept of a Rust-native filesystem with strong crash-consistency guarantees inspired by SquirrelFS and reinforced by compile-time invariants.\n",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.1",
      "citations": [
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        }
      ],
      "reasoning": "The core field value describes a Shim-based Porting approach that uses C-to-Rust FFI. The most directly supportive material notes that interoperability between Rust and existing C driver code can be enabled via FFI, making incremental adoption feasible. This aligns with the idea of a shim that exposes or translates the C driver interface to Rust, enabling integration without a full rewrite. Additionally, discussions that emphasize memory safety advantages of Rust alongside the need to bridge to existing C code through FFI reinforce the expected trade-offs noted in the field value: improved safety, but a potentially complex shim surface due to unsafe regions. Concrete statements about interoperability via FFI show that Rust can work with C driver code, which underpins the feasibility of a shim approach. Examples from Windows driver development highlight practical, real-world use of FFI to connect Rust with C/C++ driver code, illustrating both the benefits and the acceptance of FFI in driver architectures. Other excerpts emphasize that while FFI can deliver security and reliability benefits through Rust's memory safety, the shim itself introduces an attack surface and maintenance burden, especially as it tightly couples to the underlying C driver and its API. Taken together, these excerpts directly support the notion that a Shim-based C-to-Rust port is a pragmatic but complex path, with clear implications for performance overhead, safety considerations, licensing (due to the combined nature of code), and ongoing maintenance needs. The presence of examples detailing interoperability through FFI and the practical Windows case strengthens the assessment that such a strategy is feasible and widely discussed, while also highlighting the nuanced trade-offs described in the field value.",
      "confidence": "high"
    },
    {
      "field": "hardware_discovery_and_configuration_layer.x86_64_platform_bringup",
      "citations": [
        {
          "title": "ACPI and UEFI Specifications (excerpt)",
          "url": "https://uefi.org/specs/ACPI/6.5/05_ACPI_Software_Programming_Model.html",
          "excerpts": [
            "The [Root System Description Pointer (RSDP)]() structure is located in the\nsystem’s memory address space and is setup by the platform firmw",
            "This structure contains the address of the [Extended System Description Table (XSDT)]() ,\nwhich references other description tables that provide\ndata to OSPM, supplying it with knowledge of the base system’s\nimplementation and configur"
          ]
        },
        {
          "title": "ACPI/UEFI Specification (ACPI 6.5) and related references",
          "url": "https://uefi.org/sites/default/files/resources/ACPI_Spec_6_5_Aug29.pdf",
          "excerpts": [
            "Root System Description Pointer (RSDP) Structure",
            "Advanced Configuration and Power Interface (ACPI) Specification, Release 6.5",
            ".2.26.5 SMBIOS Management Information Structur"
          ]
        },
        {
          "title": "PCI Express - OSDev Wiki",
          "url": "https://wiki.osdev.org/PCI_Express",
          "excerpts": [
            "On x86 and x64 platforms, the address of each memory area is determined by the ACPI 'MCFG' table.",
            "the address of each memory area is determined by the ACPI 'MCFG' table.",
            "The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes. This extended configuration space cannot be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC).",
            "The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes. This extended configuration space cannot be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC). Instead, an Enhanced Configuration Mechanism is provided.",
            "To access a specific register within a device's PCI configuration space, you have to use the device's PCI Segment Group and bus to determine which memory mapped PCI configuration space area to use, and obtain the starting physical address and starting bus number for that memory mapped area.",
            "Physical_Address = MMIO_Starting_Physical_Address + ((Bus) << 20 | Device << 15 | Function << 12)",
            "The enhanced configuration mechanism makes use of memory mapped address space range/s to access PCI configuration space.",
            "or non-x86 systems the method varies, but usually systems provide themselves with a Devicetree which is parsed at runtime.",
            "Enhanced Configuration Mechanism"
          ]
        },
        {
          "title": "Device Tree Basics",
          "url": "https://devicetree-specification.readthedocs.io/en/latest/chapter2-devicetree-basics.html",
          "excerpts": [
            "The dma-ranges property is used to describe the direct memory access (DMA) structure of a memory-mapped bus whose devicetree parent can be accessed from DMA ... For\nexample, the architecture of PCI enables a client to probe and detect\nattached devices, and thus devicetree nodes describing PCI devices\nmight not be required. However, a device node is required to describe a\nPCI host bridge device in the system if it cannot be detected by\nprobing. **Example**\n\n[Fig. 2.1]() shows an example representation of a\nsimple devicetree that is nearly\ncomplete enough to boot a simple operating system, with the platform\ntype, CPU, memory and a single UART described. Device nodes are shown\nwith properties and values inside each node. !",
            "Description:\n\n> The *reg* property describes the address of the device’s resources\n> within the address space defined by its parent bus. Most commonly this\n> means the offsets and lengths of memory-mapped IO register blocks, but\n> may have a different meaning on some bus types. Addresses in the address\n> space defined by the root node are CPU real addresses. >\n> The value is a *<prop-encoded-array>*, composed of an arbitrary number\n> of pairs of address and length, *<address length>*. The number of\n> *<u32>* cells required to specify the address and length are\n> bus-specific and are specified by the ** and **\n> properties in the parent of the device node. If the parent node\n> specifies a value of 0 for **, the length field in the value\n> of *reg* shall be omitted.\n",
            "Property name: `ranges`",
            "Value type: `<empty>` or `<prop-encoded-array>` encoded as an arbitrary number of\n(*child-bus-address*, *parent-bus-address*, *length*) triplets. Description:\n\n> The *ranges* property provides a means of defining a mapping or\n> translation between the address space of the bus (the child address\n> space) and the address space of the bus node’s parent (the parent\n> address space). >\n> The format of the value of the *ranges* property is an arbitrary number\n> of triplets of (*child-bus-address*, *parent-bus-address*, *length*)\n>\n> * The *child-bus-address* is a physical address within the child bus’\n>   address spa",
            "Property name: `reg`"
          ]
        },
        {
          "title": "OSDev PCI Enumeration and Firmware Interfaces",
          "url": "https://wiki.osdev.org/PCI",
          "excerpts": [
            "Enumerating PCI Buses. There are 3 ways to enumerate devices on PCI buses ... However, support for them is mandatory in PCIe devices, so you can be sure ...",
            "The first step for the recursive scan is to implement a function that scans one bus. Pseudo-code might look like this:\nvoid checkBus ( uint8_t bus ) { uint8_t device ; for ( device = 0 ; device < 32 ; device ++ ) { checkDevice ( bus , device ); } "
          ]
        },
        {
          "title": "Retry deferred probe at runtime (device tree) - driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/78863378/retry-deferred-probe-at-runtime-device-tree",
          "excerpts": [
            "I've developed a device tree overlay to describe some pieces of my embedded system. The system is comprised of a Raspberry Pi, a separate custom circuit board ..."
          ]
        }
      ],
      "reasoning": "The bring-up description relies on locating core firmware interfaces and tables that the OS loader uses to discover hardware. A direct reference to the Root System Description Pointer (RSDP) establishes the starting point in the EFI Configuration Table for ACPI data, which is exactly where the OS would begin parsing ACPI/UEFI information. The associated reference to the Extended System Description Table (XSDT) shows the mechanism by which the OS finds pointers to other ACPI data structures, aligning with the need to enumerate tables such as MADT, FADT, and MCFG during bring-up. The mention of MADT indicates enumeration of interrupt controllers, which is a key piece of hardware bring-up. The explicit inclusion of MCFG (PCI Express Memory Mapped Configuration Table) points to the base address for the ECAM region used for PCIe configuration, which is essential for PCIe access during bring-up. The FADT is another foundational ACPI table referenced in the field value, signifying power-management and platform control data that the OS would parse early in initialization. SMBIOS tables, retrieved via the EFI Configuration Table, provide static hardware information such as manufacturer and memory configuration, which the OS often needs during early bring-up to present hardware info and tailor initialization. Collectively, these elements map directly to the hardware discovery and configuration responsibilities of a modern OS bring-up on x86_64, where the kernel relies on ACPI/UEFI data structures and PCIe configuration mechanisms to understand and initialize the platform. The excerpts that discuss the RSDP, XSDT, and these specific ACPI tables, as well as MCFG for PCIe ECAM, directly support the required field value. Related excerpts that discuss the broader ACPI/UEFI specifications and devicetree contexts provide reinforcing context but are not as central to the exact bring-up primitives listed in the field value, while PCIe configuration mechanism discussions support the ongoing necessity of ECAM-based access during inits. SMBIOS-related content further corroborates gathering static hardware information during bring-up. In sum, the strongest support comes from passages identifying RSDP, XSDT, MADT, MCFG/ECAM, FADT, and SMBIOS data sources as the backbone of hardware discovery and configuration on modern x86_64 systems, which aligns with the requested fine-grained field value.",
      "confidence": "high"
    },
    {
      "field": "storage_stack_strategy.block_layer_design",
      "citations": [
        {
          "title": "blk-mq.rst - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/block/blk-mq.rst",
          "excerpts": [
            "blk-mq has two group of queues: software staging queues and hardware dispatch queues. When the request arrives at the block layer, it will try the shortest path ..."
          ]
        },
        {
          "title": "Multi-Queue Block IO Queueing Mechanism (blk-mq)",
          "url": "https://docs.kernel.org/block/blk-mq.html",
          "excerpts": [
            "blk-mq has two group of queues: software staging queues and hardware dispatch\nqueues.",
            "When the request arrives at the block layer, it will try the shortest\npath possible: send it directly to the hardware queue. However, there are two\ncases that it might not do that: if there’s an IO scheduler attached at the\nlayer or if we want to try to merge requests. In both cases, requests will be\nsent to the software queue.",
            "\nThe hardware queue (represented by [`struct blk_mq_hw_ctx`](.blk_mq_hw_ctx \"blk\\_mq\\_hw\\_ctx\") ) is a `struct\nused` by device drivers to map the device submission queues (or device DMA ring\nbuffer), and are the last step of the block layer submission code before the\nlow level device driver taking ownership of the request.",
            "Missing: refactor 2019-2025 lwn.",
            "The scheduling\nhappens only between requests in the same queue, so it is not possible to merge\nrequests from different queues, otherwise there would be cache trashing and a\nneed to have a lock for each queue."
          ]
        },
        {
          "title": "SPDK NVMe and high-performance storage (SPDK news article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications. It achieves high performance by moving all of the necessary drivers into userspace and operating in a polled mode instead of relying on interrupts, which avoids kernel context switches and eliminates interrupt handling overhead.",
            "The bedrock of SPDK is a user space, polled-mode, asynchronous, lockless NVMe\ndriver that provides zero-copy, highly parallel access directly to an SSD from\na user space application.",
            "SPDK further provides a full block stack as a user\nspace library that performs many of the same operations as a block stack in\nan operating system.",
            "Feb 1, 2023 — The graph below demonstrates the I/O throughput scalability of the SPDK NVMe driver with the addition of more CPU cores to perform I/O. We ..."
          ]
        },
        {
          "title": "SPDK and Its Implementations",
          "url": "https://zhou-diyu.github.io/files/spdkp-hotstorage25.pdf",
          "excerpts": [
            "SPDK employs a block device abstraction\n\ncalled Bdev to perform the same functions as the block de-\n\nvice layer in the ker",
            "SPDK+ utilizes such instructions to construct a\n\nlow-power user-level thread within the I/O process. When\n\nthe working CPU core submits an I/O request, it switches\n\nto the low-power thread, waiting for the incoming MSI-X\n\ninterr",
            "SPDK+ enhances\n\npower efficiency by up to 49.5% with only a negligible latency\n\nincrease compared to SPD",
            "SPDK also implements a user-space,\n\nasynchronous, polling-based NVMe drive"
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: LIBAIO, SPDK, IO_URING: A PRIMER",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            " **is close to SPDK. ** io\\_uring with kernel polling\n\ncan deliver performance close to SPDK (within 10% ), thanks\n\nto the elimination of system calls from the I/O path",
            "\nthis performance needs twice as many CPU cores as SPDK"
          ]
        },
        {
          "title": "Multi-Queu on block layer in Linux Kernel - Hyunyoung2",
          "url": "https://hyunyoung2.github.io/2016/09/14/Multi_Queue/",
          "excerpts": [
            "The blk_mq API Implements a two-levels block layer design which use two separate sets of request queues. software staging queues, allocated per- ..."
          ]
        },
        {
          "title": "Storage Performance Development Kit driver",
          "url": "https://docs.openstack.org/cinder/2023.1/configuration/block-storage/drivers/spdk-volume-driver.html",
          "excerpts": [
            "Nov 12, 2019 — Storage Performance Development Kit (SPDK) is a user space, polled-mode, asynchronous, lockless NVMe driver. It provides zero-copy, highly ..."
          ]
        },
        {
          "title": "System Configuration User Guide",
          "url": "https://spdk.io/doc/system_configuration.html",
          "excerpts": [
            "VFIO device access is protected with sysfs file permissions and can be configured with chown/chmod. Please note that the VFIO device isolation is based ..."
          ]
        },
        {
          "title": "SPDK on LS2 PLATFORM",
          "url": "https://community.nxp.com/pwmxy87654/attachments/pwmxy87654/qoriq-grl%40tkb/53/1/SPDK_updated.pdf",
          "excerpts": [
            "It controls NVMe devices by directly mapping the PCI BAR into the local process and performing MMIO. ... SPDK integrates DPDK as default running environment."
          ]
        },
        {
          "title": "Log-Structured File Systems | Baeldung on Computer Science",
          "url": "https://www.baeldung.com/cs/log-structured-file-systems",
          "excerpts": [
            "To recover from a system crash, LFS employs two strategies: checkpoints, which define consistent states of the file system , and roll-forward, which is used to recover information saved since the last checkpoint. The checkpoint is a point where all the file system structures are complete and consistent."
          ]
        },
        {
          "title": "Log-structured filesystems (CS 4410, Summer 2017) - CS@Cornell",
          "url": "https://www.cs.cornell.edu/courses/cs4410/2017su/lectures/lec18-lfs.html",
          "excerpts": [
            "This is referred to as a checkpoint; once the superblock has been written, the filesystem now reflects everything that happened before the checkpoint. Note that ..."
          ]
        },
        {
          "title": "SquirrelFS (OSDI 2024)",
          "url": "https://github.com/utsaslab/squirrelfs",
          "excerpts": [
            "SquirrelFS is a file system for persistent memory (PM) written in Rust that uses soft updates for crash consistency. It uses Rust support for the typestate ..."
          ]
        },
        {
          "title": "SquirrelFS: Rust-native PM filesystem with crash-consistency",
          "url": "https://www.usenix.org/system/files/osdi24_slides-leblanc.pdf",
          "excerpts": [
            "Static checks rely only on existing Rust features",
            "Uses the typestate pattern to statically check ordering of durable updates",
            "Atomic metadata-related system calls (including rename)"
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a multi-queue block layer design inspired by the Linux blk-mq architecture, with multiple per-CPU software queues and hardware-mapped dispatch queues to scale I/O operations across cores and NVMe devices. Excerpt 16 explicitly details blk-mq having two groups of queues (software staging queues and hardware dispatch queues) and describes the path of a request through the block layer, which directly supports the concept of a multi-queue design with separation of software and hardware responsibilities. Excerpt 1 reinforces this by stating blk-mq consists of a software staging queue group and a hardware dispatch queue group, aligning with the per-CPU software queue and hardware-mapped dispatch queue idea. Excerpt 2 further supports the concept by describing how, at the block layer, requests may be routed to software queues and only subsequently sent to hardware queues, illustrating the decoupled software/hardware queue architecture and the rationale for optimal path selection. Excerpt 3 discusses the hardware context mapping (blk_mq_hw_ctx) used by device drivers to map device submission queues (DMA rings) as the final step before the real driver handles the request, which is consistent with hardware-mapped dispatch queues. Excerpt 0 indicates blk-mq is the central topic and mentions the block layer architecture, which is compatible with a blk-mq-inspired design. Excerpt 4 notes that scheduling occurs between requests within the same queue, underscoring the separation of concerns and the importance of queue-level scheduling in a multi-queue setup. Together, these excerpts collectively substantiate a design that replaces a single globally-locked I/O queue with multiple per-CPU software queues and hardware-mapped dispatch queues for better scalability on multi-core systems and high-performance NVMe devices. Excerpts discussing SPDK (7, 11-14) and related performance-focused assessments provide additional context about user-space, polling-based approaches that relate to high-performance storage paths but are secondary to the core multi-queue architectural claim. The other items expand on related storage API discussions or alternative architectures but do not directly contradict the targeted blk-mq-inspired design.",
      "confidence": "high"
    },
    {
      "field": "performance_analysis_by_strategy.native_kernel_performance",
      "citations": [
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "Intel Ethernet' s Performance Report with DPDK 23.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_23_03_Intel_NIC_performance_report.pdf",
          "excerpts": [
            "Apr 20, 2023 — Single core test case: Used to check the maximum IO throughput for a single core. The case requires at least 1x 100GbE ports for the ice DPDK ..."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "Devices are the main target of any I/O driver.",
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO.",
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO.",
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Packed virtqueue: How to reduce overhead with virtio",
          "url": "https://www.redhat.com/en/blog/packed-virtqueue-how-reduce-overhead-virtio",
          "excerpts": [
            "This layout is relatively easy to implement and to debug thus it's a good entry point for learning the virtio dataplane basics.",
            "Packed virtqueue amends it by merging the three rings in just one location in virtual environment guest memory."
          ]
        }
      ],
      "reasoning": "The most directly relevant piece discusses AF_XDP as an optimization within the Linux kernel for high-performance packet processing, and notes that this is an address family designed for efficient data path handling alongside XDP. This supports the field value's claim that native kernel interfaces can yield strong performance and tight integration with the NIC driver, potentially bypassing or reducing overhead in the traditional network stack. It also touches on the reader's familiarity with XDP and related interfaces, reinforcing that using these native pathways can achieve efficient packet processing in the driver path.\n\nThe next most relevant content compares kernel versus user-space storage or acceleration approaches. It states that SPDK (a user-space, user-driven I/O stack) can achieve higher throughput than the Linux storage stack when CPU is the bottleneck, implying that, in some workloads, native kernel storage paths may be outperformed by specialized user-space stacks, but that kernel storage has a bottleneck primarily tied to per-core efficiency and CPU limits. This informs a nuanced view of the field value: native kernel storage interfaces (like io_uring) can be state-of-the-art, but performance may still hinge on per-core efficiency and contention, so design choices should consider workload characteristics and CPU resources.\n\nOther excerpts discuss kernel-facing I/O driver architectures and device exposure models. They note that devices are the main target of I/O drivers, underscoring the central role of the kernel in I/O performance and the necessity of robust driver interfaces. They describe VFIO as a framework for exposing direct device access to userspace in a secure, IOMMU-protected environment, which is relevant when evaluating whether to keep certain drivers in-kernel versus moving to a userspace driver model with strong isolation guarantees. This supports the notion that native kernel paths can be complemented by secure, flexible userspace access paradigms without abandoning kernel-level integration altogether. The VFIO entries also emphasize unifying driver access paradigms and providing secure user-space driver environments, which is relevant to architectural decisions about how tightly to couple I/O devices to the kernel versus userspace components.\n\nOverall, the excerpts collectively support a view that native kernel interfaces (like AF_XDP) can offer strong performance and deep integration, while kernel-to-userspace exposure options (VFIO) can provide secure and flexible alternatives when necessary. The evidence for io_uring-based storage performance is less explicit here beyond the general statement that kernel storage performance can be state-of-the-art, with per-core efficiency and contention as limiting factors. This yields a balanced interpretation: native kernel paths are performant and tightly integrated for networking via AF_XDP, while modern storage and driver architectures may benefit from continued evolution (e.g., advanced kernel interfaces like io_uring) and careful workload-focused architectural decisions.",
      "confidence": "medium"
    },
    {
      "field": "storage_stack_strategy.userspace_driver_integration",
      "citations": [
        {
          "title": "SPDK NVMe and high-performance storage (SPDK news article)",
          "url": "https://spdk.io/news/2023/02/01/nvme-120m-iops/",
          "excerpts": [
            "The Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications. It achieves high performance by moving all of the necessary drivers into userspace and operating in a polled mode instead of relying on interrupts, which avoids kernel context switches and eliminates interrupt handling overhead.",
            "The bedrock of SPDK is a user space, polled-mode, asynchronous, lockless NVMe\ndriver that provides zero-copy, highly parallel access directly to an SSD from\na user space application.",
            "SPDK further provides a full block stack as a user\nspace library that performs many of the same operations as a block stack in\nan operating system."
          ]
        },
        {
          "title": "SPDK and Its Implementations",
          "url": "https://zhou-diyu.github.io/files/spdkp-hotstorage25.pdf",
          "excerpts": [
            "SPDK employs a block device abstraction\n\ncalled Bdev to perform the same functions as the block de-\n\nvice layer in the ker",
            "SPDK also implements a user-space,\n\nasynchronous, polling-based NVMe drive"
          ]
        },
        {
          "title": "Storage Performance Development Kit driver",
          "url": "https://docs.openstack.org/cinder/2023.1/configuration/block-storage/drivers/spdk-volume-driver.html",
          "excerpts": [
            "Nov 12, 2019 — Storage Performance Development Kit (SPDK) is a user space, polled-mode, asynchronous, lockless NVMe driver. It provides zero-copy, highly ..."
          ]
        },
        {
          "title": "System Configuration User Guide",
          "url": "https://spdk.io/doc/system_configuration.html",
          "excerpts": [
            "VFIO device access is protected with sysfs file permissions and can be configured with chown/chmod. Please note that the VFIO device isolation is based ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts explicitly describe SPDK enabling user-space, polled-mode drivers and a block stack that operates in user space, which directly supports the idea of integrating a userspace driver framework for direct device access and zero-copy operation. In particular, one excerpt states that SPDK moves all necessary drivers into user space and operates in a polled mode, thereby avoiding kernel-context switches and overhead, which aligns with bypassing kernel driver pathways for low-latency access. Another excerpt notes that SPDK provides user-space, polled-mode, asynchronous, lockless NVMe drivers, emphasizing zero-copy and direct access from a user-space application, which reinforces the same model. A further excerpt mentions that SPDK provides a full block stack as a user-space library, performing many of the operations of the kernel's block stack, which supports having the storage stack implemented largely in user space. Additionally, an excerpt discusses SPDK's block device abstraction (Bdev) to perform the same functions as the kernel's block device layer, which is relevant to mapping the user-space driver responsibilities to familiar storage interfaces. Related content describes SPDK in driver form and its integration context, which further substantiates a coherent user-space driver approach. Finally, an excerpt on VFIO device access notes that system configuration concerns protected device access, which is relevant to the practical aspect of mapping NVMe hardware registers into user space, albeit not SPDK-specific, and complements kernel-bypass strategies. Taken together, these excerpts collectively provide strong, direct support for a storage stack strategy centered on user-space drivers (SPDK) with kernel bypass and potential VFIO-based device access, matching the described finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "native_and_userspace_driver_architectures.microkernel_approach",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "FFI - The Rustonomicon",
          "url": "https://doc.rust-lang.org/nomicon/ffi.html",
          "excerpts": [
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped with unsafe {} as a promise to the compiler that everything contained within ...",
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped\nwith `unsafe {}` as a promise to the compiler that everything contained within\ntruly is saf",
            "The raw C API needs to be wrapped to provide memory safety and make use of higher-level concepts\nlike vectors.",
            "Unsafe blocks isolate unsafety and are a promise to\nthe compiler that the unsafety does not leak out of the block.",
            "When declaring the argument types to a foreign function, the Rust compiler\ncannot check if the declaration is correct, so specifying it correctly is part\nof keeping the binding correct at runtime."
          ]
        },
        {
          "title": "Rust — The Linux Kernel documentation",
          "url": "https://docs.kernel.org/rust/index.html",
          "excerpts": [
            "Documentation related to Rust within the kernel. To start using Rust in the kernel, please read the Quick Start guide."
          ]
        },
        {
          "title": "rust-for-linux kernel crate documentation",
          "url": "https://rust-for-linux.github.io/docs/kernel/",
          "excerpts": [
            "Common clock framework. [cred](cred/index.html \"kernel::cred mod\")"
          ]
        }
      ],
      "reasoning": "The fine-grained field describes a microkernel-style architecture where drivers and other services run in separate, unprivileged user-space processes with IPC-based communication, aiming for strong isolation and security at the cost of IPC overhead. To realize this in practice, inter-language or inter-process boundaries are often implemented via Foreign Function Interfaces (FFIs) or other cross-boundary calling mechanisms. The most directly relevant excerpt states that developers use an FFI to bridge Rust and C when enabling Rust to work alongside C, which aligns with the architectural need to connect Rust-based user-space components with existing kernel or driver code. This supports the idea that a Rust-driven, modular OS could rely on FFI to integrate with low-level, possibly C-based driver code housed in separate protection domains. Other excerpts emphasize the safety implications of using FFI: foreign functions are unsafe by default and require wrapping with an unsafe block, signaling a clear boundary where the kernel or hypervisor must enforce invariants and protect memory safety across the boundary. The raw C API often needs to be wrapped to provide memory safety and higher-level abstractions, which is consistent with designing clean, boundary-respecting interactions between user-space drivers and the kernel or kernel-like services. Additionally, isolating unsafety within explicit unsafe blocks and ensuring correct argument declarations (since the compiler cannot fully verify cross-language bindings) highlight the practical challenges of maintaining strong isolation and correctness when drivers run in separate protection domains. Collectively, these excerpts support the notion that a Rust-based, microkernel-style OS would rely on carefully designed FFI boundaries, wrapper layers, and explicit unsafety management to achieve modularity and security while contending with performance costs inherent to IPC and cross-boundary calls. The content does not directly prove every architectural claim (e.g., exact IPC implementations or formal verification in seL4), but it provides concrete evidence about the mechanism (FFI usage, safety boundaries, wrapping C APIs) that underpins such a design and the associated trade-offs.",
      "confidence": "medium"
    },
    {
      "field": "recommended_architectural_strategy.long_term_strategy",
      "citations": [
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "Userspace vs kernel space driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/15286772/userspace-vs-kernel-space-driver",
          "excerpts": [
            "I'd say go for user space if it is possible and the amount of effort to make things work is less than writing a kernel driver."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The NVMe driver is a C library that may be linked directly into an application that provides direct, zero-copy data transfer to and from NVMe SSDs.",
            "The library controls NVMe devices by directly mapping the PCI BAR into the local process and performing MMIO. I/O is submitted asynchronously via queue ...",
            "NVMe Driver Internal Memory Usage. The SPDK NVMe driver provides a zero-copy data transfer path, which means that there are no data buffers for I/O commands."
          ]
        },
        {
          "title": "Introduction to virtio-networking and vhost-net - Red Hat",
          "url": "https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net",
          "excerpts": [
            "In this post we will provide a high level solution overview of the virtio-networking architecture, based on establishing an interface between the host kernel ..."
          ]
        }
      ],
      "reasoning": "The proposal envisions a hybrid architecture that leverages virtualized device access to maximize security and compatibility while enabling high-performance paths through user-space drivers. A description of VFIO explains it as an IOMMU-enabled framework that exposes direct device access to userspace in a secure environment, which directly supports the idea of running device drivers or driver-like components in user space through a driver VM. This provides the architectural backbone for the suggested Linux-based driver VM with device passthrough. The Rust driver development article highlights using FFI to bridge Rust with existing C code, which informs how a new OS could integrate Rust-native components with established driver code or libraries, preserving interoperability while enabling language-level safety. The discussion contrasting userspace versus kernelspace drivers reinforces the strategic choice to maximize performance and simplicity for certain paths by keeping drivers in user space when possible, aligning with the goal of bypassing kernel overhead for throughput- and latency-sensitive I/O. The SPDK/NVMe-focused excerpts illustrate concrete, high-performance user-space I/O stacks that bypass the kernel to achieve zero-copy data transfer and direct device access, providing concrete architectural patterns the OS could emulate or adapt for similar devices. The accompanying details about MMIO-based device control and asynchronous I/O queues in these NVMe contexts further justify how user-space stacks can interoperate with hardware at low latency. Additionally, the driver in-kernel option is acknowledged as viable for select core devices or embedded platforms, which supports a limited, focused kernel driver strategy. The Virtio-networking excerpt adds perspective on hosting interfaces at the kernel boundary and how virtualization-based networking fits into a broader hybrid model, though it is less central than direct VFIO/user-space driver discussions. Overall, the most actionable links are the secure, user-space device access via VFIO, the Rust-to-C interop via FFI, the user-space vs kernel-space trade-off, and the concrete SPDK/DPDK-inspired patterns for high-performance I/O, with kernel drivers reserved for a narrow subset of hardware.",
      "confidence": "high"
    },
    {
      "field": "hardware_discovery_and_configuration_layer.arm64_platform_bringup",
      "citations": [
        {
          "title": "Device Tree Basics",
          "url": "https://devicetree-specification.readthedocs.io/en/latest/chapter2-devicetree-basics.html",
          "excerpts": [
            "Description:\n\n> The *reg* property describes the address of the device’s resources\n> within the address space defined by its parent bus. Most commonly this\n> means the offsets and lengths of memory-mapped IO register blocks, but\n> may have a different meaning on some bus types. Addresses in the address\n> space defined by the root node are CPU real addresses. >\n> The value is a *<prop-encoded-array>*, composed of an arbitrary number\n> of pairs of address and length, *<address length>*. The number of\n> *<u32>* cells required to specify the address and length are\n> bus-specific and are specified by the ** and **\n> properties in the parent of the device node. If the parent node\n> specifies a value of 0 for **, the length field in the value\n> of *reg* shall be omitted.\n",
            "The dma-ranges property is used to describe the direct memory access (DMA) structure of a memory-mapped bus whose devicetree parent can be accessed from DMA ... For\nexample, the architecture of PCI enables a client to probe and detect\nattached devices, and thus devicetree nodes describing PCI devices\nmight not be required. However, a device node is required to describe a\nPCI host bridge device in the system if it cannot be detected by\nprobing. **Example**\n\n[Fig. 2.1]() shows an example representation of a\nsimple devicetree that is nearly\ncomplete enough to boot a simple operating system, with the platform\ntype, CPU, memory and a single UART described. Device nodes are shown\nwith properties and values inside each node. !",
            "Property name: `reg`",
            "Value type: `<empty>` or `<prop-encoded-array>` encoded as an arbitrary number of\n(*child-bus-address*, *parent-bus-address*, *length*) triplets. Description:\n\n> The *ranges* property provides a means of defining a mapping or\n> translation between the address space of the bus (the child address\n> space) and the address space of the bus node’s parent (the parent\n> address space). >\n> The format of the value of the *ranges* property is an arbitrary number\n> of triplets of (*child-bus-address*, *parent-bus-address*, *length*)\n>\n> * The *child-bus-address* is a physical address within the child bus’\n>   address spa",
            "Property name: `ranges`"
          ]
        },
        {
          "title": "Retry deferred probe at runtime (device tree) - driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/78863378/retry-deferred-probe-at-runtime-device-tree",
          "excerpts": [
            "I've developed a device tree overlay to describe some pieces of my embedded system. The system is comprised of a Raspberry Pi, a separate custom circuit board ..."
          ]
        }
      ],
      "reasoning": "The core of the finegrained field value is that on ARM64 platforms, hardware discovery and initial setup rely on the Device Tree: the boot path passes a Flattened Device Tree blob (FDT/DTB) to the kernel, the OS parses this DT to discover devices and their properties (including the compatible string for driver matching and reg for memory-mapped registers), and the OS initializes essential subsystems (such as the GIC for interrupts and PSCI for power management). The most directly relevant content describes the reg property as the descriptor of a device's resource addresses within its parent bus's address space, which is exactly the kind of resource description DT provides for hardware devices. Related DT basics content gives a concrete example of a boot representation including platform type, CPU, memory, and a UART, illustrating how DT conveys essential platform hardware to the OS during bring-up. Additional references to the reg property header confirm its role in specifying resource addresses. The ranges property content further clarifies how DT maps address spaces, which is important for correct resource translation and memory access during device discovery. A practical DT overlay example (such as one described for a Raspberry Pi) demonstrates how a DT can be extended or adapted for a specific embedded platform, reinforcing the idea that DT-driven hardware description is used during bring-up in real systems.",
      "confidence": "high"
    },
    {
      "field": "recommended_architectural_strategy.rationale",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Introduction to virtio-networking and vhost-net - Red Hat",
          "url": "https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net",
          "excerpts": [
            "In this post we will provide a high level solution overview of the virtio-networking architecture, based on establishing an interface between the host kernel ..."
          ]
        },
        {
          "title": "Userspace vs kernel space driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/15286772/userspace-vs-kernel-space-driver",
          "excerpts": [
            "I'd say go for user space if it is possible and the amount of effort to make things work is less than writing a kernel driver."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The library controls NVMe devices by directly mapping the PCI BAR into the local process and performing MMIO. I/O is submitted asynchronously via queue ...",
            "The NVMe driver is a C library that may be linked directly into an application that provides direct, zero-copy data transfer to and from NVMe SSDs.",
            "NVMe Driver Internal Memory Usage. The SPDK NVMe driver provides a zero-copy data transfer path, which means that there are no data buffers for I/O commands."
          ]
        }
      ],
      "reasoning": "The strongest support comes from material that discusses bridging Rust with existing driver code using FFI, which aligns with a short-term plan to leverage existing C drivers from Rust via language interoperability rather than rewriting drivers from scratch. It also helps justify a pragmatic architecture where Rust-based components can invoke or interoperate with established driver code. Next, evidence about exposing or delegating device access to userspace within a protected environment supports the idea of offloading the driver problem to a virtualization-or-IOMMU-protected layer in the short term, and shows a concrete mechanism (IOMMU-protected, userspace-facing frameworks) that can avoid immediate kernel-space driver development. Architecture-level discussions of virtio and host–guest interfaces illustrate how a separation between a driver domain (host) and a Rust OS guest can provide isolation and a clean boundary for a phased approach. Additional excerpts that compare user-space versus kernel-space drivers reinforce the feasibility and trade-offs of starting in user-space and potentially moving toward a native path later. For the long-term native path, excerpts describing direct, high-performance driver interactions (direct memory mapping, MMIO, zero-copy transfers) ground the notion of returning to native, kernel-analogous capabilities when rewriting the Linux driver ecosystem would be impractical, supporting the hybrid end-state goal. While licensing considerations (GPLv2 avoidance) are mentioned in the field value, there is no explicit licensing-focused evidence in the provided excerpts, so that aspect remains unsupported by direct excerpts. Overall, the combination of Rust–C interoperability, user-space driver offloading with protected isolation, and a high-performance native path aligns with the described dual-phase rationale, with medium confidence on the exact licensing claim due to lack of explicit references in the excerpts.",
      "confidence": "medium"
    },
    {
      "field": "gpu_and_display_stack_strategy.native_stack_challenge",
      "citations": [
        {
          "title": "Direct Rendering Manager",
          "url": "https://en.wikipedia.org/wiki/Direct_Rendering_Manager",
          "excerpts": [
            "The Direct Rendering Manager (DRM) is a subsystem of the Linux kernel responsible for interfacing with GPUs of modern video cards."
          ]
        },
        {
          "title": "Linux kernel documentation - Introduction to DRM/KMS",
          "url": "https://www.kernel.org/doc/html//v6.4/gpu/introduction.html",
          "excerpts": [
            "A note on versions: this guide covers features found in the DRM tree,\nincluding the TTM memory manager, output configuration and mode setting,\nand the new vblank internals, in addition to all the regular features\nfound in current kernels."
          ]
        },
        {
          "title": "Kernel DRM/KMS Documentation",
          "url": "https://docs.kernel.org/gpu/drm-kms.html",
          "excerpts": [
            ")\n\nThe basic object structure KMS presents to userspace is fairly simple. Framebuffers (represented by [`struct drm_framebuffer`](.drm_framebuffer \"drm\\_framebuffer\") ,\nsee [Frame Buffer Abstraction]() ) feed into planes. Planes are represented by [`struct drm_plane`](.drm_plane \"drm\\_plane\") , see [Plane Abstraction]() for more\ndetails. One or more (or even no) planes feed their pixel data into a CRTC\n(represented by [`struct drm_crtc`](.drm_crtc \"drm\\_crtc\") , see [CRTC Abstraction]() )\nfor blending. The precise blending step is explained in more detail in [Plane\nComposition Properties]() and related chapters.",
            " For the output routing the first step is encoders (represented by [`struct drm_encoder`](.drm_encoder \"drm\\_encoder\") , see [Encoder Abstraction]() ).",
            " The final, and real, endpoint in the display chain is the connector (represented\nby [`struct drm_connector`](.drm_connector \"drm\\_connector\") , see [Connector\nAbstraction]() )."
          ]
        },
        {
          "title": "DRM Internals — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.18/gpu/drm-internals.html",
          "excerpts": [
            "This chapter documents DRM internals relevant to driver authors and developers working to add support for the latest features to existing drivers."
          ]
        },
        {
          "title": "DRM Internals",
          "url": "https://docs.kernel.org/gpu/drm-internals.html",
          "excerpts": [
            "This chapter documents DRM internals relevant to driver authors and developers working to add support for the latest features to existing drivers."
          ]
        },
        {
          "title": "Introduction — The Linux Kernel documentation - DRI",
          "url": "https://dri.freedesktop.org/docs/drm/gpu/introduction.html",
          "excerpts": [
            "This guide covers features found in the DRM tree, including the TTM memory manager, output configuration and mode setting, and the new vblank internals."
          ]
        },
        {
          "title": "Introduction — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.5/gpu/introduction.html",
          "excerpts": [
            "This guide covers features found in the DRM tree, including the TTM memory manager, output configuration and mode setting, and the new vblank internals."
          ]
        },
        {
          "title": "Mode Setting Helper Functions",
          "url": "https://docs.kernel.org/gpu/drm-kms-helpers.html",
          "excerpts": [
            "The DRM mode setting helper functions are common code for drivers to use if they wish. Drivers are not forced to use this code in their implementations."
          ]
        },
        {
          "title": "The Linux Kernel API",
          "url": "https://docs.kernel.org/core-api/kernel-api.html",
          "excerpts": [
            "The Linux Kernel API¶. Basic C Library Functions¶. When writing drivers, you cannot in general use routines which are from the C Library."
          ]
        },
        {
          "title": "Driver Model",
          "url": "https://docs.kernel.org/driver-api/driver-model/index.html",
          "excerpts": [
            "Contents · Driver Basics · Driver Model · Device links · Device drivers infrastructure · ioctl based interfaces · CPU and Device Power Management."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts describe the Direct Rendering Manager (DRM) as the Linux kernel subsystem that interfaces with GPUs, which is foundational to building a native display stack. This aligns directly with the field value's emphasis on DRM/KMS as the center of the modern Linux graphics stack. It's also important that the DRM/KMS documentation explains the basic object model and the flow of display data through encoders, controllers, and connectors, which supports the claim that a native stack would need to implement or faithfully reproduce these abstractions. The DRM internals documentation is highly pertinent because it targets driver authors and developers, offering insight into the internal components and how they fit together in practice, which supports the assertion that building a from-scratch DRM/KMS driver is highly specialized work. Additional context from the kernel DRM/KMS introduction and general kernel driver references reinforces the necessity of memory management (e.g., GEM, DMA-BUF) and mode-setting concepts in a native stack, which strengthens the analysis that developing a DRM driver is resource-intensive and complex. While not all excerpts are about DRM specifically, the driver-model and kernel API discussions provide important bounds about how such a subsystem would need to integrate with the rest of the kernel, further supporting the claim that a ground-up DRM/KMS stack is a substantial undertaking requiring kernel-level expertise.",
      "confidence": "high"
    },
    {
      "field": "executive_summary.strategic_impact",
      "citations": [
        {
          "title": "Linux in-kernel vs out-of-kernel drivers and plug and play ...",
          "url": "https://www.reddit.com/r/linuxhardware/comments/182uaw7/linux_inkernel_vs_outofkernel_drivers_and_plug/",
          "excerpts": [
            "Linux in-kernel drivers are preferable over out-of-kernel drivers for most users and use cases as problems with locating, installing and maintaining drivers ..."
          ]
        },
        {
          "title": "LinuxKPI - FreeBSD Wiki",
          "url": "https://wiki.freebsd.org/LinuxKPI",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "LinuxKPI: Linux Drivers on FreeBSD - cdaemon",
          "url": "https://cdaemon.com/posts/pwS7dVqV",
          "excerpts": [
            "LinuxKPI is a small compatibility layer that allows Linux drivers to run in a FreeBSD environment with minor modification."
          ]
        },
        {
          "title": "BPF licensing and Linux kernel licensing rules (GPLv2 and module/linking implications)",
          "url": "https://www.kernel.org/doc/html/v5.17/bpf/bpf_licensing.html",
          "excerpts": [
            "Linux Kernel (while being GPLv2) allows linking of proprietary kernel modules\nunder these rules: [Linux kernel licensing rule",
            "When a kernel module is loaded, the linux kernel checks which functions it\nintends to use."
          ]
        },
        {
          "title": "Linux Syscall Note (SPDX)",
          "url": "https://spdx.org/licenses/Linux-syscall-note.html",
          "excerpts": [
            "NOTE! This copyright does \\*not\\* cover user programs that use kernel\nservices by normal system calls - this is merely considered normal use\nof the kernel, and does \\*not\\* fall under the heading of \"derived work\".",
            "This note is used with the Linux kernel to clarify how user space API files should be treated. Text Text in italicized blue is omittable.",
            "Text in italicized blue is omittable (see [Matching Guidelines B.3.5](https://spdx.github.io/spdx-spec/v2.3/license-matching-guidelines-and-templates/)).",
            "the only valid version of the GPL as far as the kernel\nis concerned is \\_this\\_ particular version of the license (ie v2, not\nv2.2 or v3.x or whatever), unless explicitly otherwise stated. Linus Torvalds"
          ]
        },
        {
          "title": "The Linux kernel doesn't provide a stable ABI for modules so they ...",
          "url": "https://news.ycombinator.com/item?id=21243406",
          "excerpts": [
            "Apr 10, 2021 — The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to rebuild ...",
            "The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to ...",
            "The Linux kernel doesn't provide a stable ABI for modules so they have to be atleast recompiled each time. There are some workarounds such as DKMS to ..."
          ]
        },
        {
          "title": "Linux ABI description",
          "url": "https://docs.kernel.org/admin-guide/abi.html",
          "excerpts": [
            " This directory documents the interfaces that the developer has\n>     defined to be stable. Userspace programs are free to use these\n>     interfaces with no restrictions, and backward compatibility for\n>     them will be guaranteed for at least 2 years. Most interfaces\n>     (like syscalls) are expected to never change",
            "We have four different levels of ABI stability, as shown by the four\ndifferent subdirectories in this location. Interfaces may change levels\nof stability according to the rules described below."
          ]
        },
        {
          "title": "Symbol Namespaces",
          "url": "https://docs.kernel.org/core-api/symbol-namespaces.html",
          "excerpts": [
            "The following document describes how to use Symbol Namespaces to structure the export surface of in-kernel symbols exported through the family of EXPORT_SYMBOL ..."
          ]
        },
        {
          "title": "Symbol Namespaces — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/core-api/symbol-namespaces.html",
          "excerpts": [
            "The following document describes how to use Symbol Namespaces to structure the export surface of in-kernel symbols exported through the family of EXPORT_SYMBOL ..."
          ]
        },
        {
          "title": "EXPORT_SYMBOL in Linux Kernel - Linux Device Driver Part 29",
          "url": "https://embetronicx.com/tutorials/linux/device-drivers/export_symbol-in-linux-device-driver/",
          "excerpts": [
            "Another macro is also available to export the symbols like EXPORT_SYMBOL . That is EXPORT_SYMBOL_GPL() . EXPORT_SYMBOL exports the symbol to any loadable module . EXPORT_SYMBOL_GPL exports the symbol only to GPL-licensed modules."
          ]
        },
        {
          "title": "linux - How are intermodule dependencies resolved when...?",
          "url": "https://stackoverflow.com/questions/20412183/how-are-intermodule-dependencies-resolved-when",
          "excerpts": [
            "Modversioning is used to ensure that binary loadable modules are compatible with the kernels they are loaded upon. It is enabled with .config option CONFIG_ ..."
          ]
        },
        {
          "title": "Feasibility of adding drivers to linux kernel",
          "url": "https://softwareengineering.stackexchange.com/questions/423138/feasibility-of-adding-drivers-to-linux-kernel",
          "excerpts": [
            "Mar 8, 2021 — I want to ask how much feasible is to add tons of device driver to Linux kernel , suppose after 30 -40 years the size of Linux kernel will be ..."
          ]
        },
        {
          "title": "Confusing Usage-Guide in LICENSES/exceptions/Linux-syscall-note",
          "url": "https://lkml.org/lkml/2020/2/21/2193",
          "excerpts": [
            "For the reference, here is the Linux syscall note: NOTE! This copyright does *not* cover user programs that use kernel"
          ]
        }
      ],
      "reasoning": "The most relevant content directly addresses the driver boundary issue central to a virtualization-first approach: discussions of out-of-kernel or in-kernel driver models show why decoupling drivers from the core OS is a considered strategy. For instance, the note on in-kernel versus out-of-kernel drivers and plug-and-play highlights the benefits and tradeoffs of separating driver loading from the kernel, which supports adopting a virtualization-first or microkernel-like architecture where core OS services are distinct from driver management. A compatibility-layer example demonstrates practical feasibility across operating system boundaries by enabling Linux drivers to run in a different kernel environment, illustrating that decoupling driver ecosystems is technically achievable and not purely theoretical. Licensing-focused excerpts explain how kernel module licensing has evolved, including GPLv2 constraints and the potential for permissive licensing in a rearchitected OS, which is critical for setting a favorable licensing path if the OS chooses to diverge from GPL-licensed Linux driver practices. Additional licensing notes emphasize copyright scope and derived-work considerations, underscoring why licensing strategy is intertwined with architectural decisions. Together, these excerpts substantiate that (a) decoupling drivers from the kernel is a viable architectural direction, (b) cross-OS compatibility layers can ease transition/porting concerns, and (c) licensing implications are a consequential consideration guiding the architecture and roadmap. The content also provides context about ABI stability and module-related challenges, which reinforce why a virtualization-first approach could reduce dependency on a monolithic, stable-driver ABI and enable a more flexible development trajectory, aligning with the proposed strategic impact on architecture and development roadmap.",
      "confidence": "medium"
    },
    {
      "field": "virtualization_based_reuse_deep_dive.architectural_patterns",
      "citations": [
        {
          "title": "Our Prototype on Driver Reuse via Virtual Machines (IöTEC KIT)",
          "url": "https://os.itec.kit.edu/844.php",
          "excerpts": [
            "When we additionally include all code required for\nthe virtualization—the L4 microkernel, the VMM, and\nthe paravirtualization modifications—we still achieve a\nreuse ratio of 91% just for Linux’s NIC driver base.",
            "The engineering effort enabled us to successfully reuse\nLinux device drivers with all of our tested lab hardware.",
            "For example,\nit permits our DD/OS instances to directly translate their\nvirtual addresses into bus addresses for DMA. It also pro-\nvides performance benefit"
          ]
        },
        {
          "title": "Bridging the Gap between Software and Hardware ...",
          "url": "https://www.usenix.org/legacy/event/usenix08/tech/full_papers/santos/santos_html/index.html",
          "excerpts": [
            "In this paper we show how to significantly narrow the performance gap by improving the performance of the driver domain model."
          ]
        },
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ... Virtual I/O Device (VIRTIO) Version 1.3",
            "A Virtio device using Virtio Over PCI Bus MUST expose to guest an interface that\nmeets the specification requirements of the appropriate PCI specification: [[PCI]]() and [[PCIe]]() respectively.",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "The device MUST allow reading of any device-specific configuration field\nbefore FEATURES\\_OK is set by the drive",
            "5.7 GPU Device. virtio-gpu is a virtio based graphics adapter. It can operate in 2D mode and in 3D mode. 3D mode will offload rendering ops to the host gpu ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "Any PCI device with PCI Vendor ID 0x1AF4, and PCI Device ID 0x1000 through\n0x107F inclusive is a virtio device.",
            "The PCI Device ID is calculated by adding\n0x1040 to the Virtio Device ID, as indicated in section [5]() .",
            "Virtual I/O Device (VIRTIO) Version 1.3"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "The PCI Device ID is calculated by adding\n0x1040 to the Virtio Device ID, as indicated in section [5]() ."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "SOF VirtIO design",
          "url": "https://thesofproject.github.io/latest/developer_guides/virtualization/virtualization.html",
          "excerpts": [
            "A doorbell is\nasynchronous signaling hardware between DSP and host. This mailbox /\ndoorbell interface can be hardware-specific; thus, every hardware variant\nprovides its own set of mailbox / doorbell handling operations.",
            "Typically an IPC message exchange between the driver and the DSP takes place\nover a mailbox and a doorbell. A mailbox is a shared memory region between\nthe DSP and host.",
            "This\nvirtualization solution creates its own set of IPC operations to redirect\nIPC messages from the guest over virtual queues to the host.",
            "irtio) is a platform-agnostic virtualization standard, managed by [OASIS](https://www.oasis-open.org/) . It\ndefines a design and a protocol for implementing device IO in VMs.",
            "/) VirtIO design implements an interface to transport IPC (\nInter-Process Communication) commands and audio data between host and\nguests."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value centers on architectural patterns that enable driver reuse through virtualization. The strongest support comes from passages that describe concrete patterns like a dedicated Driver VM/Driver Domain, where a minimal OS instance owns the hardware while a separate guest runs the main OS, coupled with paravirtualized interfaces for efficient communication with the hypervisor. This is complemented by discussions of static partitioning hypervisors (e.g., Jailhouse) that minimize overhead by avoiding emulation, and the prevailing use of QEMU for device emulation with KVM for hardware acceleration, where guest drivers communicate via paravirtualized paths (PVHVM). Additional corroboration comes from notes about translating guest addresses for DMA and achieving high reuse ratios for Linux NIC drivers when virtualization layers are leveraged, illustrating tangible benefits of this approach. The virtio family and its specifications provide a standardized interface between guest devices and the host/hypervisor, which is central to enabling driver reuse across boundaries. Together, these excerpts connect directly to the described architectural patterns by outlining: (a) how to isolate drivers in a separate domain to protect the rest of the system, (b) how to use paravirtualization to minimize overhead and enable efficient interaction with hardware, (c) how a hypervisor-guest interface (virtio) standardizes device access across VMs, and (d) practical performance and reuse metrics that justify these patterns for a Rust-based OS design. The content also notes cautions about older cohosting approaches that lack strong isolation, reinforcing the preference for more modern, isolated virtualization patterns.",
      "confidence": "high"
    },
    {
      "field": "gpu_and_display_stack_strategy.shim_based_reuse_challenge",
      "citations": [
        {
          "title": "Direct Rendering Manager",
          "url": "https://en.wikipedia.org/wiki/Direct_Rendering_Manager",
          "excerpts": [
            "The Direct Rendering Manager (DRM) is a subsystem of the Linux kernel responsible for interfacing with GPUs of modern video cards."
          ]
        },
        {
          "title": "Linux kernel documentation - Introduction to DRM/KMS",
          "url": "https://www.kernel.org/doc/html//v6.4/gpu/introduction.html",
          "excerpts": [
            "A note on versions: this guide covers features found in the DRM tree,\nincluding the TTM memory manager, output configuration and mode setting,\nand the new vblank internals, in addition to all the regular features\nfound in current kernels."
          ]
        },
        {
          "title": "Introduction — The Linux Kernel documentation - DRI",
          "url": "https://dri.freedesktop.org/docs/drm/gpu/introduction.html",
          "excerpts": [
            "This guide covers features found in the DRM tree, including the TTM memory manager, output configuration and mode setting, and the new vblank internals."
          ]
        },
        {
          "title": "Introduction — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.5/gpu/introduction.html",
          "excerpts": [
            "This guide covers features found in the DRM tree, including the TTM memory manager, output configuration and mode setting, and the new vblank internals."
          ]
        },
        {
          "title": "Linux's license is GPLv2 plus a syscall exception. If it was ...",
          "url": "https://news.ycombinator.com/item?id=37320678",
          "excerpts": [
            "Aug 30, 2023 — Linux's license is GPLv2 plus a syscall exception. If it was GPLv2 alone, proprietary modules would probably be infringing."
          ]
        },
        {
          "title": "MODULE_LICENSE and EXPORT_SYMBOL_GPL",
          "url": "https://lwn.net/2001/1025/a/module-license.php3",
          "excerpts": [
            "MODULE_LICENSE() allows kernel developers to identify kernels that have been tainted by modules whose source code is not generally available."
          ]
        },
        {
          "title": "DDEKit Approach for Linux User Space Drivers",
          "url": "http://os.inf.tu-dresden.de/papers_ps/beleg_weisbach.pdf",
          "excerpts": [
            "by H Weisbach · 2011 — DDEKit-Linux enables the re-use of device drivers in Linux, although an extensive number of drivers are available for Linux. DDEKit-Linux ..."
          ]
        },
        {
          "title": "The Userspace I/O HOWTO",
          "url": "https://www.kernel.org/doc/html/v5.0/driver-api/uio-howto.html",
          "excerpts": [
            "The main part of the driver will run in user space. This simplifies development and reduces the risk of serious bugs within a kernel module."
          ]
        },
        {
          "title": "When FFI Function Calls Beat Native C",
          "url": "https://news.ycombinator.com/item?id=17171252",
          "excerpts": [
            "I think the main performance are really (1) dynamically linked C/C++ with GOT + PLT, (2) statically linked C/C++ and then (potentially) separate ..."
          ]
        }
      ],
      "reasoning": "The Direct Rendering Manager is described as a kernel subsystem that interfaces with GPUs and is central to how modern Linux graphics drivers operate, which directly informs what a compatibility shim would need to leverage. The DRM/KMS introductions illuminate how display pipelines, mode setting, and plane/CRTC structures are composed, providing concrete driver surface elements that a shim would encounter if reusing Linux drivers. Licensing concerns are explicitly tied to GPLv2 in Linux, creating significant legal and engineering hurdles for a project that would want to reuse Linux drivers or licenses; this supports the claim that a clean-room reimplementation may be required rather than direct code reuse. The existence of approaches like DDEKit, which enables running Linux device drivers in user space, provides a real-world precedent for non-kernel driver reuse strategies and helps assess architectural options beyond a pure in-kernel shim. Discussions about FFI/Syscall interfaces and interoperability further contextualize how a compatibility layer could be architected, including performance and boundary considerations. Finally, the broader commentary on kernel API stability and maintenance burden strengthens the claim that keeping a shim up-to-date with upstream changes represents a persistent challenge. Taken together, these excerpts support the view that the proposed shim-based reuse strategy faces two major, well-supported challenges: ongoing maintenance burden due to rapid upstream changes, and GPLv2 licensing complications necessitating potential clean-room implementations rather than direct code reuse. ",
      "confidence": "medium"
    },
    {
      "field": "hardware_discovery_and_configuration_layer.common_bus_enumeration",
      "citations": [
        {
          "title": "PCI Express - OSDev Wiki",
          "url": "https://wiki.osdev.org/PCI_Express",
          "excerpts": [
            "On x86 and x64 platforms, the address of each memory area is determined by the ACPI 'MCFG' table.",
            "Physical_Address = MMIO_Starting_Physical_Address + ((Bus) << 20 | Device << 15 | Function << 12)",
            "The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes. This extended configuration space cannot be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC).",
            "The PCI Express bus extends the Configuration Space from 256 bytes to 4096 bytes. This extended configuration space cannot be accessed using the legacy PCI method (through ports 0xCF8 and 0xCFC). Instead, an Enhanced Configuration Mechanism is provided.",
            "The enhanced configuration mechanism makes use of memory mapped address space range/s to access PCI configuration space.",
            "the address of each memory area is determined by the ACPI 'MCFG' table.",
            "To access a specific register within a device's PCI configuration space, you have to use the device's PCI Segment Group and bus to determine which memory mapped PCI configuration space area to use, and obtain the starting physical address and starting bus number for that memory mapped area.",
            "or non-x86 systems the method varies, but usually systems provide themselves with a Devicetree which is parsed at runtime.",
            "Enhanced Configuration Mechanism"
          ]
        },
        {
          "title": "OSDev PCI Enumeration and Firmware Interfaces",
          "url": "https://wiki.osdev.org/PCI",
          "excerpts": [
            "Enumerating PCI Buses. There are 3 ways to enumerate devices on PCI buses ... However, support for them is mandatory in PCIe devices, so you can be sure ...",
            "The first step for the recursive scan is to implement a function that scans one bus. Pseudo-code might look like this:\nvoid checkBus ( uint8_t bus ) { uint8_t device ; for ( device = 0 ; device < 32 ; device ++ ) { checkDevice ( bus , device ); } "
          ]
        },
        {
          "title": "Device Tree Basics",
          "url": "https://devicetree-specification.readthedocs.io/en/latest/chapter2-devicetree-basics.html",
          "excerpts": [
            "The dma-ranges property is used to describe the direct memory access (DMA) structure of a memory-mapped bus whose devicetree parent can be accessed from DMA ... For\nexample, the architecture of PCI enables a client to probe and detect\nattached devices, and thus devicetree nodes describing PCI devices\nmight not be required. However, a device node is required to describe a\nPCI host bridge device in the system if it cannot be detected by\nprobing. **Example**\n\n[Fig. 2.1]() shows an example representation of a\nsimple devicetree that is nearly\ncomplete enough to boot a simple operating system, with the platform\ntype, CPU, memory and a single UART described. Device nodes are shown\nwith properties and values inside each node. !",
            "Description:\n\n> The *reg* property describes the address of the device’s resources\n> within the address space defined by its parent bus. Most commonly this\n> means the offsets and lengths of memory-mapped IO register blocks, but\n> may have a different meaning on some bus types. Addresses in the address\n> space defined by the root node are CPU real addresses. >\n> The value is a *<prop-encoded-array>*, composed of an arbitrary number\n> of pairs of address and length, *<address length>*. The number of\n> *<u32>* cells required to specify the address and length are\n> bus-specific and are specified by the ** and **\n> properties in the parent of the device node. If the parent node\n> specifies a value of 0 for **, the length field in the value\n> of *reg* shall be omitted.\n",
            "Property name: `ranges`",
            "Value type: `<empty>` or `<prop-encoded-array>` encoded as an arbitrary number of\n(*child-bus-address*, *parent-bus-address*, *length*) triplets. Description:\n\n> The *ranges* property provides a means of defining a mapping or\n> translation between the address space of the bus (the child address\n> space) and the address space of the bus node’s parent (the parent\n> address space). >\n> The format of the value of the *ranges* property is an arbitrary number\n> of triplets of (*child-bus-address*, *parent-bus-address*, *length*)\n>\n> * The *child-bus-address* is a physical address within the child bus’\n>   address spa"
          ]
        },
        {
          "title": "Retry deferred probe at runtime (device tree) - driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/78863378/retry-deferred-probe-at-runtime-device-tree",
          "excerpts": [
            "I've developed a device tree overlay to describe some pieces of my embedded system. The system is comprised of a Raspberry Pi, a separate custom circuit board ..."
          ]
        }
      ],
      "reasoning": "The assertion that the OS must discover and configure PCIe devices relies on a memory-mapped configuration mechanism whose address is determined via a platform-specific table; a source explicitly states that the address of each memory-mapped area for PCIe is provided by the ACPI MCFG table, which directly supports ECAM-based access. The conventional method to access PCIe configuration space is via a programmable MMIO region whose base and addressing scheme can be described by a standard formula for combining bus, device, and function numbers to yield a physical address within that MMIO space. This concrete description shows how the ECAM-based configuration is implemented and queried by software.\nSupporting details also describe that the extended PCIe configuration space (4096 bytes) cannot be accessed by legacy methods and instead uses an Enhanced Configuration Mechanism, which aligns with the need to enumerate buses, devices, and functions and to support MSI/MSI-X capabilities for modern interrupt handling. The combination of ECAM access, extended configuration space, and the Enhanced Configuration Mechanism explicitly maps to the core tasks of hardware discovery and configuration in the field value.\nIn non-x86 contexts, the Devicetree can supply the necessary address provisioning and bus/device mapping, which broadens the applicability of the same discovery/configuration workflow to other architectures. This supports the statement that the OS should be able to initialize PCIe and other buses via either ACPI/MPCI tables or Device Tree descriptions depending on platform.\nAdditionally, an OSDev-focused discussion provides procedural guidance for enumerating PCI buses, including a recursive scan of buses, devices, and functions, which directly aligns with the field value's claim about identifying hardware across the PCIe topology. It also reinforces the practical steps for discovering and programming bus-level capabilities.\nThe field value also references how to stage USB device initialization (xHCI) and the handling of device connection via enumeration, and the cited material emphasizes the need to reset ports, assign addresses, and read descriptors to identify capabilities and drivers, which is consistent with a broad hardware discovery and driver-loading workflow that complements PCIe enumeration.\nDevice-tree basics excerpts contribute by detailing how address mappings are encoded (reg, ranges) and how a device-tree-based system describes PCI host bridge resources, further supporting the use of a device-tree-based path for hardware resource provisioning when ACPI is not used. This helps explain how the OS locates and programs hardware resources across different platforms and buses, including PCIe and USB.\nLastly, the device-tree/PCI-related OSDev references together form a coherent narrative for how to initialize devices, discover resources, and prepare for driver loading in a platform-agnostic way, which is essential to the high-level hardware discovery and configuration workflow described in the field value.",
      "confidence": "high"
    },
    {
      "field": "recommended_architectural_strategy.short_term_strategy",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "Introduction to virtio-networking and vhost-net - Red Hat",
          "url": "https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net",
          "excerpts": [
            "In this post we will provide a high level solution overview of the virtio-networking architecture, based on establishing an interface between the host kernel ..."
          ]
        },
        {
          "title": "Userspace vs kernel space driver - Stack Overflow",
          "url": "https://stackoverflow.com/questions/15286772/userspace-vs-kernel-space-driver",
          "excerpts": [
            "I'd say go for user space if it is possible and the amount of effort to make things work is less than writing a kernel driver."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "SPDK: NVMe Driver",
          "url": "https://spdk.io/doc/nvme.html",
          "excerpts": [
            "The NVMe driver is a C library that may be linked directly into an application that provides direct, zero-copy data transfer to and from NVMe SSDs.",
            "NVMe Driver Internal Memory Usage. The SPDK NVMe driver provides a zero-copy data transfer path, which means that there are no data buffers for I/O commands.",
            "The library controls NVMe devices by directly mapping the PCI BAR into the local process and performing MMIO. I/O is submitted asynchronously via queue ..."
          ]
        }
      ],
      "reasoning": "To support the proposed short-term strategy, consider: the Rust OS can leverage a C-based virtio driver ecosystem through FFI, enabling Rust code to interact with established virtio driver implementations and bindings. The virtio-networking architecture provides a concrete model for how the guest OS would access networking via the virtio-net interface, aligning with the plan to implement a minimal set of virtio clients for essential devices such as virtio-net, virtio-blk, virtio-console, virtio-gpu, and virtio-rng. The discussion on userspace vs kernel-space drivers informs the architectural choice about where drivers live—favoring a user-space/guest-leaning model with virtio rather than embedding broad hardware drivers in the guest kernel. Additional context on direct hardware access (VFIO) and deeper NVMe/PCIe driver internals offers peripheral considerations but are not central to the virtio-paravirtualization strategy described. The combination of virtio as a standardized interface, Rust-C interop via FFI, and a user-space-first approach coherently supports a minimal, secure, and testable short-term architecture where the host (Linux) manages physical hardware while the guest OS focuses on virtio-driven devices.",
      "confidence": "medium"
    },
    {
      "field": "virtualization_based_reuse_deep_dive.security_and_licensing_benefits",
      "citations": [
        {
          "title": "Driver Domain - Xen Project Wiki",
          "url": "https://wiki.xenproject.org/wiki/Driver_Domain",
          "excerpts": [
            "Having a system with a modern IOMMU (either AMD or VT-d version 2) is highly recommended. Without IOMMU support, there's nothing to stop the driver domain from using the network card's DMA engine to read and write any system memory.",
            "Putting this in a separate, unprivileged domain limits the value of attacking the network stack: even if they succeed, they have no more access than a normal unprivileged VM.",
            "Use PCI passthrough to give the VM access to the hardware NIC"
          ]
        },
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "Any PCI device with PCI Vendor ID 0x1AF4, and PCI Device ID 0x1000 through\n0x107F inclusive is a virtio device.",
            "The PCI Device ID is calculated by adding\n0x1040 to the Virtio Device ID, as indicated in section [5]() .",
            "A Virtio device using Virtio Over PCI Bus MUST expose to guest an interface that\nmeets the specification requirements of the appropriate PCI specification: [[PCI]]() and [[PCIe]]() respectively.",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "The PCI Device ID is calculated by adding\n0x1040 to the Virtio Device ID, as indicated in section [5]() .",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "SOF VirtIO design",
          "url": "https://thesofproject.github.io/latest/developer_guides/virtualization/virtualization.html",
          "excerpts": [
            "/) VirtIO design implements an interface to transport IPC (\nInter-Process Communication) commands and audio data between host and\nguests.",
            "irtio) is a platform-agnostic virtualization standard, managed by [OASIS](https://www.oasis-open.org/) . It\ndefines a design and a protocol for implementing device IO in VMs.",
            "This\nvirtualization solution creates its own set of IPC operations to redirect\nIPC messages from the guest over virtual queues to the host.",
            "Typically an IPC message exchange between the driver and the DSP takes place\nover a mailbox and a doorbell. A mailbox is a shared memory region between\nthe DSP and host.",
            "A doorbell is\nasynchronous signaling hardware between DSP and host. This mailbox /\ndoorbell interface can be hardware-specific; thus, every hardware variant\nprovides its own set of mailbox / doorbell handling operations."
          ]
        }
      ],
      "reasoning": "The finegrained field value emphasizes security and licensing benefits of virtualization-based driver reuse, including strong fault isolation via isolated VMs, the role of an IOMMU for preventing DMA attacks, the use of VFIO, and a licensing stance that treats host and guest as separate entities to avoid GPLv2 complications, with interfaces like Virtio for standard communication. The most directly relevant passages discuss hardware isolation and security through virtualization: an IOMMU is highly recommended to prevent a compromised driver from accessing system memory, and placing drivers in a separate, unprivileged domain limits attacker reach, supporting strong fault isolation. Additional supporting details describe how virtualization technologies enable hardware access for VMs via PCI passthrough, which is a practical mechanism for driver reuse across a guest OS, and how virtio-based interfaces standardize communication between host and guest devices in virtual environments. The Virtio design materials further ground the approach in a standardized virtualization stack, outlining the IPC and data paths used between guest drivers and the host, which underpins the architectural rationale for virtualization-based driver reuse. Collectively, these excerpts substantiate the architectural security benefits (isolation, IOMMU-protected DMA boundaries, and controlled device access) and the virtualization-centric interface model (Virtio and PCI passthrough) central to the field value. While there are explicit discussions of virtualization mechanisms, there are no explicit excerpts detailing GPLv2 licensing conclusions, so the licensing aspect is inferred from the virtualization model rather than directly evidenced in the excerpts.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.2.complexity",
      "citations": [
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi"
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment."
          ]
        }
      ],
      "reasoning": "The most directly supportive information comes from excerpts describing virtio bindings and abstractions, which show that virtio provides a structured, kernel-aligned interface and a set of Rust-facing abstractions for common virtio devices and transports. This supports the idea that the primary implementation focus could be on client drivers that leverage a stable, well-documented interface rather than building entirely new drivers from scratch for a broad ecosystem of hardware. The bindings excerpt specifically notes Rust bindings to the Linux kernel A, which reinforces the notion that virtio acts as a stable, interoperable layer that can simplify driver development. Following that, the excerpts describing virtio-device abstractions and specific device implementations (console, block, and general device abstractions) further illustrate that virtio encapsulates common functionality and provides default transport behavior, reducing custom implementation effort. The VFIO-related excerpts introduce virtualization technology (IOMMU, device isolation, and user-space driver capability) which is relevant for architectural choices in a virtualization-aware OS, but they are secondary to the core claim about virtio-driven driver simplicity. They contextualize how virtualization and device assignment work in practice and could influence whether a hypervisor layer is needed, but they do not directly support the assertion that virtio-focused driver work is simpler, apart from showing how isolation and user-space access can be managed in a virtualized environment.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.2.security",
      "citations": [
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:"
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi"
          ]
        }
      ],
      "reasoning": "The core claim is that a high level of hardware-enforced isolation can be achieved by employing an IOMMU-based framework (VFIO) to expose devices to user-space in a secure, protected environment, and that grouping devices into IOMMU groups strengthens isolation to prevent unintended DMA across devices. Excerpts describing VFIO as an IOMMU/device-agnostic framework for secure user-space access directly support the strategy of isolating drivers from the Rust OS guest via strict hardware-enforced boundaries. The explanations of IOMMU usage, IOVA address spaces, and the memory isolation they enable reinforce how such a separation reduces the attack surface and confines risks to the hypervisor layer and the virtio interface boundary. The material also notes that VFIO enables user-space drivers and emphasizes DMA protection and device assignment boundaries, which align with a high-security model that keeps guest OS drivers insulated from guest code execution paths. Additionally, the material on IOMMU groups highlighting isolation improvements provides concrete detail about preventing cross-device DMA leakage, further supporting a robust isolation architecture. Contextual references to virtio and its bindings illustrate the software interface layer that would be audited and secured alongside the hardware isolation mechanism, ensuring that the entry points remain tightly controlled under a defensive stack. Taken together, these excerpts corroborate a driver-strategy design that leverages hardware-enforced isolation (IOMMU, VFIO, and IOVA concepts) to achieve strong separation between host/container drivers and the Rust OS, within a rigorously auditable virtio-based interface.",
      "confidence": "high"
    },
    {
      "field": "virtualization_based_reuse_deep_dive.performance_implications",
      "citations": [
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "Any PCI device with PCI Vendor ID 0x1AF4, and PCI Device ID 0x1000 through\n0x107F inclusive is a virtio device.",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ... Virtual I/O Device (VIRTIO) Version 1.3",
            "5.7 GPU Device. virtio-gpu is a virtio based graphics adapter. It can operate in 2D mode and in 3D mode. 3D mode will offload rendering ops to the host gpu ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "The device MUST allow reading of any device-specific configuration field\nbefore FEATURES\\_OK is set by the drive"
          ]
        },
        {
          "title": "Our Prototype on Driver Reuse via Virtual Machines (IöTEC KIT)",
          "url": "https://os.itec.kit.edu/844.php",
          "excerpts": [
            "For example,\nit permits our DD/OS instances to directly translate their\nvirtual addresses into bus addresses for DMA. It also pro-\nvides performance benefit",
            "Our prototype's network performance is within 3–8% of a native Linux system.",
            "When we additionally include all code required for\nthe virtualization—the L4 microkernel, the VMM, and\nthe paravirtualization modifications—we still achieve a\nreuse ratio of 91% just for Linux’s NIC driver base.",
            "The engineering effort enabled us to successfully reuse\nLinux device drivers with all of our tested lab hardware.",
            " Paravirtualization is an enhanced\nmachine API that relocates some functionality from the\nguest OS to the VMM and hypervisor"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "SOF VirtIO design",
          "url": "https://thesofproject.github.io/latest/developer_guides/virtualization/virtualization.html",
          "excerpts": [
            "/) VirtIO design implements an interface to transport IPC (\nInter-Process Communication) commands and audio data between host and\nguests.",
            "irtio) is a platform-agnostic virtualization standard, managed by [OASIS](https://www.oasis-open.org/) . It\ndefines a design and a protocol for implementing device IO in VMs.",
            "This\nvirtualization solution creates its own set of IPC operations to redirect\nIPC messages from the guest over virtual queues to the host.",
            "Typically an IPC message exchange between the driver and the DSP takes place\nover a mailbox and a doorbell. A mailbox is a shared memory region between\nthe DSP and host.",
            "A doorbell is\nasynchronous signaling hardware between DSP and host. This mailbox /\ndoorbell interface can be hardware-specific; thus, every hardware variant\nprovides its own set of mailbox / doorbell handling operations."
          ]
        }
      ],
      "reasoning": "The most relevant information directly supports the performance implications of virtualization-based reuse. One excerpt reports that network performance is within a small margin of native systems, demonstrating that virtualization-based reuse can achieve near-native performance in practical scenarios. Another excerpt states that the code reuse ratio for NIC drivers is effectively 99.9%, with only a tiny 0.1% code-base impact due to translation modules, illustrating that substantial reuse can preserve performance while minimizing overhead. Additional text notes that including the virtualization stack (the microkernel, VMM, and paravirtualization modifications) still yields a high NIC driver reuse ratio, indicating that the overhead from virtualization layers can be kept small while maintaining most of the native performance characteristics. Other excerpts describe how a translation layer to DMA addresses and the use of virtualization efficiencies (paravirtualization) contribute to performance benefits, supporting the idea that virtualization strategies can offer performance advantages or minimal overhead when designed with reuse and efficient interfaces in mind. Together, these pieces of evidence map to the field value's core claim: virtualization introduces some overhead, but with modern techniques such as direct device passthrough (conceptually aligned with high-performance reuse through VFIO-like approaches) and paravirtualization via Virtio, the performance can be very high and often close to native, particularly for NICs and storage paths, while still delivering architecture advantages for an OS built around virtualization-friendly drivers.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.2.strategy_name",
      "citations": [
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi"
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:"
          ]
        }
      ],
      "reasoning": "The strategy value 'Virtualization-based Reuse' is supported by content describing the reuse of virtualization infrastructures rather than building all drivers from scratch. The mention of VFIO as an IOMMU/device-agnostic framework that exposes direct device access to userspace in a secure environment demonstrates leveraging virtualization-oriented pathways for device access and driver development. Descriptions of virtio bindings and virtio device abstractions reveal a structured virtualization layer that can be reused or adapted across OS projects, aligning with a virtualization-based reuse approach. References to Rust FFI bindings to the Linux kernel interfaces indicate practical reuse of existing virtualization-enabled interfaces rather than duplicating kernel-side functionality. Additional context about virtio devices (console, block) illustrates concrete components that would be integrated into a virtualization-focused architecture, reinforcing the feasibility and rationale for reusing virtualization infrastructure. Explanations of how the IOMMU isolates devices and the role of IOVA spaces in device assignment further support a design that relies on virtualization isolation and reuse of established virtualization mechanisms. Overall, these excerpts collectively substantiate using virtualization layers (virtio, VFIO, IOMMU) as the core of the driver strategy rather than crafting entirely new drivers from the ground up.",
      "confidence": "high"
    },
    {
      "field": "gpu_and_display_stack_strategy.recommended_strategy",
      "citations": [
        {
          "title": "Venus on QEMU: Enabling the new virtual Vulkan driver",
          "url": "https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/",
          "excerpts": [
            "Nov 26, 2021 — This blog post describes how to enable 3D acceleration of Vulkan applications in QEMU through the Venus experimental Vulkan driver for VirtIO-GPU with a local ..."
          ]
        },
        {
          "title": "VirGL — The Mesa 3D Graphics Library latest documentation",
          "url": "https://docs.mesa3d.org/drivers/virgl.html",
          "excerpts": [
            "VirGL is a virtual 3D GPU for use inside QEMU virtual machines, that allows the guest operating system to use the capabilities of the host GPU to accelerate 3D ..."
          ]
        },
        {
          "title": "Building the virtualization stack of the future with rust-vmm",
          "url": "https://opensource.com/article/19/3/rust-virtual-machine",
          "excerpts": [
            "The virtio-bindings are the bindings for Virtio devices generated using the Virtio Linux headers."
          ]
        },
        {
          "title": "Linux kernel documentation - Introduction to DRM/KMS",
          "url": "https://www.kernel.org/doc/html//v6.4/gpu/introduction.html",
          "excerpts": [
            "A note on versions: this guide covers features found in the DRM tree,\nincluding the TTM memory manager, output configuration and mode setting,\nand the new vblank internals, in addition to all the regular features\nfound in current kernels."
          ]
        },
        {
          "title": "Kernel DRM/KMS Documentation",
          "url": "https://docs.kernel.org/gpu/drm-kms.html",
          "excerpts": [
            " The final, and real, endpoint in the display chain is the connector (represented\nby [`struct drm_connector`](.drm_connector \"drm\\_connector\") , see [Connector\nAbstraction]() ).",
            " For the output routing the first step is encoders (represented by [`struct drm_encoder`](.drm_encoder \"drm\\_encoder\") , see [Encoder Abstraction]() ).",
            ")\n\nThe basic object structure KMS presents to userspace is fairly simple. Framebuffers (represented by [`struct drm_framebuffer`](.drm_framebuffer \"drm\\_framebuffer\") ,\nsee [Frame Buffer Abstraction]() ) feed into planes. Planes are represented by [`struct drm_plane`](.drm_plane \"drm\\_plane\") , see [Plane Abstraction]() for more\ndetails. One or more (or even no) planes feed their pixel data into a CRTC\n(represented by [`struct drm_crtc`](.drm_crtc \"drm\\_crtc\") , see [CRTC Abstraction]() )\nfor blending. The precise blending step is explained in more detail in [Plane\nComposition Properties]() and related chapters."
          ]
        }
      ],
      "reasoning": "The proposed strategy centers on leveraging a virtualized graphics path rather than native driver development. The supporting content explicitly describes VirGL as a virtual 3D GPU for guest OSes running inside QEMU, enabling acceleration by using the host GPU capabilities through a virtualization boundary. This establishes a concrete mechanism for graphics acceleration in a VM without in-kernel or hardware-specific drivers. In addition, a Venus backend is documented as enabling a virtual Vulkan driver for VirtIO-GPU within QEMU, delivering Vulkan performance close to native by providing a thin transport layer and reducing the need to write or maintain hardware drivers. Together, these pieces validate that a virtualization-first approach with virtio-gpu and Venus/Vulkan can deliver practical, high-performance graphics support early in the OS development life cycle, while avoiding shim-heavy or native driver maintenance costs.\n\nSupporting context from virtualization and DRM/GPU documentation shows how virtual GPUs are structured and exposed (drivers, encoders, connectors, and planes concepts in DRM/KMS) which helps justify integrating a virtio-gpu path in the OS design as a first-class graphics surface inside virtual environments. A discussion of building out the virtualization stack via rust-vmm and virtio tooling reinforces that leveraging the established virtio ecosystem is a scalable and future-proof direction for a research OS, aligning with the goal of focusing on the OS and its application ecosystem rather than writing hardware drivers.\n",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.2.licensing",
      "citations": [
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi"
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment."
          ]
        }
      ],
      "reasoning": "The licensing question hinges on whether a guest or host driver interface can be exposed in a way that avoids a derivative-work relationship with the host license. An excerpt that mentions Rust FFI bindings to the Linux kernel suggests there is an official boundary to interact with Linux components while keeping implementation boundaries, which is central to licensing concerns. Excerpts describing virtio abstractions provide context for how a new OS might expose standardized interfaces to drivers, which is relevant to decoupling from the host's drivers and licenses. However, none of the excerpts explicitly states licensing terms or derivatives; they mainly describe interfaces, abstractions, and how user-space or guest drivers might interact with host/kernel components. Taken together, these excerpts offer contextual support for the architectural approach (FFI bindings, virtio interfaces) but do not directly confirm the favorable licensing claim. Therefore, the connection is indirect and interventionist rather than definitive. Excerpts about VFIO/IOMMU further illustrate device isolation and sharing models that impact where licensing boundaries might lie, though again without explicit licensing guidance.",
      "confidence": "low"
    },
    {
      "field": "driver_strategy_decision_matrix.0.time_to_first_boot",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        }
      ],
      "reasoning": "The target finegrained field value claims a very long timeline to boot a system when rewriting drivers from C to Rust, noting the Rust-for-Linux effort as experimental and not production-ready. Excerpts that discuss using a Foreign Function Interface to bridge Rust and C show a path toward incremental adoption rather than an immediate, full replacement, which aligns with a long, staged timeline rather than a quick rewrite. Specifically, the description of Rust being used to enable Rust to work alongside C via FFI supports the notion of gradual integration rather than wholesale replacement, which is consistent with a lengthy roadmap. Additional content noting that Rust adoption in kernel driver contexts is being explored for security and reliability improvements further supports the idea that this is an evolving effort, not a production-ready rewrite. Finally, surrounding discussions about Rust in Windows driver contexts illustrate broader industry interest in Rust for drivers, but they do not contradict the Linux-focused timeline assertion; they instead provide contextual support that Rust-driven driver development is generally a gradual, architecture-changing process rather than an immediate, monolithic rewrite. Taken together, these excerpts support the notion of a long, incremental, and uncertain pathway to replacing C drivers with Rust in the kernel, rather than a short, decisive migration. ",
      "confidence": "low"
    },
    {
      "field": "virtualization_based_reuse_deep_dive.key_technologies",
      "citations": [
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ... Virtual I/O Device (VIRTIO) Version 1.3",
            "Any PCI device with PCI Vendor ID 0x1AF4, and PCI Device ID 0x1000 through\n0x107F inclusive is a virtio device.",
            "5.7 GPU Device. virtio-gpu is a virtio based graphics adapter. It can operate in 2D mode and in 3D mode. 3D mode will offload rendering ops to the host gpu ...",
            "A Virtio device using Virtio Over PCI Bus MUST expose to guest an interface that\nmeets the specification requirements of the appropriate PCI specification: [[PCI]]() and [[PCIe]]() respectively.",
            "The device MUST allow reading of any device-specific configuration field\nbefore FEATURES\\_OK is set by the drive",
            "Virtual I/O Device (VIRTIO) Version 1.3",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "SOF VirtIO design",
          "url": "https://thesofproject.github.io/latest/developer_guides/virtualization/virtualization.html",
          "excerpts": [
            "/) VirtIO design implements an interface to transport IPC (\nInter-Process Communication) commands and audio data between host and\nguests.",
            "This\nvirtualization solution creates its own set of IPC operations to redirect\nIPC messages from the guest over virtual queues to the host.",
            "Typically an IPC message exchange between the driver and the DSP takes place\nover a mailbox and a doorbell. A mailbox is a shared memory region between\nthe DSP and host.",
            "A doorbell is\nasynchronous signaling hardware between DSP and host. This mailbox /\ndoorbell interface can be hardware-specific; thus, every hardware variant\nprovides its own set of mailbox / doorbell handling operations."
          ]
        }
      ],
      "reasoning": "The key technology described is the Virtio standard, which defines a family of devices used in virtual environments and is mentioned across multiple excerpts as the foundation for paravirtualized I/O. One excerpt explicitly describes virtio as the family of devices used in virtual environments, which directly supports the notion that virtio-net, virtio-blk, and virtio-gpu are part of the architecture. Additional excerpts enumerate specific Virtio device types (e.g., virtio-gpu) and discuss the general requirement and behavior of Virtio devices, including the mechanism by which Virtio devices are exposed and interact with guests. Other excerpts discuss the Virtio design in the context of virtualization, recognizing IPC-like interactions and the concept of a mediated interface between guest and host, which aligns with the idea of virtqueues or similar shared-memory/buffer-based communication patterns. Additional references mention the importance of PCI/virtio device discovery and identity, reinforcing that Virtio devices are a central, standardized mechanism for device virtualization. Taken together, these excerpts support the core claim that Virtio and its device family are the foundational technologies discussed in the field value, including specific device types and the architectural approach to virtualization-driven I/O. The references to Virtio-based GPU and PCI/Virtio device handling further corroborate the notion of a standardized, multi-device Virtio ecosystem as described in the field value, while mentions of IPC structures and doorbells provide contextual support for how such devices communicate in practice. The more specialized aspects in the field value, such as vhost-net, vhost-user, VFIO, and SR-IOV, are not as directly evidenced in the excerpts and thus contribute less to the stated field value, though they are conceptually related to Virtio-based I/O acceleration and direct hardware access.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.0.strategy_name",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer.",
            "swiotlb also provides “sync” APIs that correspond to the dma\\_sync\\_\\*() APIs that\na driver may use when control of a buffer transitions between the CPU and the\ndevice."
          ]
        }
      ],
      "reasoning": "The field value describes a strategy of directly reusing Linux drivers by rewriting or adapting them rather than building new ones from ground up. An excerpt stating that Rust can bridge to C via a Foreign Function Interface underlines a practical approach to leverage existing Linux driver implementations without complete rewrites. An excerpt noting that the driver code would use a wrapper around the existing C API and would be employed by Rust drivers indicates alignment with reuse of existing Linux driver interfaces through adaptation rather than full replacement. Multiple excerpts discuss the DMA-mapping and kernel interfaces that Rust drivers would need to interact with; this supports the feasibility and architectural requirement of reuse via adaptation of established Linux DMA/mapping mechanisms rather than inventing new abstractions from scratch. Together, these excerpts collectively substantiate a strategy where Linux drivers are reused or adapted (via FFI and existing kernel interfaces) instead of writing entirely new drivers for the Rust-based OS, with DMA/mapping infrastructure provided by the kernel as the integration point.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.0.security",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability.",
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day.",
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve."
          ]
        }
      ],
      "reasoning": "The field value asserts that Rust's memory safety guarantees, enforced at compile time, eliminate whole classes of common C vulnerabilities (such as buffer overflows, use-after-free, and data races) in the safe portions of the kernel code, and that this significantly improves security compared to a pure C implementation. Directly supporting this, an excerpt explicitly states that Rust is a memory-safe language designed to address memory-related and kernel security issues, and to enhance Linux driver security and reliability. This provides concrete evidence that adopting Rust could improve the security posture by mitigating well-known kernel vulnerabilities in the safe paths. Additional excerpts reinforce this by describing Rust-driven reductions in attack surface and improved security in Windows driver contexts through Rust's FFI-enabled interoperability and the resulting benefits of memory safety, which aligns with the same security-oriented rationale for kernel components. A further excerpt notes the use of FFI to bridge Rust and C, illustrating a practical architectural approach to gradually introduce Rust while preserving existing security boundaries, which is relevant to the broader architectural implications of adopting Rust in kernel driver code. Overall, the strongest, most direct support comes from the explicit claim about memory safety addressing kernel security issues, with corroborating mentions of reduced attack surface and safer, more secure driver development in practice. The remaining excerpts provide supportive context about interoperability and incremental adoption but are less central to the precise claim about memory-safety-driven security gains in the kernel.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.2.time_to_first_boot",
      "citations": [
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi"
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:",
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality."
          ]
        }
      ],
      "reasoning": "The central claim is that the new Rust-based OS can achieve a short time to first boot by leveraging an established Linux driver ecosystem and allowing user-space or FFI-driven interaction with drivers rather than implementing a large driver stack from scratch. Direct evidence supporting this includes: the existence of Rust FFI bindings to the Linux kernel interfaces, which directly enables Rust OS components to call into mature Linux driver code without reimplementing drivers. The VFIO description reinforces the feasibility of exposing device access to user-space drivers in a secure, IOMMU-protected environment, which is a practical path to reuse Linux driver models and avoid kernel-mode driver development from scratch. The discussion of IOMMU groups enabling PCI device assignment and user-space drivers (such as DPDK) illustrates concrete mechanisms for safely using existing Linux driver stacks from a user-space or OS-abstraction layer, further reducing new-driver required work. Collectively, these excerpts support the idea that a Rust OS can focus on core OS logic while leaning on Linux's driver ecosystem for hardware support, aligning with a short time-to-boot advantage. Additional excerpts describing virtio abstractions and default MMIO transports show concrete virtualization interfaces that can be layered atop or integrated with Linux-based driver mechanisms, further supporting a fast-start architecture that reuses established I/O paths rather than building them anew.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.2.performance",
      "citations": [
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:"
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi",
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A"
          ]
        }
      ],
      "reasoning": "The driver-strategy performance claim rests on how securely and efficiently a userspace driver can access devices and how much overhead the virtualization/virtual I/O stack imposes. Excerpts describing VFIO as an IOMMU-based, device-agnostic framework that enables user-space drivers and isolates devices into IOMMU groups directly support the possibility of high-throughput, near-native performance when using passthrough, since devices can be accessed with strong isolation and minimal host intervention. Additional excerpt details explain that IOMMU creates an address space (IOVA) and that devices are connected to separate addresses, which underpins stable, high-performance device assignment and low cross-device DMA interference—foundational elements for achieving high performance with VT-d/VFIO passthrough and SR-IOV-type configurations. Context about VFIO enabling user-space drivers (and related mentions of DPDK) reinforces the practical viability of high-performance, specialized I/O pathways in a OS architecture. On the other hand, content describing virtio stack abstractions and default MMIO transports indicates there is an overhead in the standard virtio path, and modern implementations with optimizations (e.g., vhost, packed rings) are used to mitigate that overhead. While the excerpts do not provide exact numeric benchmarks or tiered performance figures, they collectively support the core idea that performance tiers are achievable through selective use of VFIO passthrough and SR-IOV enhancements, contrasted with the baseline overhead of standard virtio, and that such architectures rely on secure, isolated IOMMU mechanisms and optimized I/O paths.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.3.security",
      "citations": [
        {
          "title": "Software Sandboxing Basics",
          "url": "https://blog.emilua.org/2025/01/12/software-sandboxing-basics/",
          "excerpts": [
            "Fortunately there’s a well researched model that solves\nthis problem for us: capability-based security.",
            "There’s even a programming\nlanguage based on the actor model and capability-based security:\n[the Pony programming language",
            "A capability isn’t only a reference\nto a resource, but the associated access rights as well. Owning a capability is\nthe same as also having access rights to perform actions",
            "Now that we have messaging solved with the actor model, let’s jump into\nsandboxing (security models) again.",
            ")*” is\none of the most well known phrases within the UNIX culture. If we can send file\ndescriptors then we have a really broad range of resources that we can work with\nfrom sandboxed processes.",
            "The initial versions of the contributed\npatchset were rejected due to the use of the syscall `write` in operations\nthat’d use the credentials of the calling process for permission\nchecks",
            "It’s important to notice that kernel developers will respect the convention\nwhether suid binaries are allowed on our Linux distro or not."
          ]
        },
        {
          "title": "Capsicum, Fuchsia, seL4, and Linux Sandboxing — OCAP-related discussion",
          "url": "https://github.com/dckc/awesome-ocap",
          "excerpts": [
            "Capsicum Capsicum is a lightweight OS capability and sandbox framework that extends the POSIX API, providing several new OS primitives to support object ...",
            "t's based on a\nmicrokernel, Zircon ,\nwith a capability security model. * 2021-06 cr0 blog: A few thoughts on Fuchsia security Julien Tinnes. Sandboxing is trivial. In fact a new process with access to no capabilities",
            "seL4 is the world's first\noperating-system kernel with an end-to-end proof of implementation\ncorrectness and security enforcement; it is available as open\nsource.",
            "2024-03: Fuchsia OS powers all Nest Hub devices – Fuchsia's capability-based component framework is now deployed across all Nest Hub models, replacing the ..."
          ]
        },
        {
          "title": "Capability-based security",
          "url": "https://en.wikipedia.org/wiki/Capability-based_security",
          "excerpts": [
            "Capability-based security refers to the principle of designing user programs such that they directly share capabilities with each other.",
            "Notable research and commercial systems employing capability-based security include the following:",
            "* [FreeBSD](/wiki/FreeBSD \"FreeBSD\") [Capsicum](/wiki/Capsicum_\\(Unix\\) \"Capsicum \\\\(Unix\\\\)\")",
            "* [Genode](/wiki/Genode \"Genode\")",
            "* [Fuchsia](/wiki/Fuchsia_\\(operating_system\\) \"Fuchsia \\\\(operating system\\\\)\")",
            "* [HarmonyOS](/wiki/HarmonyOS \"HarmonyOS\") ( [OpenHarmony](/wiki/OpenHarmony \"OpenHarmony\") )",
            "* [RedoxOS](/wiki/RedoxOS \"RedoxOS\")",
            "* [L4 microkernel family](/wiki/L4_microkernel_family \"L4 microkernel family\") :\n      + OKL4 from Open Kernel Labs\n      + [SeL4](/wiki/SeL4 \"SeL4\") from NICTA"
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "IOMMU groups are available in Red Hat Enterprise Linux 7, which is a significant improvement over the legacy KVM device assignment that is available in Red Hat Enterprise Linux 6. This appendix highlights the following:",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment."
          ]
        }
      ],
      "reasoning": "The field value emphasizes strong security benefits from driver isolation in user-space, with containment of faults to individual processes and enforcement of least privilege via sandboxing and hardware-assisted protection. Excerpts that describe software sandboxing basics and capability-based security provide direct support for isolating drivers, restricting their privileges, and preventing kernel-wide failure. The discussion of capability-based security lists several real-world systems and describes how capabilities grant access rights and containment, aligning with the principle of least privilege for driver isolation. Additionally, material on sandboxing frameworks and OCAP-related discussions reinforces approaches to dividing driver functionality into secure, restricted components, which is exactly the architectural stance described in the field value. Excerpts about Rust's memory safety and FFI bridges explain why integrating drivers in a safer language and interfacing through controlled boundaries can further reduce risk and improve reliability, complementing the isolation strategy. Documentation about DMA/IOMMU and VFIO demonstrates hardware-assisted isolation mechanisms that can enforce memory protection across user-space driver boundaries, which supports the claim that crashes and faults remain contained rather than affecting the kernel. In sum, there is cohesive support across sandboxing concepts (policy and model), capability-based security (permissions model), and hardware-enforced isolation (IOMMU/VFIO) that together validate the security-oriented driver strategy. Excerpts about Rust's FFI and memory safety provide rationale for safer integration, while DMA/IOMMU and VFIO-related materials supply concrete mechanisms for enforcing the isolation in practice.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.3.complexity",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        }
      ],
      "reasoning": "Written in pieces, the excerpts collectively illustrate why writing drivers from scratch is complex. First, bridging Rust with existing C code via a Foreign Function Interface is commonly employed to enable Rust work alongside C, which helps but does not magically eliminate low-level hardware interaction challenges: interoperability via FFI is a practical bridge rather than a complete simplification. Second, Rust is highlighted as memory-safe by default, with adoption aimed at addressing memory-related and kernel security issues, which improves safety but does not remove the need to understand hardware interfaces and kernel mechanisms. Third, DMA and DMA-mapping are central to device driver operation, with the DMA-mapping layer described as an architecture-independent abstraction that drivers must interact with; drivers written in Rust will still need access to and proper use of this mapping layer. Fourth, core DMA/IOMMU documentation emphasizes the need to call dma_map_* before issuing DMA commands and to perform dma_unmap_* when done, underscoring the mechanical and correctness burdens of DMA integration in drivers. Fifth, additional DMA/IOMMU and VFIO-related material reinforces that device access controls, memory translations, and isolation are non-trivial concerns that persist even with higher-level language safety guarantees, further contributing to overall driver complexity. In summary, while Rust and FFI can reduce certain kinds of risks and enable safer, incremental adoption, the foundational tasks—understanding and interfacing with DMA, IOMMU, and hardware devices—remain fundamentally challenging when writing drivers from scratch. The cited excerpts collectively illustrate the shift toward safer, higher-level implementation while acknowledging that the technical architecture must still accommodate low-level hardware interfaces and DMA mechanics for robust driver development.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.3.performance",
      "citations": [
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly references user-space drivers such as DPDK and explains that VFIO, which uses IOMMU groups, enables such user-space access to devices in a secure environment. This directly supports the notion that bypassing the kernel with user-space drivers is a path to very high performance for certain workloads. The next two excerpts discuss using Foreign Function Interfaces to bridge Rust with Linux driver code, which aligns with the proposed approach of building a new OS in Rust that interoperates with existing driver code rather than rewriting everything, thereby influencing architectural decisions around where performance-critical components run. Additional excerpts describe the DMA/mapping and IOMMU infrastructure, which are prerequisites for high-performance I/O in a driver architecture that includes user-space drivers; they explain how DMA access is set up and managed, which is essential for achieving high throughput when bypassing kernel paths. Together, these excerpts form a chain of evidence showing how kernel-bypass (via VFIO/DPDK), language interoperation (FFI with Rust), and low-level DMA/IOMMU support collectively enable the high-performance but resource-intensive approach described in the field value.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.0.performance",
      "citations": [
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it.",
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer.",
            "swiotlb also provides “sync” APIs that correspond to the dma\\_sync\\_\\*() APIs that\na driver may use when control of a buffer transitions between the CPU and the\ndevice."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability.",
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        }
      ],
      "reasoning": "The strongest support comes from excerpts that describe the role of DMA in achieving performance. One excerpt notes that DMA transfers move data directly between RAM and the device, and that without DMA, achieving reasonable I/O performance is difficult, implying that a Rust driver that properly leverages DMA can attain good performance. Additional excerpts describe the need for proper DMA-mapping abstractions and map/unmap lifecycle to interface with the kernel's DMA subsystem, which is essential for safe and efficient high-performance driver operation. The DMA-related documentation excerpts reinforce the idea that correct DMA setup and synchronization are critical parts of performance, and the swiotlb-related excerpts discuss memory buffering and synchronization that can influence throughput and latency in DMA paths. Together, these sources support the notion that a native Rust driver, with correct DMA integration and safe resource management, could reach performance on par with or exceed a traditional C-based driver, while benefiting from Rust's safety guarantees to reduce bugs and long-term maintenance costs. The remaining excerpts discuss Rust/C interoperability and memory-safety benefits more generally, which provide indirect context for why a Rust-based approach could be both safe and performant, but do not directly assert performance outcomes.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.0.complexity",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb also provides “sync” APIs that correspond to the dma\\_sync\\_\\*() APIs that\na driver may use when control of a buffer transitions between the CPU and the\ndevice.",
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer."
          ]
        }
      ],
      "reasoning": "The most relevant evidence concerns the feasibility and implications of using existing Linux drivers through FFI and how such an approach interacts with kernel subsystems and security concerns. The discussion that Rust can bridge to C via a foreign function interface highlights how a Rust-based system might interoperate with the Linux driver model, which is central to evaluating a strategy that points to Linux drivers instead of reimplementing them. The material that notes interoperability between Rust and existing C/C++ driver code reinforces that such an incremental approach is possible but comes with complexities related to integration and safety. Statements that emphasize memory safety and the kernel risk profile when adopting Rust drivers support the broader notion that architectural choices will impact security and reliability requirements, which in turn affect complexity. Documents describing DMA-mapping layers, the need for device driver abstractions to interface with the kernel's DMA subsystem, and the use of wrappers around C APIs provide direct evidence of the low-level architectural dependencies a new OS would inherit if it relies on Linux DMA/IOMMU and related subsystems; these dependencies are precisely the kinds of internal, evolving interfaces that complicate reuse and reimplementation. The notes about swiotlb, dma_map_* / dma_unmap_* lifecycles, and related APIs illustrate concrete API surfaces that must be accommodated, negotiated, or reimplemented, further underscoring the depth of coupling to kernel internals. Taken together, these excerpts collectively substantiate that reusing Linux driver code via FFI would entail contending with a broad and evolving set of kernel subsystems (memory management, locking, device model) and a substantial architectural surface area, which aligns with the claim of very high complexity. The occasional mention that a C API wrapper or incremental integration can be used suggests that there are pathways but does not mitigate the overarching architectural burden highlighted by the core argument.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.3.strategy_name",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ..."
          ]
        },
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr"
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly discuss using Rust in the Linux driver ecosystem and how to bridge Rust with existing driver code via Foreign Function Interfaces. One excerpt explicitly describes Linux driver development with Rust and using an FFI bridge to connect Rust with the C kernel interfaces, which is a central consideration when choosing between native (kernel-space) and user-space driver strategies since it addresses cross-language integration and where the driver code can live. Another excerpt emphasizes Rust's memory-safety advantages and its aim to improve Linux driver security and reliability, which are key factors when evaluating whether to implement drivers in native kernel space or in user space with additional safety layers. Additional excerpts describe interoperability between Rust and C driver code, highlighting incremental Rust adoption and the potential to leverage existing kernel drivers rather than rewriting drivers entirely, which is highly relevant to strategic decisions about native vs user-space deployment. Supporting context from the vm-virtio set shows Rust bindings and abstractions related to virtio devices and kernel interfaces, illustrating practical paths to connect Rust code with kernel-side functionality, further informing the strategic decision. Collectively, these excerpts map the decision space between writing drivers in native kernel space versus exposing or interfacing through user-space mechanisms, by illustrating FFI bridges, memory-safety benefits, and practical Rust-in-Linux driver patterns.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.0.maintenance",
      "citations": [
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer.",
            "swiotlb also provides “sync” APIs that correspond to the dma\\_sync\\_\\*() APIs that\na driver may use when control of a buffer transitions between the CPU and the\ndevice."
          ]
        }
      ],
      "reasoning": "The most directly supportive material discusses the existence of a DMA-mapping layer and the need for abstractions to interface with the kernel's DMA subsystem. Specifically, materials describe that devices and drivers must interact with DMA mappings and that such mappings depend on kernel-provided interfaces. This supports the idea that a driver written in Rust would require abstractions to work with the kernel's DMA/mapping subsystem, and that changes in these interfaces across kernel releases would impose ongoing maintenance costs. The references to the DMA-mapping layer existing to hide complexity, and to the necessity of wrapping or interfacing with dma_map_* and dma_unmap_* APIs, illustrate how upstream changes would ripple into driver code and necessitate constant adaptation. Additional excerpts discuss the IOMMU/DMA context and the need for memory areas for DMA, reinforcing that driver maintenance is tightly coupled to the kernel's DMA/API surface. The excerpts touching on FFI (bridging Rust with C) and general security or Windows/Rust adoption provide helpful context but are less central to the explicit maintenance burden caused by upstream API churn. Overall, the accumulation of kernel DMA mapping responsibilities and the requirement for host-provided abstractions suggests a substantial, possibly unsustainable maintenance burden for a separate OS project if the driver must constantly adapt to kernel changes. This aligns with the described \"Sisyphean task\" of maintaining such a rewritten driver against upstream evolution, as the field value asserts. The least directly supportive items still contribute to the narrative by illustrating how drivers rely on stable or evolving kernel interfaces and how bridging strategies (FFI) interact with those interfaces, but they do not as directly substantiate the maintenance burden claim as the DMA/IOMMU-focused material does.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.1.strategy_name",
      "citations": [
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        }
      ],
      "reasoning": "The target field describes a strategy that relies on interfacing C and Rust via FFI to port or wrap existing C driver code into Rust, enabling incremental adoption. An excerpt that discusses leveraging Rust's FFI to enable interoperability between Rust and existing C/C++ driver code directly supports this conceptual approach, showing how a shim-like boundary can be established to port or wrap drivers. Another excerpt states that to enable Rust to work alongside C, developers use an FFI to bridge the two languages, which is the core mechanism of a shim-based porting strategy. A third excerpt notes that Rust is memory-safe and aims to address kernel security and reliability, which provides rationale for preferring Rust in a driver context and thus reinforces the value of a bridging/shim approach to gradually replace C code. While other excerpts emphasize Rust adoption in specific vendor contexts or domains (e.g., Windows Surface), they still illustrate the broader driver-development trend of using Rust with FFI, thereby contributing additional supporting context for the strategy's viability. The most directly relevant portions are those that explicitly describe FFI-based interoperability between Rust and C/C++ driver code, followed by context about Rust's safety and its suitability for kernel/driver use; less directly relevant but contextually supportive excerpts discuss Rust adoption in driver projects in other environments.",
      "confidence": "high"
    },
    {
      "field": "driver_strategy_decision_matrix.2.maintenance",
      "citations": [
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "The vm-virtio workspace provides virtio abstractions and implementation for the virtio queue and devices.",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr",
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A"
          ]
        }
      ],
      "reasoning": "- The existence of virtio abstractions and a default MMIO transport implementation indicates a structured, reusable interface for virtual devices. This can reduce ongoing maintenance work because driver logic is encapsulated behind a stable abstraction layer, rather than bespoke device-specific code scattered across the stack. The statement that the virtio workspace provides these abstractions and implementations supports the idea of a coherent, mature interface that other drivers (including a Rust OS virtio client) can depend on.\n- The note that there are Rust FFI bindings to the corresponding Linux kernel abstractions further supports lower maintenance needs by enabling a ready-made, well-supported bridge between the Rust OS drivers and the Linux-side implementations. If bindings exist to established kernel interfaces, changes on the kernel side can be absorbed through the bindings, potentially minimizing bespoke maintenance in the Rust OS.\n- Additional context about bindings and virtio devices reinforces the ecosystem's maturity: a set of concrete device abstractions (virtio-console, virtio-blk, virtio-bindings) suggests a stable, multi-component stack around virtio, which is typically associated with backward compatibility and infrequent sweeping changes. This supports the notion that maintaining virtio-based client drivers may incur infrequent updates, contributing to a low long-term maintenance burden. However, none of the excerpts explicitly state stability or backward-compatibility guarantees; the linkage is inferential from the existence of mature abstractions and bindings.\n",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.3.time_to_first_boot",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer.",
            "Device drivers\nneed to be able to set up memory areas for direct memory access (DMA)\ntransfers, though; that means Rust drivers will need a set of\nabstractions to interface with the kernel's DMA-mapping subsystem.",
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA.",
            "Once DMA is completed and mapping is no longer\nrequired, driver performs dma\\_unmap\\_\\*() calls to unmap the region."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "vm-virtio",
          "url": "https://github.com/rust-vmm/vm-virtio",
          "excerpts": [
            "`virtio-bindings` \\-> provides Rust FFI bindings to the corresponding Linux\n  kernel A",
            "`virtio-blk` \\-> contains abstractions for parsing and executing a request of\n  the block devi",
            "`virtio-console` \\-> contains abstractions for the virtio console device",
            "`virtio-device` \\-> provides abstractions for the common functionality of the\n  virtio devices, and a default implementation for the MMIO\n  transport operations (read, wr"
          ]
        }
      ],
      "reasoning": "The statement that a new OS with drivers written from the ground up would face a massive, long-term engineering effort is aligned with the described breadth of driver responsibilities, including DMA setup, memory mappings, and device-specific interfaces. The indicated use of a Foreign Function Interface to bridge Rust and C shows a viable path to leveraging existing driver code or interfaces rather than starting entirely anew, which could shorten timelines but does not remove the scale of the challenge. Additional context on the DMA/IOMMU subsystem highlights why driver development is inherently complex, since devices require careful memory management and mapping, and kernel interfaces (like VFIO and IOMMU concepts) define non-trivial integration points. Together, these excerpts support the view that the time-to-first-boot for a Rust-based OS with a broad driver footprint would be very long if drivers must be written largely from scratch, while suggesting there are architectural levers (FFI, existing abstractions) that could ease some portion of that effort.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.1.security",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability.",
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        }
      ],
      "reasoning": "The most directly supportive information notes that Rust is memory-safe by default and aims to address memory-related issues and kernel security, which aligns with the positive security attributes of the Rust components. Additional excerpts describe interoperability through Foreign Function Interfaces, emphasizing easier integration with existing C/C++ driver code and the potential for security improvements through safer interfaces. Other excerpts discuss Rust-based driver development to reduce the attack surface on Windows and to enhance reliability, which further supports the memory-safety and security benefits side. However, there is an implied tension: bridging Rust with existing C drivers via FFI can create an unsafe boundary where bugs in the shim or in the underlying C driver could compromise the system, suggesting a potential large attack surface. This combination of memory-safety benefits plus the caveat about unsafe shims and C-driver interactions provides a mixed assessment that mirrors the requested field value. Excerpt mentioning a wrapper around the C API used by Rust drivers hints at complexity in the interaction layer, which can contribute to the broader security risk discussion, though it is less explicit about the full attack-surface implications than the more direct statements about safety and interoperability.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.1.complexity",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        }
      ],
      "reasoning": "The most relevant information comes from discussions that explicitly address using a Foreign Function Interface to bridge Rust with existing C driver code. This establishes that the architectural approach centers on cross-language interoperation, a core driver decision that directly affects complexity. Additionally, sources highlighting Rust's memory-safety advantages and security implications reinforce why this approach is attractive and how it shapes implementation concerns related to safety boundaries across language borders. The remaining excerpts reinforce the same theme in different domains (Linux or Windows driver contexts) and note that FFI is used to enable interoperability, which further substantiates that the chosen path centers on bridging languages through FFI, a factor that typically contributes to complexity. The excerpt focusing on DMA-related wrappers around the C API adds context about vendor or project constraints when crossing language boundaries, which can influence where complexity concentrates (e.g., peripheral access patterns). Overall, while the excerpts confirm that FFI-based interfacing and safety goals are central to the architecture, they do not provide explicit, in-depth details of the manual translation workload or unsafe block budgeting, so the inferred complexity is based on general implications of cross-language interop and safety considerations. ",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.1.maintenance",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability.",
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        }
      ],
      "reasoning": "The fine-grained field value centers on a shim that wraps a legacy or external C driver, with maintenance risk arising from tight coupling to specific driver versions and underlying OS APIs. Excerpts that describe using a Foreign Function Interface to bridge Rust and C underscore a fundamental source of maintenance complexity: the shim must stay in sync with the C API and any kernel/driver changes. This is the core mechanism by which brittleness and frequent updates can arise, since changes to the C driver or Linux kernel APIs can compel changes in the shim or its bindings. Excerpts that frame Rust as a memory-safe layer with potential security and reliability gains, while valuable for overall architecture, do not directly state maintenance fragility but imply that the wrapper layer could become a target for update-driven maintenance cycles. The mention of a wrapper around the C API used by Rust drivers in the context of DMA suggests an additional indirection layer whose behavior must be kept compatible with evolving C interfaces, reinforcing the maintenance concern. Together, these excerpts support the notion that a Rust-C shim in a Linux driver stack introduces coupling points (FFI boundaries and C API wrappers) that are likely sources of ongoing maintenance work as APIs evolve, which aligns with the high-maintenance risk described in the target field value, albeit without explicit, direct claims about brittleness.",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.3.maintenance",
      "citations": [
        {
          "title": "LKML Discussion: DMA API and IOMMU (March 6, 2025)",
          "url": "https://lkml.org/lkml/2025/3/6/1236",
          "excerpts": [
            "There is a lot of state tied to the struct device lifecycle that the DMA API and iommu implicitly manages."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Linux kernel DMA/IOMMU documentation",
          "url": "https://docs.kernel.org/arch/x86/iommu.html",
          "excerpts": [
            "Well behaved drivers call dma\\_map\\_\\*() calls before sending command to device\nthat needs to perform DMA."
          ]
        },
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "DMA transfers move data directly between RAM and the device of interest,\nwithout involving the CPU. It is difficult to get any sort of reasonable\nI/O performance without DMA, so almost all devices support it.",
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface. Drivers written in Rust will need to do DMA, so they will need access to\nthe mapping layer."
          ]
        },
        {
          "title": "VFIO IOMMU overview (Red Hat doc)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/app-iommu",
          "excerpts": [
            "VFIO uses IOMMU groups to isolate devices and prevent unintentional Direct Memory Access (DMA) between two devices running on the same host physical machine. VFIO enables user-space drivers such as the _Data Plane Development Kit (DPDK)_ , as well as the more common [PCI device assignment](chap-Guest_virtual_machine_device_configuration) . VFIO uses IOMMU groups to isolate devices and prevent unintentional _Direct Memory Access (DMA)_ between two devices running on the same host physical machine, which would impact host and guest functionality.",
            "An IOMMU creates a virtual address space for the device, where each I/O Virtual Address (IOVA) may translate to different addresses in the physical system memory. When the translation is completed, the devices are connected to a different address within the physical system's memory. Without an IOMMU, all devices have a shared, flat view of the physical memory because they lack memory address translation. With an IOMMU, devices receive the IOVA space as a new address space, which is useful for device assignment."
          ]
        }
      ],
      "reasoning": "The most relevant information comes from discussions that explicitly connect maintenance to who owns and controls the driver stack and its APIs. When the maintenance burden is described as being borne entirely by the OS development team, with the benefit of stable internal APIs as a result, this directly supports the target field value about sole ownership and the stability of internal interfaces. Related excerpts emphasize the adoption of Rust with foreign function interfaces to bridge to existing kernel code, which informs the architectural decision to leverage external interfaces rather than duplicating all kernel integration logic and thus shapes the maintenance profile. Additional excerpts highlight the complexity and lifecycle considerations of DMA/IOMMU-related APIs and mappings in the kernel, underscoring external maintenance challenges that the OS would inherit if it relies on kernel and DMA subsystems. This combination of direct ownership statements, architectural implications of FFI-based integration, and external subsystem maintenance considerations coherently supports a nuanced view of the maintenance burden and API stability described in the finegrained field value. ",
      "confidence": "medium"
    },
    {
      "field": "driver_strategy_decision_matrix.1.performance",
      "citations": [
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts describe the core mechanism central to the field value: using a Foreign Function Interface to bridge Rust with existing C code. One excerpt states that to enable Rust to work alongside C, developers use an FFI to bridge the two languages, directly aligning with the idea of cross-language calls that could introduce non-zero-cost overhead. Another excerpt emphasizes interoperability through Rust's FFI features, highlighting that this approach allows Rust to integrate with existing driver code, which is central to the architectural decision of leveraging existing ecosystems while adopting Rust. While these excerpts do not quantify the performance impact, they establish the architectural pattern that the field value is concerned with—FFI-based cross-language driver interaction that can carry performance implications. Additional excerpts discuss Rust as a memory-safe option to address kernel security and reliability, which informs architectural trade-offs (safety and security benefits) but do not provide performance details. A further excerpt mentions a wrapper around the C API used by Rust drivers in the context of DMA, which touches on how Rust may interact with low-level hardware interfaces, though it still does not quantify performance overhead. Finally, other excerpts reiterate the broader context of Rust adoption for driver development on Windows and Linux, reinforcing the relevance of FFI-enabled interoperation to architectural choices without giving performance figures. Collectively, the excerpts support the existence and use of FFI-based interoperation as a design choice, but do not provide explicit evidence of the asserted performance overhead, making the support for the exact performance claim indirect and incomplete.",
      "confidence": "low"
    },
    {
      "field": "driver_strategy_decision_matrix.1.time_to_first_boot",
      "citations": [
        {
          "title": "Surface is advancing Windows driver development by adopting Rust (Microsoft Surface IT Pro blog)",
          "url": "https://techcommunity.microsoft.com/blog/surfaceitpro/safer-drivers-stronger-devices/4431411",
          "excerpts": [
            "Interoperability: Leveraging Rust’s Foreign Function Interface (FFI) features, developers can enable interoperability between Rust and existing C/C++ driver code, making it easier to adopt Rust incrementally in existing projects. The result? Drivers that are not only more secure, but also easier to maintain and evolve.",
            "Surface is advancing Windows driver development by adopting Rust, a memory-safe programming language, to improve device security and reliability ...",
            "By integrating Rust drivers into our shipping products, we’re reducing the attack surface with memory safety and improving reliability for our customers who depend on Surface every day."
          ]
        },
        {
          "title": "Linux Driver Development with Rust - Apriorit",
          "url": "https://www.apriorit.com/dev-blog/rust-for-linux-driver",
          "excerpts": [
            "To enable Rust to work alongside C, developers use a Foreign Function Interface (FFI) to bridge the two languages.",
            "Rust is a memory-safe language by default, and its adoption is designed both to address memory-related and kernel security issues and to enhance Linux driver security and reliability."
          ]
        },
        {
          "title": "Resistance to Rust abstractions for DMA mapping | Hacker News",
          "url": "https://news.ycombinator.com/item?id=42928121",
          "excerpts": [
            "It is a wrapper around the C API. It has no impact on current code and it will only be used by Rust drivers using DMA, the people of Rust for Linux were clear ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that a pragmatic, faster approach to achieving functionality (time to first boot) comes from leveraging mature, existing C driver code instead of a full rewrite. Excerpts that explicitly describe using a Foreign Function Interface to bridge Rust with existing C drivers demonstrate the exact mechanism of incremental adoption and code reuse, directly supporting the claim of a faster path to enabling hardware support. Additional excerpts that reiterate the benefit of Rust's interoperability with C for driver development further reinforce this pragmatic strategy. Excerpts that discuss security improvements or general benefits of Rust (without tying them to the incremental reuse of C driver code) provide contextual strengths of the approach but are less directly connected to the specific claim about time-to-first-boot via reusing existing code. One excerpt notes that the approach has no impact on current code and will be used by Rust drivers, which supports compatibility and incremental transition albeit in a narrower scope, while another reiterates the purpose of FFI in bridging Rust with C for Linux drivers, aligning with the core claim about a faster, targeted enablement path.",
      "confidence": "medium"
    },
    {
      "field": "phased_hardware_enablement_roadmap",
      "citations": [
        {
          "title": "Linux Rust and DMA-mapping—Jonathan Corbet (LWN), January 30, 2025",
          "url": "https://lwn.net/Articles/1006805/",
          "excerpts": [
            "The DMA-mapping layer exists to hide most of\nthese problems from device drivers behind an architecture-independent\ninterface."
          ]
        },
        {
          "title": "Reddit discussion: Memory safe DMA transfers in Rust",
          "url": "https://www.reddit.com/r/rust/comments/7wco91/eir_memory_safe_dma_transfers/",
          "excerpts": [
            "The correct solution is simply to use an IOMMU and remove the mapping before the buffer is freed, so the hardware cannot DMA into freed memory. >",
            ". If you don't have an IOMMU, you have to instead trust the hardware to cancel DMA when told (since it can DMA anywhere anytime anyway), and tell it to cancel the transfer instead of removing the IOMMU entry."
          ]
        },
        {
          "title": "Buffer Sharing and Synchronization (dma-buf)",
          "url": "https://docs.kernel.org/driver-api/dma-buf.html",
          "excerpts": [
            "The dma-buf subsystem provides the framework for sharing buffers for hardware (DMA) access across multiple device drivers and subsystems, and for synchronizing ..."
          ]
        },
        {
          "title": "Hector Martin: Re: [PATCH v8 2/2] rust: add dma coherent ...",
          "url": "https://lkml.org/lkml/2025/2/4/8",
          "excerpts": [
            "Feb 4, 2025 — Re: [PATCH v8 2/2] rust: add dma coherent allocator abstraction. From, Hector Martin <>. On 2025/01/31 22:54, Jason Gunthorpe wrote: > ..."
          ]
        },
        {
          "title": "dma.txt",
          "url": "https://www.kernel.org/doc/Documentation/usb/dma.txt",
          "excerpts": [
            "It's not using a streaming DMA mapping, so it's good for small transfers on systems where the I/O would otherwise thrash an IOMMU mapping. (See ..."
          ]
        },
        {
          "title": "The Price of Safety: Evaluating IOMMU Performance",
          "url": "https://research.ibm.com/haifa/dept/stt/pubs/iommu-performance-ols07.pdf",
          "excerpts": [
            "by M Ben-Yehuda · Cited by 119 — IOMMUs translate device DMA addresses to machine addresses, restricting device access to granted memory, and create unique address spaces for DMA operations."
          ]
        },
        {
          "title": "Linux swiotlb Documentation",
          "url": "https://docs.kernel.org/core-api/swiotlb.html",
          "excerpts": [
            "swiotlb is a memory buffer allocator used by the Linux kernel DMA layer."
          ]
        },
        {
          "title": "FFI - The Rustonomicon",
          "url": "https://doc.rust-lang.org/nomicon/ffi.html",
          "excerpts": [
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped with unsafe {} as a promise to the compiler that everything contained within ...",
            "ereferencing raw pointers or calling functions that have been marked\nunsafe are only allowed inside unsafe blocks. Unsafe blocks isolate unsafety and are a promise to\nthe compiler that the unsafety does not leak out of the block.",
            "Foreign functions are assumed to be unsafe so calls to them need to be wrapped\nwith `unsafe {}` as a promise to the compiler that everything contained within\ntruly is saf",
            "The raw C API needs to be wrapped to provide memory safety and make use of higher-level concepts\nlike vectors.",
            "Unsafe blocks isolate unsafety and are a promise to\nthe compiler that the unsafety does not leak out of the block.",
            "The `#[link(...)]` attribute is used to\ninstruct the linker to link against the snappy library so the symbols are\nresolved.",
            "When declaring the argument types to a foreign function, the Rust compiler\ncannot check if the declaration is correct, so specifying it correctly is part\nof keeping the binding correct at runtime.",
            "The `extern` block can be extended to cover the entire snappy API:",
            "The `extern` block is a list of function signatures in a foreign library, in\nthis case with the platform's C ABI."
          ]
        },
        {
          "title": "Driver in kernel::platform - Rust",
          "url": "https://rust-for-linux.github.io/docs/kernel/platform/trait.Driver.html",
          "excerpts": [
            "type Data: PointerWrapper + Send + Sync + DeviceRemoval = (). Data stored on device by driver. Corresponds to the data set or retrieved via the kernel's ..."
          ]
        },
        {
          "title": "Rust — The Linux Kernel documentation",
          "url": "https://docs.kernel.org/rust/index.html",
          "excerpts": [
            "Documentation related to Rust within the kernel. To start using Rust in the kernel, please read the Quick Start guide."
          ]
        },
        {
          "title": "vfio_ioctls - Rust",
          "url": "https://docs.rs/vfio-ioctls",
          "excerpts": [
            "In other words, the VFIO framework allows safe, non-privileged, userspace drivers. Why do we want that? Virtual machines often make use of direct device access ..."
          ]
        },
        {
          "title": "What is SPDK",
          "url": "https://spdk.io/doc/about.html",
          "excerpts": [
            "This provides zero-copy, highly parallel access directly to an SSD from a user space application. The driver is written as a C library with a single public ...",
            "The Storage Performance Development Kit (SPDK) provides a set of tools and libraries for writing high performance, scalable, user-mode storage applications.",
            "Polling hardware for completions instead of relying on interrupts, which lowers both total latency and latency variance.",
            "SPDK further provides a full block stack as a user space library that performs many of the same operations as a block stack in an operating system.",
            "The bedrock of SPDK is a user space, polled-mode, asynchronous, lockless NVMe driver. This provides zero-copy, highly parallel access directly to an SSD from a ... SPDK: What is SPDK\n",
            "The bedrock of SPDK is a user space, polled-mode, asynchronous, lockless NVMe driver. This provides zero-copy, highly parallel access directly to an SSD from a ... SPDK: What is SPDK\n",
            "Moving all of the necessary drivers into userspace, which avoids syscalls and enables zero-copy access from the application.",
            "Avoiding all locks in the I/O path, instead relying on message passing."
          ]
        },
        {
          "title": "IPC",
          "url": "https://docs.sel4.systems/Tutorials/ipc.html",
          "excerpts": [
            "Interprocess communication (IPC) is the microkernel mechanism for synchronous transmission of small amounts of data and capabilities between processes."
          ]
        },
        {
          "title": "Message passing",
          "url": "https://www.qnx.com/developers/docs/8.0/com.qnx.doc.neutrino.user_guide/topic/security_MessagePassing.html",
          "excerpts": [
            "Our basic model of operation relies on message passing between the OS kernel, process manager and other services."
          ]
        },
        {
          "title": "Linux Kernel vs. DPDK: HTTP Performance Showdown | Hacker News",
          "url": "https://news.ycombinator.com/item?id=31982026",
          "excerpts": [
            "This was a fascinating read and the kernel does quite nicely in comparison - 66% of DPDK performance is amazing. That said, the article ...",
            "DPDK offloads TCP packet processing from the kernel to user space, achieving higher efficiency and throughput than the kernel's interrupt- ...",
            "Jul 4, 2022 — This post is about comparing two different networking stacks (kernel vs DPDK) to see how they handle a specific (and extreme) HTTP workload."
          ]
        },
        {
          "title": "[seL4] Re: seL4 vs QNX and Linux benchmarks - Devel - Lists",
          "url": "https://lists.sel4.systems/hyperkitty/list/devel@sel4.systems/message/ROR6PFGGH7TPIKRDGKTOYOSXS234QI4N/",
          "excerpts": [
            "Missing: driver isolation"
          ]
        },
        {
          "title": "seL4 Design Principles",
          "url": "https://microkerneldude.org/2020/03/11/sel4-design-principles/",
          "excerpts": [
            "Mar 11, 2020 — But not all of seL4's IPC functionality is equally performance-critical. IPC allows transferring capabilities, which is an inherently more ...",
            "The prime example of a hot operation is IPC: in a microkernel-based system, all system services are provided by user-level servers that are ...",
            "Mar 11, 2020 — kernel moves scheduling context from client to server; kernel unblocks server, which executes the request; having handled the request, server ..."
          ]
        },
        {
          "title": "Message Passing",
          "url": "https://www.qnx.com/developers/docs/6.5.0SP1.update/com.qnx.doc.neutrino_getting_started/s1_msg.html",
          "excerpts": [
            "In QNX lingo, the application is a client requesting the data from a server. This client/server model introduces several process states associated with message ..."
          ]
        },
        {
          "title": "Microkernel Operating Systems",
          "url": "http://www.minix3.org/docs/jorrit-herder/disi08-talk.pdf",
          "excerpts": [
            "All code runs at highest privilege level (breaches POLA). – No proper fault isolation (any bug can be fatal). – Huge amount of code in kernel (highly ..."
          ]
        },
        {
          "title": "rust-for-linux kernel crate documentation",
          "url": "https://rust-for-linux.github.io/docs/kernel/",
          "excerpts": [
            "The kernel crate. This crate contains the kernel APIs that have been ported or wrapped for usage by Rust code in the kernel and is shared by all of them. [clk](clk/index.html \"kernel::clk mod\")",
            "Common clock framework. [cred](cred/index.html \"kernel::cred mod\")",
            "Credentials management. [delay](delay/index.html \"kernel::delay mod\")",
            "Delay functions for operations like sleeping. [device](device/index.html \"kernel::device mod\")",
            "Generic devices that are part of the kernel’s driver model. [driver](driver/index.html \"kernel::driver mod\")",
            "Generic support for drivers of different buses (e.g., PCI, Platform, Amba, etc.). [error](error/index.html \"kernel::error mod\")",
            "Kernel errors. [file](file/index.html \"kernel::file mod\")",
            "Files and file descriptors. [fs](fs/index.html \"kernel::fs mod\")",
            "File systems. [gpio](gpio/index.html \"kernel::gpio mod\")",
            "Support for gpio device drivers. [hwrng](hwrng/index.html \"kernel::hwrng mod\")",
            "Hardware Random Number Generator. [io\\_buffer](io_buffer/index.html \"kernel::io_buffer mod\"",
            "Buffers used in IO. [io\\_mem](io_mem/index.html \"kernel::io_mem mod\"",
            "Memory-mapped IO. [iov\\_iter](iov_iter/index.html \"kernel::iov_iter mod\"",
            "IO vector iterators. [irq](irq/index.html \"kernel::irq mod\")",
            "Interrupts and interrupt chips. [kasync](kasync/index.html \"kernel::kasync mod\")",
            "Kernel async functionality. [kunit](kunit/index.html \"kernel::kunit mod\")",
            "KUnit-based macros for Rust unit tests. [linked\\_list](linked_list/index.html \"kernel::linked_list mod\"",
            "Linked lists. [miscdev](miscdev/index.html \"kernel::miscdev mod\")",
            "Miscellaneous devices. [mm](mm/index.html \"kernel::mm mod\")",
            "Memory management."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - Virtual Function I/O",
          "url": "https://www.kernel.org/doc/html/v6.3/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Rust for Linux",
          "url": "https://rust-for-linux.com/",
          "excerpts": [
            "Rust for Linux is the project adding support for the Rust language to the Linux kernel. This website is intended as a hub of links, documentation and resources."
          ]
        },
        {
          "title": "The First Rust-Written Network PHY Driver Set To Land In ...",
          "url": "https://www.phoronix.com/news/Linux-6.8-Rust-PHY-Driver",
          "excerpts": [
            "Dec 17, 2023 — The first Rust network driver is set to be introduced. Merged this week to net-next.git ahead of Linux 6.8 is landing the net-phy-rust branch."
          ]
        },
        {
          "title": "Rust for Linux",
          "url": "https://en.wikipedia.org/wiki/Rust_for_Linux",
          "excerpts": [
            "In December 2023, the first drivers written in Rust were accepted, and released in version 6.8. ... \"The First Rust-Written Network PHY Driver Set To Land In ..."
          ]
        },
        {
          "title": "The First Rust-Written Network PHY Driver Set To Land In ...",
          "url": "https://www.phoronix.com/forums/forum/hardware/general-hardware/1429237-the-first-rust-written-network-phy-driver-set-to-land-in-linux-6-8",
          "excerpts": [
            "Dec 17, 2023 — Phoronix: The First Rust-Written Network PHY Driver Set To Land In Linux 6.8 Since Linux 6.1 when the very initial Rust infrastructure was ..."
          ]
        },
        {
          "title": "rust-vmm/vfio-ioctls: Safe wrappers for VFIO",
          "url": "https://github.com/rust-vmm/vfio-ioctls",
          "excerpts": [
            "Dec 20, 2021 — The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Microkernel Architecture in Practice",
          "url": "https://www.numberanalytics.com/blog/microkernel-architecture-in-practice",
          "excerpts": [
            "Jun 11, 2025 — In this article, we will explore real-world examples and case studies of microkernel architecture in different domains, including operating systems, embedded ..."
          ]
        },
        {
          "title": "[PDF] seL4 Reference Manual Version 13.0.0",
          "url": "https://sel4.systems/Info/Docs/seL4-manual-latest.pdf",
          "excerpts": [
            "This means, the seL4 capability model is an instance of a segregated (or partitioned) capability system, where capabilities are managed by the ...",
            "In seL4, device drivers run in user space to keep them out of the trusted computing base.",
            "The seL4 microkernel provides a capability-based access-control model.",
            "Device primitives allow device drivers to be implemented as unprivileged applications.",
            "IOSpace This object represents the address space associated with a hardware device. It rep-\nresents the right to modify a device’s address spac",
            "IOPageTable This object represents a node in the multilevel page-table structure used by IOMMU\nhardware to translate hardware memory accesses.",
            "Page capabilities are used to represent the actual frames that are mapped into the I/O address\nspace. A Page can be mapped into either a VSpace or an IOSpace but never into both at the same\ntime.",
            "Interrupts are delivered as notifications.",
            "This is the seL4 Reference Manual, version 13.0.0, covering Kernel Services and Objects, Capability-based Access Control, and SystemCalls."
          ]
        },
        {
          "title": "MINIX 3 isolates drivers in unprivileged processes. - ResearchGate",
          "url": "https://www.researchgate.net/figure/MINIX-3-isolates-drivers-in-unprivileged-processes_fig5_224596767",
          "excerpts": [
            "This work explores the principles and practice of isolating low-level device drivers in order to improve OS dependability."
          ]
        },
        {
          "title": "Linux Kernel vs DPDK: HTTP Performance Showdown",
          "url": "https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/",
          "excerpts": [
            "Jul 4, 2022 — In the end, DPDK still maintains a solid 51% percent performance lead over the kernel. Whether that is a lot or a little depends on your ...",
            "Jul 4, 2022 — In this post I will use a simple HTTP benchmark to do a head-to-head performance comparison between the Linux kernel's network stack, and a kernel-bypass stack ...",
            "DPDK performance clocks in at an impressive 1.19M req/s right out of the gate.",
            "The Linux kernel is designed to be fast, but it is also designed to be multi-purpose, so it isn't *perfectly* optimized for high-speed networking by defaul"
          ]
        },
        {
          "title": "iSCSI vs NVMe-oF: Performance Comparison - StarWind",
          "url": "https://www.starwindsoftware.com/blog/iscsi-vs-nvme-of-performance-comparison/",
          "excerpts": [
            "In contrast, the SPDK iSCSI target's 4K read and write latency is about 50 times higher than that of local NVMe, at 0.806 ms and 0.906 ms ...",
            "Apr 4, 2024 — In contrast, the SPDK iSCSI target's 4K read and write latency is about 50 times higher than that of local NVMe, at 0.806 ms and 0.906 ms ..."
          ]
        },
        {
          "title": "Analyzing the Performance of Linux Networking ...",
          "url": "https://liu.diva-portal.org/smash/get/diva2:1789103/FULLTEXT01.pdf",
          "excerpts": [
            "From the analysis, it was concluded that DPDK had the best overall performance when considering packet loss, throughput, latency, and packet rate. io_uring ...",
            "\n\nOf the three different applications and frameworks, DPDK performs best in all aspects con-\n\ncerning throughput, packet loss, packet rate, and latency.",
            "DPDK was able to generate\n\nthroughput values that were around five times higher than io\\_uring, and the Linux network\n\nstack performed the worst with more than seven times lower throughput than DPD",
            "In\n\nterms of latency, DPDK also outperformed io\\_uring and the Linux network stack, in some\n\ncases with values 100 times lowe",
            "io\\_uring clearly\n\noutperformed the standard Linux network stack in terms of throughput, latency and packet\n\nloss. However, in terms of latency io\\_uring and the standard Linux network stack appear to\n\nperform equal",
            "Between io\\_uring and the Linux network\n\nstack, io\\_uring was able to outperform the Linux network stack in terms of throughput and\n\npacket loss, while the difference in latency was determined to be similar between the tw",
            " also the best performing with a through-\n\nput of 25 Gbit/s compared to io\\_uring with 5.0 Gb/s and the standard Linux network stack\n\nimplementation which performed the worst with a maximum of 3.5 Gbit/s",
            "The latency of the different applications was also in favor of DPDK as can be seen in the\n\nhistogram shown in Figure 5.5, where the mean latency is about 10 _µ_ s lower than io\\_uring\n\nin Figure 5.11 and the standard Linux network stack in Figure 5.1"
          ]
        },
        {
          "title": "Fault isolation for device drivers - IEEE Xplore",
          "url": "https://ieeexplore.ieee.org/document/5270357/",
          "excerpts": [
            "Abstract: This work explores the principles and practice of isolating low-level device drivers in order to improve OS dependability."
          ]
        },
        {
          "title": "Frequently Asked Questions - The seL4 Microkernel",
          "url": "https://sel4.systems/About/FAQ.html",
          "excerpts": [
            "seL4 provides the mechanisms for user-mode device drivers, especially the ability to map device memory to drivers and forward IRQs as (asynchronous) messages.",
            "nel, runs all device drivers in user mode.",
            "Note that to build a system from scratch on a microkernel, you will need basic OS services such as drivers, file system and networking. Some of the tools and ...",
            "Capabilities are an OS abstraction for managing access rights.",
            "as seL4, any operation is authorised by a\ncapability.",
            "The fast path is an add-on frontend to the kernel which performs the simple cases of some common operations quickly. It is key to the high IPC performance seL4 ... H",
            "How can threads communicate? Communication can happen via message-passing IPC or shared memory.",
            "IPC only\nmakes sense for short messages; there is an implementation-defined,\narchitecture-dependent limit on the message size of a few hundred bytes, but\ngenerally messages should be kept to a few dozen bytes so that they fit into\nregisters. For longer messages, a shared buffer should be used.",
            "Or a trusted server could be given\nread access to a sender’s buffer and write access to a receiver’s buffer and\ncopies the data directly from the sender’s to the receiver’s address space. Shared-buffer access can be synchronised via Notification objects.",
            "How does message-passing work? As is characteristic to members of the L4 microkernel family, seL4 uses synchronous IPC . This means a rendez-vous communication model, where the\nmessage is exchanged when both sender and receiver are ready. If both are\nrunning on the same core, this means that one partner will block until the other\ninvokes the IPC operation.",
            "In seL4, IPC is via Endpoint objects. An Endpoint can be considered a mailbox\nthrough which the sender and receiver exchange the message through a handshake."
          ]
        },
        {
          "title": "Performance of microkernel vs monolithic kernel",
          "url": "https://cs.stackexchange.com/questions/29854/performance-of-microkernel-vs-monolithic-kernel",
          "excerpts": [
            "Aug 20, 2014 — A monolithic kernel will be faster, but security will be harder to implement and it will be less modular and less easy to customize."
          ]
        },
        {
          "title": "[PDF] A Lightweight Method for Building Reliable Operating Systems ...",
          "url": "http://www.minix3.org/doc/reliable-os.pdf",
          "excerpts": [
            "This paper discusses our lightweight approach and reports on its performance and reliability. It also compares our design to other proposals for protecting ..."
          ]
        },
        {
          "title": "Rust reference drivers",
          "url": "https://rust-for-linux.com/rust-reference-drivers",
          "excerpts": [
            " are a solution to these deadlocks: they are drivers that subsystem maintainers are allowed to introduce in their subsystem without dropping the existing C driver. This allows maintainers:\n\n1. To bootstrap abstractions for new drivers, i.e. not the \"duplicate\"/\"rewritten\" one, but future new drivers that would use those abstractions; while avoiding breaking the \"no dead code\" rule. 2. To serve as a reference for existing C maintainers on how such drivers would look like in Rust, as \"live\" documentation, e.g. like how [LWN featured a 1:1 comparison](https://lwn.net/Articles/863459/) between a C and Rust driver.\nAnd it has to be buildable at all times. 3. To use all the in-tree kernel infrastructure and to prepare their subsystem for Rust over time, e.g. setting up tests and CI. 4. To learn over time, especially for subsystems that have several maintainers where not everybody may have time for it at a given moment. Reading Rust patches from time to time for APIs one is familiar with can help a lot. 5. And, most importantly, to evaluate if the effort is worth it for their subsystem. For instance, maintainers may ask themselves:\n   \n   \n       + \n         \"How much can we write in safe code?\" + \n         \"How many issues has the reference driver had over time vs. the C one? Did Rust help prevent some?\" + \n         \"How hard is it to maintain the Rust side? Do we have enough resources in our subsystem?\" + \n         etc. A Rust reference driver does not necessarily need to be considered a real driver, e.g. it could be behind `EXPERT` , be tagged `(EXPERIMENTAL)` , staging... The first driver that took advantage of this framework was [`drivers/net/phy/ax88796b_rust.rs`](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/drivers/net/phy/ax88796b_rust.rs?h=v6.8-rc1) , merged in v6.8:\n\n```kconfig\nconfig AX88796B_RUST_PHY\n\tbool \"Rust reference driver for Asix PHYs\"\n\tdepends on RUST_PHYLIB_ABSTRACTIONS && AX88796B_PHY\n\thelp\n\t  Uses the Rust reference driver for Asix PHYs (ax88796b_rust.ko). The features are equivalent.\nIt supports the Asix Electronics PHY\n\t  found in the X-Surf 100 AX88796B package. ```\n\n[](/branches \"Previous chapter\") [](/rust-version-policy \"Next chapter\")\n\n[](/branches \"Previous chapter\") [](/rust-version-policy \"Next chap"
          ]
        },
        {
          "title": "HongMeng kernel (HM) design and Linux driver reuse",
          "url": "https://www.usenix.org/system/files/osdi24-chen-haibo.pdf",
          "excerpts": [
            "he Linux Driver Container (LDC) provides\n\nall necessary Linux KAPIs by reusing the Linux code base\n\nas a userspace runtime, allowing existing Linux drivers\n\nto run without modificatio",
            "h ABI-compliant shim",
            "ABI-compliant shim that identifies and\n\nredirects Linux syscalls to IPC",
            " reuses Linux drivers (Figure 9 ) through a driver con-\n\ntainer, which strikes to find a sweet spot between compatibil-\n\nity, engineering effort, and critical-path perform",
            "river con-\n\ntainer, which strikes to find a sweet spot between compatibil-\n\nity, engineering effort, and critical-path perform",
            "er. The Linux driver container and other non-\n\nperformance-critical OS services, such as CPU frequency\n\ngovernor and power manager, remain in IC2 (",
            "In practice, supporting long-term support (LTS) kernel dis-\n\ntributions is sufficient for reusing most drivers (currently, _HM_\n\nsupports 4.4, 4.19, and ",
            "base creates a virtual timer and a virtual IRQ chip to provide\n\nthe interrupt request and reserves a linear mapped space for\n\nfunctions like virt\\_to\\_phys",
            "The twin driver\n\nrewrites the data handling procedure and can thus be enforced\n\nwith weaker isolation (placed at IC1 in kernel space), resulting\n\nin significantly better performan",
            "twin driver",
            " The control planes, which\n\ncontain cumbersome procedures like init/suspend/resume,\n\nremain in the LDC ( ❺ ). ",
            "he Linux Driver Container (LDC) provides\n\nall necessary Linux KAPIs by reusing the Linux code base\n\nas a userspace runtime, allowing existing Linux drivers\n\nto run without modificati",
            "or performance-\n\ncritical drivers like the Universal Flash Storage (UFS) driver\n\n(others are integrated transparently w/o modification",
            " places the performance-critical OS services in IC1 (ker-\n\nnel space), including the process manager, memory manager,\n\nFS, and native driver container, and coalesces FS with the\n\nmemory manag",
            "HM integrates ex- isting software ecosystems by achieving full Linux API/ABI compatibility through ABI-compliant shim that identifies and."
          ]
        },
        {
          "title": "Simple IPC cycle costs and microkernel overview",
          "url": "https://www.researchgate.net/figure/Simple-IPC-cycle-costs_tbl1_3694140",
          "excerpts": [
            "Microkernel has many intriguing features like security, fault-tolerance, modularity and customizability, which recently stimulate a resurgent interest in both academia and industry (including seL4, QNX and Google's Fuchsia OS). However, IPC (inter-process communication), which is known as the Achilles' Heel of microkernels, is still the major factor for the overall (poor) OS performance.",
            "IPC also plays a vital role in monolithic kernels like Android Linux, as mobile applications frequently communicate with plenty of user-level services through IPC.",
            "The wait/signal performance is comparable to the round-trip IPC performance of fast micro-kernels such as L4 (242 cycles on a 166 MHz Pen- tium [15]) and seL4 (448 cycles on an ARM processor [14] ), although in fairness, IPC involves an address space switch as well as a thread switch.",
            "Microkernels advocate a minimal TCB, that is restricted to\nan Inter-Partition Communication (IPC) infrastructure, a scheduler and memory\nmanagement. Other functionalities are implemented in user-space, isolated from\nthe system’s critical functionalities."
          ]
        },
        {
          "title": "Driver Domain - Xen Project Wiki",
          "url": "https://wiki.xenproject.org/wiki/Driver_Domain",
          "excerpts": [
            "A driver domain is unprivileged Xen domain that has been given responsibility for a particular piece of hardware. It runs a minimal kernel with only that hardware driver and the backend driver for that device class. Thus, if the hardware driver fails, the other domains (including Dom0) will survive and, when the driver domain is restarted, will be able to use the hardware again. As disk driver domains are not currently supported, this page will describe the setup for network driver domains.",
            "Benefits",
            "Performance",
            "This will eliminate dom0 as a bottleneck. All device backend in dom0 will result dom0 to have bad response latency.",
            "Enhanced reliability",
            "Enhanced security",
            "Requirements",
            "Having a system with a modern IOMMU (either AMD or VT-d version 2) is highly recommended. Without IOMMU support, there's nothing to stop the driver domain from using the network card's DMA engine to read and write any system memory.",
            "setup",
            "Hardware drivers are the most failure-prone part of an operating system. It would be good for safety if you could isolate a driver from the rest of the system so that, when it failed, it could just be restarted without affecting the rest of the machine.",
            "Putting this in a separate, unprivileged domain limits the value of attacking the network stack: even if they succeed, they have no more access than a normal unprivileged VM.",
            "Install the xen-related hotplug scripts",
            "Use PCI passthrough to give the VM access to the hardware NIC",
            "Set up network topology in the VM",
            "Configure guests"
          ]
        },
        {
          "title": "Bridging the Gap between Software and Hardware ...",
          "url": "https://www.usenix.org/legacy/event/usenix08/tech/full_papers/santos/santos_html/index.html",
          "excerpts": [
            "In this paper we show how to significantly narrow the performance gap by improving the performance of the driver domain model."
          ]
        },
        {
          "title": "7.1 What are Virtualization Modes or Domain Types?",
          "url": "https://docs.oracle.com/cd/E50245_01/E50249/html/vmcon-vm-modes.html",
          "excerpts": [
            "The main virtualization modes are paravirtualized (PVM), hardware virtualized (HVM), and hardware virtualized with paravirtualized drivers (PVHVM)."
          ]
        },
        {
          "title": "7.2. Virtualization Modes (Domain Types)",
          "url": "https://docs.oracle.com/cd/E26996_01/E18549/html/BABEIIEF.html",
          "excerpts": [
            "Virtual machines may run in one of two main modes, paravirtualized (PVM) or hardware virtualized (HVM). In paravirtualized mode, the kernel of the guest ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ... When VIRTIO\\_F\\_NOTIFICATION\\_DATA has been negotiated, the value has the\nfollowing format:\n\nbe32 {  \nvqn : 16;  \nnext\\_off : 15;  \nnext\\_wrap : 1;  \n};\n\nSee [2\\.7.23]()",
            " Device\n   \n   \n",
            "  The virtio block device is a simple virtual block device (ie. disk).",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU documentation",
          "url": "https://qemu-project.gitlab.io/qemu/interop/vhost-user.html",
          "excerpts": [
            "It implements the control plane needed to establish virtqueue sharing with a user space process on the same host.",
            "This protocol is aiming to complement the `ioctl` interface used to\ncontrol the vhost implementation in the Linux kernel.",
            "While vhost-user was initially developed targeting Linux, nowadays it is supported on any platform that provides the following features: A way for requesting ...",
            "The _back-end_ is the consumer of the virtqueue",
            "The _front-end_ and _back-end_ can be either a client (i.e. connecting) or\nserver (listening) in the socket communication.",
            "The protocol defines 2 sides of the communication, _front-end_ and _back-en",
            "In the current implementation QEMU is the _front-end_ , and the _back-end_ is the external process consuming the virtio queues, for example a\nsoftware Ethernet switch running in user space, such as Snabbswitch,\nor a block device back-end processing read & write to a virtual\nd",
            "It uses communication over a Unix\ndomain socket to share file descriptors in the ancillary data of the\nmessage.",
            "The vhost-user protocol complements the ioctl interface, establishing virtqueue sharing between a user space process and the front-end (like QEMU) and back-end ... Vhost-user Protocol — QEMU documentation",
            "While vhost-user was initially developed targeting Linux, nowadays it\nis supported on any platform that provides the following features:",
            " A way for requesting shared memory represented by a file descriptor\n  so it can be passed over a UNIX domain socket and then mapped by the\n  other process. * \n  AF\\_UNIX sockets with SCM\\_RIGHTS, so QEMU and the other process can\n  exchange messages through it, including ancillary data when needed. * \n  Either eventfd o"
          ]
        },
        {
          "title": "A journey to the vhost-users realm",
          "url": "https://www.redhat.com/en/blog/journey-vhost-users-realm",
          "excerpts": [
            "Sep 24, 2019 — Vhost-user provides a reliable and secure mechanism to offload the network processing to DPDK based applications. It integrates with the vIOMMU ...",
            "VFIO stands for Virtual Function I/O. However, Alex Williamson, maintainer of vfio-pci kernel driver suggested calling it ",
            " “Versatile Framework for userspace I/O,” which might be more accurate. VFIO is basically a framework for building userspace drivers that provides:",
            "VFIO exposes a user friendly API creating character devices (in /dev/vfio/) that support ioctl calls for describing the device, the I/O regions and their read/write/mmap offsets on the device descriptor as well as mechanisms for describing and registering interrupt notifications.",
            "The virtio Poll Mode Driver (virtio-pmd) is one of the many drivers that use the PMD API and offers applications written with DPDK a fast and lock-free access to a virtio device providing the basic functionality of packet reception and transmission using virtio’s virtqueues.",
            "The last element that glues everything together is QEMU’s virtio device model. It has two main tasks:\n    * It emulates a virtio device that shows up in the guest in a specific PCI port that can be probed and configured seamlessly by the guest. In addition, it maps  the ioeventfd to the emulated device’s Memory-mapped I/O space and the irqfd to it’s Global System Interrup"
          ]
        },
        {
          "title": "8. Virtio_user as Exceptional Path - Documentation",
          "url": "https://doc.dpdk.org/guides-17.11/howto/virtio_user_as_exceptional_path.html",
          "excerpts": [
            "Virtio_user with vhost-kernel backend is a solution for exceptional path, such as KNI which exchanges packets with kernel networking stack."
          ]
        },
        {
          "title": "[PDF] Virtio Devices Emulation in SPDK Based On VFIO-USER Protocol",
          "url": "https://kvm-forum.qemu.org/2022/Virtio%20devices%20emulation%20in%20SPDK%20based%20on%20vfio-user%20protocol%20KVM_FORUM_2022%20by%20Changpeng%20Liu%20and%20Xiaodong%20Liu.pdf",
          "excerpts": [
            "VFIO-USER is a protocol that allows a device to be emulated in a separate process outside of a Virtual Machine Monitor (VMM). ▫ The VFIO-USER specification ..."
          ]
        },
        {
          "title": "Virtio on Linux",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "Virtio is an open standard that defines a protocol for communication between drivers and devices of different types, see Chapter 5 (“Device Types”) of the ...",
            "Virtio is an open standard that defines a protocol for communication between drivers and devices of different types, see Chapter 5 (“Device Types”) of the ...",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor.",
            "The virtio spec defines these transport methods in detail, including device discovery, capabilities and interrupt handling. The communication between the driver ..."
          ]
        },
        {
          "title": "A Sledgehammer Approach to Reuse of Legacy Device ...",
          "url": "https://os.itec.kit.edu/834.php",
          "excerpts": [
            "The reused device driver achieved throughput within 2.5% of native Linux, but with 1.46x more CPU utilization for sending, and 2.3x more CPU ...",
            "Sep 19, 2004 — Our flexible approach, requiring only minimal support infrastructure, allows us to run any existing device driver, independently of the OS or ..."
          ]
        },
        {
          "title": "KVM Irqfd Introduction",
          "url": "https://xzpeter.org/htmls/2017_12_07_kvm_irqfd/kvm_irqfd_implementation.html",
          "excerpts": [
            "Irqfd in KVM is implemented based on eventfd in Linux. Eventfd is a simple and fast notification mechanism in Linux kernel to deliver a single and simple ..."
          ]
        },
        {
          "title": "Virtual Machine Network Driver for Microsoft Device Emulator",
          "url": "https://www.microsoft.com/en-us/download/details.aspx?id=2863",
          "excerpts": [
            "The Virtual Machine Network Driver allows the Device emulator's OS (or even the Virtual PC OS, as the case may be) to emulate its own network connection."
          ]
        },
        {
          "title": "Dom0 - Xen Project Wiki",
          "url": "https://wiki.xenproject.org/wiki/Dom0",
          "excerpts": [
            "Dom0 is a privileged domain that starts first and manages the DomU unprivileged domains. The Xen hypervisor is not usable without Dom0."
          ]
        },
        {
          "title": "Virtio devices and drivers overview: Who is who",
          "url": "https://www.redhat.com/en/blog/virtio-devices-and-drivers-overview-headjack-and-phone",
          "excerpts": [
            "Jun 11, 2020 — This section provides a brief overview of the virtio devices, virtio drivers, examples of the different architectures you can use and the different components.",
            "Jun 11, 2020 — This three-part series will take you through the main virtio data plane layouts: the split virtqueue and the packed virtqueue."
          ]
        },
        {
          "title": "Windows VirtIO Drivers",
          "url": "https://pve.proxmox.com/wiki/Windows_VirtIO_Drivers",
          "excerpts": [
            "Aug 28, 2023 — VirtIO Drivers are paravirtualized drivers for kvm/Linux (see http://www.linux-kvm.org/page/Virtio). In short, they enable direct (paravirtualized) access to ..."
          ]
        },
        {
          "title": "Diagnosing Performance Overheads in the Xen Virtual ...",
          "url": "https://www.usenix.org/events/vee05/full_papers/p13-menon.pdf",
          "excerpts": [
            "by A Menon · 2005 · Cited by 763 — Xenoprof, a system-wide profiling toolkit, is used to analyze performance overheads in Xen VMs, focusing on network I/O, and identify main sources of overhead."
          ]
        },
        {
          "title": "Assessment of spatial isolation in Jailhouse: Towards a generic ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S1389128624002342?dgcid=rss_sd_all",
          "excerpts": [
            "The paper introduces the methodology followed for testing spatial isolation and suggests key generic points to consider for assessing hypervisors on this ...",
            "However, significant overhead is introduced in Jailhouse ... Without these protections, this concern becomes more pronounced in Jailhouse and static partitioning ..."
          ]
        },
        {
          "title": "[PDF] Shedding Light on Static Partitioning Hypervisors for Arm-based ...",
          "url": "https://arxiv.org/pdf/2303.11186",
          "excerpts": [
            "by J Martins · 2023 · Cited by 39 — Xen follows the same approach as. Jailhouse, but presents higher overhead. The CAmkES VMM has the highest overhead due to the large number of ..."
          ]
        },
        {
          "title": "A Quantitative Analysis of the Xen Virtualization Overhead",
          "url": "https://homepages.dcc.ufmg.br/~fabricio/download/ccc07.pdf",
          "excerpts": [
            "by F Benevenuto · Cited by 5 — We use four benchmarks which stress different aspects of the virtual system. We compare the performance of the bench- marks on a single virtual machine (VM) ..."
          ]
        },
        {
          "title": "High Performance NVMe Virtualization with SPDK and vfio-user - SNIA",
          "url": "https://www.snia.org/educational-library/high-performance-nvme-virtualization-spdk-and-vfio-user-2021",
          "excerpts": [
            "The most popular example is virtio-blk or virtio-scsi. These devices can be constructed using either the host system operating system (KVM ...",
            "In this talk, we'll cover a new, standardized protocol for virtual machines to use to communicate with other processes that can be used to emulate any PCI ..."
          ]
        },
        {
          "title": "Device-driver reuse via virtual machines",
          "url": "https://scispace.com/pdf/device-driver-reuse-via-virtual-machines-53mvoikrww.pdf",
          "excerpts": [
            "To reuse and isolate a device driver, we execute it and its native OS within a virtual\n\nmachine. The VM virtualizes much of the machine, but not the devices controlled\n\nby the reused drivers: the VM selectively disables virtualization to permit the de-\n\nvice driver OS (DD/OS) to directly access the devices that it controls, thus permit-\n\nting the DD/OS to access the devices’ registers, and to receive hardware int",
            "The VM and hypervisor, however, inhibit the DD/OS from seeing and accessing\n\ndevices which belong to, or are shared by, other VMs",
            "The basis of the reuse approach is to run\n\nthe device drivers in virtual machines along with their original operating system",
            "In this thesis we present the design of a pragmatic driver-reuse system that runs\n\nthe drivers inside virtual machines, along with their original operating systems",
            "The virtual machines help us solve several problems: they protect the invariants of\n\nthe novel operating system from the invariants of the reused drivers, they permit\n\neasy resource provisioning for the reused drivers (by running the drivers in their\n\noriginal operating systems), they permit substantial code reuse, and they enhance\n\ndependabil",
            " introduce a new technique for creating virtual machines — pre-\n\nvirtualization — which combines many of the best features of traditional virtual-\n\nization and para-virtualization, to give good performance and rapid deploym",
            "VMware has demonstrated an alternative approach for reusing binary drivers in\n\ntheir VMWare Workstation product [SVL01] — a cohosting approach. They run\n\nthe drivers within their entire original OS, and share the machine with the new OS\n\n(in this case, VMware’s hypervisor), which is similar to the approach of t",
            "The driver OS shares the\n\nmachine with the new OS via time-division multiplexing. The multiplexing uses\n\nworld switching — the driver OS runs fully privileged when it owns the CPU, but\n\nthen completely relinquishes the CPU when it switches to the new OS, switching\n\neverything including interrupt handlers, page tables, exception handlers, ",
            "This cohosting method offers no trust guarantees: both operating systems run\n\nfully privileged in supervisor mode and can interfere with each other, particularly\n\nif the driver OS malfunction",
            "\nArchitecture\n\n",
            "We call the device driver operating system the device-driver OS\n\n(DD/OS). To connect the drivers to the surrounding system, we install translation\n\nmodules into the DD/OS that communicate across the VM boundary with clients\n\nof the devices, and which convert client requests into internal DD/OS operatio",
            "3.4.5\nShared hardware and recursion",
            "Device drivers assume exclusive access to the hardware device. In many cases\n\nexclusiveness can be guaranteed by partitioning the system and only giving device\n\naccess to a single DD/O",
            "Inherently shared resources, such as the PCI bus and\n\nPCI configuration space, are incompatible with partitioning and require shared and\n\nsynchronized access for multiple DD/OSe",
            "To simplify VM device discovery, additional virtual devices can\n\nbe registered",
            "6.2\nFuture work",
            "We demonstrated driver reuse and pre-virtualization by running Linux drivers\n\nin virtual machines constructed from pre-virtualization, on a research operating\n\nsystem based on the L4Ka::Pistachio microkerne",
            "This dissertation proposes solutions for reusing device drivers in a manner to pro-\n\nmote novel operating system construction. We focus on insulating the novel oper-\n\n148\n\nating system from the invariants and faults of the reused devi",
            "We show how to design the modular runtime support of our approach, par-\n\nticularly with a focus on reliability and performan",
            "We provide reference implementations for running Linux on the Xen hyper-\n\nvisor and the L4Ka::Pistachio microkern",
            "We demonstrated driver reuse and pre-virtualization by running Linux drivers\n\nin virtual machines constructed from pre-virtualization",
            ": Domain model example: database appliance"
          ]
        },
        {
          "title": "Unmodified Device Driver Reuse and Improved System Dependability via Virtual Machines",
          "url": "https://www.usenix.org/event/osdi04/tech/full_papers/levasseur/levasseur.pdf",
          "excerpts": [
            "by J LeVasseur · Cited by 370 — We propose a method to reuse unmodified device drivers and to improve system dependability using vir- tual machines. We run the unmodified device driver, with.",
            "approach enables extensive reuse of existing and unmod-\nified drivers, independent of the OS or device vendor,\nsignificantly reducing the barrier to building new OS en-\ndeavors.",
            "By allowing distinct device drivers to reside in\nseparate virtual machines, this technique isolates faults\ncaused by defective or malicious drivers, thus improving\na system’s dependability.",
            "Our\nprototype’s network performance is within 3–8% of a\nnative Linux system.",
            "Each additional virtual machine in-\ncreases the CPU utilization by about 0.12",
            "We have\nsuccessfully reused a wide variety of unmodified Linux\nnetwork, disk, and PCI device drivers.",
            "In a fully virtualized environment, some device drivers\ncannot be replaced dynamically. Linux, for example,\ndoes not allow substituting the PCI bus driver. In those\ncases, full hardware emulation is required by the VMM.",
            "We implemented a driver reuse system according to the\narchitecture described in the prior sections, and assessed\nthe architecture’s performance, resource, and engineer-\ning cost"
          ]
        },
        {
          "title": "DPDK Virtio-User and Vhost Documentation",
          "url": "https://doc.dpdk.org/guides/howto/virtio_user_as_exception_path.html",
          "excerpts": [
            "In this regard, virtio-user and vhost in kernel space are an alternative to DPDK KNI\nfor transferring packets between a DPDK packet processing application and the kernel stack.",
            "virtio-user, was originally introduced with the vhost-user\nbackend as a high performance solution for IPC (Inter-Process Communication)\nand user space container networking",
            "virtio-user can be used in conjunction with the vhost-kernel backend\nas a solution for dealing with exception path packets\nwhich need to be injected into the Linux kernel for processing there.",
            "virtio-user can be used in conjunction with the vhost-kernel backend\nas a solution for dealing with exception path packets\nwhich need to be injected into the Linux kernel for processing there.",
            "vhost-net is designed to be a networking solution, and, as such,\n  has lots of networking related features,\n  such as multi queue support, TSO, multi-segment buffer support, ",
            " The overview of an application using virtio-user as exception path is shown\nin [Fig. 9.14]()",
            "\n  \n  \n  vhost-net"
          ]
        },
        {
          "title": "Firecracker Issue 3707 (Vhost-net improvements for networking)",
          "url": "https://github.com/firecracker-microvm/firecracker/issues/3707",
          "excerpts": [
            "Firecracker doesn't have fast enough networking for many users.",
            "Firecracker networking benchmarks claiming tens of Gbps use TCP and use network offloads (TSO).",
            "However, often for real users these assumptions can't be met.",
            "Under such conditions - UDP and no offloads - firecracker network performance is dismal.",
            "We can get only 1-2Gbps.",
            "This is really understandable once you realize how the tap code works.",
            "Other kvm-based virtualization systems can do much better, they typically do this by utilizing vhost-net host kernel acceleration.",
            "I'm considering working on adding vhost-net support to firecracker.",
            "Vhost-net is a host kernel API, exposing tap device in a format compatible with virtio-net guest driver.",
            "With shared memory and ioctl's dance it's possible to greatly speed up the data path for networking.",
            "Host vhost-net kernel thread would copy network data directly into the kvm guest memory.",
            "It's possible to set it up in such a way that firecracker process is not touching neither the data nor interrupts when doing networking.",
            "This saves loads of context switches, greatly improves latency and reduces CPU usage."
          ]
        },
        {
          "title": "Hands-on vhost-net — Red Hat Virtio Networking",
          "url": "https://www.redhat.com/en/blog/hands-vhost-net-do-or-do-not-there-no-try",
          "excerpts": [
            "In this post we have guided you through the process of creating a VM with QEMU and vhost-net, inspecting both guest and host in order to understand the ins and outs of this architecture."
          ]
        },
        {
          "title": "seL4/camkes-vm: Virtual Machine built as a ...",
          "url": "https://github.com/seL4/camkes-vm",
          "excerpts": [
            "This repo provides various CAmkES components, templates, interfaces and libraries that can be used to create a CAmkES VMM application."
          ]
        },
        {
          "title": "Storage driver domains",
          "url": "https://wiki.xenproject.org/wiki/Storage_driver_domains",
          "excerpts": [
            "Jul 25, 2014 — This guide will focus on setting up a Linux or FreeBSD guest that will serve disk backends to other guests."
          ]
        },
        {
          "title": "Motūrus OS (Motor OS) - a Rust-first Operating System",
          "url": "https://www.reddit.com/r/rust/comments/190znk5/mot%C5%ABrus_os_motor_os_a_rustfirst_operating_system/",
          "excerpts": [
            "A Rust-first operating system for x64 VMs. What does \"Rust-first\" mean here? It means not only that both the (micro) kernel and the drivers are implemented in ..."
          ]
        },
        {
          "title": "CAmkES CAMKES-VMM Documentation",
          "url": "https://docs.sel4.systems/projects/camkes-vm/",
          "excerpts": [
            "This project is for running virtualised Linux guests on seL4 for ARM and x86 platforms. The `camkes-vm` implements\na virtual machine monitor (VMM) server, facilitating the initialisation, booting and run-time management of a guest OS.",
            "-------\n\nThis project contains various reference CAmkES applications that incorporate the VMM component. These applications include:\n\n* `vm_minimal`: A simple VMM application, launching a single guest Linux with a buildroot RAM-based file system\n* `vm_cross_connector`: Application demonstrating communication between a Linux guest user-level process and a native seL4 component. This leveraging\n  cross vm communication channels established between the VMM and the seL4 component.",
            "| QEMU Virt | 64-bit | Supported | Unsupported | Supported | Supported | Supported | Supported | Multi-VM support depends on porting the TimeServer to QEMU (See https://github.com/sel4/global-components/tree/master/components/TimeServer) |",
            "## Passthrough Ethernet\n\nThe ethernet device is not accessible to the guest:\n\n```\n$ ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n",
            "Cross VM Connectors:* A series of shared dataports and events are established between `vm0` and the `StringReverse` component",
            "Ethernet Driver, UDPSever, Echo, Firewall:* A passthrough ethernet configuration demo. The guest VM is configured to use the Ethernet driver component through a virtio configuration",
            "Passthrough hardware storage (SATA/USB)*: A hardware configuration to boot the CentOS installation",
            "See the `apps/x86/` subdirectory for all the supported virtual machine manager apps, their implementations and the platforms they target.",
            "Arm Features",
            "Arm Features\n------------\n\nSee the below feature matrix for the various features the CAmkES ARM VMM implements/facilitates across the various supported platforms.",
            "| Plaform | Guest Mode | SMP Support | Multiple VM Support | Virtio PCI | Virtio Console | Virtio Net | Cross VM Connector Support | *Notes* |",
            "Currently the supported platforms include:\n\n* Exynos5 (exynos5410, exynos5422)\n* TK1\n* TX1\n* TX2\n* QEMU ARM virt machine\n* ZCU102\n* x86\n* x86\\_6",
            "| --- | --- | --- | --- | --- | --- | --- | --- | --- |",
            "Arm Platform Configuration Notes\n--------------------------------\n\n### Exynos5422, TX1, TX2, ZCU102, QEMU ARM Virt configuration"
          ]
        },
        {
          "title": "rust-vmm ecosystem and Rust-based virtio device backends (Linaro blog)",
          "url": "https://www.linaro.org/blog/rust-device-backends-for-every-hypervisor/",
          "excerpts": [
            "As the name might suggest, rust-vmm is written in Rust.",
            "rust-vmm is organized into many crates that abstract individual aspects of virtualization. Of particular interest for us are vm-memory (abstraction of mmap’d guest memory), virtio-queue (tools for handling virtio descriptors) and vhost-user-backend (implementation of the vhost-user protocol).",
            "Virtio has standardized how paravirtualized guests talk with host devices.",
            "The guest can then efficiently issue requests to a device without caring about the details of the backend implementation.",
            "This removes the need for hypervisor-specific drivers in the guest.",
            "Efficient shared-memory structures also reduce the number of context switches to the hypervisor.",
            "The vhost protocol describes the coordination between the control and data plane happens.",
            "Moving complex handling of untrusted, guest-controlled memory into the kernel may also be undesired from a security perspective.",
            "vhost-user was modeled after vhost but allows offloading backend tasks to a user-space daemon.",
            "Adding Xen support to this range of daemons was mostly enabled by adding support for Xen memory mappings to the core rust-vmm libs.",
            "The virtio Rust ecosystem",
            "As part of project Orko we have been busy improving the ecosystem of edge-computing.",
            "Today, we maintain a quickly growing number of backend daemons under the umbrella of the vhost-device repository.",
            "rust-vmm is written in Rust. While user-space backends may run as unprivileged users and use sandboxing, virtual devices are still a prime target for attackers.",
            "Rust’s type and memory-safety guarantees as well as native speed make it a good choice here.",
            "Overall, rust-vmm is maturing into a versatile and highly reusable ecosystem.",
            "The flexibility makes it a great choice to demonstrate new virtualization technologies and the rigorously tested Rust code base provides confidence in code and changes.",
            "virtio Rust ecosystem",
            "As the name might suggest, rust-vmm is written in Rust. While user-space backends may run as unprivileged users and use sandboxing, virtual devices are still a prime target for attackers.",
            "rust-vmm was founded. Initially starting off with bits from crosvm, rust-vmm components saw use as building blocks for Firecracker and Cloud Hypervisor. As universal, hypervisor agnostic building blocks, these components also were a perfect match for Linaro’s vision of reusable device backends.",
            "daemons for GPIO, I2C, random number generators, SCMI, SCSI, VSOCK, sound and video devices.",
            "The hypervisor can then treat every device class the same, regardless of the actual type.",
            "Virtiofsd, while developed outside of the rust-vmm umbrella, also is based on the same rust-vmm libraries as the vhost-device ones."
          ]
        },
        {
          "title": "SquirrelFS: Rust-native PM filesystem with crash-consistency",
          "url": "https://www.usenix.org/system/files/osdi24_slides-leblanc.pdf",
          "excerpts": [
            "Static checks rely only on existing Rust features",
            "Uses the typestate pattern to statically check ordering of durable updates",
            "Atomic metadata-related system calls (including rename)",
            "Evaluated on 128GB Intel Optane DC Persistent Memory Module",
            "Compared against Ext4-DAX, NOVA, and WineFS",
            "Typestate-checked Synchronous Soft Updates for crash consistency"
          ]
        },
        {
          "title": "30.1. General note on [U]EFI x86_64 support",
          "url": "https://docs.kernel.org/arch/x86/x86_64/uefi.html",
          "excerpts": [
            "Below are general EFI setup guidelines on the x86\\_64 platform,\nregardless of whether you use the EFI stub or a separate bootloader",
            "Booting Linux kernel on UEFI x86_64 platform can either be done using the <The EFI Boot Stub> or using a separate bootloader. x86_64 platform with EFI/UEFI ..."
          ]
        },
        {
          "title": "EFI stub - Gentoo Wiki",
          "url": "https://wiki.gentoo.org/wiki/EFI_stub",
          "excerpts": [
            "An EFI stub (aka EFI boot stub) [1] is a kernel that is an EFI executable, ie that can directly be booted from the UEFI firmware.See more"
          ]
        },
        {
          "title": "efi-stub.rst",
          "url": "https://www.kernel.org/doc/Documentation/admin-guide/efi-stub.rst",
          "excerpts": [
            "On the x86 and ARM platforms, a kernel zImage/bzImage can masquerade\nas a PE/COFF image, thereby convincing EFI firmware loaders to load\nit as an EFI executable.",
            "The bzImage located in arch/x86/boot/bzImage must be copied to the EFI\nSystem Partition (ESP) and renamed with the extension \".efi\".",
            "The arm64 EFI stub lives in drivers/firmware/efi/libstub/arm64.c and drivers/firmware/efi/libstub/arm64-stub.c. By using the EFI boot stub it's possible to boot ...See more",
            "By using the EFI boot stub it's possible to boot a Linux kernel without the use of a conventional EFI boot loader, such as grub or elilo.",
            "For arm64, there is no compressed kernel support, so the Image itself\nmasquerades as a PE/COFF image and the EFI stub is linked into the\nkernel. The arm64 EFI stub lives in drivers/firmware/efi/libstub/arm64.c\nand drivers/firmware/efi/libstub/arm64-stub.c.",
            "For ARM the EFI stub is implemented in\narch/arm/boot/compressed/efi-header.S and\ndrivers/firmware/efi/libstub/arm32-stub.c."
          ]
        },
        {
          "title": "booting.rst",
          "url": "https://www.kernel.org/doc/Documentation/arm64/booting.rst",
          "excerpts": [
            "This document is based on the ARM booting document by Russell King and is ... - The primary CPU must jump directly to the first instruction of the kernel image.",
            "This document is based on the ARM booting document by Russell King and is relevant to all public releases of the AArch64 Linux kernel. The AArch64 exception ..."
          ]
        },
        {
          "title": "Linux and the Devicetree",
          "url": "https://docs.kernel.org/devicetree/usage-model.html",
          "excerpts": [
            "This article describes how Linux uses the device tree. An overview of the device tree data format can be found on the device tree usage page at devicetree.org."
          ]
        },
        {
          "title": "[PDF] ARM64 Kernel Booting Process - NXP Community",
          "url": "https://community.nxp.com/pwmxy87654/attachments/pwmxy87654/Layerscape%40tkb/153/1/ARM64%20Kernel%20Booting%20Process.pdf",
          "excerpts": [
            "Setup the device tree. Requirement: MANDATORY. The device tree blob (dtb) must be placed on an 8-byte boundary and must not exceed 2 megabytes in size. Since ..."
          ]
        },
        {
          "title": "Introduction — QEMU documentation",
          "url": "https://www.qemu.org/docs/master/system/introduction.html",
          "excerpts": [
            "QEMU provides a virtual machine model to run a guest OS, emulating hardware components and supporting various device models."
          ]
        },
        {
          "title": "Writing Virtio Drivers - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/writing_virtio_drivers.html",
          "excerpts": [
            "This document serves as a basic guideline for driver programmers that need to hack a new virtio driver or understand the essentials of the existing ones."
          ]
        },
        {
          "title": "QEMU - ArchWiki",
          "url": "https://wiki.archlinux.org/title/QEMU",
          "excerpts": [
            "Jul 20, 2025 — When used as a machine emulator, QEMU can run OSes and programs made for one machine (e.g. an ARM board) on a different machine (e.g. your x86 ..."
          ]
        },
        {
          "title": "QEMU/Options - Gentoo Wiki",
          "url": "https://wiki.gentoo.org/wiki/QEMU/Options",
          "excerpts": [
            "May 6, 2025 — QEMU options include display, machine, processor, RAM, hard drive, optical drives, boot order, graphics cards, networking, and more."
          ]
        },
        {
          "title": "The EFI Boot Stub",
          "url": "https://docs.kernel.org/admin-guide/efi-stub.html",
          "excerpts": [
            "For the ARM and arm64 architectures, a device tree must be provided to\nthe kernel. Normally firmware shall supply the device tree via the\nEFI CONFIGURATION TABLE. However, the “dtb=” command line option can\nbe used to override the firmware supplied device tree, or to supply\none when firmware is unable to.",
            "The EFI boot stub is enabled with the CONFIG\\_EFI\\_STUB kernel optio",
            "By using the EFI boot stub it's possible to boot a Linux kernel without the use of a conventional EFI boot loader, such as grub or elilo.See",
            "\")\n\nThe bzImage located in arch/x86/boot/bzImage must be copied to the EFI\nSystem Partition (ESP) and renamed with the extension “.efi”.",
            "For ARM the EFI stub is implemented in\narch/arm/boot/compressed/efi-header.S and\ndrivers/firmware/efi/libstub/arm32-stub.c.",
            "The arm64 EFI stub lives in drivers/firmware/efi/libstub/arm64.c\nand drivers/firmware/efi/libstub/arm64-stub.c.",
            "Arguments to the kernel can be passed after bzImage.efi, e.g. :\n\n```\nfs0:> bzImage.efi console=ttyS0 root=/dev/sda4\n```",
            "However, the “dtb=” command line option can\nbe used to override the firmware supplied device tree, or to supply\none when firmware is unable to.",
            "Like most boot loaders, the EFI stub allows the user to specify\nmultiple initrd files using the “initrd=” option.",
            "For ARM the\narch/arm/boot/zImage should be copied to the system partition, and it\nmay not need to be renamed."
          ]
        },
        {
          "title": "EFI boot stub - ArchWiki",
          "url": "https://wiki.archlinux.org/title/EFI_boot_stub",
          "excerpts": [
            "An **EFI boot stub** (aka **EFI stub** ) is a kernel that is an EFI executable, i.e. that can directly be booted from the UEFI.",
            "If you do not want to create a permanent boot entry it is possible to launch the kernel from UEFI Shell since it is a normal UEFI application:",
            "### Using UEFI Shell",
            "If your motherboard has a good UEFI implementation, it is possible to embed the kernel parameters within a UEFI boot entry and for the motherboard to boot Arch directly.",
            "To create a boot entry with _efibootmgr_ that will load the kernel:",
            "```\n# efibootmgr --create --disk /dev/sdX --part Y --label \"Arch Linux\" --loader /vmlinuz-linux --unicode 'root= block_device_identifier rw initrd=\\initramfs-linux.img'\n```"
          ]
        },
        {
          "title": "ARM64 Booting Documentation",
          "url": "https://www.kernel.org/doc/Documentation/arm64/booting.txt",
          "excerpts": [
            "The device tree blob (dtb) must be placed on an 8-byte boundary and must not exceed 2 megabytes in size.",
            "Booting AArch64 Linux",
            "The DT or ACPI tables must describe a GICv2 interrupt controller.",
            "The boot loader is expected to find and initialise all RAM that the\nkernel will use for volatile data storage in the system.",
            "The device tree blob passed by this CPU must contain\n  an 'enable-method' property for each cpu nod",
            "The primary CPU must jump directly to the first instruction of the\n  kernel imag"
          ]
        },
        {
          "title": "QEMU virt ARM/Platform Documentation",
          "url": "https://qemu-project.gitlab.io/qemu/system/arm/virt.html",
          "excerpts": [
            "The `virt` board is a platform which does not correspond to any\nreal hardware; it is designed for use in virtual machines.",
            "The `virt` board automatically generates a device tree blob (“dtb”)\nwhich it passes to the guest. This provides information about the\naddresses, interrupt lines and other configuration of the various devices\nin the syste",
            "The virt board supports:\n\n* PCI/PCIe devices\n* CXL Fixed memory windows, root bridges and devices. * Flash memory\n* Either one or two PL011 UARTs for the NonSecure World\n* An RTC\n* The fw\\_cfg device that allows a guest to obtain data from QEMU\n* A PL061 GPIO controller\n* An optional SMMUv3 IOMMU\n* hotpluggable DIMMs\n* hotpluggable NVDIMMs\n* An MSI controller (GICv2M or ITS)",
            "The `virt` board automatically generates a device tree blob (“dtb”)\nwhich it passes to the guest. This provides information about the\naddresses, interrupt lines and other configuration of the various devices\nin the system. Guest code can rely on and hard-code the following\naddresse",
            "Guest code can rely on and hard-code the following\naddresses:\n\n* \n  Flash memory starts at address 0x0000\\_0000\n* \n  RAM starts at 0x4000\\_00",
            "All other information about device locations may change between\nQEMU versions, so guest code must look in the DTB.",
            "For guests using the Linux kernel boot protocol (this means any\n  non-ELF file passed to the QEMU `-kernel` option) the address\n  of the DTB is passed in a register ( `r2` for 32-bit guests,\n  or `x0` for 64-bit g",
            "For guests booting as “bare-metal” (any other kind of boot),\n  the DTB is at the start of RAM (0x4000\\_00",
            "QEMU supports two types of guest image boot for `virt` , and\nthe way for the guest code to locate the dtb binary differs",
            " For guests using the Linux kernel boot protocol (this means any\n  non-ELF file passed to the QEMU `-kernel` option) the address\n  of the DTB is passed in a register ( `r2` for 32-bit guests,\n  or `x0` for 64-bit gu",
            " For guests booting as “bare-metal” (any other kind of boot),\n  the DTB is at the start of RAM (0x4000\\_000"
          ]
        },
        {
          "title": "QEMU ARM Virtualization Documentation",
          "url": "https://www.qemu.org/docs/master/system/target-arm.html",
          "excerpts": [
            "If you don’t care about reproducing the idiosyncrasies of a particular\nbit of hardware, such as small amount of RAM, no PCI or other hard\ndisk, etc., and just want to run Linux, the best option is to use the `virt` board. This is a platform which doesn’t correspond to any\nreal hardware and is designed for use in virtual machines.",
            "QEMU has generally good support for Arm guests. It has support for\nnearly fifty different machines. The reason we support so many is that\nArm hardware is much more widely varying than x86 hardware.",
            "Arm CPUs\nare generally built into “system-on-chip” (SoC) designs created by\nmany different companies with different devices"
          ]
        },
        {
          "title": "Understanding the (Embedded) Linux boot process",
          "url": "https://kleinembedded.com/understanding-the-embedded-linux-boot-process/",
          "excerpts": [
            "Mar 31, 2023 — In UEFI mode, the entire bootloader is stored directly on the ESP and consists of a main executable (e.g. grubx64.efi ) and a core image ( core."
          ]
        },
        {
          "title": "Where to go after Exiting UEFI Boot Services. : r/osdev",
          "url": "https://www.reddit.com/r/osdev/comments/1163xmx/where_to_go_after_exiting_uefi_boot_services/",
          "excerpts": [
            "The system is just a QEMU x86_64 using x86_64 OVMF UEFI firmware. The boot loader passes to the Kernel the memory map and frame buffer from ..."
          ]
        },
        {
          "title": "ARM boot documentation - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/arm/Booting",
          "excerpts": [
            "Essentially, the boot loader should provide (as a minimum) the following: 1. Setup and initialise the RAM. 2. Initialise one serial port. 3. Detect the machine ..."
          ]
        },
        {
          "title": "ARM64 Architecture — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.1/arm64/index.html",
          "excerpts": [
            "Booting AArch64 Linux · ARM64 CPU Feature Registers · ARM64 ELF hwcaps · HugeTLBpage on ARM64 · Legacy instructions · Memory Layout on AArch64 Linux · Memory ..."
          ]
        },
        {
          "title": "arm - Why Device Tree Structure (DTS) file is needed both ...",
          "url": "https://stackoverflow.com/questions/30711327/why-device-tree-structure-dts-file-is-needed-both-in-bootloader-and-kernel-sou",
          "excerpts": [
            "Why Device Tree Structure (DTS) file is needed both in bootloader and kernel source code? · 2. It's an issue that people have been trying to work ..."
          ]
        },
        {
          "title": "kernel parameters documentation",
          "url": "https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt",
          "excerpts": [
            "acpi_irq_isa= [HW,ACPI] If irq_balance, mark listed IRQs used by ISA Format: <irq>,<irq>... acpi_irq_pci= [HW,ACPI] If irq_balance, clear listed IRQs for use by ...",
            "nobl causes the internal blacklist of systems known to behave incorrectly in some ways with respect to system suspend and resume to be ignored (use wisely)."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html",
          "excerpts": [
            "... ACPI] Limit processor to maximum C-state max_cstate=9 overrides any DMI blacklist limit. processor.nocst [HW,ACPI] Ignore the _CST method to determine C ..."
          ]
        },
        {
          "title": "Cannot boot after UEFI Firmware update - Support and Help",
          "url": "https://discourse.ubuntu.com/t/cannot-boot-after-uefi-firmware-update/57935",
          "excerpts": [
            "In addition, UEFI firmware update resets to defaults. That usually is UEFI Secure Boot on and others. Did you change settings to install Ubuntu ..."
          ]
        },
        {
          "title": "Devicetree (DT) ABI",
          "url": "https://docs.kernel.org/devicetree/bindings/ABI.html",
          "excerpts": [
            "Certainly a stable binding means that a newer kernel will not break on an older device tree, but that doesn't mean the binding is frozen for all time. Grant ..."
          ]
        },
        {
          "title": "How does UEFI know which bootloader configuration to use",
          "url": "https://superuser.com/questions/1728445/how-does-uefi-know-which-bootloader-configuration-to-use",
          "excerpts": [
            "Jun 26, 2022 — UEFI knows nothing - it will usually just present a menu for the bootloaders that it finds. It won't present the menu if it finds only one bootloader."
          ]
        },
        {
          "title": "[SOLVED] EFI stub loading of kernel directly from firmware instead of ...",
          "url": "https://www.linuxquestions.org/questions/slackware-14/efi-stub-loading-of-kernel-directly-from-firmware-instead-of-using-elilo-or-grub-4175733835-new/",
          "excerpts": [
            "I'm doing this on an old 2011 acer Veriton X2610 that has UEFI firmware. I just press F12 to boot other OS from the menu, but beware, if I need ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/kernel-parameters.html",
          "excerpts": [
            "max_cstate= [HW,ACPI] Limit processor to maximum C-state max_cstate=9 overrides any DMI blacklist limit. processor.nocst [HW,ACPI] Ignore the _CST method to ..."
          ]
        },
        {
          "title": "'virt' Generic Virtual Platform (virt)",
          "url": "https://www.qemu.org/docs/master/system/riscv/virt.html",
          "excerpts": [
            "\n\nThe `virt` board is a platform which does not correspond to any real hardware;\nit is designed for use in virtual machines. It is the recommended board type\nif you simply want to run a guest such as Linux and do not care about\nreproducing the idiosyncrasies and limitations of a particular bit of\nreal-world hardware.",
            "The `virt` machine automatically generates a device tree blob (“dtb”)\nwhich it passes to the guest, if there is no `-dtb` option. This provides\ninformation about the addresses, interrupt lines and other configuration of\nthe various devices in the system. Guest software should discover the devices\nthat are present in the generated DTB. If users want to provide their own DTB, they can use the `-dtb` option"
          ]
        },
        {
          "title": "Booting into UEFI Firmware Settings not working",
          "url": "https://www.reddit.com/r/linuxquestions/comments/1d1xvrg/booting_into_uefi_firmware_settings_not_working/",
          "excerpts": [
            "I recently installed Kali Linux on my Acer A515-54-30BQ laptop. Now, I'm trying to boot from a USB drive to reinstall Windows."
          ]
        },
        {
          "title": "oasis-tcs/virtio-spec: OASIS Virtual I/O Device TC - GitHub",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "This repository includes the authoritative source of the VIRTIO (Virtual I/O) Specification document. VIRTIO document describes the specifications of the ...",
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms.",
            "OASIS Virtual I/O Device TC: Source code for the VIRTIO (Virtual I/O) Specification maintained by the OASIS Virtual I/O Device (VIRTIO) Technical Committee"
          ]
        },
        {
          "title": "Chapter 10. KVM Paravirtualized (virtio) Drivers",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/virtualization_host_configuration_and_guest_installation_guide/chap-virtualization_host_configuration_and_guest_installation_guide-para_virtualized_drivers",
          "excerpts": [
            "Paravirtualized drivers enhance the performance of guests, decreasing guest I/O latency and increasing throughput to near bare-metal levels."
          ]
        },
        {
          "title": "ACPI on Arm — Linux Kernel Documentation",
          "url": "https://docs.kernel.org/arch/arm64/arm-acpi.html",
          "excerpts": [
            "The only defined method for passing ACPI tables to the kernel on Arm\nis via the UEFI system configuration table. Just so it is explicit, this\nmeans that ACPI is only supported on platforms that boot via UEFI.",
            "If no command line parameters are used,\nthe kernel will try to use DT for device enumeration; if there is no DT\npresent, the kernel will try to use ACPI tables, but only if they are present.",
            "If acpi=force is used\non the command line, the kernel will attempt to use ACPI tables first, but\nfall back to DT if there are no ACPI tables present.",
            "ACPI support in drivers and subsystems for Arm should never be mutually\nexclusive with DT support at compile time.",
            "ACPI can be used for Armv8 and Armv9 systems designed to follow\nthe BSA (Arm Base System Architecture) [0] and BBR (Arm\nBase Boot Requirements) [1] specifications.",
            "At boot time the kernel will only use one description method depending on\nparameters passed from the boot loader (including kernel bootargs). Regardless of whether DT or ACPI is used, the kernel must always be capable\nof booting with either scheme (in kernels with both schemes enabled at compile\ntime).",
            "For servers in general, the\nresponsibility for hardware behaviour cannot solely be the domain of the\nkernel, but rather must be split between the platform and the kernel, in\norder to allow for orderly change over time.",
            " no ACPI tables present. The basic idea is that\nthe kernel will not fail to boot unless it absolutely has no other choice. ",
            "Device descriptions in ACPI should use standard recognized ACPI interfaces. These may contain less information than is typically provided via a Device\nTree description for the same device."
          ]
        },
        {
          "title": "Arch Linux ACPI BIOS Error thread excerpt (ACPI/UEFI discussion)",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=303736",
          "excerpts": [
            "] ACPI: Early table checksum verification disabled",
            "] ACPI: RSDP 0x000000008BD0D014 000024 (v02 LENOVO)",
            "] ACPI: XSDT 0x000000008BCEC188 000104 (v01 LENOVO CB-01    00000001      01000013)",
            "] ACPI: FACP 0x000000008BCEE000 00010C (v05 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: DSDT 0x000000008BCB8000 03192B (v02 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: UEFI 0x000000008BD0C000 000236 (v01 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: SSDT 0x000000008BD0A000 0017D5 (v02 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: SSDT 0x000000008BD06000 0031D1 (v02 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: SSDT 0x000000008BD02000 003390 (v02 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: UEFI 0x000000008BD01000 000042 (v01 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: SSDT 0x000000008BCFF000 000046 (v02 LENOVO CB-01    00000001 ACPI 00040000)",
            "] ACPI: TPM2 0x000000008BCFE000 000034 (v03 LENOVO CB-01    00000001 ACPI 00040000)"
          ]
        },
        {
          "title": "Zephyr QEMU Emulation for ARM AArch64 Virt KVM",
          "url": "https://docs.zephyrproject.org/latest/boards/qemu/kvm_arm64/doc/index.html",
          "excerpts": [
            "QEMU Emulation for ARM AArch64 Virt KVM",
            "This board configuration will use QEMU to run a KVM guest on an AArch64 host."
          ]
        },
        {
          "title": "'virt' generic virtual platform (virt) - QEMU - Read the Docs",
          "url": "https://qemu.readthedocs.io/en/master/system/arm/virt.html",
          "excerpts": [
            "The virt board is a platform which does not correspond to any real hardware; it is designed for use in virtual machines."
          ]
        },
        {
          "title": "Xen on ARM and the Device Tree vs. ACPI debate",
          "url": "https://xenproject.org/blog/xen-on-arm-and-the-device-tree-vs-acpi-debate/",
          "excerpts": [
            "Device Tree (or DT) and Advanced Configuration & Power Interface (or ACPI) are both standards which are used for describing a hardware platform ..."
          ]
        },
        {
          "title": "Booting AArch64 Linux",
          "url": "https://docs.kernel.org/arch/arm64/booting.html",
          "excerpts": [
            "This document is based on the ARM booting document by Russell King and is relevant to all public releases of the AArch64 Linux kernel."
          ]
        },
        {
          "title": "booting.rst",
          "url": "https://www.kernel.org/doc/Documentation/arch/arm64/booting.rst",
          "excerpts": [
            "This document is based on the ARM booting document by Russell King and is ... If an initrd/initramfs is passed to the kernel at boot, it must reside ...",
            "The device tree blob (dtb) must be placed on an 8-byte boundary and must\nnot exceed 2 megabytes in size."
          ]
        },
        {
          "title": "uefi.txt - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/x86/x86_64/uefi.txt",
          "excerpts": [
            "Booting Linux kernel on UEFI x86_64 platform requires bootloader support. Elilo with x86_64 support can be used. 3. x86_64 platform with EFI/UEFI firmware."
          ]
        },
        {
          "title": "UEFI and ACPI in Arm System Architecture",
          "url": "https://uefi.org/sites/default/files/resources/UEFI%20and%20ACPI%20in%20Arm%20System%20Architecture_Wei.pdf",
          "excerpts": [
            "Oct 12, 2023 — Arm Base Boot Requirements www.uefi.org. 3. Page 4. Arm Base Boot Requirements (BBR). • Expands to include common firmware interfaces, but."
          ]
        },
        {
          "title": "Arm® Base Boot Requirements 1.0 Platform ...",
          "url": "https://documentation-service.arm.com/static/5fb7e4b4d77dd807b9a80c87",
          "excerpts": [
            "Oct 6, 2020 — This document identifies the Arm and industry standard firmware interfaces applicable to the Arm 64-bit architecture. They include the PSCI, ..."
          ]
        },
        {
          "title": "ACPI vs. Device Tree: The Linux Approach to Hardware ...",
          "url": "https://www.linkedin.com/pulse/acpi-vs-device-tree-linux-approach-hardware-tushar-vyavahare-cbxac",
          "excerpts": [
            "ACPI dominates x86 due to its powerful dynamic configuration & power management capabilities. Device Trees thrive on ARM & RISC-V due to their lightweight, ..."
          ]
        },
        {
          "title": "BUD17-205 Updates on SBSA-SBBR. ...",
          "url": "https://static.linaro.org/connect/bud17/Presentations/BUD17-205%20Updates%20on%20SBSA-SBBR.pdf",
          "excerpts": [
            "SBSA/SBBR. ○SBSA/SBBR are the BSA/BBR for the enterprise systems. ○Developed using feedback from vendors across the industry (Silicon vendors, OSVs, ..."
          ]
        },
        {
          "title": "UEFI/ARM Linux Kernel Documentation",
          "url": "https://docs.kernel.org/arch/arm/uefi.html",
          "excerpts": [
            "The “stub” is a feature that extends the Image/zImage into a valid UEFI\nPE/COFF executable, including a loader application that makes it possible to\nload the kernel directly from the UEFI shell, boot menu, or one of the\nlightweight bootloaders like Gummiboot or rEFInd. The kernel image built with stub support remains a valid kernel image for\nbooting in non-UEFI environments.",
            "UEFI kernel support on the ARM architectures (arm and arm64) is only available\nwhen boot is performed through the stub.",
            "When booting in UEFI mode, the stub deletes any memory nodes from a provided DT. Instead, the kernel reads the UEFI memory map.",
            "The stub populates the FDT /chosen node with (and the kernel scans for) the\nfollowing parameters:"
          ]
        },
        {
          "title": "arm-acpi.rst - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/Documentation/arm64/arm-acpi.rst",
          "excerpts": [
            "ACPI can be used for ARMv8 general purpose servers designed to follow the ARM SBSA (Server Base System Architecture) [0] and SBBR (Server Base Boot Requirements) ..."
          ]
        },
        {
          "title": "[Software] [Solved] EFI message on boot: EFI stub: Loaded ...",
          "url": "https://forums.debian.net/viewtopic.php?t=158188",
          "excerpts": [
            "Feb 2, 2024 — When booted with UEFI, the boot loader (grub2) loads the kernel into memory, then use the \"EFI Handover Protocol\" to start the kernel. Then, the ..."
          ]
        },
        {
          "title": "SystemReady Band Integration and Testing Guide",
          "url": "https://documentation-service.arm.com/static/676a886427eda361ad4e3098?token=",
          "excerpts": [
            "Dec 24, 2024 — • SBBR recipe in BBR. • SBSA (only for servers). The Advanced Configuration and Power Interface (ACPI) describes the hardware resources that ..."
          ]
        },
        {
          "title": "Base Boot Requirements (BBR)",
          "url": "https://developer.arm.com/Architectures/Base%20Boot%20Requirements",
          "excerpts": [
            "The Base Boot Requirements, with supplements, defines a standard set of boot and runtime interfaces provided by firmware running on Arm 64-bit Architecture ..."
          ]
        },
        {
          "title": "ARM-software/ebbr: Embedded Base Boot Requirements ...",
          "url": "https://github.com/ARM-software/ebbr",
          "excerpts": [
            "The Embedded Base Boot Requirements specification defines requirements for embedded systems to enable inter-operability between SoCs, hardware platforms, ..."
          ]
        },
        {
          "title": "ACPI in Linux",
          "url": "https://www.kernel.org/doc/ols/2005/ols2005v1-pages-59-76.pdf",
          "excerpts": [
            "by L Brown · Cited by 37 — The ACPICA software can be hosted on any operating system by writing a small and rel- atively simple OS Services Layer (OSL) be- tween the ACPI subsystem and ..."
          ]
        },
        {
          "title": "The Linux/x86 Boot Protocol",
          "url": "https://www.kernel.org/doc/html/v5.6/x86/boot.html",
          "excerpts": [
            "1.16. EFI Handover Protocol",
            "The boot loader *must* fill out the following fields in bp:",
            "efi_main(void *handle, efi_system_table_t *table, struct boot_params *bp)",
            "The boot loader is required to load the kernel/initrd(s)\nfrom the boot media and jump to the EFI handover protocol entry point\nwhich is hdr->handover\\_offset bytes from the beginning of\nstartup\\_{32,64"
          ]
        },
        {
          "title": "Give guidance on reproducible builds · Issue #1865",
          "url": "https://github.com/coreinfrastructure/best-practices-badge/issues/1865",
          "excerpts": [
            "Sep 22, 2022 — Reproducible builds is about blasting away all the useless, meaningless differences: the timestamps of files created during the build, the ..."
          ]
        },
        {
          "title": "Developer best practices: Reproducible builds",
          "url": "https://internetcomputer.org/docs/building-apps/best-practices/reproducible-builds",
          "excerpts": [
            "Apr 10, 2025 — Ideally, provide an executable recipe or script to recreate the build environment using tools such as Docker or Nix."
          ]
        },
        {
          "title": "Blog - KernelCI Foundation",
          "url": "https://kernelci.org/blog/",
          "excerpts": [
            "On-going progress with Chrome OS tests using QEMU and hardware in LAVA; Support for CIP kernel configs; Initial work to enable CIP Core images (Debian based) ...",
            "The Hardware section focuses on the physical devices used for testing, providing insights into how different kernels perform across various hardware platforms."
          ]
        },
        {
          "title": "Custom result handling — LAVA 2025.06 documentation",
          "url": "https://docs.lavasoftware.org/lava/custom-result-handling.html",
          "excerpts": [
            "It hosts data collected by LKFT project. Main source of testing data for LKFT is LAVA. The CI loop for LKFT is constructed from Jenkins (building artifacts) ..."
          ]
        },
        {
          "title": "Tag: kernelci",
          "url": "https://bootlin.com/blog/tag/kernelci/",
          "excerpts": [
            "Since April 2016, we have our own automated testing infrastructure to validate the Linux kernel on a large number of hardware platforms."
          ]
        },
        {
          "title": "Introduction to LAVA — LAVA 2025.06 documentation",
          "url": "https://docs.lavasoftware.org/lava/index.html",
          "excerpts": [
            "LAVA supports data extraction to make it easier to produce a frontend which is directly relevant to particular groups of developers. LAVA is not a build farm - ..."
          ]
        },
        {
          "title": "Linaro's Linux Kernel Functional Test framework | LKFT",
          "url": "https://lkft.linaro.org/",
          "excerpts": [
            "The mission of LKFT is to improve the quality of the Linux kernel by performing functional testing on Arm hardware."
          ]
        },
        {
          "title": "Fuzzing the Kernel with syzkaller. Part 1: Setting up on Mac and ...",
          "url": "https://slava-moskvin.medium.com/fuzzing-the-kernel-with-syzkaller-part-1-setting-up-on-mac-and-crashing-a-vulnerable-driver-b2a3949ea575",
          "excerpts": [
            "syzkaller is one of the best fuzzers for the Linux kernel. It supports coverage (through KCOV) and provides a way to declaratively describe syscalls you want ... It s",
            "The docs even tell you how to set up fuzzing not only for x86\\_64, but also for arm/arm64, riscv64, and s390x",
            "The client was even shorter and looked like this",
            "The next step was to execute `make extract` so that `syzkaller` can extract all necessary constants for every relevant architecture",
            "Yes, it’s 100% possible to use arm64 mac as a host machine for `syzkaller` , and it's also possible to fuzz an out-of-tree kernel module.",
            "syzkaller is one of the best fuzzers for the Linux kernel. It supports coverage (through KCOV) and provides a way to declaratively describe syscalls you want ..."
          ]
        },
        {
          "title": "KCOV: code coverage for fuzzing",
          "url": "https://docs.kernel.org/dev-tools/kcov.html",
          "excerpts": [
            "KCOV collects and exposes kernel code coverage information in a form suitable for coverage-guided fuzzing. Coverage data of a running kernel is exported via the ...",
            "KCOV follows a predefined format for both global and common handles. Each handle is a u64 integer. Currently, only the one top and the lower 4 bytes are used."
          ]
        },
        {
          "title": "kcov: code coverage for fuzzing",
          "url": "https://www.kernel.org/doc/html/v4.14/dev-tools/kcov.html",
          "excerpts": [
            "kcov exposes kernel code coverage information in a form suitable for coverage- guided fuzzing (randomized testing). Coverage data of a running kernel is ..."
          ]
        },
        {
          "title": "Using syzkaller, part 4: Driver fuzzing - Collabora",
          "url": "https://www.collabora.com/news-and-blog/blog/2020/06/26/using-syzkaller-part-4-driver-fuzzing/",
          "excerpts": [
            "A fuzzer can be an invaluable tool during the development and debugging process if we can make it fuzz the particular code we're interested in."
          ]
        },
        {
          "title": "syzbot: automated kernel testing",
          "url": "https://lpc.events/event/2/contributions/237/attachments/61/71/syzbot_automated_kernel_testing.pdf",
          "excerpts": [
            "○ The crash was triggered by the reproducer on fresh machine. ○ Wrong source. ○ Wrong config. ○ No debugging configs. ○ Different hardware. ○ .."
          ]
        },
        {
          "title": "Kernel-Fuzzing.pdf",
          "url": "https://freebsdfoundation.org/wp-content/uploads/2021/01/Kernel-Fuzzing.pdf",
          "excerpts": [
            "Jul 22, 2021 — syzkaller is an open-source coverage-guided kernel fuzzer by Dmitry Vyukov. It originally tar- geted Linux but has since expanded to support ...See more"
          ]
        },
        {
          "title": "Industry Practice of Coverage-Guided Enterprise Linux ...",
          "url": "http://wingtecher.com/themes/WingTecherResearch/assets/papers/fse19-linux-kernel.pdf",
          "excerpts": [
            "by H Shi · 2019 · Cited by 36 — In this paper, we adapt the syzkaller, one of the widely-used kernel fuzzers, to the enterprise Linux kernel used in various real business services, and address ..."
          ]
        },
        {
          "title": "Reproducible builds — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v5.8/kbuild/reproducible-builds.html",
          "excerpts": [
            "This document covers the various reasons why building the kernel may be unreproducible, and how to avoid them."
          ]
        },
        {
          "title": "SameKernel",
          "url": "https://kernelnewbies.org/SameKernel",
          "excerpts": [
            "SameKernel is idea to make kernel reproducible (deterministic, verifiable build). So that each time you build given kernel version (with same configuration),"
          ]
        },
        {
          "title": "Customizing the Kernel (GNU Guix Cookbook)",
          "url": "https://guix.gnu.org/cookbook/en/html_node/Customizing-the-Kernel.html",
          "excerpts": [
            "There are two ways to create a kernel with a custom kernel configuration. The first is to provide a standard defconfig to customize-linux procedure."
          ]
        },
        {
          "title": "KernelCI architecture",
          "url": "https://docs.kernelci.org/intro/architecture/",
          "excerpts": [
            "e labs are managed by independent community members. The KernelCI project does not run or manage any hardware lab. KernelCI hardware labs receives test request directly from KernelCI’s Maestro, so they need to be accessible from the internet and tokens handed to KernelCI sysadmin.\nToday, KernelCI supports only LAVA based lab",
            "The KernelCI architecture",
            "The KernelCI architecture",
            "The KernelCI architecture | KernelCI[KernelCI](/)"
          ]
        },
        {
          "title": "LKFT 101: An Introduction",
          "url": "https://lkft.linaro.org/lkft-101/",
          "excerpts": [
            "LKFT relies heavily on the Linaro Automated Validation Architecture (LAVA) to automate kernel testing on hardware.",
            "Adding a test to test-definitions is the first step in adding it to LKFT. github.com/linaro/lava-test-plans. lava-test-plans contains the LAVA job templates ..."
          ]
        },
        {
          "title": "KernelCI Documentation",
          "url": "https://docs.kernelci.org/",
          "excerpts": [
            "Welcome to the KernelCI documentation website! *\n\nThe KernelCI project’s mission is to ensure the quality, stability and long-term maintenance of the Linux kernel. To achieve that, we are building an ecosystem to foster collaboration around Linux kernel testing and validation, facilitating the process of executing tests for a given subsystem or the whole kernel and following up on the results, **helping you catch issues early and maintain kernel qua",
            "documentation",
            "The upstream Linux kernel test project\nKernelCI[KernelCI](/)",
            "The KernelCI project's mission is to ensure the quality, stability and long-term maintenance of the Linux kernel.",
            "KernelCI architecture"
          ]
        },
        {
          "title": "Reproducible builds - Linux Kernel documentation",
          "url": "https://docs.kernel.org/kbuild/reproducible-builds.html",
          "excerpts": [
            "Reproducible builds",
            "It is generally desirable that building the same source code with\nthe same set of tools is reproducible, i.e. the output is always\nexactly the same.",
            "The kernel embeds timestamps in three places:",
            "This must\nbe overridden using the [KBUILD\\_BUILD\\_TIMESTAMP](kbuild.html) variabl",
            "If you are building from a git commit, you could use its commit date. The kernel does *not* use the `__DATE__` and `__TIME__` macros,\nand enables warnings if they are use",
            "If you incorporate external\ncode that does use these, you must override the timestamp they\ncorrespond to by setting the [SOURCE\\_DATE\\_EPOCH](https://reproducible-builds.org/docs/source-date-epoch/) environment\nvariabl",
            "User, host",
            "The kernel embeds the building user and host names in\n`/proc/version`. These must be overridden using the\n[KBUILD\\_BUILD\\_USER and KBUILD\\_BUILD\\_HOST](kbuild.html) variab",
            "Absolute filenames",
            "When the kernel is built out-of-tree, debug information may include\nabsolute filenames for the source files. This must be overridden by\nincluding the `-fdebug-prefix-map` option in the [KCFLAGS](kbuild.html) variable.",
            "Depending on the compiler used, the `__FILE__` macro may also expand\nto an absolute filename in an out-of-tree build. Kbuild automatically\nuses the `-fmacro-prefix-map` option to prevent this, if it is\nsupported.",
            "Generated files in source packages",
            "The build processes for some programs under the `tools/`\nsubdirectory do not completely support out-of-tree builds. This may\ncause a later source package build using e.g. `make rpm-pkg` to\ninclude generated files.",
            "You should ensure the source tree is\npristine by running `make mrproper` or `git clean -d -f -x` before\nbuilding a source package.",
            "Module signing",
            "If you enable `CONFIG_MODULE_SIG_ALL`, the default behaviour is to\ngenerate a different temporary key for each build, resulting in the\nmodules being unreproducible.",
            "One approach to this is to divide up the build process so that the\nunreproducible parts can be treated as sources:",
            "1. Generate a persistent signing key. Add the certificate for the key\n   to the kernel sour",
            "2. Set the `CONFIG_SYSTEM_TRUSTED_KEYS` symbol to include the\n   signing key’s certificate, set `CONFIG_MODULE_SIG_KEY` to an\n   empty string, and disable `CONFIG_MODULE_SIG_ALL`. Build the kernel and mo",
            "3. Create detached signatures for the modules, and publish them as\n   sourc",
            "4. Perform a second build that attaches the module signatures. It\n   can either rebuild the modules or use the output of step",
            "Structure randomisation",
            "If you enable `CONFIG_RANDSTRUCT`, you will need to pre-generate\nthe random seed in `scripts/basic/randstruct.seed` so the same\nvalue is used by each build.",
            "Debug info conflicts",
            "This is not a problem of unreproducibility, but of generated files\nbeing *too* reproducible. Once you set all the necessary variables for a reproducible build, a\nvDSO’s debug information may be identical even for different kernel\nversions. This can result in file conflicts between debug information\npackages for the different kernel versions. To avoid this, you can make the vDSO different for different\nkernel versions by including an arbitrary string of “salt” in it. This is specified by the Kconfig symbol `CONFIG_BUILD_SALT",
            "Git",
            "Uncommitted changes or different commit ids in git can also lead\nto different compilation results. For example, after executing\n`git reset HEAD^`, even if the code is the same, the\n`include/config/kernel.release` generated during compilation\nwill be different, which will eventually lead to binary differences."
          ]
        },
        {
          "title": "Getting started - The Kani Rust Verifier",
          "url": "https://model-checking.github.io/kani/",
          "excerpts": [
            "Kani is an open-source verification tool that uses model checking to analyze Rust programs. Kani is useful for checking both safety and correctness of Rust code ..."
          ]
        },
        {
          "title": "The Prusti Project: Formal Verification for Rust",
          "url": "https://pm.inf.ethz.ch/publications/AstrauskasBilyFialaGrannanMathejaMuellerPoliSummers22.pdf",
          "excerpts": [
            "by V Astrauskas · Cited by 82 — Besides building upon unoptimised MIR, Prusti reuses the compiler's type and borrow checker to ensure that user-written Prusti annotations follow typing rules."
          ]
        },
        {
          "title": "Creusot helps you prove your code is correct in an automated fashion.",
          "url": "https://github.com/creusot-rs/creusot",
          "excerpts": [
            "Creusot is a deductive verifier for Rust code. It verifies your code is safe from panics, overflows, and assertion failures. By adding annotations you can ..."
          ]
        },
        {
          "title": "Prusti processing stage - Prusti dev guide - Viper Project",
          "url": "https://viperproject.github.io/prusti-dev/dev-guide/pipeline/prusti.html",
          "excerpts": [
            "The MIR is a CFG-based representation that encodes Rust in a highly simplified (and therefore easier to programmatically process) manner. The output of this ..."
          ]
        },
        {
          "title": "Syzbot: 7 years of continuous kernel fuzzing",
          "url": "https://lpc.events/event/17/contributions/1521/attachments/1272/2698/LPC'23_%20Syzbot_%207%20years%20of%20continuous%20kernel%20fuzzing.pdf",
          "excerpts": [
            "Nov 15, 2023 — ○ Syzbot uses syzkaller for the actual fuzzing. ○ ~17k findings ... syzbot contributes to the overload of Linux kernel maintainers.See more"
          ]
        },
        {
          "title": "bsod-kernel-fuzzing",
          "url": "https://github.com/0xf4b1/bsod-kernel-fuzzing",
          "excerpts": [
            "This repository contains the implementations described in BSOD: Binary-only Scalable fuzzing Of device Drivers."
          ]
        },
        {
          "title": "kcov: code coverage for fuzzing — The Linux Kernel 5.10.0 ...",
          "url": "https://www.infradead.org/~mchehab/kernel_docs/dev-tools/kcov.html",
          "excerpts": [
            "kcov exposes kernel code coverage information in a form suitable for coverage- guided fuzzing (randomized testing)."
          ]
        },
        {
          "title": "How Open Source Projects are Using Kani to Write Better ...",
          "url": "https://aws.amazon.com/blogs/opensource/how-open-source-projects-are-using-kani-to-write-better-software-in-rust/",
          "excerpts": [
            "Nov 6, 2023 — An open source automated reasoning tool developed by Amazon Web Services for analyzing and proving properties of your Rust code."
          ]
        },
        {
          "title": "Verification Pipeline - Prusti dev guide - Viper Project",
          "url": "https://viperproject.github.io/prusti-dev/dev-guide/pipeline/summary.html",
          "excerpts": [
            "This chapter summarizes the steps that take place when the user runs Prusti on a given Rust file. ... MIR is encoded into VIR - Prusti's intermediate ..."
          ]
        },
        {
          "title": "[PDF] Creusot: a Foundry for the Deductive Verification of Rust Programs",
          "url": "https://inria.hal.science/hal-03737878v1/document",
          "excerpts": [
            "We present the foundations of Creusot, a tool for the formal specification and deductive verification of Rust code. A first originality comes from Creusot's ..."
          ]
        },
        {
          "title": "Creusot: A Foundry for the Deductive Verification of Rust Programs",
          "url": "https://link.springer.com/chapter/10.1007/978-3-031-17244-1_6",
          "excerpts": [
            "We present the foundations of Creusot, a tool for the formal specification and deductive verification of Rust code."
          ]
        },
        {
          "title": "The Rust Developer's Toolbox: Best Static Code Analysis Tools",
          "url": "https://www.in-com.com/blog/the-rust-developers-toolbox-best-static-code-analysis-tools/",
          "excerpts": [
            "Kani is a formal verification tool purpose-built for analyzing Rust programs at the level of the LLVM intermediate representation (IR). Developed and maintained by AWS, Kani aims to make formal verification of Rust code practical and scalable by performing *bounded model checking* (BMC). This approach systematically explores all possible program states up to a user-specified bound to prove or refute properties about the code.",
            "Kani is particularly well-suited to safety-critical systems, embedded software, cryptographic libraries, and other contexts where developers want high confidence that their Rust code is free from certain classes of bugs. By modeling all feasible execution paths within specified bounds, Kani can detect subtle logical errors that are difficult to uncover with testing or conventional static analysis. Key features include:\n\n* **Bounded model checking**  \n  Systematically analyzes all possible execution paths up to a given bound to ensure correctness properties hold under all scenarios within those limits.",
            "* **Support for Rust assertions**  \n  Verifies standard Rust `assert` statements, ensuring that developer-defined safety and correctness conditions always hold within the chosen bounds. * **Harness-based verification model**  \n  Lets developers write *verification harnesses*, which are specialized entry points used to describe the conditions and inputs Kani should verify, offering fine-grained control over analysis scope.",
            "-\n\nPrusti is a static verifier for Rust programs that brings formal verification techniques into everyday development workflows. Built on top of the Rust compiler, Prusti allows developers to write formal specifications such as preconditions, postconditions, and invariants directly in Rust code using contracts. It then uses automated reasoning to verify these specifications, helping ensure that code behaves correctly in all possible executions. Unlike typical static analysis tools that focus on style or common bug patterns, Prusti targets deep logical correctness. It is designed to catch subtle bugs that might only appear under specific conditions and to provide machine-checked guarantees that certain errors are impossible. By integrating closely with Rust’s ownership and type systems, Prusti enhances the language’s safety model with user-defined behavioral contracts.",
            "Miri** offers a unique approach by interpreting Rust code to detect undefined behavior at compile time, especially valuable when working with unsafe cod",
            "Flowistry is a sophisticated static analysis and visualization tool for Rust that focuses on understanding *dataflow* in Rust programs. Built as a Rust analyzer extension and command-line tool, Flowistry helps developers see how data moves through their code, making ownership, borrowing, and mutation patterns transparent in a way that is often hard to grasp just from reading source code.",
            "For security, **cargo-audit** and **cargo-scan** play vital roles. cargo-audit protects against known supply chain vulnerabilities by checking dependencies for published advisories, while cargo-scan focuses on your own source code, finding insecure patterns before they ship. These tools make sure that the code you write and the libraries you depend on remain",
            "Advanced static analysis and formal verification tools, including **MIRAI**, **Prusti**, **Creusot**, **Kani**, **Seer**, and **Rudra**, address deeper correctness and safety challenges. They help catch subtle logic errors, prove critical invariants, or verify memory safet"
          ]
        },
        {
          "title": "ucsb-seclab/difuze: Fuzzer for Linux Kernel Drivers",
          "url": "https://github.com/ucsb-seclab/difuze",
          "excerpts": [
            "Fuzzer for Linux Kernel Drivers ... This tells MangoFuzz to run for 1000 iterations against all ioctl command value pairs pertaining to the chb ioctl/driver."
          ]
        },
        {
          "title": "Device Emulation — QEMU documentation",
          "url": "https://qemu-project.gitlab.io/qemu/system/device-emulation.html",
          "excerpts": [
            "QEMU supports the emulation of a large number of devices from peripherals such network cards and USB devices to integrated systems on a chip (SoCs)."
          ]
        },
        {
          "title": "How to set up syzbot",
          "url": "https://gitlab.ispras.ru/p.nikulshin/syz-analyzer/-/blob/2e35aef210508f6827e3ac0a8fd02559d488763b/docs/setup_syzbot.md",
          "excerpts": [
            "How to set up syzbot. This doc will be useful to you: should you wish to hack on user interface bits like the dashboard / mailing list integration or ..."
          ]
        },
        {
          "title": "[PDF] Towards an Open Format for Scalable System Telemetry - arXiv",
          "url": "https://arxiv.org/pdf/2101.10474",
          "excerpts": [
            "The pipeline provides a set of reusable components and APIs that enable easy deployment of telemetry probes for host and container workload monitoring, as well ..."
          ]
        },
        {
          "title": "syz-ci/syz-ci.go - third_party/syzkaller - Git at Google",
          "url": "https://fuchsia.googlesource.com/third_party/syzkaller/+/usb-fuzzer/syz-ci/syz-ci.go",
          "excerpts": [
            "// syz-ci is a continuous fuzzing system for syzkaller. // It runs several syz-manager's, polls and rebuilds images for managers. // and polls and rebuilds ..."
          ]
        },
        {
          "title": "Fuzzing Kernel Drivers with Interface Awareness",
          "url": "https://www.blackhat.com/docs/eu-17/materials/eu-17-Corina-Difuzzing-Android-Kernel-Drivers.pdf",
          "excerpts": [
            "Fuzzing kernel drivers involves sending random input to trigger bugs. Modeling each syscall is difficult, especially for ioctls, and generating correct ..."
          ]
        },
        {
          "title": "Building a Pixel kernel with KASAN+KCOV",
          "url": "https://groups.google.com/g/syzkaller/c/UfypyanhQHc",
          "excerpts": [
            "I build a pixel xl kernel with KASAN+KCOV enabled, but the produced kernel image is too large to flash to pixel boot partition."
          ]
        },
        {
          "title": "Hunting Bugs in Linux Kernel With KASAN: How to Use it ...",
          "url": "https://slava-moskvin.medium.com/kernel-quest-from-brain-surgery-to-bug-hunting-with-kasan-0d276383f308",
          "excerpts": [
            "Setting Up KASAN​​ To make use of KASAN, we have to recompile the kernel. So make sure that you have about 40 GB of free space to store the ..."
          ]
        },
        {
          "title": "kcov(1) — Arch manual pages",
          "url": "https://man.archlinux.org/man/kcov.1.en",
          "excerpts": [
            "This manual page documents briefly the kcov command. kcov is a code coverage tester for ELF binaries, Python scripts and shell scripts. It allows collecting ..."
          ]
        },
        {
          "title": "Testing in QEMU",
          "url": "https://www.qemu.org/docs/master/devel/testing/main.html",
          "excerpts": [
            "QEMU’s testing infrastructure is fairly complex as it covers\neverything from unit testing and exercising specific sub-systems all\nthe way to full blown functional tests.",
            "To get an overview of the\ntests you can run `make check-help` from either the source or build\ntree.",
            "The KVM unit tests are designed to run as a Guest OS under KVM but\nthere is no reason why they can’t exercise the TCG as well.",
            "The LTP is focused on exercising the syscall interface of a Linux\nkernel. It checks that syscalls behave as documented and strives to\nexercise as many corner cases as possible. It is a useful test suite\nto run to exercise QEMU’s linux-user code:",
            "GCC gcov support",
            "`gcov` is a GCC tool to analyze the testing coverage by\ninstrumenting the tested code. To use it, configure QEMU with `--enable-gcov` option and build. Then run the tests as usual."
          ]
        },
        {
          "title": "syzkaller and syzbot discussions",
          "url": "https://groups.google.com/g/syzkaller/c/O9rWGc824eU/m/2DKWhBLkCgAJ",
          "excerpts": [
            "\n> syzkaller is a program for finding bugs by generating/testing various\n  \n> testcases regarding system calls (a method for calling functionality\n  \n> provided",
            " is a program for executing testcases syzkaller has generated\n  \n> on a platform called Google Compute Engine and for trying to generate\n  \n> programs for reproducing bugs (in other words, identify conditions for\n  \n> triggering problematic",
            " syzkaller generates reproducers and\n  \ncan run on GCE "
          ]
        },
        {
          "title": "An In-depth Analysis of Duplicated Linux Kernel Bug Reports",
          "url": "https://dl.acm.org/doi/10.1145/3663529.3663828",
          "excerpts": [
            "One of the representative approaches to automatically locate bug-inducing changes is cause bisection.",
            "It bisects a range of code change history and locates the change introducing the bug.",
            "First, we construct a dataset consisting of 1,070 publicly disclosed bugs by Syzbot."
          ]
        },
        {
          "title": "FAQ | KernelCI",
          "url": "https://docs.kernelci.org/faq/",
          "excerpts": [
            "The core KernelCI tools are designed to facilitate working with LAVA labs, but this is not a requirement and other test lab frameworks are also ..."
          ]
        },
        {
          "title": "Linaro/lava-test-plans",
          "url": "https://github.com/Linaro/lava-test-plans",
          "excerpts": [
            "The lava-test-plans project makes it easier to generate LAVA job definition files. It generates the LAVA job definition file from a set of templates."
          ]
        },
        {
          "title": "Test definitions in version control — LAVA 2025.06 ...",
          "url": "https://docs.lavasoftware.org/lava/test-repositories.html",
          "excerpts": [
            "LAVA supports git version control for use with test definitions, and this is the recommended way to host and use test definitions for LAVA. When a repository is ..."
          ]
        },
        {
          "title": "Understanding KernelCI KCIDB: A Guide for Future Contributors",
          "url": "https://abhishekk.hashnode.dev/understanding-kernelci-kcidb-a-guide-for-future-contributors",
          "excerpts": [
            "KCIDB: This project offers a web-based dashboard that showcases test results obtained from continuous integration testing of the Linux kernel."
          ]
        },
        {
          "title": "BlueOS Kernel – Written in Rust, compatible with POSIX",
          "url": "https://news.ycombinator.com/item?id=44669026",
          "excerpts": [
            "It would be momentous to simply get Linux drivers to comply with a kernel-agnostic ABI. ... operating system written in the Rust language from the ..."
          ]
        },
        {
          "title": "Fuzzing with cargo-fuzz - Rust Fuzz Book",
          "url": "https://rust-fuzz.github.io/book/cargo-fuzz.html",
          "excerpts": [
            "cargo-fuzz is the recommended tool for fuzz testing Rust code. cargo-fuzz is itself not a fuzzer, but a tool to invoke a fuzzer."
          ]
        },
        {
          "title": "Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation",
          "url": "https://arxiv.org/html/2505.02464v1",
          "excerpts": [
            "However, Rust builds are not reproducible in all use cases and depend on multiple factors, for example, the absolute build path.",
            "To fuzz the target programs, FourFuzz utilizes afl.rs in version 0.15.3 which is based on AFL + + version 4.10c.",
            " \nWe implement partial instrumentation to focus the fuzzing efforts on unsafe Rust code to significantly increase the efficiency and probability of triggering and detecting vulnerabilities in Rust programs.",
            "This is challenging as Rust uses a complex compilation toolchain:\nThis includes multiple intermediate representations and several compiler stages facilitating Rust-specific code analysis.",
            " as MirChecker  [ 42 ] are static analysis frameworks that utilize intermediate code representations to detect different bug type",
            "FourFuzz improves the fuzzing process by utilizing partial instrumentation which prioritizes code locations that contain potentially memory unsafe Rust code.",
            "Rust is a powerful systems programming language that emphasizes on performance while providing strong safety guarantees."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio",
          "url": "http://wiki.osdev.org/Virtio",
          "excerpts": [
            "VirtIO is a standardized interface which allows virtual machines access to simplified \"virtual\" devices, such as block devices, network adapters and consoles."
          ]
        },
        {
          "title": "virtio-drivers/examples/aarch64/src/main.rs at master",
          "url": "https://github.com/rcore-os/virtio-drivers/blob/master/examples/aarch64/src/main.rs",
          "excerpts": [
            "VirtIO guest drivers in Rust. Contribute to rcore-os/virtio-drivers development by creating an account on GitHub."
          ]
        },
        {
          "title": "How can DPDK access devices from user space?",
          "url": "https://codilime.com/blog/how-can-dpdk-access-devices-from-user-space/",
          "excerpts": [
            "DPDK is a set of libraries for implementing user space drivers for NICs. Read on to see how DPDK works and can access the hardware directly."
          ]
        },
        {
          "title": "Userspace Networking with DPDK",
          "url": "https://www.linuxjournal.com/content/userspace-networking-dpdk",
          "excerpts": [
            "DPDK is a fully open-source project that operates in userspace. It's a multi-vendor and multi-architecture project, and it aims at achieving high I/O ..."
          ]
        },
        {
          "title": "PCI passthrough via OVMF",
          "url": "https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF",
          "excerpts": [
            "Select the graphics and audio devices you want to passthrough and then select LOAD VFIO. ... A guide with decriptions of the differences is available from redhat[ ...",
            "It is now possible to passthrough a graphics card, offering the virtual machine native graphics performance which is useful for graphic-intensive tasks."
          ]
        },
        {
          "title": "Overview of Single Root I/O Virtualization (SR-IOV)",
          "url": "https://learn.microsoft.com/en-us/windows-hardware/drivers/network/overview-of-single-root-i-o-virtualization--sr-iov-",
          "excerpts": [
            "Sep 27, 2024 — SR-IOV allows a device, such as a network adapter, to separate access to its resources among various PCIe hardware functions."
          ]
        },
        {
          "title": "Single-root input/output virtualization",
          "url": "https://en.wikipedia.org/wiki/Single-root_input/output_virtualization",
          "excerpts": [
            "Single root input/output virtualization (SR-IOV) is a specification that allows the isolation of PCI Express resources for manageability and performance ..."
          ]
        },
        {
          "title": "Emulated Devices - NVIDIA Docs",
          "url": "https://docs.nvidia.com/doca/archive/doca-v1.5.2/emulated-devices/index.html",
          "excerpts": [
            "VirtIO emulated devices enables users to create VirtIO-net emulated PCIe devices in the system where the NVIDIA® BlueField®-2 DPU is ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "In this specification, virtio devices are implemented over MMIO, Channel I/O and PCI bus transports, earlier drafts have been implemented on other buses not ..."
          ]
        },
        {
          "title": "bryansteiner/gpu-passthrough-tutorial",
          "url": "https://github.com/bryansteiner/gpu-passthrough-tutorial",
          "excerpts": [
            "In this tutorial, I will create a GPU passthrough setup. Specifically, I will be passing through an NVIDIA GPU to my guest VM while using an AMD GPU for my ...",
            "In this post, I will be giving detailed instructions on how to run a KVM setup with GPU passthrough. This setup uses a Linux host installed with Pop!_",
            "Adding the virtIO drivers can be done by attaching its relevant ISO to the Windows VM during creation. Fedora provides the virtIO drivers for direct download."
          ]
        },
        {
          "title": "HOWTO do Linux kernel development",
          "url": "https://www.kernel.org/doc/html/v4.16/process/howto.html",
          "excerpts": [
            "If you add patches to your mail, make sure they are plain readable text as stated in Documentation/process/submitting-patches.rst. Kernel developers don't want ..."
          ]
        },
        {
          "title": "Mitigating Security Risks in Linux with KLAUS: A Method ...",
          "url": "https://www.usenix.org/system/files/usenixsecurity23-wu-yuhang.pdf",
          "excerpts": [
            "by Y Wu · 2023 · Cited by 6 — KLAUS, as a new testing tool, offers an effective and efficient method for uncovering bug patch incorrectness in the Linux kernel. Moreover, it ..."
          ]
        },
        {
          "title": "Does Linux really need a driver ABI interface for the kernel?",
          "url": "https://forum.level1techs.com/t/does-linux-really-need-a-driver-abi-interface-for-the-kernel/143044",
          "excerpts": [
            "So, I keep hearing that “Oh, but desktop Linux will never fly without a (stable) driver ABI interface for the Kernel”."
          ]
        },
        {
          "title": "What are the best guides to follow for this process? : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/15700a4/what_are_the_best_guides_to_follow_for_this/",
          "excerpts": [
            "I want to passthrough a single GPU, achieve similar performance to bare metal windows on the guest system, and be able to switch between my guest and my host."
          ]
        },
        {
          "title": "OASIS Virtual I/O Device (VIRTIO) TC",
          "url": "https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=virtio",
          "excerpts": [
            "PCI devices of the VIRTIO family as found in virtual environments are not all that different from physical PCI devices. Treating them similarly allows the guest to use standard PCI drivers and discovery mechanisms.",
            "The TC intends to define formal specifications for virtual device buses (including PCI) for a variety of devices, including network devices.",
            "The expected TC deliverables include:\n\n* Specification of feature negotiation, configuration and queues, from both driver and device points of view\n* Specification of device-specific configuration\n* Non-normative code examples for operation of guest/host side of buffers\n* Non-normative guide for creating devices which also support previous mode(s)",
            "**[VIRTIO-v1.2]** *Virtual I/O Device (VIRTIO) Version 1.2*. Edited by Michael S. Tsirkin and Cornelia Huck. 01 July 2022. OASIS Committee Specification 01. <https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html",
            "Specification development will be based upon the [\"Virtio PCI Card Specification\"](http://ozlabs.org/~rusty/virtio-spec/virtio-0.9.5.pdf) v0.9.5, seeking solutions that support portability, simplicity, least-surprise for driver authors, extensibility, and performance.",
            "\n\n\n\n---\n\n\n\n**Technical Work Produced by the Committee**\n\n**[VIRTIO-v1.2]** *Virtual I/O Device (VIRTIO) Version 1.2*. Edited by Michael S. Tsirkin and Cornelia Huck. 01 July 2022. OASIS Committee Specification 01.",
            "**External Resources**\n\n* [virtio: Towards a De-Facto Standard For Virtual I/O Devices](http://ozlabs.org/~rusty/virtio-spec/virtio-paper.pdf)",
            "**\n\nThere are no approved expository work products for this TC yet. ---"
          ]
        },
        {
          "title": "rust-vmm/vhost-device",
          "url": "https://github.com/rust-vmm/vhost-device",
          "excerpts": [
            "This repository hosts various 'vhost-user' device backends in their own crates. See their individual README.md files for specific information about those ...",
            "vhost-user' device backends workspace",
            "Implementing a proper VirtIO device requires co-ordination between the\nspecification, drivers and backend implementations.",
            "As these can all\nbe in flux during development it was decided introducing a staging\nworkspace which would allow developers to work within the main rust-vmm\nproject while clearly marking the backends as not production ready. To be included in the staging workspace there must at least be:",
            "* A public proposal to extend the [VIRTIO specification](https://github.com/oasis-tcs/virtio-spec)",
            "* A public implementation of a device driver",
            "* Documentation pointing to the above",
            "Here is the list of device backends in **staging",
            "\n* \n  ",
            "\n* \n  ",
            "\n* \n  ",
            "\n* \n ",
            "\n* \n ",
            "\n* \n ",
            "\n* \n ",
            "\n* \n "
          ]
        },
        {
          "title": "Building the virtualization stack of the future with rust-vmm",
          "url": "https://opensource.com/article/19/3/rust-virtual-machine",
          "excerpts": [
            "The kvm-bindings crate represents the Rust Foreign Function Interface (FFI) to KVM kernel headers.",
            "Vhost has support for both user-land and kernel-land drivers, but users can also plug virtio-devices to their custom backend.",
            "Virtio devices and rate limiting: Virtio has a frontend-backend architecture. Currently in rust-vmm, the frontend is implemented in the virtio-devices crate, and the backend lies in the vhost package.",
            "the frontend is implemented in the virtio-devices crate, and the backend lies in the vhost package.",
            "The virtio-bindings are the bindings for Virtio devices generated using the Virtio Linux headers.",
            "\nEach box on the right side of the diagram is a GitHub repository corresponding to one package, which in Rust is called a crate"
          ]
        },
        {
          "title": "Introduction to virtio-networking and vhost-net - Red Hat",
          "url": "https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net",
          "excerpts": [
            "In this post we will provide a high level solution overview of the virtio-networking architecture, based on establishing an interface between the host kernel ..."
          ]
        },
        {
          "title": "(VIRTIO) Virtual I/O Devices",
          "url": "https://marz.utk.edu/my-courses/cosc562c/virtio/",
          "excerpts": [
            "PCI Transport. PCI transport will use the vendor ID 0x1AF4 and the device ID is dependent on the virtio device. The following table describes the device types."
          ]
        },
        {
          "title": "vhost-user for snabbswitch, a virtio-net with QEMU/KVM and an ...",
          "url": "https://www.virtualopensystems.com/en/solutions/guides/snabbswitch-qemu/",
          "excerpts": [
            "Vhost-user Overview  A UNIX domain socket based mechanism allows to set up the resources used by a number of Vrings shared between two userspace processes, which will be placed in shared memory . The mechanism also configures the necessary eventfds to signal when a Vring gets a kick event from either side."
          ]
        },
        {
          "title": "vDPA",
          "url": "https://vdpa-dev.gitlab.io/",
          "excerpts": [
            "Overview. A vDPA device means a type of device whose datapath complies with the virtio specification, but whose control path is vendor specific . vDPA devices can be both physically located on the hardware or emulated by software."
          ]
        },
        {
          "title": "Introduction to vDPA kernel framework - Red Hat",
          "url": "https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework",
          "excerpts": [
            "This post provides a high level overview of the vDPA (virtio data path acceleration) kernel framework merged to the Linux kernel in March 2020."
          ]
        },
        {
          "title": "2021/12/02 - QEMU virtio configurations - memzero",
          "url": "https://blog.memzero.de/toying-with-virtio/",
          "excerpts": [
            "I wanted to document some minimal virtio device configurations with qemu and the required Linux kernel configuration to enable those devices."
          ]
        },
        {
          "title": "Virtio-console — Project ACRN™ v 1.6 documentation",
          "url": "https://projectacrn.github.io/1.6/developer-guides/hld/virtio-console.html",
          "excerpts": [
            "The Virtio-console is a simple device for data input and output. The console's virtio device ID is 3 and can have from 1 to 16 ports."
          ]
        },
        {
          "title": "Secure and Fast microVM for Serverless Computing",
          "url": "https://aws.amazon.com/blogs/opensource/firecracker-open-source-secure-fast-microvm-serverless/",
          "excerpts": [
            "Nov 27, 2018 — Firecracker allows you to create micro Virtual Machines or microVMs. Firecracker is minimalist by design – it includes only what you need to run ...See more"
          ]
        },
        {
          "title": "Paravirtualized Devices in crosvm - a Performance Panacea for ...",
          "url": "https://prilik.com/blog/post/crosvm-paravirt/",
          "excerpts": [
            "Paravirtualized devices are the “secret sauce” that enable modern VMs to reach near-native speeds in IO heavy workloads such as file access, networking, and ..."
          ]
        },
        {
          "title": "VirtIO-net Emulated Devices",
          "url": "https://docs.nvidia.com/networking/display/BlueFieldDPUOSv396/VirtIO-net+Emulated+Devices",
          "excerpts": [
            "Sep 5, 2023 — This feature enables users to create VirtIO-net emulated PCIe devices in the system where the NVIDIA® BlueField®-2 DPU is connected."
          ]
        },
        {
          "title": "drivers package - github.com/kata-containers/runtime ...",
          "url": "https://pkg.go.dev/github.com/kata-containers/runtime/virtcontainers/device/drivers",
          "excerpts": [
            "GenericDevice refers to a device that is neither a VFIO device, block device or VhostUserDevice. func NewGenericDevice ¶. func NewGenericDevice( ..."
          ]
        },
        {
          "title": "How to use virtio-fs — Cloud Hypervisor documentation",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/docs/fs.html",
          "excerpts": [
            "Start the VM​​ Once the daemon is running, the option --fs from cloud-hypervisor needs to be used. Direct kernel boot is the preferred option, but we can boot ..."
          ]
        },
        {
          "title": "vdev virtio-console",
          "url": "https://www.qnx.com/developers/docs/8.0/com.qnx.doc.hypervisor.user/topic/vdev_ref/vdev_virtio-console.html",
          "excerpts": [
            "The virtio-console vdev provides an interface for console devices. Normally the device appears as a PCI device, but if you specify the loc and intr options, ..."
          ]
        },
        {
          "title": "crosvm README (ChromiumOS virtualization concepts)",
          "url": "https://chromium.googlesource.com/chromiumos/platform/crosvm/+/master/README.md",
          "excerpts": [
            "By default crosvm runs in multiprocess mode. Each device that supports running inside of a sandbox will run in a jailed child process of crosvm. The ... ```\ncrosvm run -- rwdisk \"${ROOT_IMAGE}\" - p \"root=/dev/vda\" vmlinux\n```"
          ]
        },
        {
          "title": "Firecracker official website (as provided)",
          "url": "https://firecracker-microvm.github.io/",
          "excerpts": [
            "Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based ...See more The\n fast startup time and low memory overhead of each microVM enables you to pack thousands of microVMs onto the\n same machine. This means that every function, container, or container group can be encapsulated with a\n virtual machine barrier, enabling workloads from different customers to run on the same machine, without any\n tradeoffs to security or efficiency. F",
            "Firecracker is an [alternative to QEMU](https://www.redhat.com/en/blog/all-you-need-know-about-kvm-userspace) , an established VMM with a general purpose and broad feature set that allows it to host a variety of guest\n operating systems.",
            "You can control the Firecracker process via a RESTful API that enables common actions such as configuring\n the number of vCPUs or starting the machine.",
            "achine. It provides built-in rate limiters, which allows you to\n granularly control network and storage resources used by thousands of microVMs on the same machine.",
            "You can\n create and configure rate limiters via the Firecracker API and define flexible rate limiters that support\n bursts or specific bandwidth/operations limitations.",
            "Firecracker also provides a metadata service that\n securely shares configuration information between the host and guest operating system. You can set up and\n configure the metadata service using the Firecracker API.",
            "Each Firecracker microVM is further isolated with\n common Linux user-space security barriers by a companion program called \"jailer\".",
            "Firecracker is an [alternative to QEMU](https://www.redhat.com/en/blog/all-you-need-know-about-kvm-userspace) that is purpose-built for running serverless functions and containers safely and efficiently, and nothing\n more. Firecracker is written in Rust, provides a minimal required device model to the guest operating\n system while excluding non-essential functionality (only 5 emulated devices are available: virtio-net,\n virtio-block, virtio-vsock, serial console, and a minimal keyboard controller used only to stop the\n microVM). This, along with a streamlined kernel loading process enables a < 125 ms startup time and a\n < 5 MiB memory footprint. The Firecracker process also provides a RESTful control API, handles resource\n rate limiting for microVMs, and provides a microVM metadata service to enable the sharing of configuration\n data between the host and guest.",
            "What operating systems are supported by Firecracker? Firecracker supports Linux host and guest operating systems with kernel versions 4.14 and above, as well\n as [OSv](http://blog.osv.io/blog/2019/04/19/making-OSv-run-on-firecraker/) guests. The long-term support plan is still under discussion.",
            "What is the open source license for Firecracker? Firecracker is [licensed](https://github.com/firecracker-microvm/firecracker/blob/master/LICENSE) under Apache License, version 2.0, allowing you to freely use, copy, and distribute your changes under the\n terms of your choice."
          ]
        },
        {
          "title": "How AWS Firecracker Works - a deep dive",
          "url": "https://unixism.net/2019/10/how-aws-firecracker-works-a-deep-dive/",
          "excerpts": [
            "\nWhen you have operating systems that can’t be modified, but come with drivers for some hardware devices, it makes sense to emulate them. That way, it becomes possible to run these operating systems unmodified.",
            "The concept behind VirtIO is very simple. It specifies a way for the guest and host to communicate efficiently. It defines various device types like: network, block, console, entropy, memory balloon and SCSI host.",
            "VirtIO is relatively simple to program compared to programming real hardware and it is also designed to be very high performance.",
            "The performance is also the result of not having to emulate whole, real hardware devices.",
            "Firecracker takes advantage of this. VirtIO was developed initially by Rusty Russell for [LGuest](https://en.wikipedia.org/wiki/Lguest), which made its way into the kernel in 2007, but was removed in 2017.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "* VirtIO Block (emulated by Firecracker in [devices/src/virtio/block.rs](https://github.com/firecracker-microvm/firecracker/blob/master/devices/src/virtio/block.rs))",
            "* VirtIO Net (emulated by Firecracker in [devices/src/virtio/net.rs](https://github.com/firecracker-microvm/firecracker/blob/master/devices/src/virtio/net.rs))",
            "VirtIO needs a bit more explanation. Real hardware devices have all kinds of quirks and are fairly complicated to program.",
            "VirtIO needs a bit more explanation. Real hardware devices have all kinds of quirks and are fairly complicated to program.",
            "The concept behind VirtIO is very simple. It specifies a way for the guest and host to communicate efficiently."
          ]
        },
        {
          "title": "Virtio-net Feature Bits",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2410/Virtio-net+Feature+Bits",
          "excerpts": [
            "Nov 12, 2024 — Per virtio spec, virtio the device negotiates with the virtio driver on the supported features when the driver probes the device."
          ]
        },
        {
          "title": "VFIO virtual device - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/virt/kvm/devices/vfio.html",
          "excerpts": [
            "The created device tracks VFIO files (group or device) in use by the VM and features of those groups/devices important to the correctness and acceleration of ..."
          ]
        },
        {
          "title": "rust-vmm/vhost",
          "url": "https://github.com/rust-vmm/vhost",
          "excerpts": [
            "The vhost workspace hosts libraries related to the vhost and vhost-user protocols. It currently consists of the following crates."
          ]
        },
        {
          "title": "Minijail - Book of crosvm",
          "url": "https://crosvm.dev/book/appendix/minijail.html",
          "excerpts": [
            "On Linux hosts, crosvm uses minijail to sandbox the child devices. The minijail C library is utilized via a Rust wrapper so as not to repeat the intricate ..."
          ]
        },
        {
          "title": "Consider changing the vhost-user-backend API · Issue #279",
          "url": "https://github.com/rust-vmm/vhost/issues/279",
          "excerpts": [
            "Nov 20, 2024 — Usually in rust-vmm a repository can be a workspace and contain multiple crates (e.g. vhost, vhost-device), but this is just a development ..."
          ]
        },
        {
          "title": "syzkaller/docs/contributing.md at master",
          "url": "https://github.com/google/syzkaller/blob/master/docs/contributing.md",
          "excerpts": [
            "To contribute code or syscall descriptions, at the very least you need to be able to build and run syzkaller, see the instructions here."
          ]
        },
        {
          "title": "syzkaller is an unsupervised coverage-guided kernel fuzzer",
          "url": "https://github.com/google/syzkaller",
          "excerpts": [
            "Documentation. Initially, syzkaller was developed with Linux kernel fuzzing in mind, but now it's being extended to support other OS kernels as ..."
          ]
        },
        {
          "title": "Next VirtIO device for Project Stratos? - Stratos-dev - op-lists.linaro.org",
          "url": "https://op-lists.linaro.org/archives/list/stratos-dev@op-lists.linaro.org/thread/VPLKMBWYB4PG2X5MTACUIW5SBGBP5HVF/",
          "excerpts": [
            "Neither of these have up-streamed the specification to OASIS but there is an implementation of the mac80211_hwsim in the Linux kernel. I found evidence of a ..."
          ]
        },
        {
          "title": "Linux Kernel patch submission checklist - No comment",
          "url": "https://www.infradead.org/~mchehab/kernel_docs/process/submit-checklist.html",
          "excerpts": [
            "Linux Kernel patch submission checklist¶. Here are some basic things that developers should do if they want to see their kernel patch submissions accepted ..."
          ]
        },
        {
          "title": "Check if your code follows kernel code style — kw documentation",
          "url": "https://kworkflow.org/tutorials/codestyle.html",
          "excerpts": [
            "The Linux kernel has a very clear and well-defined code style automated by a tool named checkpatch.pl . You can use this script to check a file or a patch, and ..."
          ]
        },
        {
          "title": "Vhost Processing - SPDK Documentation",
          "url": "https://spdk.io/doc/vhost_processing.html",
          "excerpts": [
            "The purpose of virtio and [virtio] specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms. Virtio devices use virtqueues to transport data efficiently. Virtqueue is a set of three different single-producer, single-consumer ring structures designed to store generic scatter-gatter I/O.",
            "\nThis document is intended to provide an overview of how Vhost works behind the scenes. Code snippets used in this document might have been simplified for the sake of readability and should not be used as an API or implementation reference. Reading from the [Virtio specification",
            "interop/vhost-user.html) describes the protocol as follows:\n\n> ‍[Vhost-user protocol] is aiming to complement the ioctl interface used to control the vhost implementation in the Linux kernel. It implements the control plane needed to establish virtqueue sharing with a user space process on the same host. It uses communication over a Unix domain socket to share file descriptors in the ancillary data of the message.",
            "The initial vhost implementation is a part of the Linux kernel and uses ioctl interface to communicate with userspace applications. What makes it possible for SPDK to expose a vhost device is Vhost-user protocol.",
            "Vhost is a protocol for devices accessible via inter-process communication. It uses the same virtqueue layout as Virtio to allow Vhost devices to be mapped directly to Virtio devices.",
            "is a Vhost-user back-end server. It exposes Unix domain sockets and allows external applications to connect.",
            "One of major Vhost-user use cases is networking (DPDK) or storage (SPDK) offload in QEMU.",
            "QEMU-based VM communicates with SPDK Vhost-SCSI device. QEMU/SPDK vhost data flow",
            "The front-end and back-end can be either a client (i.e. connecting) or server (listening) in the socket communication. SPDK vhost is a Vhost-user back-end server."
          ]
        },
        {
          "title": "vhost-user back ends",
          "url": "https://www.qemu.org/docs/master/system/devices/vhost-user.html",
          "excerpts": [
            "vhost-user back ends are way to service the request of VirtIO devices outside of QEMU itself. To do this there are a number of things required.",
            "These are simple stub devices that ensure the VirtIO device is visible\nto the guest.",
            "The code is mostly boilerplate although each device has\na `chardev` option which specifies the ID of the `--chardev` device that connects via a socket to the vhost-user _daemon_ .",
            "The vhost-user-device is a generic development device intended for\nexpert use while developing new backends.",
            "to specify\nall the required parameters including:\n\n> *\n>   Device `virtio-id`\n> *\n>   The `num_vqs` it needs and their `vq_size`\n> *\n>   The `config_size`",
            "This is a separate process that is connected to by QEMU via a socket\nfollowing the [Vhost-user Protocol"
          ]
        },
        {
          "title": "CodeLinaro / le / platform / external / rust-vmm / vhost-device - GitLab",
          "url": "https://git.codelinaro.org/clo/le/platform/external/rust-vmm/vhost-device",
          "excerpts": [
            "Typically they have been tested with QEMU although the rust-vmm project does provide a vhost-user frontend crate for rust based VMMs. While ..."
          ]
        },
        {
          "title": "Timekeeping Virtualization for X86-Based Architectures",
          "url": "https://docs.kernel.org/virt/kvm/x86/timekeeping.html",
          "excerpts": [
            "In general, the HPET is recommended as a high precision (compared to PIT /RTC) time source which is independent of local variation (as there is only one HPET in ..."
          ]
        },
        {
          "title": "cloud-hypervisor/docs/io_throttling.md at main",
          "url": "https://github.com/cloud-hypervisor/cloud-hypervisor/blob/main/docs/io_throttling.md",
          "excerpts": [
            "Cloud Hypervisor now supports I/O throttling on virtio-block and virtio-net devices. This support is based on the rate-limiter module from Firecracker."
          ]
        },
        {
          "title": "Cloud Hypervisor documentation - GitHub Pages",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/README.html",
          "excerpts": [
            "A large part of the Cloud Hypervisor code is based on either the Firecracker or the crosvm projects implementations. Both of these are VMMs written in Rust with ..."
          ]
        },
        {
          "title": "Firecracker: start a VM in less than a second - Julia Evans",
          "url": "https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/",
          "excerpts": [
            "I've been using Firecracker to start relatively large VMs – Ubuntu VMs running systemd as an init system – and it takes maybe 2-3 seconds for them to boot."
          ]
        },
        {
          "title": "Getting Started with Firecracker | Harry Hodge",
          "url": "https://harryhodge.co.uk/posts/2024/01/getting-started-with-firecracker/",
          "excerpts": [
            "We start by manually setting up the TAP network interface for the firecracker VM. This could get fiddly if we want to run multiple VMs. 1# Setup ..."
          ]
        },
        {
          "title": "Paravirtualized KVM features — QEMU documentation",
          "url": "https://www.qemu.org/docs/master/system/i386/kvm-pv.html",
          "excerpts": [
            "Paravirtualized KVM features are represented as CPU flags. The following features are enabled by default for any CPU model when KVM acceleration is enabled."
          ]
        },
        {
          "title": "Chapter 8. KVM Guest Timing Management - Red Hat Documentation",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/chap-kvm_guest_timing_management",
          "excerpts": [
            "KVM avoids these issues by providing guest virtual machines with a paravirtualized clock (kvm-clock). However, it is still important to test timing before ..."
          ]
        },
        {
          "title": "Chapter 14. KVM Guest Timing Management | 6",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/virtualization_host_configuration_and_guest_installation_guide/chap-virtualization_host_configuration_and_guest_installation_guide-kvm_guest_timing_management",
          "excerpts": [
            "KVM avoids these issues by providing guest virtual machines with a paravirtualized clock (kvm-clock). However, it is still important to test timing before ..."
          ]
        },
        {
          "title": "A detailed view of the vhost user protocol and its ...",
          "url": "https://access.redhat.com/solutions/3394851",
          "excerpts": [
            "Jun 14, 2024 — The vhost user protocol consists of a control path and a data path. All control information is exchanged via a Unix socket. This includes ..."
          ]
        },
        {
          "title": "vhost-device-vsock - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/vhost-device-vsock",
          "excerpts": [
            "The crate introduces a vhost-device-vsock device that enables communication between an application running in the guest ie inside a VM and an application ..."
          ]
        },
        {
          "title": "Firecracker GitHub Repository - Getting Started / Design & Features",
          "url": "https://github.com/firecracker-microvm/firecracker",
          "excerpts": [
            "Firecracker\nhas a minimalist design. It excludes unnecessary devices and guest-facing\nfunctionality to reduce the memory footprint and attack surface area of each\nmicroVM",
            " Firecracker is open sourced\nunder [Apache version 2.0",
            "Firecracker consists of a single micro Virtual Machine Manager process that\nexposes an API endpoint to the host once started.",
            "The main component of Firecracker is a virtual machine monitor (VMM) that uses\nthe Linux Kernel Virtual Machine (KVM) to create and run microVMs."
          ]
        },
        {
          "title": "How to write an actually working Linux kernel module in Rust?",
          "url": "https://users.rust-lang.org/t/how-to-write-an-actually-working-linux-kernel-module-in-rust/97293",
          "excerpts": [
            "I've been trying to write a simple hello world kernel module in Rust, but none of the examples available on GitHub appear to be working."
          ]
        },
        {
          "title": "How to call a Rust struct's method from C using FFI? - Stack Overflow",
          "url": "https://stackoverflow.com/questions/54156498/how-to-call-a-rust-structs-method-from-c-using-ffi",
          "excerpts": [
            "I am trying to call a public function (located inside a Rust struct's impl block) from a C program using the FFI. Calling regular pub fn s has not been too ..."
          ]
        },
        {
          "title": "Not matching ABI between C++ and Rust",
          "url": "https://users.rust-lang.org/t/not-matching-abi-between-c-and-rust/112791",
          "excerpts": [
            "Missing: porting drivers"
          ]
        },
        {
          "title": "Licenses",
          "url": "https://www.rust-lang.org/policies/licenses",
          "excerpts": [
            "The Rust Programming Language and all other official projects, including this website, are generally dual-licensed: Apache License, Version 2.0 ..."
          ]
        },
        {
          "title": "Will Rust in the Linux kernel make it non-GPL?",
          "url": "https://www.bleepingcomputer.com/forums/t/782256/will-rust-in-the-linux-kernel-make-it-non-gpl/",
          "excerpts": [
            "Feb 5, 2023 — GPL code, however, can't be used in MIT licensed code. Edit: Here is a ... Rust code can be licensed however the author of the code wants."
          ]
        },
        {
          "title": "PortingCToRust - Wiki",
          "url": "https://wiki.alopex.li/PortingCToRust",
          "excerpts": [
            "I decided to port the minimp3 library from C to Rust. I want a pure Rust MP3 decoder crate to exist under a permissive license."
          ]
        },
        {
          "title": "Linux NVMe-oF Initiator + Linux SPDK NVMe-oF Target - StarWind",
          "url": "https://www.starwindsoftware.com/blog/nvme-part-1-linux-nvme-initiator-linux-spdk-nvmf-target/",
          "excerpts": [
            "Linux SPDK NVMe-oF Target provides 20 000 IOPS gain under all 64KB patterns. Under 4k blocks, things look even better: SPDK Target provides 300 ...",
            "This article examines Linux NVMe-oF initiator and target performance, using a Linux SPDK NVMe-oF target and a Linux NVMe-oF initiator."
          ]
        },
        {
          "title": "QXL vs VirtIO GPU vs VirGL GPU - trivial benchmark on my setup",
          "url": "https://www.reddit.com/r/Proxmox/comments/1auvdlg/qxl_vs_virtio_gpu_vs_virgl_gpu_trivial_benchmark/",
          "excerpts": [
            "So I just tested with Portal 2 and VirtIO GPU & QXL gave me between 2 and 10 fps, while VirGL gave me about 180 fps. VirGL also gave about 600 - ..."
          ]
        },
        {
          "title": "How much performance does VFIO hit.",
          "url": "https://www.reddit.com/r/VFIO/comments/utow7o/how_much_performance_does_vfio_hit/",
          "excerpts": [
            "I've switched to dual GPU VFIO headless setup on Pascal cards and assuming that everything is correctly optimized You'll get between 97-99% ...",
            "I notice a like 2/5% loss compared to bare metal (without any bloat installed). I think this is an acceptable loss. I do use two gpu's instead ..."
          ]
        },
        {
          "title": "io - What is the impact of different Linux I/O APIs on performance ...",
          "url": "https://unix.stackexchange.com/questions/728807/what-is-the-impact-of-different-linux-i-o-apis-on-performance-benchmarking-of-nv",
          "excerpts": [
            "I am benchmarking NVMe SSDs on my Linux server, with the aim to achieve the IOPS, BW and Latency values as mentioned in the product specifications."
          ]
        },
        {
          "title": "A Guide to Porting C/C++ to Rust | Hacker News",
          "url": "https://news.ycombinator.com/item?id=13513807",
          "excerpts": [
            "This is an awesome start for a great manual on the area. I'm very inspired by the experience I had in doing this, and hope that we do more."
          ]
        },
        {
          "title": "Rust support in the Linux kernel - Hacker News",
          "url": "https://news.ycombinator.com/item?id=29485465",
          "excerpts": [
            "Rust should not be used in the Linux kernel until it supports 100% of the architectures supported by Linux itself. > Rust should not be used in ..."
          ]
        },
        {
          "title": "Is it possible to rewrite the Linux kernel in Rust? - Quora",
          "url": "https://www.quora.com/Is-it-possible-to-rewrite-the-Linux-kernel-in-Rust",
          "excerpts": [
            "Sure, it could be done. But the Linux kernel is enormous these days, with 27.8 million lines of code as of 2020, and probably more now."
          ]
        },
        {
          "title": "VFIO: Tuning your Windows gaming VM for optimal performance",
          "url": "https://angrysysadmins.tech/index.php/2022/07/grassyloki/vfio-tuning-your-windows-gaming-vm-for-optimal-performance/",
          "excerpts": [
            "Jul 29, 2022 — This will be a guide on advanced tuning for a VFIO gaming VM. If you're starting from scratch, read through the Arch Wiki guide on PCI passtrhough via OVMF."
          ]
        },
        {
          "title": "I'm shocked the Rust community is pushing an MIT licensed Rust ...",
          "url": "https://users.rust-lang.org/t/im-shocked-the-rust-community-is-pushing-an-mit-licensed-rust-rewrite-of-gnu-coreutils/126110?page=2",
          "excerpts": [
            "The Rust code that is being integrated to the Linux kernel is under GPLv2 as the rest of the kernel. On the other hand most Rust projects (Linux ..."
          ]
        },
        {
          "title": "Rust is Officially in the Linux Kernel : r/programming",
          "url": "https://www.reddit.com/r/programming/comments/1l6y59c/rust_is_officially_in_the_linux_kernel/",
          "excerpts": [
            "Rust was already officially in the Linux kernel for other device drivers before Nova."
          ]
        },
        {
          "title": "Rust for Linux: Rust Version Policy and Kernel Integration",
          "url": "https://rust-for-linux.com/rust-version-policy",
          "excerpts": [
            "The kernel supports a minimum version of Rust, starting with Rust 1.78.0.",
            "The project\n\n1. [Contact](/contact)\n2. [Contributing](/contributing)\n3. [Rust kernel policy](/rust-kernel-policy)\n4. [Branches](/branches)\n5. [Rust reference drivers](/rust-reference-drivers)\n6. [Rust version policy](/rust-version-policy)"
          ]
        },
        {
          "title": "QEMU/KVM with VFIO passthrough kinda slow - Reddit",
          "url": "https://www.reddit.com/r/VFIO/comments/v9eian/qemukvm_with_vfio_passthrough_kinda_slow/",
          "excerpts": [
            "Recently I have set up a Win10 vm for gmaing on linux host system (Debian 11) and its performance is low. My Host Configuration"
          ]
        },
        {
          "title": "[SOLVED] KVM/QEMU GPU Passthrough Performance Issues ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=280628",
          "excerpts": [
            "User experienced poor performance (lagging, 2 fps in PUBG) with KVM/QEMU GPU passthrough, but the issue was later solved."
          ]
        },
        {
          "title": "~15-20% CPU performance penalty under KVM : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/r49an8/1520_cpu_performance_penalty_under_kvm/",
          "excerpts": [
            "I'm getting a Cinebench R23 single core score of ~1580 points, while under QEMU it is reduced to ~1300, with a big variance - between 1220 and 1380.See more"
          ]
        },
        {
          "title": "Introduction to KVM and SR-IOV",
          "url": "https://dzone.com/articles/introduction-to-kvm-sr-iov",
          "excerpts": [
            "Mar 20, 2024 — Improved I/O performance: Direct access for VMs to network hardware speeds up data processing and minimizes latency. · Efficient resource sharing ..."
          ]
        },
        {
          "title": "To FUSE or Not to FUSE: Performance of User-Space File ...",
          "url": "https://www.usenix.org/system/files/conference/fast17/fast17-vangoor.pdf",
          "excerpts": [
            "by BKR Vangoor · 2017 · Cited by 237 — FUSE consists of a kernel part and a user-level dae- mon. The kernel part is implemented as a Linux kernel module that, when loaded, registers a ...",
            "by BKR Vangoor · 2017 · Cited by 237 — Thus, overheads caused by the FUSE's user-kernel communi- cations explain the performance degradation. File Reads. Figure 5 shows different ..."
          ]
        },
        {
          "title": "fuse - A File Systems Geek",
          "url": "https://fsgeek.ca/tag/fuse/",
          "excerpts": [
            "FUSE remains slower than a native file system in many scenarios however (e.g., they use EXT4 as a benchmark comparison) though the performance seems to be much ..."
          ]
        },
        {
          "title": "White Paper",
          "url": "https://www.lannerinc.com/support/download-center/category/38-white-papers?download=46:throughput-improvements-with-the-dpdk",
          "excerpts": [
            "We provide application exam- ples for writing the software to acquire data from the Intel DPDK instead of the Linux kernel. With the Intel DPDK and Lanner ..."
          ]
        },
        {
          "title": "Development of Ethernet Based Real-Time Applications in ...",
          "url": "https://accelconf.web.cern.ch/icalepcs2019/papers/mopha044.pdf",
          "excerpts": [
            "by G Gaio · Cited by 2 — The goal of the test was to validate the mainline Linux kernel together with the DPDK platform as a possible alternative to the present ..."
          ]
        },
        {
          "title": "Improving the performance of a Windows 11 Guest on QEMU",
          "url": "https://leduccc.medium.com/improving-the-performance-of-a-windows-10-guest-on-qemu-a5b3f54d9cf5",
          "excerpts": [
            "This guide is intended for existing KVM/QEMU Libvirt setups and will help you reach near native performance on your Windows 10 or 11 Guest.See more"
          ]
        },
        {
          "title": "RFUSE: Modernizing Userspace Filesystem Framework ...",
          "url": "https://www.usenix.org/system/files/fast24-cho.pdf",
          "excerpts": [
            "by KJ Cho · 2024 · Cited by 8 — However, the complex internal stack of FUSE leads to notable performance overhead, which becomes even more prominent in modern hardware ..."
          ]
        },
        {
          "title": "How to have good graphics performance in KVM-based ...",
          "url": "https://www.reddit.com/r/VFIO/comments/1hexcq6/how_to_have_good_graphics_performance_in_kvmbased/",
          "excerpts": [
            "I'm wondering what should I do to have good VM graphics performance. I thought that it is just enough to buy a GPU with SR-IOV, and my VMs will ..."
          ]
        },
        {
          "title": "Virtio-blk latency measurements and analysis",
          "url": "https://www.linux-kvm.org/page/Virtio/Block/Latency",
          "excerpts": [
            "The results give a 33% virtualization overhead.",
            "The time spent in QEMU servicing a read request made up 29 us or a 23% overhead compared to a host read request.",
            "The following numbers for the layers of the stack are derived from the previous numbers by subtracting successive latency readings:",
            "95 ns latency (13% of total) is high. The guest should be filling in virtio-blk read commands and talking to the virtio-blk PCI device, there isn't much interesting work going on inside the guest.",
            "The QEMU 29393 ns latency (~15% of total) is high."
          ]
        },
        {
          "title": "MODULE_LICENSE and EXPORT_SYMBOL_GPL",
          "url": "https://lwn.net/2001/1025/a/module-license.php3",
          "excerpts": [
            "MODULE_LICENSE() allows kernel developers to identify kernels that have been tainted by modules whose source code is not generally available."
          ]
        },
        {
          "title": "2024 VFIO Performance Loss vs Bare Metal - Virtualization",
          "url": "https://forum.level1techs.com/t/2024-vfio-performance-loss-vs-bare-metal/221503",
          "excerpts": [
            "Dec 11, 2024 — In demanding games (Squad) I see a 30-40% loss in performance versus bare metal. In older titles (CS:Source, World in Conflict) the performance drop is more ..."
          ]
        },
        {
          "title": "Low Latency Storage Optimizations for Proxmox, KVM, & ...",
          "url": "https://kb.blockbridge.com/technote/proxmox-tuning-low-latency-storage/",
          "excerpts": [
            "This technote describes how to optimize I/O latency in a performance-critical virtual environment consisting of KVM, QEMU, and Proxmox."
          ]
        },
        {
          "title": "[PDF] QEMU & KVM Automated Performance Benchmarking @ SUSE",
          "url": "https://kvm-forum.qemu.org/2022/QEMU%20%26%20KVM%20Automated%20Performance%20Benchmarking%20%40%20SUSE-1.pdf",
          "excerpts": [
            "More combinations: — Run benchmarks CPU bench, I/O bench and MEM bench in a 15-SP3 VM, on a 15-SP3 host. — Run benchmarks CPU bench, I/O bench and MEM bench ..."
          ]
        },
        {
          "title": "Virtualization Tuning and Optimization Guide | 7",
          "url": "https://docs.redhat.com/de/documentation/red_hat_enterprise_linux/7/html-single/virtualization_tuning_and_optimization_guide/index",
          "excerpts": [
            "Within this guide, you can find tips and suggestions for making full use of KVM performance features and options for your host systems and virtualized guests."
          ]
        },
        {
          "title": "NVMe over Fabrics Target - SPDK",
          "url": "https://spdk.io/doc/nvmf.html",
          "excerpts": [
            "The Linux kernel also implements an NVMe-oF target and host, and SPDK is tested for interoperability with the Linux kernel implementations."
          ]
        },
        {
          "title": "What is the overhead on a FUSE filesystem compared ...",
          "url": "https://news.ycombinator.com/item?id=43517538",
          "excerpts": [
            "What is the overhead on a FUSE filesystem compared to being implemented in the kernel? The overhead is quite high, because of the additional context ..."
          ]
        },
        {
          "title": "The Linux Kernel Archives",
          "url": "https://www.kernel.org/",
          "excerpts": [
            "The Linux Kernel Archives ; mainline: 6.17-rc3, 2025-08-24, [tarball] ; stable: 6.16.3, 2025-08-23, [tarball], [pgp] ; stable: 6.15.11 [EOL], 2025-08-20, [tarball] ..."
          ]
        },
        {
          "title": "ABI stable symbols - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/admin-guide/abi-stable.html",
          "excerpts": [
            "Documents the interfaces that the developer has defined to be stable. Userspace programs are free to use these interfaces with no restrictions."
          ]
        },
        {
          "title": "Kernel index",
          "url": "https://lwn.net/Kernel/Index/",
          "excerpts": [
            "This index covers articles published in the LWN.net Kernel Page. All articles from the beginning of 2004 have been entered here."
          ]
        },
        {
          "title": "70% game performance in vfio vm compared to bare metal",
          "url": "https://forum.level1techs.com/t/70-game-performance-in-vfio-vm-compared-to-bare-metal/178705",
          "excerpts": [
            "Nov 22, 2021 — The GPU is pegged at 100% on bare metal but only like 40-70% in the VM. In both, there is “normal”/expected cpu usage. I feel like I'm missing something."
          ]
        },
        {
          "title": "Accelerate Your NVMe Drives with SPDK - Intel",
          "url": "https://www.intel.com/content/www/us/en/developer/articles/technical/accelerating-your-nvme-drives-with-spdk.html",
          "excerpts": [
            "When comparing the SPDK userspace NVMe driver to the Linux Kernel 4.4, the latency is up to 10x lower: Figure 3. A comparison of latency between ..."
          ]
        },
        {
          "title": "Linux Kernel Enforcement Statement",
          "url": "https://www.kernel.org/doc/html/v4.17/process/kernel-enforcement-statement.html",
          "excerpts": [
            "As developers of the Linux kernel, we have a keen interest in how our software is used and how the license for our software is enforced. Compliance with the ..."
          ]
        },
        {
          "title": "DDEKit and DDE for Linux",
          "url": "https://os.inf.tu-dresden.de/ddekit/",
          "excerpts": [
            "On this page you find our implementation of a DDE that allows running Linux kernel drivers in Linux user space without requiring any modifications to the driver ..."
          ]
        },
        {
          "title": "The Linux kernel has an incredibly strong userspace API ...",
          "url": "https://news.ycombinator.com/item?id=40816716",
          "excerpts": [
            "Jun 28, 2024 — The Linux kernel has an incredibly strong userspace API stability guarantee. I think in practice this guarantee is unmatched in modern ..."
          ]
        },
        {
          "title": "iwlwifi(4) - FreeBSD Manual Pages",
          "url": "https://man.freebsd.org/cgi/man.cgi?iwlwifi(4)",
          "excerpts": [
            "DESCRIPTION The iwlwifi driver provides support for Intel Wireless network devices. ... iwlwifi driver module parameters and are mapped automatically by linuxkpi."
          ]
        },
        {
          "title": "What is the meaning of \"driver core\" in the context of Linux kernel ...",
          "url": "https://stackoverflow.com/questions/47584527/what-is-the-meaning-of-driver-core-in-the-context-of-linux-kernel-device-drive",
          "excerpts": [
            "The driver core is the generic code that manages drivers, devices, buses, classes, etc. It is not tied to a specific bus or device."
          ]
        },
        {
          "title": "The Biggest Linux Gaslight Of Them All: Stable Driver API",
          "url": "https://www.reddit.com/r/linuxsucks/comments/18s688e/the_biggest_linux_gaslight_of_them_all_stable/",
          "excerpts": [
            "Except this isn't a problem, because the drivers would be out of tree. The whole POINT of a stable driver API is so that drivers can be moved ...",
            "One of the top Linux kernel maintainers wrote this: https://github.com/torvalds/linux/blob/master/Documentation/process/stable-api-nonsense.rst ..."
          ]
        },
        {
          "title": "Possible solution to the drm-kmod kernel mismatch after ...",
          "url": "https://forums.freebsd.org/threads/possible-solution-to-the-drm-kmod-kernel-mismatch-after-upgrade-from-bapt.96058/",
          "excerpts": [
            "Dec 13, 2024 — It's a thorny problem that often comes up after upgrades from a minor version that has not reached EOL yet. bapt@ has posted a solution to freebsd-ports."
          ]
        },
        {
          "title": "When FFI Function Calls Beat Native C",
          "url": "https://news.ycombinator.com/item?id=17171252",
          "excerpts": [
            "I think the main performance are really (1) dynamically linked C/C++ with GOT + PLT, (2) statically linked C/C++ and then (potentially) separate ..."
          ]
        },
        {
          "title": "Compatibility layers are the future of Linux gaming and native builds ...",
          "url": "https://www.reddit.com/r/linux_gaming/comments/11wu0kw/compatibility_layers_are_the_future_of_linux/",
          "excerpts": [
            "Proton or other compatibility layers are far superior to native Linux builds of games. In fact they are even superior to native windows versions of games."
          ]
        },
        {
          "title": "Torvalds' New Patch Improves Linux Security and ...",
          "url": "https://linuxsecurity.com/news/security-projects/torvalds-new-patch-implications-for-security-performance",
          "excerpts": [
            "Nov 11, 2024 — Torvalds' recent patch enhances Linux performance and addresses critical security against Meltdown and Spectre attacks."
          ]
        },
        {
          "title": "And I'm not at all interested in some \"ZFS shim layer\" thing either If ...",
          "url": "https://news.ycombinator.com/item?id=22005901",
          "excerpts": [
            "Linux absolutely is especially hostile. Windows will generally try to support existing drivers, even binary-only ones, and give plenty of notice for API changes ..."
          ]
        },
        {
          "title": "QEMU/virtual machines with GPU pass through even possible on ...",
          "url": "https://www.reddit.com/r/linux4noobs/comments/15vtwgt/qemuvirtual_machines_with_gpu_pass_through_even/",
          "excerpts": [
            "I've been trying to get GPU pass through to work on mint cinnamon 21.2 but everything I found has only worked for arch and fedora."
          ]
        },
        {
          "title": "Linux Kernel 6.14: Enhanced Drivers, Security ...",
          "url": "https://thenewstack.io/linux-kernel-6-14-enhanced-drivers-security-performance-improvements/",
          "excerpts": [
            "Mar 25, 2025 — Drivers help increase the operating system's stability and performance, enabling new development and greater integration of Linux with cloud ..."
          ]
        },
        {
          "title": "NDSS 2024: Linux Driver Runtime (LDR)",
          "url": "https://www.ndss-symposium.org/wp-content/uploads/2024-9-paper.pdf",
          "excerpts": [
            "LDR is a sandbox environment\nthat provides necessary runtime support for driver execution\ninside the secure OS which works as the operating system\ninside the TrustZone secure world (SW).",
            "sium.org\n\ndriver module for a target secure peripheral, which we refer\nto as a SW driver, into LDR and orchestrate and mediate its\ninteractions with the secure OS as well as the normal world\n(NW) Linux kernel so that the driver module can properly\ninitialize and operate on the target peripheral.",
            "To mitigate the\nsecurity concerns introduced by the SW driver’s dependency\non the NW Linux kernel, LDR provides library function (e.g.,\nmemory management, I/O operation, etc.) support for the SW\ndriver by reusing the secure OS library functions. As for the\nremaining ones which the secure OS does not implement, LDR\nredirects them to the NW Linux kernel yet with security checks\non both passed arguments and return values.",
            "Library functions can be directly supported by the secure\nOS and the Linux kernel subsystem functions need to be redirected\nto and handled by the NW Linux kernel.",
            "During runtime, the twin drivers work\ncooperatively to facilitate SW driver support (§IV-D). The\nSW driver is loaded into LDR, interacts with the secure\nperipheral devices and provides secure I/O services to other\nSW components.",
            "ing NW driver is a helper\ndriver running inside the NW Linux kernel to assist with SW\ndriver initialization and subsystem call redirection.",
            "Considering that SW drivers themselves have large code\nbase and may contain potential security flaws, we create\nan isolated execution environment to protect the secure OS\nfrom the untrusted SW drivers and prevent an untrusted SW\ndriver from jeopardizing other drivers’ code integrity and data\nprivacy.",
            "e create an isolated execution domain for\na set of functionally-related SW drivers and prevent such\nSW drivers from accessing memory regions belonging to the\nsecure OS and other IEDs (§IV-E1).",
            "e provide\n\n4\n\na carefully-defined IED call gate that intercepts and mediates\neach dependency function call issued by a SW driver (§IV-",
            "Finally, we propose several driver state maintenance schemes\nthat securely maintain both SW and NW driver states so that no\nsensitive SW driver information is leaked to the Linux kernel\nduring driver initialization and execution (§IV-F)",
            "Redirecting All Function Calls to Linux Kernel.\nAnother\nalternative approach is to only put driver LKMs inside the\nsecure OS and redirect all driver dependency function invoca-\ntions back to the NW Linux kernel, as shown in Figure 1-(b",
            "Such total function call redirection approach does not enlarge\nTCB since the complex Linux kernel subsystems still reside in\nthe NW and can be realized with relatively small engineering\nefforts.",
            "However, such approach may result in performance\ndowngrade as well as security issues. Firstly, redirecting every\nsingle function invocation may result in huge performance\noverhead for drivers with frequent kernel function calls, e.g.,\nsensor devices with bus I/O at a high sampling rate.",
            "e deploy an MMA8451 accelerometer driver[18]\ninside the secure OS following this approach. The results show\nthat the sampling rate drops to 405.89, 49.3% lower than the\ndevice’s default sampling rate of 8",
            "t the kernel I/O library\nfunctions primarily help drivers to interact with the underlying\ndevices.\nTherefore, the untrusted Linux kernel may corrupt the\nraw data by hooking these I/O functions and the SW cannot\nacquire integral and genuine raw data.",
            "LDR\nintroduces minimal performance overhead to real-world\nuse cases."
          ]
        },
        {
          "title": "The Linux Kernel Driver Interface",
          "url": "https://docs.kernel.org/process/stable-api-nonsense.html",
          "excerpts": [
            "This is being written to try to explain why Linux does not have a binary kernel interface, nor does it have a stable kernel interface."
          ]
        },
        {
          "title": "Linux's license is GPLv2 plus a syscall exception. If it was ...",
          "url": "https://news.ycombinator.com/item?id=37320678",
          "excerpts": [
            "Aug 30, 2023 — Linux's license is GPLv2 plus a syscall exception. If it was GPLv2 alone, proprietary modules would probably be infringing."
          ]
        },
        {
          "title": "Tutorial On Rump Kernel Servers and Clients",
          "url": "https://www.netbsd.org/docs/rump/sptut.html",
          "excerpts": [
            "This document acts as a tutorial on how to configure and use unmodified NetBSD kernel drivers as userspace services with utilities available from the NetBSD ..."
          ]
        },
        {
          "title": "The rump kernel: A tool for driver development and a toolkit ...",
          "url": "https://www.netbsd.org/gallery/presentations/justin/2015_AsiaBSDCon/justincormack-abc2015.pdf",
          "excerpts": [
            "by J Cormack · Cited by 6 — The NetBSD rump kernel is a way to run device drivers outside the kernel. Until recently the most common use was as a tool for running tests on NetBSD ..."
          ]
        },
        {
          "title": "The Linux Kernel API",
          "url": "https://docs.kernel.org/core-api/kernel-api.html",
          "excerpts": [
            "The Linux Kernel API¶. Basic C Library Functions¶. When writing drivers, you cannot in general use routines which are from the C Library."
          ]
        },
        {
          "title": "Driver Model",
          "url": "https://docs.kernel.org/driver-api/driver-model/index.html",
          "excerpts": [
            "Contents · Driver Basics · Driver Model · Device links · Device drivers infrastructure · ioctl based interfaces · CPU and Device Power Management."
          ]
        },
        {
          "title": "LWN.net - Non-free drivers",
          "url": "https://lwn.net/Articles/425142/",
          "excerpts": [
            "It's not about \"never\", as there is no point in keeping stable kernel API and ABI forever. It's about keeping them stable for a reasonable amount of time, in a way that administrators and 3rd party developers can actually understand and adapt to.",
            "Locking ABIs down means you just won't innovate that part of the system any longer, and you'll put up with what eventually turn out to be insufficient designs, as new requirements evolve, for the sake of stability."
          ]
        },
        {
          "title": "Hacker News discussion on Linux driver API stability",
          "url": "https://news.ycombinator.com/item?id=30868346",
          "excerpts": [
            "This is by design. Linux doesn't *want* to pay the maintenance and performance costs of guaranteeing a stable in-kernel API/ABI: [https://www.kernel.org/doc/Documentation/process/stable-api-"
          ]
        },
        {
          "title": "Linux kernel licensing rules",
          "url": "https://android.googlesource.com/kernel/common/+/b207caff4176/Documentation/process/license-rules.rst",
          "excerpts": [
            " space API ( UAPI ) header files , which describe the interface of |\n|  | user \\- space programs to the kernel are a special case . According to the |\n|  | note in the kernel COPYING file , the syscall interface is a clear boundary , |\n|  | which does not extend the GPL requirements to any software which uses it to |\n|  | communicate "
          ]
        },
        {
          "title": "DDEKit Approach for Linux User Space Drivers",
          "url": "http://os.inf.tu-dresden.de/papers_ps/beleg_weisbach.pdf",
          "excerpts": [
            "by H Weisbach · 2011 — DDEKit-Linux enables the re-use of device drivers in Linux, although an extensive number of drivers are available for Linux. DDEKit-Linux ..."
          ]
        },
        {
          "title": "The Userspace I/O HOWTO",
          "url": "https://www.kernel.org/doc/html/v5.0/driver-api/uio-howto.html",
          "excerpts": [
            "The main part of the driver will run in user space. This simplifies development and reduces the risk of serious bugs within a kernel module."
          ]
        },
        {
          "title": "Linux device driver ports - Breaking new ground",
          "url": "https://genodians.org/skalk/2021-04-06-dde-linux-experiments",
          "excerpts": [
            "Apr 6, 2021 — In my following blog post(s) I want to elaborate on recent attemtps to renew and enhance the way Linux device drivers are ported to Genode."
          ]
        },
        {
          "title": "Chapter 5. KVM Paravirtualized (virtio) Drivers",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/chap-kvm_para_virtualized_virtio_drivers",
          "excerpts": [
            "Paravirtualized drivers enhance the performance of guests, decreasing guest I/O latency and increasing throughput almost to bare-metal levels. It is recommended ..."
          ]
        },
        {
          "title": "PCI-passthrough: Windows gaming... on Linux - bitgrounds.tech",
          "url": "https://bitgrounds.tech/posts/pci-passthrough/",
          "excerpts": [
            "VFIO allows for near native performance from within the guest system , partly because it provides DMA mapping and isolation using hardware ..."
          ]
        },
        {
          "title": "Can I set the virtio network device to a faster speed?",
          "url": "https://www.reddit.com/r/Proxmox/comments/j2t3or/can_i_set_the_virtio_network_device_to_a_faster/",
          "excerpts": [
            "I think VirtIO goes up to 10G, but it depends on the driver. What OS? Did you install the VirtIO drivers, if necessary?"
          ]
        }
      ],
      "reasoning": "The roadmap described in the field value centers on establishing a minimal, bootable system that relies on Virtio-based, standardized virtual devices within a VM (console, block, network, RNG, GPU) and runs the OS in a guest under a hypervisor such as QEMU/KVM. Direct supporting material in the excerpts includes: - Excerpts that describe Virtio as a standardized mechanism for virtual devices in VMs, including Virtio's transport over MMIO/Pci and its use for general I/O in virtual environments, with reference to virtio itself and its device families. These excerpts provide the foundational virtualization primitives needed by a Rust-based OS to key off Virtio for bootstrapping and runtime I/O within a VM, rather than relying on physical hardware drivers. - Excerpts describing vhost-user and VFIO as mechanisms to connect user-space drivers or backends to the hypervisor/host, which align with a design where the guest OS interacts with virtual devices via a VMM interface, enabling safe, high-performance device access in a minimal environment. - Excerpts on SPDK, Rust-based driver ecosystems, and Rust in kernel/user-space contexts (Rust for Linux, FFI practices) which contextualize how a Rust-based OS could build drivers or driver-like components (Virtio backends) that interact with VMM/device passthrough through FFI at the boundary of the guest. These provide indirect support for the roadmap's emphasis on a Rust-based foundation using Virtio as the virtualization abstraction, and a virtio-enabled, minimal bootable system running inside a VM. - Additional material on VIRTIO-related components and the virtualization stack (vhost, vhost-user, VFIO) that informs the architecture choices for a minimal, virtio-driven system, and the boundary conditions for driver placement (user-space vs kernel inside the VM) as relevant to a foundational phase. The excerpts collectively support mapping the field value to concrete implementation decisions: start with Virtio devices as the stable interface, use a hypervisor like QEMU/KVM for the guest, implement essential Virtio drivers in Rust (console, net, blk, rng, gpu), and leverage VFIO/vhost for high-performance or safe user-space driver boundaries where appropriate.",
      "confidence": "medium"
    },
    {
      "field": "phased_hardware_enablement_roadmap.key_activities",
      "citations": [
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio on Linux",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "Virtio is an open standard that defines a protocol for communication between drivers and devices of different types, see Chapter 5 (“Device Types”) of the ...",
            "Virtio is an open standard that defines a protocol for communication between drivers and devices of different types, see Chapter 5 (“Device Types”) of the ..."
          ]
        },
        {
          "title": "Virtio",
          "url": "http://wiki.osdev.org/Virtio",
          "excerpts": [
            "VirtIO is a standardized interface which allows virtual machines access to simplified \"virtual\" devices, such as block devices, network adapters and consoles."
          ]
        },
        {
          "title": "virtio-drivers/examples/aarch64/src/main.rs at master",
          "url": "https://github.com/rcore-os/virtio-drivers/blob/master/examples/aarch64/src/main.rs",
          "excerpts": [
            "VirtIO guest drivers in Rust. Contribute to rcore-os/virtio-drivers development by creating an account on GitHub."
          ]
        },
        {
          "title": "rust-vmm/vhost-device",
          "url": "https://github.com/rust-vmm/vhost-device",
          "excerpts": [
            "This repository hosts various 'vhost-user' device backends in their own crates. See their individual README.md files for specific information about those ..."
          ]
        },
        {
          "title": "Building the virtualization stack of the future with rust-vmm",
          "url": "https://opensource.com/article/19/3/rust-virtual-machine",
          "excerpts": [
            "The virtio-bindings are the bindings for Virtio devices generated using the Virtio Linux headers."
          ]
        },
        {
          "title": "How AWS Firecracker Works - a deep dive",
          "url": "https://unixism.net/2019/10/how-aws-firecracker-works-a-deep-dive/",
          "excerpts": [
            "VirtIO is relatively simple to program compared to programming real hardware and it is also designed to be very high performance."
          ]
        }
      ],
      "reasoning": "The most directly relevant information shows that Virtio is an established standard for connecting guest software to virtual devices and that there exist Rust-based or Rust-friendly Virtio driver implementations. A source explicitly mentions VirtIO guest drivers implemented in Rust, which aligns with the target of building client-side drivers inside a Rust OS. Additional sources describe the Virtio specification and its transport/interaction model, which provides the concrete driver interfaces your OS code would implement against. Documents outlining Virtio on Linux and the Virtio spec give the necessary architectural context for how guest drivers discover and negotiate with virtual devices, which is essential when designing a Rust OS that exposes virtio-console, virtio-blk, virtio-net, virtio-rng, and virtio-gpu devices to a guest. Several entries summarize Virtio's architecture and the nature of its device models (MMIO/PCI/vhost-backed paths), reinforcing the expected driver surface area and integration points your OS would need to support. Cited material also points to practical examples and surrounding tooling (e.g., vhost, vhost-user backends) that influence how virtio devices are exposed to guests and how drivers may be structured to interact with backends or hypervisors. Taken together, these excerpts support the field value's claims about implementing a set of Virtio drivers (console, blk, net, rng, gpu) inside a Rust-based OS and running as a guest under a standard hypervisor, while also acknowledging the need to align with Virtio specifications and common guest-host interfaces.",
      "confidence": "high"
    },
    {
      "field": "phased_hardware_enablement_roadmap.goal",
      "citations": [
        {
          "title": "Virtio",
          "url": "http://wiki.osdev.org/Virtio",
          "excerpts": [
            "VirtIO is a standardized interface which allows virtual machines access to simplified \"virtual\" devices, such as block devices, network adapters and consoles."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "In this specification, virtio devices are implemented over MMIO, Channel I/O and PCI bus transports, earlier drafts have been implemented on other buses not ..."
          ]
        },
        {
          "title": "bryansteiner/gpu-passthrough-tutorial",
          "url": "https://github.com/bryansteiner/gpu-passthrough-tutorial",
          "excerpts": [
            "In this tutorial, I will create a GPU passthrough setup. Specifically, I will be passing through an NVIDIA GPU to my guest VM while using an AMD GPU for my ..."
          ]
        },
        {
          "title": "virtio-drivers/examples/aarch64/src/main.rs at master",
          "url": "https://github.com/rcore-os/virtio-drivers/blob/master/examples/aarch64/src/main.rs",
          "excerpts": [
            "VirtIO guest drivers in Rust. Contribute to rcore-os/virtio-drivers development by creating an account on GitHub."
          ]
        },
        {
          "title": "rust-vmm/vhost-device",
          "url": "https://github.com/rust-vmm/vhost-device",
          "excerpts": [
            "This repository hosts various 'vhost-user' device backends in their own crates. See their individual README.md files for specific information about those ...",
            "vhost-user' device backends workspace",
            "Implementing a proper VirtIO device requires co-ordination between the\nspecification, drivers and backend implementations.",
            "As these can all\nbe in flux during development it was decided introducing a staging\nworkspace which would allow developers to work within the main rust-vmm\nproject while clearly marking the backends as not production ready. To be included in the staging workspace there must at least be:"
          ]
        },
        {
          "title": "How AWS Firecracker Works - a deep dive",
          "url": "https://unixism.net/2019/10/how-aws-firecracker-works-a-deep-dive/",
          "excerpts": [
            "VirtIO is relatively simple to program compared to programming real hardware and it is also designed to be very high performance.",
            "The performance is also the result of not having to emulate whole, real hardware devices.",
            "Firecracker takes advantage of this. VirtIO was developed initially by Rusty Russell for [LGuest](https://en.wikipedia.org/wiki/Lguest), which made its way into the kernel in 2007, but was removed in 2017.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "* VirtIO Block (emulated by Firecracker in [devices/src/virtio/block.rs](https://github.com/firecracker-microvm/firecracker/blob/master/devices/src/virtio/block.rs))",
            "* VirtIO Net (emulated by Firecracker in [devices/src/virtio/net.rs](https://github.com/firecracker-microvm/firecracker/blob/master/devices/src/virtio/net.rs))",
            "VirtIO needs a bit more explanation. Real hardware devices have all kinds of quirks and are fairly complicated to program.",
            "VirtIO needs a bit more explanation. Real hardware devices have all kinds of quirks and are fairly complicated to program.",
            "The concept behind VirtIO is very simple. It specifies a way for the guest and host to communicate efficiently."
          ]
        },
        {
          "title": "Virtio-net Feature Bits",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2410/Virtio-net+Feature+Bits",
          "excerpts": [
            "Nov 12, 2024 — Per virtio spec, virtio the device negotiates with the virtio driver on the supported features when the driver probes the device."
          ]
        },
        {
          "title": "VFIO virtual device - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/virt/kvm/devices/vfio.html",
          "excerpts": [
            "The created device tracks VFIO files (group or device) in use by the VM and features of those groups/devices important to the correctness and acceleration of ..."
          ]
        }
      ],
      "reasoning": "To support the fine-grained field value, the most directly relevant information describes virtio as the open standard for virtual I/O devices and how guest drivers interact with virtio devices in virtualized environments. Evidence that virtio is a standardized interface enabling guest access to virtualized devices such as block and network devices informs the core architecture of a minimal Rust OS that relies on a virtual hardware interface rather than real hardware. The documentation of virtio specifications, including references to its transport mechanisms and device discovery, directly aligns with bringing up a bootable OS atop a virtualized platform. The Rust-specific material showing Virtio guest drivers in Rust and the Rust-based virtio driver examples demonstrate practical pathways for implementing Rust-based OS components that talk to virtio devices without touching physical hardware. Additional material about vhost/vhost-user backends and the rust-vmm ecosystem provides architectural patterns for how a user-space or VMM component can back virtio devices, which is useful for a Rust OS designed to run in a VM with a stable virtio interface. In particular, explicit mentions of VirtIO drivers written in Rust, and of virtio-related Rust ecosystems, support the goal of a Rust OS that interacts with standardized virtual hardware. Paravirtualization and Virtio's design to enable high-performance I/O through shared memory and queues further solidify that a minimal OS can bootstrap using virtio devices in a controlled VM environment rather than coupling to physical drivers. Collectively, these excerpts corroborate the intended focus on a bootable Rust OS leveraging virtio as the virtual hardware abstraction, and they provide concrete references for implementing such an architecture.",
      "confidence": "high"
    },
    {
      "field": "phased_hardware_enablement_roadmap.phase_name",
      "citations": [
        {
          "title": "Virtio",
          "url": "http://wiki.osdev.org/Virtio",
          "excerpts": [
            "VirtIO is a standardized interface which allows virtual machines access to simplified \"virtual\" devices, such as block devices, network adapters and consoles."
          ]
        },
        {
          "title": "How AWS Firecracker Works - a deep dive",
          "url": "https://unixism.net/2019/10/how-aws-firecracker-works-a-deep-dive/",
          "excerpts": [
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "The concept behind VirtIO is very simple. It specifies a way for the guest and host to communicate efficiently. It defines various device types like: network, block, console, entropy, memory balloon and SCSI host.",
            "VirtIO is relatively simple to program compared to programming real hardware and it is also designed to be very high performance."
          ]
        },
        {
          "title": "rust-vmm/vhost-device",
          "url": "https://github.com/rust-vmm/vhost-device",
          "excerpts": [
            "This repository hosts various 'vhost-user' device backends in their own crates. See their individual README.md files for specific information about those ...",
            "vhost-user' device backends workspace",
            "Implementing a proper VirtIO device requires co-ordination between the\nspecification, drivers and backend implementations."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "In this specification, virtio devices are implemented over MMIO, Channel I/O and PCI bus transports, earlier drafts have been implemented on other buses not ..."
          ]
        },
        {
          "title": "Virtio on Linux",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor.",
            "The virtio spec defines these transport methods in detail, including device discovery, capabilities and interrupt handling. The communication between the driver ..."
          ]
        }
      ],
      "reasoning": "Foundational System with Virtio is about establishing a core, standard interface for virtual devices that underpins the OS's I/O and virtualization stack. The first excerpt foregrounds Virtio as a standardized interface that enables a guest (virtual machine) to access virtual devices such as block devices and networks, which is a core piece of a Virtio-based foundational system. That framing is complemented by the Virtio documentation excerpts, which describe Virtio as the open standard governing how devices are discovered, configured, and communicated with in virtual environments, including how transports (MMIO, PCI, etc.) and device types are defined. Together, these excerpts support the idea that Virtio underpins a foundational I/O abstraction layer in a new OS, enabling consistent, high-performance device access across guest and host boundaries. Additional excerpts discuss the vhost/vhost-user backends and Rust-based or user-space backends, which are concrete mechanisms to implement Virtio devices in practice, reinforcing the notion of a foundational Virtio stack with clear frontend/backend boundaries. The excerpts that discuss Virtio in Linux kernel and SPDK contexts further reinforce Virtio's role as a standard, portable, high-performance I/O abstraction that a foundational system would rely on when building open source OS capabilities around virtualization I/O.\n",
      "confidence": "high"
    },
    {
      "field": "phased_hardware_enablement_roadmap.exit_criteria",
      "citations": [
        {
          "title": "What is SPDK",
          "url": "https://spdk.io/doc/about.html",
          "excerpts": [
            "This provides zero-copy, highly parallel access directly to an SSD from a user space application. The driver is written as a C library with a single public ..."
          ]
        },
        {
          "title": "How AWS Firecracker Works - a deep dive",
          "url": "https://unixism.net/2019/10/how-aws-firecracker-works-a-deep-dive/",
          "excerpts": [
            "The performance is also the result of not having to emulate whole, real hardware devices.",
            "* VirtIO Block (emulated by Firecracker in [devices/src/virtio/block.rs](https://github.com/firecracker-microvm/firecracker/blob/master/devices/src/virtio/block.rs))",
            "VirtIO is relatively simple to program compared to programming real hardware and it is also designed to be very high performance.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel.",
            "VirtIO now has a [specification](http://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html) and device drivers are available in-tree in the Linux kernel."
          ]
        },
        {
          "title": "Virtio-net Feature Bits",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2410/Virtio-net+Feature+Bits",
          "excerpts": [
            "Nov 12, 2024 — Per virtio spec, virtio the device negotiates with the virtio driver on the supported features when the driver probes the device."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Specification v1.3",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "In this specification, virtio devices are implemented over MMIO, Channel I/O and PCI bus transports, earlier drafts have been implemented on other buses not ..."
          ]
        },
        {
          "title": "bryansteiner/gpu-passthrough-tutorial",
          "url": "https://github.com/bryansteiner/gpu-passthrough-tutorial",
          "excerpts": [
            "In this tutorial, I will create a GPU passthrough setup. Specifically, I will be passing through an NVIDIA GPU to my guest VM while using an AMD GPU for my ..."
          ]
        },
        {
          "title": "Virtio",
          "url": "http://wiki.osdev.org/Virtio",
          "excerpts": [
            "VirtIO is a standardized interface which allows virtual machines access to simplified \"virtual\" devices, such as block devices, network adapters and consoles."
          ]
        },
        {
          "title": "virtio-drivers/examples/aarch64/src/main.rs at master",
          "url": "https://github.com/rcore-os/virtio-drivers/blob/master/examples/aarch64/src/main.rs",
          "excerpts": [
            "VirtIO guest drivers in Rust. Contribute to rcore-os/virtio-drivers development by creating an account on GitHub."
          ]
        },
        {
          "title": "How can DPDK access devices from user space?",
          "url": "https://codilime.com/blog/how-can-dpdk-access-devices-from-user-space/",
          "excerpts": [
            "DPDK is a set of libraries for implementing user space drivers for NICs. Read on to see how DPDK works and can access the hardware directly."
          ]
        },
        {
          "title": "Userspace Networking with DPDK",
          "url": "https://www.linuxjournal.com/content/userspace-networking-dpdk",
          "excerpts": [
            "DPDK is a fully open-source project that operates in userspace. It's a multi-vendor and multi-architecture project, and it aims at achieving high I/O ..."
          ]
        }
      ],
      "reasoning": "The phase you describe relies on a standard, well-supported path for booting and operating a guest OS inside a VM using VirtIO devices. Excerpt describing VirtIO as a standardized interface enabling virtual machines to access simple devices establishes the baseline capability: the OS will boot with virtio-blk as the root filesystem and interact with virtio-console for a shell, while networking should be provided by virtio-net and a DHCP process should succeed through that interface. The Rust-based VirtIO driver excerpt shows concrete precedent for implementing VirtIO guest drivers in Rust, which aligns with your plan to write a Rust-based OS and manage VirtIO devices from user space or kernel space with Rust code. Additional references describe the VirtIO specification itself and the various transports (MMIO, PCI, etc.), which underpin how the OS will discover and configure virtio devices at boot time and during runtime. Citations that discuss VirtIO networking, VirtIO-GPU, and VirtIO-RNG provide direct corroboration that such devices exist in standard VirtIO ecosystems and can be wired into an OS boot sequence and runtime user experience. The combination of VirtIO as a standardized interface, concrete Rust VirtIO driver examples, and explicit VirtIO device coverage for network, console, block, GPU, and RNG support directly supports the stated exit criteria, including booting from a virtio-blk root filesystem, obtaining an IP via DHCP on virtio-net, interacting through virtio-console, rendering via virtio-gpu, and seeding cryptographic material via virtio-rng. The included VirtIO specification and device model discussions reinforce that this is a supported, standards-based path, which is essential for a robust Rust-based OS architecture in a VM context.",
      "confidence": "high"
    }
  ],
  "outputSchema": {
    "type": "object",
    "properties": {
      "executive_summary": {
        "type": "object",
        "properties": {
          "key_findings": {
            "type": "string",
            "description": "The main conclusions from the research, highlighting that direct reuse is infeasible but alternative strategies exist."
          },
          "primary_recommendation": {
            "type": "string",
            "description": "The recommended architectural path, emphasizing virtualization for initial support and selective native development long-term."
          },
          "strategic_impact": {
            "type": "string",
            "description": "The overall impact of the driver strategy on the OS architecture, licensing, and development roadmap."
          }
        },
        "required": [
          "key_findings",
          "primary_recommendation",
          "strategic_impact"
        ],
        "additionalProperties": false
      },
      "feasibility_of_direct_ffi_reuse": {
        "type": "object",
        "properties": {
          "technical_barriers": {
            "type": "string",
            "description": "Analysis of the lack of a stable in-kernel API/ABI and the deep, complex dependencies drivers have on Linux subsystems."
          },
          "legal_and_licensing_barriers": {
            "type": "string",
            "description": "Explanation of how the GPLv2 license creates 'derivative works', forcing license adoption and creating legal risks."
          },
          "conclusion": {
            "type": "string",
            "description": "The final assessment that direct reuse via a simple FFI is not a viable or sustainable strategy."
          }
        },
        "required": [
          "technical_barriers",
          "legal_and_licensing_barriers",
          "conclusion"
        ],
        "additionalProperties": false
      },
      "recommended_architectural_strategy": {
        "type": "object",
        "properties": {
          "short_term_strategy": {
            "type": "string",
            "description": "The recommended initial approach, focusing on virtualization-based reuse to achieve rapid hardware support."
          },
          "long_term_strategy": {
            "type": "string",
            "description": "The recommended long-term vision, outlining a hybrid model that combines virtualization with selective native and user-space driver development."
          },
          "rationale": {
            "type": "string",
            "description": "The reasoning behind the recommendation, highlighting benefits in speed, security, and licensing."
          }
        },
        "required": [
          "short_term_strategy",
          "long_term_strategy",
          "rationale"
        ],
        "additionalProperties": false
      },
      "driver_strategy_decision_matrix": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "strategy_name": {
              "type": "string",
              "description": "The name of the driver strategy being evaluated (e.g., Virtualization-based Reuse)."
            },
            "complexity": {
              "type": "string",
              "description": "The estimated technical and engineering complexity of implementing the strategy."
            },
            "performance": {
              "type": "string",
              "description": "The expected performance characteristics, including throughput and latency."
            },
            "security": {
              "type": "string",
              "description": "The security posture, including isolation and attack surface implications."
            },
            "time_to_first_boot": {
              "type": "string",
              "description": "The estimated time and effort required to get a functional system with basic hardware support."
            },
            "licensing": {
              "type": "string",
              "description": "The licensing constraints and implications, particularly concerning GPLv2."
            },
            "maintenance": {
              "type": "string",
              "description": "The long-term maintenance burden and cost of the strategy."
            }
          },
          "required": [
            "strategy_name",
            "complexity",
            "performance",
            "security",
            "time_to_first_boot",
            "licensing",
            "maintenance"
          ],
          "additionalProperties": false
        },
        "description": "A comparative analysis of the four primary driver strategies: Direct Reuse (Rewrite), Shim-based Porting, Virtualization-based Reuse, and Native/User-space Drivers. Each strategy is evaluated across key axes including complexity, performance, security, time-to-first-boot, licensing, and maintenance."
      },
      "virtualization_based_reuse_deep_dive": {
        "type": "object",
        "properties": {
          "architectural_patterns": {
            "type": "string",
            "description": "Description of patterns like driver VMs, static partitioning (Jailhouse), and QEMU/KVM."
          },
          "key_technologies": {
            "type": "string",
            "description": "Explanation of technologies like virtio, vhost/vhost-user for performance, and VFIO/SR-IOV for direct hardware access."
          },
          "performance_implications": {
            "type": "string",
            "description": "Analysis of performance, noting near-native speeds with VFIO and manageable overhead with virtio."
          },
          "security_and_licensing_benefits": {
            "type": "string",
            "description": "How virtualization provides strong isolation and avoids GPLv2 derivative work issues."
          }
        },
        "required": [
          "architectural_patterns",
          "key_technologies",
          "performance_implications",
          "security_and_licensing_benefits"
        ],
        "additionalProperties": false
      },
      "shim_based_porting_deep_dive": {
        "type": "object",
        "properties": {
          "approach_description": {
            "type": "string",
            "description": "Explanation of creating a compatibility layer to translate Linux kernel API calls, citing FreeBSD's LinuxKPI as an example."
          },
          "maintenance_challenges": {
            "type": "string",
            "description": "The high cost and effort required to track the constantly changing Linux internal APIs (upstream churn)."
          },
          "licensing_risks": {
            "type": "string",
            "description": "The significant risk of creating a GPLv2 derivative work, which is legally problematic for non-GPL operating systems."
          }
        },
        "required": [
          "approach_description",
          "maintenance_challenges",
          "licensing_risks"
        ],
        "additionalProperties": false
      },
      "native_and_userspace_driver_architectures": {
        "type": "object",
        "properties": {
          "native_in_kernel_drivers": {
            "type": "string",
            "description": "Analysis of writing drivers from scratch in Rust to run in the kernel, offering high performance but with high complexity."
          },
          "userspace_drivers": {
            "type": "string",
            "description": "Analysis of running drivers in isolated user-space processes for enhanced security, and using frameworks like DPDK/SPDK for extreme performance."
          },
          "microkernel_approach": {
            "type": "string",
            "description": "How microkernel architectures (e.g., Redox, seL4) mandate user-space drivers to prioritize security and modularity."
          }
        },
        "required": [
          "native_in_kernel_drivers",
          "userspace_drivers",
          "microkernel_approach"
        ],
        "additionalProperties": false
      },
      "licensing_implications_of_gplv2": {
        "type": "object",
        "properties": {
          "derivative_work_analysis": {
            "type": "string",
            "description": "Explanation of how linking code with the Linux kernel creates a 'derivative work' under the GPLv2."
          },
          "technical_enforcement_mechanisms": {
            "type": "string",
            "description": "How the `EXPORT_SYMBOL_GPL` macro technically restricts access to core kernel functions to GPL-compatible modules."
          },
          "virtualization_as_a_compliance_strategy": {
            "type": "string",
            "description": "How running Linux in a VM is considered 'mere aggregation', providing a clean legal separation and avoiding GPL obligations for the guest OS."
          }
        },
        "required": [
          "derivative_work_analysis",
          "technical_enforcement_mechanisms",
          "virtualization_as_a_compliance_strategy"
        ],
        "additionalProperties": false
      },
      "performance_analysis_by_strategy": {
        "type": "object",
        "properties": {
          "native_kernel_performance": {
            "type": "string",
            "description": "Performance profile of native kernel drivers, including modern APIs like XDP and io_uring."
          },
          "userspace_framework_performance": {
            "type": "string",
            "description": "Performance profile of user-space frameworks like DPDK and SPDK, which offer the highest throughput by bypassing the kernel."
          },
          "virtualized_driver_performance": {
            "type": "string",
            "description": "Performance profile of virtualized drivers, including near-native speeds with VFIO passthrough and high throughput with virtio."
          }
        },
        "required": [
          "native_kernel_performance",
          "userspace_framework_performance",
          "virtualized_driver_performance"
        ],
        "additionalProperties": false
      },
      "security_architecture_for_drivers": {
        "type": "object",
        "properties": {
          "threat_model": {
            "type": "string",
            "description": "Key threats to drivers, including memory corruption, privilege escalation, and DMA attacks."
          },
          "isolation_strategies": {
            "type": "string",
            "description": "Methods for sandboxing drivers, such as user-space processes (with VFIO) and dedicated driver VMs."
          },
          "key_defenses": {
            "type": "string",
            "description": "Critical security mechanisms like IOMMU for DMA protection, capability-based I/O, and measured boot for integrity."
          },
          "attack_surface_analysis": {
            "type": "string",
            "description": "How FFI expands the attack surface via `unsafe` code, while virtualization reduces it by isolating drivers."
          }
        },
        "required": [
          "threat_model",
          "isolation_strategies",
          "key_defenses",
          "attack_surface_analysis"
        ],
        "additionalProperties": false
      },
      "concurrency_models_and_driver_api": {
        "type": "object",
        "properties": {
          "rcu_like_model": {
            "type": "string",
            "description": "Using Read-Copy-Update for scalable, lock-free reads of shared data structures."
          },
          "per_cpu_data_model": {
            "type": "string",
            "description": "Using per-CPU data to eliminate lock contention for frequently updated, core-local state."
          },
          "message_passing_model": {
            "type": "string",
            "description": "Using an asynchronous, event-driven model that aligns well with Rust's `async/await` and ownership principles."
          }
        },
        "required": [
          "rcu_like_model",
          "per_cpu_data_model",
          "message_passing_model"
        ],
        "additionalProperties": false
      },
      "hardware_discovery_and_configuration_layer": {
        "type": "object",
        "properties": {
          "x86_64_platform_bringup": {
            "type": "string",
            "description": "The process for x86_64, which requires parsing UEFI and ACPI tables (RSDP, XSDT, MADT, MCFG)."
          },
          "arm64_platform_bringup": {
            "type": "string",
            "description": "The process for ARM64, which primarily relies on parsing a Flattened Device Tree (FDT/DTB) from the bootloader."
          },
          "common_bus_enumeration": {
            "type": "string",
            "description": "The need to implement protocols for discovering devices on buses like PCIe (via ECAM) and USB."
          }
        },
        "required": [
          "x86_64_platform_bringup",
          "arm64_platform_bringup",
          "common_bus_enumeration"
        ],
        "additionalProperties": false
      },
      "maintenance_cost_of_tracking_linux_apis": {
        "type": "object",
        "properties": {
          "the_upstream_churn_problem": {
            "type": "string",
            "description": "Explanation of why maintaining out-of-tree drivers is a 'nightmare' due to Linux's lack of a stable in-kernel API."
          },
          "impact_on_shims_and_ports": {
            "type": "string",
            "description": "How API churn requires a massive, continuous engineering investment to keep any compatibility layer synchronized."
          },
          "mitigation_through_automation": {
            "type": "string",
            "description": "The necessity of heavy automation (CI, codegen, semantic patching) to manage the maintenance burden."
          }
        },
        "required": [
          "the_upstream_churn_problem",
          "impact_on_shims_and_ports",
          "mitigation_through_automation"
        ],
        "additionalProperties": false
      },
      "phased_hardware_enablement_roadmap": {
        "type": "object",
        "properties": {
          "phase_number": {
            "type": "number",
            "description": "The sequential number of the phase in the roadmap."
          },
          "phase_name": {
            "type": "string",
            "description": "A descriptive name for the phase (e.g., Foundational System with Virtio)."
          },
          "goal": {
            "type": "string",
            "description": "The primary objective of this phase."
          },
          "key_activities": {
            "type": "string",
            "description": "The main tasks and technologies to be implemented in this phase."
          },
          "exit_criteria": {
            "type": "string",
            "description": "The measurable conditions that must be met to complete the phase."
          }
        },
        "required": [
          "phase_number",
          "phase_name",
          "goal",
          "key_activities",
          "exit_criteria"
        ],
        "additionalProperties": false
      },
      "gpu_and_display_stack_strategy": {
        "type": "object",
        "properties": {
          "native_stack_challenge": {
            "type": "string",
            "description": "The extreme complexity and resource requirements of building a native DRM/KMS stack from scratch."
          },
          "shim_based_reuse_challenge": {
            "type": "string",
            "description": "The high maintenance cost of tracking DRM API churn and the legal hurdles of GPLv2."
          },
          "recommended_strategy": {
            "type": "string",
            "description": "The recommended path is to use `virtio-gpu` with the Venus (Vulkan) backend for the fastest route to modern, hardware-accelerated graphics in a VM."
          }
        },
        "required": [
          "native_stack_challenge",
          "shim_based_reuse_challenge",
          "recommended_strategy"
        ],
        "additionalProperties": false
      },
      "storage_stack_strategy": {
        "type": "object",
        "properties": {
          "block_layer_design": {
            "type": "string",
            "description": "Recommendation to adopt a multi-queue block layer model inspired by Linux's `blk-mq`."
          },
          "userspace_driver_integration": {
            "type": "string",
            "description": "Recommendation to use a framework like SPDK with VFIO for kernel-bypass to achieve maximum performance."
          },
          "filesystem_and_consistency": {
            "type": "string",
            "description": "Recommendation to develop a Rust-native filesystem (e.g., log-structured) that leverages Rust's type system for provable crash consistency."
          }
        },
        "required": [
          "block_layer_design",
          "userspace_driver_integration",
          "filesystem_and_consistency"
        ],
        "additionalProperties": false
      },
      "networking_stack_strategy": {
        "type": "object",
        "properties": {
          "tcp_ip_stack_choice": {
            "type": "string",
            "description": "Recommendation to adopt a mature Rust stack like `smoltcp` or design a new one for safety."
          },
          "performance_architecture": {
            "type": "string",
            "description": "Recommendation to integrate with user-space frameworks (DPDK/AF_XDP) and support kernel/hardware TLS offload."
          },
          "api_design": {
            "type": "string",
            "description": "Recommendation to provide both a POSIX-compatible sockets API for portability and a native, modern `async` API."
          }
        },
        "required": [
          "tcp_ip_stack_choice",
          "performance_architecture",
          "api_design"
        ],
        "additionalProperties": false
      },
      "developer_experience_and_reliability_pipeline": {
        "type": "object",
        "properties": {
          "ci_cd_and_build_infrastructure": {
            "type": "string",
            "description": "The need for reproducible builds, hardware-in-the-loop testing (LAVA), and emulation harnesses (QEMU)."
          },
          "testing_and_verification_strategies": {
            "type": "string",
            "description": "The use of coverage-guided fuzzing (Syzkaller), static analysis, and formal methods (Kani, Miri) to ensure safety."
          },
          "crash_triage_and_telemetry": {
            "type": "string",
            "description": "The importance of automated crash analysis (Syzbot) and centralized test result dashboards (SQUAD)."
          }
        },
        "required": [
          "ci_cd_and_build_infrastructure",
          "testing_and_verification_strategies",
          "crash_triage_and_telemetry"
        ],
        "additionalProperties": false
      }
    },
    "required": [
      "executive_summary",
      "feasibility_of_direct_ffi_reuse",
      "recommended_architectural_strategy",
      "driver_strategy_decision_matrix",
      "virtualization_based_reuse_deep_dive",
      "shim_based_porting_deep_dive",
      "native_and_userspace_driver_architectures",
      "licensing_implications_of_gplv2",
      "performance_analysis_by_strategy",
      "security_architecture_for_drivers",
      "concurrency_models_and_driver_api",
      "hardware_discovery_and_configuration_layer",
      "maintenance_cost_of_tracking_linux_apis",
      "phased_hardware_enablement_roadmap",
      "gpu_and_display_stack_strategy",
      "storage_stack_strategy",
      "networking_stack_strategy",
      "developer_experience_and_reliability_pipeline"
    ],
    "additionalProperties": false
  }
}